//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24817639
// Cuda compilation tools, release 10.0, V10.0.130
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64

	// .globl	vec_initIndex
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 func_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;
.global .align 4 .u32 truc;
.global .align 4 .u32 barrier;
.global .align 4 .u32 barrier2;
.global .align 1 .b8 $str1[14] = {78, 65, 78, 32, 118, 97, 108, 117, 101, 32, 37, 102, 10, 0};
.global .align 1 .b8 $str2[16] = {107, 122, 32, 45, 62, 32, 37, 100, 32, 32, 37, 102, 32, 32, 10, 0};
.global .align 1 .b8 $str3[16] = {90, 85, 84, 32, 37, 100, 32, 32, 37, 100, 32, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str4[110] = {109, 95, 112, 111, 115, 105, 116, 58, 37, 108, 100, 32, 32, 32, 112, 95, 105, 109, 58, 37, 108, 100, 32, 32, 32, 112, 95, 112, 115, 102, 58, 37, 108, 100, 32, 32, 120, 95, 112, 115, 102, 58, 37, 100, 32, 32, 32, 32, 121, 95, 112, 115, 102, 58, 37, 100, 32, 32, 32, 32, 32, 32, 32, 32, 111, 95, 112, 111, 115, 105, 116, 58, 37, 108, 100, 32, 32, 32, 32, 32, 32, 110, 58, 37, 108, 100, 32, 32, 32, 32, 118, 95, 105, 109, 58, 37, 102, 32, 32, 32, 118, 95, 112, 115, 102, 58, 37, 102, 10, 0};
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry vec_initIndex(
	.param .u32 vec_initIndex_param_0,
	.param .u64 vec_initIndex_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_initIndex_param_0];
	ld.param.u64 	%rd1, [vec_initIndex_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u32 	[%rd4], %r1;

BB0_2:
	ret;
}

	// .globl	vec_computeLocalMaxima
.visible .entry vec_computeLocalMaxima(
	.param .u32 vec_computeLocalMaxima_param_0,
	.param .u64 vec_computeLocalMaxima_param_1,
	.param .u32 vec_computeLocalMaxima_param_2,
	.param .u64 vec_computeLocalMaxima_param_3,
	.param .u32 vec_computeLocalMaxima_param_4,
	.param .u32 vec_computeLocalMaxima_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [vec_computeLocalMaxima_param_0];
	ld.param.u64 	%rd2, [vec_computeLocalMaxima_param_1];
	ld.param.u32 	%r5, [vec_computeLocalMaxima_param_2];
	ld.param.u64 	%rd3, [vec_computeLocalMaxima_param_3];
	ld.param.u32 	%r6, [vec_computeLocalMaxima_param_4];
	ld.param.u32 	%r7, [vec_computeLocalMaxima_param_5];
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %nctaid.x;
	mov.u32 	%r14, %ctaid.x;
	mad.lo.s32 	%r15, %r12, %r13, %r14;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r15, %r16, %r17;
	setp.ge.s32	%p1, %r1, %r8;
	@%p1 bra 	BB1_8;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd1, %rd4, %rd5;
	mov.u32 	%r18, 0;
	st.global.u32 	[%rd1], %r18;
	mul.lo.s32 	%r19, %r5, %r5;
	rem.s32 	%r20, %r1, %r19;
	div.s32 	%r21, %r20, %r5;
	rem.s32 	%r22, %r20, %r5;
	max.s32 	%r23, %r6, %r7;
	sub.s32 	%r24, %r21, %r23;
	sub.s32 	%r25, %r22, %r23;
	or.b32  	%r26, %r24, %r25;
	setp.gt.s32	%p2, %r26, -1;
	add.s32 	%r27, %r23, %r21;
	setp.lt.s32	%p3, %r27, %r5;
	and.pred  	%p4, %p2, %p3;
	add.s32 	%r28, %r23, %r22;
	setp.lt.s32	%p5, %r28, %r5;
	and.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB1_8;
	bra.uni 	BB1_2;

BB1_2:
	neg.s32 	%r2, %r6;
	setp.gt.s32	%p7, %r2, %r6;
	@%p7 bra 	BB1_7;

	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd8, %rd6, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];

BB1_4:
	setp.lt.f32	%p8, %f1, %f2;
	mov.u32 	%r30, %r2;
	@%p8 bra 	BB1_8;

BB1_5:
	add.s32 	%r4, %r30, 1;
	setp.lt.s32	%p9, %r30, %r6;
	mov.u32 	%r30, %r4;
	@%p9 bra 	BB1_5;

	setp.lt.s32	%p10, %r4, %r6;
	@%p10 bra 	BB1_4;

BB1_7:
	mov.u32 	%r29, 1;
	st.global.u32 	[%rd1], %r29;

BB1_8:
	ret;
}

	// .globl	vec_eraseNonLocalMaxima
.visible .entry vec_eraseNonLocalMaxima(
	.param .u32 vec_eraseNonLocalMaxima_param_0,
	.param .u64 vec_eraseNonLocalMaxima_param_1,
	.param .u64 vec_eraseNonLocalMaxima_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_eraseNonLocalMaxima_param_0];
	ld.param.u64 	%rd1, [vec_eraseNonLocalMaxima_param_1];
	ld.param.u64 	%rd2, [vec_eraseNonLocalMaxima_param_2];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB2_3;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r12, [%rd5];
	setp.ne.s32	%p2, %r12, 0;
	@%p2 bra 	BB2_3;

	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	mov.u32 	%r13, -1082130432;
	st.global.u32 	[%rd8], %r13;

BB2_3:
	ret;
}

	// .globl	vec_set
.visible .entry vec_set(
	.param .u32 vec_set_param_0,
	.param .u64 vec_set_param_1,
	.param .f64 vec_set_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_set_param_0];
	ld.param.u64 	%rd1, [vec_set_param_1];
	ld.param.f64 	%fd1, [vec_set_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB3_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f64 	[%rd4], %fd1;

BB3_2:
	ret;
}

	// .globl	vec_add
.visible .entry vec_add(
	.param .u32 vec_add_param_0,
	.param .u64 vec_add_param_1,
	.param .u64 vec_add_param_2,
	.param .u64 vec_add_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_add_param_0];
	ld.param.u64 	%rd1, [vec_add_param_1];
	ld.param.u64 	%rd2, [vec_add_param_2];
	ld.param.u64 	%rd3, [vec_add_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB4_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB4_2:
	ret;
}

	// .globl	vec_sub
.visible .entry vec_sub(
	.param .u32 vec_sub_param_0,
	.param .u64 vec_sub_param_1,
	.param .u64 vec_sub_param_2,
	.param .u64 vec_sub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_sub_param_0];
	ld.param.u64 	%rd1, [vec_sub_param_1];
	ld.param.u64 	%rd2, [vec_sub_param_2];
	ld.param.u64 	%rd3, [vec_sub_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB5_2:
	ret;
}

	// .globl	vec_subFloat
.visible .entry vec_subFloat(
	.param .u32 vec_subFloat_param_0,
	.param .u64 vec_subFloat_param_1,
	.param .u64 vec_subFloat_param_2,
	.param .u64 vec_subFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_subFloat_param_0];
	ld.param.u64 	%rd1, [vec_subFloat_param_1];
	ld.param.u64 	%rd2, [vec_subFloat_param_2];
	ld.param.u64 	%rd3, [vec_subFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

BB6_2:
	ret;
}

	// .globl	vec_mul
.visible .entry vec_mul(
	.param .u32 vec_mul_param_0,
	.param .u64 vec_mul_param_1,
	.param .u64 vec_mul_param_2,
	.param .u64 vec_mul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_mul_param_0];
	ld.param.u64 	%rd1, [vec_mul_param_1];
	ld.param.u64 	%rd2, [vec_mul_param_2];
	ld.param.u64 	%rd3, [vec_mul_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB7_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB7_2:
	ret;
}

	// .globl	vec_mul_fl
.visible .entry vec_mul_fl(
	.param .u32 vec_mul_fl_param_0,
	.param .u64 vec_mul_fl_param_1,
	.param .u64 vec_mul_fl_param_2,
	.param .u64 vec_mul_fl_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_mul_fl_param_0];
	ld.param.u64 	%rd1, [vec_mul_fl_param_1];
	ld.param.u64 	%rd2, [vec_mul_fl_param_2];
	ld.param.u64 	%rd3, [vec_mul_fl_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB8_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

BB8_2:
	ret;
}

	// .globl	vec_mul_fl_pow
.visible .entry vec_mul_fl_pow(
	.param .u32 vec_mul_fl_pow_param_0,
	.param .u64 vec_mul_fl_pow_param_1,
	.param .u64 vec_mul_fl_pow_param_2,
	.param .u64 vec_mul_fl_pow_param_3,
	.param .f32 vec_mul_fl_pow_param_4
)
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<75>;
	.reg .f64 	%fd<23>;
	.reg .b64 	%rd<30>;


	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r6, [vec_mul_fl_pow_param_0];
	ld.param.u64 	%rd4, [vec_mul_fl_pow_param_1];
	ld.param.u64 	%rd5, [vec_mul_fl_pow_param_2];
	ld.param.u64 	%rd6, [vec_mul_fl_pow_param_3];
	ld.param.f32 	%f13, [vec_mul_fl_pow_param_4];
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r15;
	setp.ge.s32	%p2, %r1, %r6;
	@%p2 bra 	BB9_22;

	cvta.to.global.u64 	%rd7, %rd4;
	cvta.to.global.u64 	%rd8, %rd6;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f32 	%f12, [%rd10];
	setp.gt.f32	%p3, %f12, 0f00000000;
	cvta.to.global.u64 	%rd11, %rd5;
	add.s64 	%rd1, %rd11, %rd9;
	add.s64 	%rd2, %rd7, %rd9;
	mov.u32 	%r74, 1;
	@%p3 bra 	BB9_24;

	ld.global.f32 	%f8, [%rd1];
	st.global.f32 	[%rd2], %f8;
	bra.uni 	BB9_22;

BB9_23:
	mul.f32 	%f13, %f13, 0f3F000000;
	ld.global.f32 	%f12, [%rd10];
	add.s32 	%r74, %r74, 1;

BB9_24:
	cvt.f64.f32	%fd1, %f12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	cvt.f64.f32	%fd2, %f13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd2;
	}
	bfe.u32 	%r17, %r4, 20, 11;
	add.s32 	%r18, %r17, -1012;
	mov.b64 	 %rd12, %fd2;
	shl.b64 	%rd3, %rd12, %r18;
	setp.eq.s64	%p4, %rd3, -9223372036854775808;
	abs.f64 	%fd3, %fd1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd2;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd9, [retval0+0];
	
	//{
	}// Callseq End 0
	setp.lt.s32	%p5, %r3, 0;
	and.pred  	%p1, %p5, %p4;
	@!%p1 bra 	BB9_4;
	bra.uni 	BB9_3;

BB9_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd9;
	}
	xor.b32  	%r20, %r19, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd9;
	}
	mov.b64 	%fd9, {%r21, %r20};

BB9_4:
	setp.eq.f32	%p6, %f12, 0f00000000;
	@%p6 bra 	BB9_7;
	bra.uni 	BB9_5;

BB9_7:
	selp.b32	%r22, %r3, 0, %p4;
	mov.u32 	%r23, 0;
	or.b32  	%r24, %r22, 2146435072;
	setp.lt.s32	%p10, %r4, 0;
	selp.b32	%r25, %r24, %r22, %p10;
	mov.b64 	%fd9, {%r23, %r25};
	bra.uni 	BB9_8;

BB9_5:
	setp.gt.s32	%p7, %r3, -1;
	@%p7 bra 	BB9_8;

	cvt.rzi.f64.f64	%fd14, %fd2;
	setp.neu.f64	%p8, %fd14, %fd2;
	selp.f64	%fd9, 0dFFF8000000000000, %fd9, %p8;

BB9_8:
	add.f64 	%fd22, %fd2, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd22;
	}
	and.b32  	%r27, %r26, 2146435072;
	setp.ne.s32	%p11, %r27, 2146435072;
	@%p11 bra 	BB9_9;

	setp.gtu.f64	%p12, %fd3, 0d7FF0000000000000;
	@%p12 bra 	BB9_19;

	abs.f64 	%fd15, %fd2;
	setp.gtu.f64	%p13, %fd15, 0d7FF0000000000000;
	@%p13 bra 	BB9_19;

	and.b32  	%r28, %r4, 2147483647;
	setp.ne.s32	%p14, %r28, 2146435072;
	@%p14 bra 	BB9_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd2;
	}
	setp.eq.s32	%p15, %r29, 0;
	@%p15 bra 	BB9_18;
	bra.uni 	BB9_14;

BB9_18:
	setp.gt.f64	%p18, %fd3, 0d3FF0000000000000;
	selp.b32	%r38, 2146435072, 0, %p18;
	mov.u32 	%r39, 0;
	xor.b32  	%r40, %r38, 2146435072;
	setp.lt.s32	%p19, %r4, 0;
	selp.b32	%r41, %r40, %r38, %p19;
	setp.eq.f32	%p20, %f12, 0fBF800000;
	selp.b32	%r42, 1072693248, %r41, %p20;
	mov.b64 	%fd22, {%r39, %r42};
	bra.uni 	BB9_19;

BB9_9:
	mov.f64 	%fd22, %fd9;

BB9_19:
	setp.eq.f32	%p21, %f13, 0f00000000;
	setp.eq.f32	%p22, %f12, 0f3F800000;
	or.pred  	%p23, %p22, %p21;
	selp.f64	%fd16, 0d3FF0000000000000, %fd22, %p23;
	ld.global.f32 	%f9, [%rd1];
	cvt.f64.f32	%fd17, %f9;
	mul.f64 	%fd18, %fd16, %fd17;
	cvt.rn.f32.f64	%f10, %fd18;
	st.global.f32 	[%rd2], %f10;
	abs.f32 	%f4, %f10;
	setp.gtu.f32	%p24, %f4, 0f7F800000;
	setp.lt.s32	%p25, %r74, 20;
	and.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB9_23;
	bra.uni 	BB9_20;

BB9_14:
	and.b32  	%r30, %r3, 2147483647;
	setp.ne.s32	%p16, %r30, 2146435072;
	@%p16 bra 	BB9_15;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd1;
	}
	setp.ne.s32	%p17, %r31, 0;
	mov.f64 	%fd22, %fd9;
	@%p17 bra 	BB9_19;

	shr.s32 	%r32, %r4, 31;
	and.b32  	%r33, %r32, -2146435072;
	add.s32 	%r34, %r33, 2146435072;
	or.b32  	%r35, %r34, -2147483648;
	selp.b32	%r36, %r35, %r34, %p1;
	mov.u32 	%r37, 0;
	mov.b64 	%fd22, {%r37, %r36};
	bra.uni 	BB9_19;

BB9_15:
	mov.f64 	%fd22, %fd9;
	bra.uni 	BB9_19;

BB9_20:
	setp.le.f32	%p27, %f4, 0f7F800000;
	@%p27 bra 	BB9_22;

	ld.global.f32 	%f11, [%rd1];
	st.global.f32 	[%rd2], %f11;
	cvt.f64.f32	%fd19, %f11;
	add.u64 	%rd23, %SP, 0;
	add.u64 	%rd24, %SPL, 0;
	st.local.f64 	[%rd24], %fd19;
	mov.u64 	%rd25, $str1;
	cvta.global.u64 	%rd26, %rd25;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd26;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd23;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r63, [retval0+0];
	
	//{
	}// Callseq End 1

BB9_22:
	ret;
}

	// .globl	vec_div
.visible .entry vec_div(
	.param .u32 vec_div_param_0,
	.param .u64 vec_div_param_1,
	.param .u64 vec_div_param_2,
	.param .u64 vec_div_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_div_param_0];
	ld.param.u64 	%rd1, [vec_div_param_1];
	ld.param.u64 	%rd2, [vec_div_param_2];
	ld.param.u64 	%rd3, [vec_div_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB10_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB10_2:
	ret;
}

	// .globl	vec_negate
.visible .entry vec_negate(
	.param .u32 vec_negate_param_0,
	.param .u64 vec_negate_param_1,
	.param .u64 vec_negate_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_negate_param_0];
	ld.param.u64 	%rd1, [vec_negate_param_1];
	ld.param.u64 	%rd2, [vec_negate_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	neg.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB11_2:
	ret;
}

	// .globl	vec_addScalar
.visible .entry vec_addScalar(
	.param .u32 vec_addScalar_param_0,
	.param .u64 vec_addScalar_param_1,
	.param .u64 vec_addScalar_param_2,
	.param .f64 vec_addScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_addScalar_param_0];
	ld.param.u64 	%rd1, [vec_addScalar_param_1];
	ld.param.u64 	%rd2, [vec_addScalar_param_2];
	ld.param.f64 	%fd1, [vec_addScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB12_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB12_2:
	ret;
}

	// .globl	vec_addScalarFloat
.visible .entry vec_addScalarFloat(
	.param .u32 vec_addScalarFloat_param_0,
	.param .u64 vec_addScalarFloat_param_1,
	.param .u64 vec_addScalarFloat_param_2,
	.param .f32 vec_addScalarFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_addScalarFloat_param_0];
	ld.param.u64 	%rd1, [vec_addScalarFloat_param_1];
	ld.param.u64 	%rd2, [vec_addScalarFloat_param_2];
	ld.param.f32 	%f1, [vec_addScalarFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB13_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	add.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

BB13_2:
	ret;
}

	// .globl	vec_subScalar
.visible .entry vec_subScalar(
	.param .u32 vec_subScalar_param_0,
	.param .u64 vec_subScalar_param_1,
	.param .u64 vec_subScalar_param_2,
	.param .f64 vec_subScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_subScalar_param_0];
	ld.param.u64 	%rd1, [vec_subScalar_param_1];
	ld.param.u64 	%rd2, [vec_subScalar_param_2];
	ld.param.f64 	%fd1, [vec_subScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB14_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB14_2:
	ret;
}

	// .globl	vec_mulScalar
.visible .entry vec_mulScalar(
	.param .u32 vec_mulScalar_param_0,
	.param .u64 vec_mulScalar_param_1,
	.param .u64 vec_mulScalar_param_2,
	.param .f64 vec_mulScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_mulScalar_param_0];
	ld.param.u64 	%rd1, [vec_mulScalar_param_1];
	ld.param.u64 	%rd2, [vec_mulScalar_param_2];
	ld.param.f64 	%fd1, [vec_mulScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB15_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB15_2:
	ret;
}

	// .globl	vec_mulScalarFloat
.visible .entry vec_mulScalarFloat(
	.param .u32 vec_mulScalarFloat_param_0,
	.param .u64 vec_mulScalarFloat_param_1,
	.param .u64 vec_mulScalarFloat_param_2,
	.param .f32 vec_mulScalarFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_mulScalarFloat_param_0];
	ld.param.u64 	%rd1, [vec_mulScalarFloat_param_1];
	ld.param.u64 	%rd2, [vec_mulScalarFloat_param_2];
	ld.param.f32 	%f1, [vec_mulScalarFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB16_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

BB16_2:
	ret;
}

	// .globl	vec_divScalar
.visible .entry vec_divScalar(
	.param .u32 vec_divScalar_param_0,
	.param .u64 vec_divScalar_param_1,
	.param .u64 vec_divScalar_param_2,
	.param .f64 vec_divScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_divScalar_param_0];
	ld.param.u64 	%rd1, [vec_divScalar_param_1];
	ld.param.u64 	%rd2, [vec_divScalar_param_2];
	ld.param.f64 	%fd1, [vec_divScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB17_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB17_2:
	ret;
}

	// .globl	vec_scalarAdd
.visible .entry vec_scalarAdd(
	.param .u32 vec_scalarAdd_param_0,
	.param .u64 vec_scalarAdd_param_1,
	.param .f64 vec_scalarAdd_param_2,
	.param .u64 vec_scalarAdd_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarAdd_param_0];
	ld.param.u64 	%rd1, [vec_scalarAdd_param_1];
	ld.param.f64 	%fd1, [vec_scalarAdd_param_2];
	ld.param.u64 	%rd2, [vec_scalarAdd_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB18_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB18_2:
	ret;
}

	// .globl	vec_scalarSub
.visible .entry vec_scalarSub(
	.param .u32 vec_scalarSub_param_0,
	.param .u64 vec_scalarSub_param_1,
	.param .f64 vec_scalarSub_param_2,
	.param .u64 vec_scalarSub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarSub_param_0];
	ld.param.u64 	%rd1, [vec_scalarSub_param_1];
	ld.param.f64 	%fd1, [vec_scalarSub_param_2];
	ld.param.u64 	%rd2, [vec_scalarSub_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB19_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	sub.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB19_2:
	ret;
}

	// .globl	vec_scalarMul
.visible .entry vec_scalarMul(
	.param .u32 vec_scalarMul_param_0,
	.param .u64 vec_scalarMul_param_1,
	.param .f64 vec_scalarMul_param_2,
	.param .u64 vec_scalarMul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarMul_param_0];
	ld.param.u64 	%rd1, [vec_scalarMul_param_1];
	ld.param.f64 	%fd1, [vec_scalarMul_param_2];
	ld.param.u64 	%rd2, [vec_scalarMul_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB20_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB20_2:
	ret;
}

	// .globl	vec_scalarDiv
.visible .entry vec_scalarDiv(
	.param .u32 vec_scalarDiv_param_0,
	.param .u64 vec_scalarDiv_param_1,
	.param .f64 vec_scalarDiv_param_2,
	.param .u64 vec_scalarDiv_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarDiv_param_0];
	ld.param.u64 	%rd1, [vec_scalarDiv_param_1];
	ld.param.f64 	%fd1, [vec_scalarDiv_param_2];
	ld.param.u64 	%rd2, [vec_scalarDiv_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB21_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	div.rn.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB21_2:
	ret;
}

	// .globl	vec_lt
.visible .entry vec_lt(
	.param .u32 vec_lt_param_0,
	.param .u64 vec_lt_param_1,
	.param .u64 vec_lt_param_2,
	.param .u64 vec_lt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_lt_param_0];
	ld.param.u64 	%rd1, [vec_lt_param_1];
	ld.param.u64 	%rd2, [vec_lt_param_2];
	ld.param.u64 	%rd3, [vec_lt_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB22_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB22_2:
	ret;
}

	// .globl	vec_lte
.visible .entry vec_lte(
	.param .u32 vec_lte_param_0,
	.param .u64 vec_lte_param_1,
	.param .u64 vec_lte_param_2,
	.param .u64 vec_lte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_lte_param_0];
	ld.param.u64 	%rd1, [vec_lte_param_1];
	ld.param.u64 	%rd2, [vec_lte_param_2];
	ld.param.u64 	%rd3, [vec_lte_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB23_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB23_2:
	ret;
}

	// .globl	vec_eq
.visible .entry vec_eq(
	.param .u32 vec_eq_param_0,
	.param .u64 vec_eq_param_1,
	.param .u64 vec_eq_param_2,
	.param .u64 vec_eq_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_eq_param_0];
	ld.param.u64 	%rd1, [vec_eq_param_1];
	ld.param.u64 	%rd2, [vec_eq_param_2];
	ld.param.u64 	%rd3, [vec_eq_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB24_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB24_2:
	ret;
}

	// .globl	vec_gte
.visible .entry vec_gte(
	.param .u32 vec_gte_param_0,
	.param .u64 vec_gte_param_1,
	.param .u64 vec_gte_param_2,
	.param .u64 vec_gte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_gte_param_0];
	ld.param.u64 	%rd1, [vec_gte_param_1];
	ld.param.u64 	%rd2, [vec_gte_param_2];
	ld.param.u64 	%rd3, [vec_gte_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB25_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB25_2:
	ret;
}

	// .globl	vec_gt
.visible .entry vec_gt(
	.param .u32 vec_gt_param_0,
	.param .u64 vec_gt_param_1,
	.param .u64 vec_gt_param_2,
	.param .u64 vec_gt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_gt_param_0];
	ld.param.u64 	%rd1, [vec_gt_param_1];
	ld.param.u64 	%rd2, [vec_gt_param_2];
	ld.param.u64 	%rd3, [vec_gt_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB26_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB26_2:
	ret;
}

	// .globl	vec_ne
.visible .entry vec_ne(
	.param .u32 vec_ne_param_0,
	.param .u64 vec_ne_param_1,
	.param .u64 vec_ne_param_2,
	.param .u64 vec_ne_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_ne_param_0];
	ld.param.u64 	%rd1, [vec_ne_param_1];
	ld.param.u64 	%rd2, [vec_ne_param_2];
	ld.param.u64 	%rd3, [vec_ne_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB27_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB27_2:
	ret;
}

	// .globl	vec_ltScalar
.visible .entry vec_ltScalar(
	.param .u32 vec_ltScalar_param_0,
	.param .u64 vec_ltScalar_param_1,
	.param .u64 vec_ltScalar_param_2,
	.param .f64 vec_ltScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_ltScalar_param_0];
	ld.param.u64 	%rd1, [vec_ltScalar_param_1];
	ld.param.u64 	%rd2, [vec_ltScalar_param_2];
	ld.param.f64 	%fd1, [vec_ltScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB28_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB28_2:
	ret;
}

	// .globl	vec_lteScalar
.visible .entry vec_lteScalar(
	.param .u32 vec_lteScalar_param_0,
	.param .u64 vec_lteScalar_param_1,
	.param .u64 vec_lteScalar_param_2,
	.param .f64 vec_lteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_lteScalar_param_0];
	ld.param.u64 	%rd1, [vec_lteScalar_param_1];
	ld.param.u64 	%rd2, [vec_lteScalar_param_2];
	ld.param.f64 	%fd1, [vec_lteScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB29_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB29_2:
	ret;
}

	// .globl	vec_eqScalar
.visible .entry vec_eqScalar(
	.param .u32 vec_eqScalar_param_0,
	.param .u64 vec_eqScalar_param_1,
	.param .u64 vec_eqScalar_param_2,
	.param .f64 vec_eqScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_eqScalar_param_0];
	ld.param.u64 	%rd1, [vec_eqScalar_param_1];
	ld.param.u64 	%rd2, [vec_eqScalar_param_2];
	ld.param.f64 	%fd1, [vec_eqScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB30_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB30_2:
	ret;
}

	// .globl	vec_gteScalar
.visible .entry vec_gteScalar(
	.param .u32 vec_gteScalar_param_0,
	.param .u64 vec_gteScalar_param_1,
	.param .u64 vec_gteScalar_param_2,
	.param .f64 vec_gteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_gteScalar_param_0];
	ld.param.u64 	%rd1, [vec_gteScalar_param_1];
	ld.param.u64 	%rd2, [vec_gteScalar_param_2];
	ld.param.f64 	%fd1, [vec_gteScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB31_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB31_2:
	ret;
}

	// .globl	vec_gtScalar
.visible .entry vec_gtScalar(
	.param .u32 vec_gtScalar_param_0,
	.param .u64 vec_gtScalar_param_1,
	.param .u64 vec_gtScalar_param_2,
	.param .f64 vec_gtScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_gtScalar_param_0];
	ld.param.u64 	%rd1, [vec_gtScalar_param_1];
	ld.param.u64 	%rd2, [vec_gtScalar_param_2];
	ld.param.f64 	%fd1, [vec_gtScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB32_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB32_2:
	ret;
}

	// .globl	vec_neScalar
.visible .entry vec_neScalar(
	.param .u32 vec_neScalar_param_0,
	.param .u64 vec_neScalar_param_1,
	.param .u64 vec_neScalar_param_2,
	.param .f64 vec_neScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_neScalar_param_0];
	ld.param.u64 	%rd1, [vec_neScalar_param_1];
	ld.param.u64 	%rd2, [vec_neScalar_param_2];
	ld.param.f64 	%fd1, [vec_neScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB33_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB33_2:
	ret;
}

	// .globl	vec_acos
.visible .entry vec_acos(
	.param .u32 vec_acos_param_0,
	.param .u64 vec_acos_param_1,
	.param .u64 vec_acos_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r4, [vec_acos_param_0];
	ld.param.u64 	%rd1, [vec_acos_param_1];
	ld.param.u64 	%rd2, [vec_acos_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB34_14;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd16, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd16;
	}
	abs.f64 	%fd1, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	setp.lt.s32	%p2, %r14, 1071801958;
	@%p2 bra 	BB34_9;
	bra.uni 	BB34_2;

BB34_9:
	mul.f64 	%fd62, %fd1, %fd1;
	mov.f64 	%fd63, 0dBFB3823B180754AF;
	mov.f64 	%fd64, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0d3FB333333320F91B;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mul.f64 	%fd88, %fd62, %fd87;
	fma.rn.f64 	%fd10, %fd88, %fd1, %fd1;
	setp.lt.s32	%p6, %r2, 0;
	@%p6 bra 	BB34_11;

	mov.f64 	%fd89, 0dBC91A62633145C07;
	add.rn.f64 	%fd90, %fd10, %fd89;
	neg.f64 	%fd95, %fd90;
	bra.uni 	BB34_12;

BB34_2:
	mov.f64 	%fd19, 0d3FF0000000000000;
	sub.f64 	%fd2, %fd19, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	add.s32 	%r16, %r3, -1048576;
	mov.b64 	%fd18, {%r15, %r16};
	// inline asm
	rsqrt.approx.ftz.f64 %fd17, %fd18;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd17;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd17;
	}
	add.s32 	%r19, %r18, -1048576;
	mov.b64 	%fd20, {%r17, %r19};
	mul.f64 	%fd21, %fd18, %fd17;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd18;
	fma.rn.f64 	%fd24, %fd23, %fd20, %fd21;
	neg.f64 	%fd25, %fd24;
	fma.rn.f64 	%fd26, %fd17, %fd25, %fd19;
	fma.rn.f64 	%fd27, %fd26, %fd20, %fd20;
	fma.rn.f64 	%fd28, %fd24, %fd25, %fd18;
	fma.rn.f64 	%fd3, %fd28, %fd27, %fd24;
	setp.lt.s32	%p3, %r3, 1;
	@%p3 bra 	BB34_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd3;
	}
	add.s32 	%r21, %r20, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd3;
	}
	mov.b64 	%fd29, {%r22, %r21};
	mov.f64 	%fd30, 0dBEBAC2FE66FAAC4B;
	mov.f64 	%fd31, 0d3EC715B371155F70;
	fma.rn.f64 	%fd32, %fd31, %fd2, %fd30;
	mov.f64 	%fd33, 0d3ED9A9B88EFCD9B8;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0d3EDD0F40A8A0C4C3;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3EF46D4CFA9E0E1F;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3F079C168D1E2422;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3F1C9A88C3BCA540;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0d3F31C4E64BD476DF;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3F46E8BA60009C8F;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F5F1C71C62B05A2;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3F76DB6DB6DC9F2C;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0d3F9333333333329C;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3FB5555555555555;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mul.f64 	%fd55, %fd2, %fd54;
	fma.rn.f64 	%fd94, %fd55, %fd29, %fd29;
	bra.uni 	BB34_5;

BB34_11:
	mov.f64 	%fd91, 0d3C91A62633145C07;
	add.rn.f64 	%fd95, %fd10, %fd91;

BB34_12:
	mov.f64 	%fd92, 0d3FF921FB54442D18;
	add.rn.f64 	%fd94, %fd92, %fd95;
	bra.uni 	BB34_13;

BB34_4:
	mov.f64 	%fd56, 0d0000000000000000;
	mul.rn.f64 	%fd94, %fd1, %fd56;

BB34_5:
	setp.gt.s32	%p4, %r3, -1;
	@%p4 bra 	BB34_7;

	mov.f64 	%fd57, 0d7FF0000000000000;
	mul.rn.f64 	%fd94, %fd94, %fd57;

BB34_7:
	setp.gt.s32	%p5, %r2, -1;
	@%p5 bra 	BB34_13;

	mov.f64 	%fd58, 0dBCA1A62633145C07;
	add.rn.f64 	%fd59, %fd94, %fd58;
	neg.f64 	%fd60, %fd59;
	mov.f64 	%fd61, 0d400921FB54442D18;
	add.rn.f64 	%fd94, %fd61, %fd60;

BB34_13:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd94;

BB34_14:
	ret;
}

	// .globl	vec_acosh
.visible .entry vec_acosh(
	.param .u32 vec_acosh_param_0,
	.param .u64 vec_acosh_param_1,
	.param .u64 vec_acosh_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<160>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r22, [vec_acosh_param_0];
	ld.param.u64 	%rd1, [vec_acosh_param_1];
	ld.param.u64 	%rd2, [vec_acosh_param_2];
	mov.u32 	%r23, %tid.x;
	mov.u32 	%r24, %ntid.y;
	mov.u32 	%r25, %ctaid.y;
	mov.u32 	%r26, %tid.y;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %nctaid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r30, %r27, %r28, %r29;
	mov.u32 	%r31, %ntid.x;
	mad.lo.s32 	%r1, %r30, %r31, %r23;
	setp.ge.s32	%p1, %r1, %r22;
	@%p1 bra 	BB35_21;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	add.f64 	%fd156, %fd1, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd156;
	}
	setp.gt.u32	%p2, %r66, 1127219199;
	@%p2 bra 	BB35_12;
	bra.uni 	BB35_2;

BB35_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd156;
	}
	mov.u32 	%r68, -1023;
	setp.gt.s32	%p11, %r66, 1048575;
	@%p11 bra 	BB35_14;

	mul.f64 	%fd156, %fd156, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd156;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd156;
	}
	mov.u32 	%r68, -1077;

BB35_14:
	add.s32 	%r51, %r66, -1;
	setp.lt.u32	%p12, %r51, 2146435071;
	@%p12 bra 	BB35_16;
	bra.uni 	BB35_15;

BB35_16:
	shr.u32 	%r53, %r66, 20;
	add.s32 	%r69, %r68, %r53;
	and.b32  	%r54, %r66, -2146435073;
	or.b32  	%r55, %r54, 1072693248;
	mov.b64 	%fd157, {%r67, %r55};
	setp.lt.s32	%p14, %r55, 1073127583;
	@%p14 bra 	BB35_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd157;
	}
	add.s32 	%r58, %r57, -1048576;
	mov.b64 	%fd157, {%r56, %r58};
	add.s32 	%r69, %r69, 1;

BB35_18:
	add.f64 	%fd109, %fd157, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd110, %fd109;
	neg.f64 	%fd111, %fd109;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd110, %fd112;
	fma.rn.f64 	%fd114, %fd113, %fd113, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd110, %fd110;
	add.f64 	%fd116, %fd157, 0dBFF0000000000000;
	mul.f64 	%fd117, %fd116, %fd115;
	fma.rn.f64 	%fd118, %fd116, %fd115, %fd117;
	mul.f64 	%fd119, %fd118, %fd118;
	mov.f64 	%fd120, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd121, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd122, %fd121, %fd119, %fd120;
	mov.f64 	%fd123, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd124, %fd122, %fd119, %fd123;
	mov.f64 	%fd125, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd126, %fd124, %fd119, %fd125;
	mov.f64 	%fd127, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd128, %fd126, %fd119, %fd127;
	mov.f64 	%fd129, 0d3F624924923BE72D;
	fma.rn.f64 	%fd130, %fd128, %fd119, %fd129;
	mov.f64 	%fd131, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd132, %fd130, %fd119, %fd131;
	mov.f64 	%fd133, 0d3FB5555555555554;
	fma.rn.f64 	%fd134, %fd132, %fd119, %fd133;
	sub.f64 	%fd135, %fd116, %fd118;
	add.f64 	%fd136, %fd135, %fd135;
	neg.f64 	%fd137, %fd118;
	fma.rn.f64 	%fd138, %fd137, %fd116, %fd136;
	mul.f64 	%fd139, %fd115, %fd138;
	mul.f64 	%fd140, %fd119, %fd134;
	fma.rn.f64 	%fd141, %fd140, %fd118, %fd139;
	xor.b32  	%r59, %r69, -2147483648;
	mov.u32 	%r60, -2147483648;
	mov.u32 	%r61, 1127219200;
	mov.b64 	%fd142, {%r59, %r61};
	mov.b64 	%fd143, {%r60, %r61};
	sub.f64 	%fd144, %fd142, %fd143;
	mov.f64 	%fd145, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd146, %fd144, %fd145, %fd118;
	neg.f64 	%fd147, %fd144;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	sub.f64 	%fd149, %fd148, %fd118;
	sub.f64 	%fd150, %fd141, %fd149;
	mov.f64 	%fd151, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd152, %fd144, %fd151, %fd150;
	add.f64 	%fd158, %fd146, %fd152;
	bra.uni 	BB35_19;

BB35_2:
	fma.rn.f64 	%fd26, %fd1, %fd156, %fd156;
	// inline asm
	rsqrt.approx.ftz.f64 %fd25, %fd26;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd25;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd25;
	}
	add.s32 	%r34, %r33, -1048576;
	mov.b64 	%fd27, {%r32, %r34};
	mul.f64 	%fd28, %fd26, %fd25;
	neg.f64 	%fd29, %fd28;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd26;
	fma.rn.f64 	%fd31, %fd30, %fd27, %fd28;
	neg.f64 	%fd32, %fd31;
	mov.f64 	%fd33, 0d3FF0000000000000;
	fma.rn.f64 	%fd34, %fd25, %fd32, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd27, %fd27;
	fma.rn.f64 	%fd36, %fd31, %fd32, %fd26;
	fma.rn.f64 	%fd37, %fd36, %fd35, %fd31;
	add.f64 	%fd3, %fd156, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd3;
	}
	setp.lt.u32	%p3, %r35, 1071994197;
	setp.lt.s32	%p4, %r35, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB35_10;
	bra.uni 	BB35_3;

BB35_10:
	add.f64 	%fd84, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd85, %fd3, %fd84;
	mul.f64 	%fd86, %fd3, %fd85;
	neg.f64 	%fd87, %fd86;
	sub.f64 	%fd88, %fd3, %fd86;
	mul.f64 	%fd89, %fd88, %fd88;
	mov.f64 	%fd90, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd91, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd92, %fd91, %fd89, %fd90;
	mov.f64 	%fd93, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd94, %fd92, %fd89, %fd93;
	mov.f64 	%fd95, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd96, %fd94, %fd89, %fd95;
	mov.f64 	%fd97, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd98, %fd96, %fd89, %fd97;
	mov.f64 	%fd99, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd100, %fd98, %fd89, %fd99;
	mov.f64 	%fd101, 0d3F899999999D70C4;
	fma.rn.f64 	%fd102, %fd100, %fd89, %fd101;
	mov.f64 	%fd103, 0d3FB5555555555462;
	fma.rn.f64 	%fd104, %fd102, %fd89, %fd103;
	mul.f64 	%fd105, %fd89, %fd104;
	fma.rn.f64 	%fd106, %fd105, %fd88, %fd87;
	add.f64 	%fd155, %fd3, %fd106;
	bra.uni 	BB35_11;

BB35_15:
	mov.f64 	%fd107, 0d7FF0000000000000;
	fma.rn.f64 	%fd108, %fd156, %fd107, %fd107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd156;
	}
	mov.b32 	 %f2, %r52;
	setp.eq.f32	%p13, %f2, 0f00000000;
	selp.f64	%fd158, 0dFFF0000000000000, %fd108, %p13;

BB35_19:
	add.f64 	%fd159, %fd158, 0d3FE62E42FEFA39EF;
	bra.uni 	BB35_20;

BB35_3:
	add.f64 	%fd153, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd153;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd153;
	}
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p6, %r62, 1048575;
	@%p6 bra 	BB35_5;

	mul.f64 	%fd153, %fd153, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd153;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd153;
	}
	mov.u32 	%r64, -1077;

BB35_5:
	add.s32 	%r38, %r62, -1;
	setp.lt.u32	%p7, %r38, 2146435071;
	@%p7 bra 	BB35_7;
	bra.uni 	BB35_6;

BB35_7:
	shr.u32 	%r40, %r62, 20;
	add.s32 	%r65, %r64, %r40;
	and.b32  	%r41, %r62, -2146435073;
	or.b32  	%r42, %r41, 1072693248;
	mov.b64 	%fd154, {%r63, %r42};
	setp.lt.s32	%p9, %r42, 1073127583;
	@%p9 bra 	BB35_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd154;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd154;
	}
	add.s32 	%r45, %r44, -1048576;
	mov.b64 	%fd154, {%r43, %r45};
	add.s32 	%r65, %r65, 1;

BB35_9:
	add.f64 	%fd40, %fd154, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd41, %fd40;
	neg.f64 	%fd42, %fd40;
	fma.rn.f64 	%fd44, %fd42, %fd41, %fd33;
	fma.rn.f64 	%fd45, %fd44, %fd44, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd41, %fd41;
	add.f64 	%fd47, %fd154, 0dBFF0000000000000;
	mul.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd47, %fd46, %fd48;
	mul.f64 	%fd50, %fd49, %fd49;
	mov.f64 	%fd51, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd52, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	mov.f64 	%fd54, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd55, %fd53, %fd50, %fd54;
	mov.f64 	%fd56, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd57, %fd55, %fd50, %fd56;
	mov.f64 	%fd58, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd59, %fd57, %fd50, %fd58;
	mov.f64 	%fd60, 0d3F624924923BE72D;
	fma.rn.f64 	%fd61, %fd59, %fd50, %fd60;
	mov.f64 	%fd62, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd63, %fd61, %fd50, %fd62;
	mov.f64 	%fd64, 0d3FB5555555555554;
	fma.rn.f64 	%fd65, %fd63, %fd50, %fd64;
	sub.f64 	%fd66, %fd47, %fd49;
	add.f64 	%fd67, %fd66, %fd66;
	neg.f64 	%fd68, %fd49;
	fma.rn.f64 	%fd69, %fd68, %fd47, %fd67;
	mul.f64 	%fd70, %fd46, %fd69;
	mul.f64 	%fd71, %fd50, %fd65;
	fma.rn.f64 	%fd72, %fd71, %fd49, %fd70;
	xor.b32  	%r46, %r65, -2147483648;
	mov.u32 	%r47, -2147483648;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd73, {%r46, %r48};
	mov.b64 	%fd74, {%r47, %r48};
	sub.f64 	%fd75, %fd73, %fd74;
	mov.f64 	%fd76, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd77, %fd75, %fd76, %fd49;
	neg.f64 	%fd78, %fd75;
	fma.rn.f64 	%fd79, %fd78, %fd76, %fd77;
	sub.f64 	%fd80, %fd79, %fd49;
	sub.f64 	%fd81, %fd72, %fd80;
	mov.f64 	%fd82, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd83, %fd75, %fd82, %fd81;
	add.f64 	%fd155, %fd77, %fd83;
	bra.uni 	BB35_11;

BB35_6:
	mov.f64 	%fd38, 0d7FF0000000000000;
	fma.rn.f64 	%fd39, %fd153, %fd38, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd153;
	}
	mov.b32 	 %f1, %r39;
	setp.eq.f32	%p8, %f1, 0f00000000;
	selp.f64	%fd155, 0dFFF0000000000000, %fd39, %p8;

BB35_11:
	setp.eq.s32	%p10, %r66, 0;
	selp.f64	%fd159, %fd156, %fd155, %p10;

BB35_20:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd159;

BB35_21:
	ret;
}

	// .globl	vec_asin
.visible .entry vec_asin(
	.param .u32 vec_asin_param_0,
	.param .u64 vec_asin_param_1,
	.param .u64 vec_asin_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<83>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r3, [vec_asin_param_0];
	ld.param.u64 	%rd1, [vec_asin_param_1];
	ld.param.u64 	%rd2, [vec_asin_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB36_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f1, %r2;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FE26666;
	@%p2 bra 	BB36_3;
	bra.uni 	BB36_2;

BB36_3:
	mul.f64 	%fd55, %fd1, %fd1;
	mov.f64 	%fd56, 0dBFB3823B180754AF;
	mov.f64 	%fd57, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0d3FB333333320F91B;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mul.f64 	%fd81, %fd55, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd1, %fd1;
	bra.uni 	BB36_4;

BB36_2:
	abs.f64 	%fd7, %fd1;
	mov.f64 	%fd8, 0d3FE0000000000000;
	mov.f64 	%fd9, 0dBFE0000000000000;
	fma.rn.f64 	%fd6, %fd9, %fd7, %fd8;
	// inline asm
	rsqrt.approx.ftz.f64 %fd5, %fd6;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd5;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd5;
	}
	add.s32 	%r15, %r14, -1048576;
	mov.b64 	%fd10, {%r13, %r15};
	mul.f64 	%fd11, %fd6, %fd5;
	neg.f64 	%fd12, %fd11;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd6;
	fma.rn.f64 	%fd14, %fd13, %fd10, %fd11;
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF0000000000000;
	fma.rn.f64 	%fd17, %fd5, %fd15, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd10, %fd10;
	fma.rn.f64 	%fd19, %fd14, %fd15, %fd6;
	fma.rn.f64 	%fd20, %fd19, %fd18, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd6;
	}
	setp.lt.s32	%p3, %r16, 0;
	selp.f64	%fd21, 0dFFF8000000000000, %fd20, %p3;
	setp.equ.f64	%p4, %fd6, 0d0000000000000000;
	selp.f64	%fd22, %fd6, %fd21, %p4;
	mov.f64 	%fd23, 0dBFB3823B180754AF;
	mov.f64 	%fd24, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd25, %fd24, %fd6, %fd23;
	mov.f64 	%fd26, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd27, %fd25, %fd6, %fd26;
	mov.f64 	%fd28, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd29, %fd27, %fd6, %fd28;
	mov.f64 	%fd30, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd31, %fd29, %fd6, %fd30;
	mov.f64 	%fd32, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd33, %fd31, %fd6, %fd32;
	mov.f64 	%fd34, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd35, %fd33, %fd6, %fd34;
	mov.f64 	%fd36, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd37, %fd35, %fd6, %fd36;
	mov.f64 	%fd38, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd39, %fd37, %fd6, %fd38;
	mov.f64 	%fd40, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd41, %fd39, %fd6, %fd40;
	mov.f64 	%fd42, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd43, %fd41, %fd6, %fd42;
	mov.f64 	%fd44, 0d3FB333333320F91B;
	fma.rn.f64 	%fd45, %fd43, %fd6, %fd44;
	mov.f64 	%fd46, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd47, %fd45, %fd6, %fd46;
	mul.f64 	%fd48, %fd6, %fd47;
	mul.f64 	%fd49, %fd22, 0dC000000000000000;
	mov.f64 	%fd50, 0d3C91A62633145C07;
	fma.rn.f64 	%fd51, %fd49, %fd48, %fd50;
	add.f64 	%fd52, %fd49, 0d3FE921FB54442D18;
	add.f64 	%fd53, %fd52, %fd51;
	add.f64 	%fd54, %fd53, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd54;
	}
	and.b32  	%r19, %r2, -2147483648;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd82, {%r17, %r20};

BB36_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd82;

BB36_5:
	ret;
}

	// .globl	vec_asinh
.visible .entry vec_asinh(
	.param .u32 vec_asinh_param_0,
	.param .u64 vec_asinh_param_1,
	.param .u64 vec_asinh_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<77>;
	.reg .f64 	%fd<161>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r23, [vec_asinh_param_0];
	ld.param.u64 	%rd1, [vec_asinh_param_1];
	ld.param.u64 	%rd2, [vec_asinh_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r28, %r25, %r26, %r27;
	mov.u32 	%r29, %nctaid.x;
	mov.u32 	%r30, %ctaid.x;
	mad.lo.s32 	%r31, %r28, %r29, %r30;
	mov.u32 	%r32, %ntid.x;
	mad.lo.s32 	%r1, %r31, %r32, %r24;
	setp.ge.s32	%p1, %r1, %r23;
	@%p1 bra 	BB37_20;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd1;
	}
	and.b32  	%r34, %r2, 2147483647;
	mov.b64 	%fd157, {%r33, %r34};
	setp.gt.u32	%p2, %r34, 1138753535;
	@%p2 bra 	BB37_11;
	bra.uni 	BB37_2;

BB37_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %fd157;
	}
	mov.u32 	%r75, -1023;
	setp.gt.s32	%p10, %r73, 1048575;
	@%p10 bra 	BB37_13;

	mul.f64 	%fd157, %fd157, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %fd157;
	}
	mov.u32 	%r75, -1077;

BB37_13:
	add.s32 	%r54, %r73, -1;
	setp.lt.u32	%p11, %r54, 2146435071;
	@%p11 bra 	BB37_15;
	bra.uni 	BB37_14;

BB37_15:
	shr.u32 	%r56, %r73, 20;
	add.s32 	%r76, %r75, %r56;
	and.b32  	%r57, %r73, -2146435073;
	or.b32  	%r58, %r57, 1072693248;
	mov.b64 	%fd158, {%r74, %r58};
	setp.lt.s32	%p13, %r58, 1073127583;
	@%p13 bra 	BB37_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd158;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd158;
	}
	add.s32 	%r61, %r60, -1048576;
	mov.b64 	%fd158, {%r59, %r61};
	add.s32 	%r76, %r76, 1;

BB37_17:
	add.f64 	%fd110, %fd158, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd111, %fd110;
	neg.f64 	%fd112, %fd110;
	mov.f64 	%fd113, 0d3FF0000000000000;
	fma.rn.f64 	%fd114, %fd112, %fd111, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd114, %fd114;
	fma.rn.f64 	%fd116, %fd115, %fd111, %fd111;
	add.f64 	%fd117, %fd158, 0dBFF0000000000000;
	mul.f64 	%fd118, %fd117, %fd116;
	fma.rn.f64 	%fd119, %fd117, %fd116, %fd118;
	mul.f64 	%fd120, %fd119, %fd119;
	mov.f64 	%fd121, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd122, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd123, %fd122, %fd120, %fd121;
	mov.f64 	%fd124, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd125, %fd123, %fd120, %fd124;
	mov.f64 	%fd126, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd127, %fd125, %fd120, %fd126;
	mov.f64 	%fd128, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd129, %fd127, %fd120, %fd128;
	mov.f64 	%fd130, 0d3F624924923BE72D;
	fma.rn.f64 	%fd131, %fd129, %fd120, %fd130;
	mov.f64 	%fd132, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd133, %fd131, %fd120, %fd132;
	mov.f64 	%fd134, 0d3FB5555555555554;
	fma.rn.f64 	%fd135, %fd133, %fd120, %fd134;
	sub.f64 	%fd136, %fd117, %fd119;
	add.f64 	%fd137, %fd136, %fd136;
	neg.f64 	%fd138, %fd119;
	fma.rn.f64 	%fd139, %fd138, %fd117, %fd137;
	mul.f64 	%fd140, %fd116, %fd139;
	mul.f64 	%fd141, %fd120, %fd135;
	fma.rn.f64 	%fd142, %fd141, %fd119, %fd140;
	xor.b32  	%r62, %r76, -2147483648;
	mov.u32 	%r63, -2147483648;
	mov.u32 	%r64, 1127219200;
	mov.b64 	%fd143, {%r62, %r64};
	mov.b64 	%fd144, {%r63, %r64};
	sub.f64 	%fd145, %fd143, %fd144;
	mov.f64 	%fd146, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd147, %fd145, %fd146, %fd119;
	neg.f64 	%fd148, %fd145;
	fma.rn.f64 	%fd149, %fd148, %fd146, %fd147;
	sub.f64 	%fd150, %fd149, %fd119;
	sub.f64 	%fd151, %fd142, %fd150;
	mov.f64 	%fd152, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd153, %fd145, %fd152, %fd151;
	add.f64 	%fd159, %fd147, %fd153;
	bra.uni 	BB37_18;

BB37_2:
	mul.rn.f64 	%fd25, %fd1, %fd1;
	mov.f64 	%fd26, 0d3FF0000000000000;
	fma.rn.f64 	%fd24, %fd1, %fd1, %fd26;
	// inline asm
	rsqrt.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd23;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd23;
	}
	add.s32 	%r37, %r36, -1048576;
	mov.b64 	%fd27, {%r35, %r37};
	mul.f64 	%fd28, %fd24, %fd23;
	neg.f64 	%fd29, %fd28;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd24;
	fma.rn.f64 	%fd31, %fd30, %fd27, %fd28;
	neg.f64 	%fd32, %fd31;
	fma.rn.f64 	%fd33, %fd23, %fd32, %fd26;
	fma.rn.f64 	%fd34, %fd33, %fd27, %fd27;
	fma.rn.f64 	%fd35, %fd31, %fd32, %fd24;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd31;
	add.f64 	%fd37, %fd36, 0d3FF0000000000000;
	div.rn.f64 	%fd38, %fd25, %fd37;
	add.f64 	%fd3, %fd157, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd3;
	}
	setp.lt.u32	%p3, %r38, 1071994197;
	setp.lt.s32	%p4, %r38, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB37_10;
	bra.uni 	BB37_3;

BB37_10:
	add.f64 	%fd85, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd86, %fd3, %fd85;
	mul.f64 	%fd87, %fd3, %fd86;
	neg.f64 	%fd88, %fd87;
	sub.f64 	%fd89, %fd3, %fd87;
	mul.f64 	%fd90, %fd89, %fd89;
	mov.f64 	%fd91, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd92, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	mov.f64 	%fd94, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd95, %fd93, %fd90, %fd94;
	mov.f64 	%fd96, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd97, %fd95, %fd90, %fd96;
	mov.f64 	%fd98, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd99, %fd97, %fd90, %fd98;
	mov.f64 	%fd100, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd101, %fd99, %fd90, %fd100;
	mov.f64 	%fd102, 0d3F899999999D70C4;
	fma.rn.f64 	%fd103, %fd101, %fd90, %fd102;
	mov.f64 	%fd104, 0d3FB5555555555462;
	fma.rn.f64 	%fd105, %fd103, %fd90, %fd104;
	mul.f64 	%fd106, %fd90, %fd105;
	fma.rn.f64 	%fd107, %fd106, %fd89, %fd88;
	add.f64 	%fd160, %fd3, %fd107;
	bra.uni 	BB37_19;

BB37_14:
	mov.f64 	%fd108, 0d7FF0000000000000;
	fma.rn.f64 	%fd109, %fd157, %fd108, %fd108;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd157;
	}
	mov.b32 	 %f2, %r55;
	setp.eq.f32	%p12, %f2, 0f00000000;
	selp.f64	%fd159, 0dFFF0000000000000, %fd109, %p12;

BB37_18:
	add.f64 	%fd160, %fd159, 0d3FE62E42FEFA39EF;
	bra.uni 	BB37_19;

BB37_3:
	add.f64 	%fd155, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd155;
	}
	mov.u32 	%r71, -1023;
	setp.gt.s32	%p6, %r69, 1048575;
	@%p6 bra 	BB37_5;

	mul.f64 	%fd155, %fd155, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd155;
	}
	mov.u32 	%r71, -1077;

BB37_5:
	add.s32 	%r41, %r69, -1;
	setp.lt.u32	%p7, %r41, 2146435071;
	@%p7 bra 	BB37_7;
	bra.uni 	BB37_6;

BB37_7:
	shr.u32 	%r43, %r69, 20;
	add.s32 	%r72, %r71, %r43;
	and.b32  	%r44, %r69, -2146435073;
	or.b32  	%r45, %r44, 1072693248;
	mov.b64 	%fd156, {%r70, %r45};
	setp.lt.s32	%p9, %r45, 1073127583;
	@%p9 bra 	BB37_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd156;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd156;
	}
	add.s32 	%r48, %r47, -1048576;
	mov.b64 	%fd156, {%r46, %r48};
	add.s32 	%r72, %r72, 1;

BB37_9:
	add.f64 	%fd41, %fd156, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd42, %fd41;
	neg.f64 	%fd43, %fd41;
	fma.rn.f64 	%fd45, %fd43, %fd42, %fd26;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd42, %fd42;
	add.f64 	%fd48, %fd156, 0dBFF0000000000000;
	mul.f64 	%fd49, %fd48, %fd47;
	fma.rn.f64 	%fd50, %fd48, %fd47, %fd49;
	mul.f64 	%fd51, %fd50, %fd50;
	mov.f64 	%fd52, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd53, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0d3F624924923BE72D;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mov.f64 	%fd65, 0d3FB5555555555554;
	fma.rn.f64 	%fd66, %fd64, %fd51, %fd65;
	sub.f64 	%fd67, %fd48, %fd50;
	add.f64 	%fd68, %fd67, %fd67;
	neg.f64 	%fd69, %fd50;
	fma.rn.f64 	%fd70, %fd69, %fd48, %fd68;
	mul.f64 	%fd71, %fd47, %fd70;
	mul.f64 	%fd72, %fd51, %fd66;
	fma.rn.f64 	%fd73, %fd72, %fd50, %fd71;
	xor.b32  	%r49, %r72, -2147483648;
	mov.u32 	%r50, -2147483648;
	mov.u32 	%r51, 1127219200;
	mov.b64 	%fd74, {%r49, %r51};
	mov.b64 	%fd75, {%r50, %r51};
	sub.f64 	%fd76, %fd74, %fd75;
	mov.f64 	%fd77, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd78, %fd76, %fd77, %fd50;
	neg.f64 	%fd79, %fd76;
	fma.rn.f64 	%fd80, %fd79, %fd77, %fd78;
	sub.f64 	%fd81, %fd80, %fd50;
	sub.f64 	%fd82, %fd73, %fd81;
	mov.f64 	%fd83, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd84, %fd76, %fd83, %fd82;
	add.f64 	%fd160, %fd78, %fd84;
	bra.uni 	BB37_19;

BB37_6:
	mov.f64 	%fd39, 0d7FF0000000000000;
	fma.rn.f64 	%fd40, %fd155, %fd39, %fd39;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd155;
	}
	mov.b32 	 %f1, %r42;
	setp.eq.f32	%p8, %f1, 0f00000000;
	selp.f64	%fd160, 0dFFF0000000000000, %fd40, %p8;

BB37_19:
	cvta.to.global.u64 	%rd6, %rd1;
	and.b32  	%r65, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd160;
	}
	or.b32  	%r67, %r66, %r65;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd160;
	}
	mov.b64 	%fd154, {%r68, %r67};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd154;

BB37_20:
	ret;
}

	// .globl	vec_atan
.visible .entry vec_atan(
	.param .u32 vec_atan_param_0,
	.param .u64 vec_atan_param_1,
	.param .u64 vec_atan_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<56>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_atan_param_0];
	ld.param.u64 	%rd1, [vec_atan_param_1];
	ld.param.u64 	%rd2, [vec_atan_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB38_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.leu.f64	%p2, %fd2, 0d3FF0000000000000;
	mov.f64 	%fd55, %fd2;
	@%p2 bra 	BB38_3;

	rcp.approx.ftz.f64 	%fd5, %fd2;
	neg.f64 	%fd6, %fd2;
	mov.f64 	%fd7, 0d3FF0000000000000;
	fma.rn.f64 	%fd8, %fd6, %fd5, %fd7;
	fma.rn.f64 	%fd9, %fd8, %fd8, %fd8;
	fma.rn.f64 	%fd10, %fd9, %fd5, %fd5;
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	selp.f64	%fd55, 0d0000000000000000, %fd10, %p3;

BB38_3:
	cvta.to.global.u64 	%rd6, %rd1;
	mul.f64 	%fd11, %fd55, %fd55;
	mov.f64 	%fd12, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd13, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd14, %fd13, %fd11, %fd12;
	mov.f64 	%fd15, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd16, %fd14, %fd11, %fd15;
	mov.f64 	%fd17, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd18, %fd16, %fd11, %fd17;
	mov.f64 	%fd19, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd20, %fd18, %fd11, %fd19;
	mov.f64 	%fd21, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd22, %fd20, %fd11, %fd21;
	mov.f64 	%fd23, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd24, %fd22, %fd11, %fd23;
	mov.f64 	%fd25, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd26, %fd24, %fd11, %fd25;
	mov.f64 	%fd27, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd28, %fd26, %fd11, %fd27;
	mov.f64 	%fd29, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd30, %fd28, %fd11, %fd29;
	mov.f64 	%fd31, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd32, %fd30, %fd11, %fd31;
	mov.f64 	%fd33, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd34, %fd32, %fd11, %fd33;
	mov.f64 	%fd35, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd36, %fd34, %fd11, %fd35;
	mov.f64 	%fd37, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd38, %fd36, %fd11, %fd37;
	mov.f64 	%fd39, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd40, %fd38, %fd11, %fd39;
	mov.f64 	%fd41, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd42, %fd40, %fd11, %fd41;
	mov.f64 	%fd43, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd44, %fd42, %fd11, %fd43;
	mov.f64 	%fd45, 0d3FC99999999840D2;
	fma.rn.f64 	%fd46, %fd44, %fd11, %fd45;
	mov.f64 	%fd47, 0dBFD555555555544C;
	fma.rn.f64 	%fd48, %fd46, %fd11, %fd47;
	mul.f64 	%fd49, %fd11, %fd48;
	fma.rn.f64 	%fd50, %fd49, %fd55, %fd55;
	mov.f64 	%fd51, 0d3FF921FB54442D18;
	sub.f64 	%fd52, %fd51, %fd50;
	setp.gt.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd53, %fd52, %fd50, %p4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r13, %r15;
	mov.b64 	%fd54, {%r12, %r16};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd54;

BB38_4:
	ret;
}

	// .globl	vec_atanh
.visible .entry vec_atanh(
	.param .u32 vec_atanh_param_0,
	.param .u64 vec_atanh_param_1,
	.param .u64 vec_atanh_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<45>;
	.reg .f64 	%fd<91>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r12, [vec_atanh_param_0];
	ld.param.u64 	%rd1, [vec_atanh_param_1];
	ld.param.u64 	%rd2, [vec_atanh_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB39_11;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd13, %fd1;
	add.f64 	%fd14, %fd13, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	sub.f64 	%fd16, %fd15, %fd13;
	div.rn.f64 	%fd2, %fd14, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd2;
	}
	setp.lt.u32	%p2, %r22, 1071994197;
	setp.lt.s32	%p3, %r22, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB39_9;
	bra.uni 	BB39_2;

BB39_9:
	add.f64 	%fd63, %fd2, 0d4000000000000000;
	div.rn.f64 	%fd64, %fd2, %fd63;
	mul.f64 	%fd65, %fd2, %fd64;
	neg.f64 	%fd66, %fd65;
	sub.f64 	%fd67, %fd2, %fd65;
	mul.f64 	%fd68, %fd67, %fd67;
	mov.f64 	%fd69, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd70, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	mov.f64 	%fd72, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd73, %fd71, %fd68, %fd72;
	mov.f64 	%fd74, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd75, %fd73, %fd68, %fd74;
	mov.f64 	%fd76, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd77, %fd75, %fd68, %fd76;
	mov.f64 	%fd78, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd79, %fd77, %fd68, %fd78;
	mov.f64 	%fd80, 0d3F899999999D70C4;
	fma.rn.f64 	%fd81, %fd79, %fd68, %fd80;
	mov.f64 	%fd82, 0d3FB5555555555462;
	fma.rn.f64 	%fd83, %fd81, %fd68, %fd82;
	mul.f64 	%fd84, %fd68, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd67, %fd66;
	add.f64 	%fd90, %fd2, %fd85;
	bra.uni 	BB39_10;

BB39_2:
	add.f64 	%fd88, %fd2, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd88;
	}
	mov.u32 	%r43, -1023;
	setp.gt.s32	%p5, %r41, 1048575;
	@%p5 bra 	BB39_4;

	mul.f64 	%fd88, %fd88, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd88;
	}
	mov.u32 	%r43, -1077;

BB39_4:
	add.s32 	%r25, %r41, -1;
	setp.lt.u32	%p6, %r25, 2146435071;
	@%p6 bra 	BB39_6;
	bra.uni 	BB39_5;

BB39_6:
	shr.u32 	%r27, %r41, 20;
	add.s32 	%r44, %r43, %r27;
	and.b32  	%r28, %r41, -2146435073;
	or.b32  	%r29, %r28, 1072693248;
	mov.b64 	%fd89, {%r42, %r29};
	setp.lt.s32	%p8, %r29, 1073127583;
	@%p8 bra 	BB39_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd89;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd89;
	}
	add.s32 	%r32, %r31, -1048576;
	mov.b64 	%fd89, {%r30, %r32};
	add.s32 	%r44, %r44, 1;

BB39_8:
	add.f64 	%fd19, %fd89, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd20, %fd19;
	neg.f64 	%fd21, %fd19;
	fma.rn.f64 	%fd23, %fd21, %fd20, %fd15;
	fma.rn.f64 	%fd24, %fd23, %fd23, %fd23;
	fma.rn.f64 	%fd25, %fd24, %fd20, %fd20;
	add.f64 	%fd26, %fd89, 0dBFF0000000000000;
	mul.f64 	%fd27, %fd26, %fd25;
	fma.rn.f64 	%fd28, %fd26, %fd25, %fd27;
	mul.f64 	%fd29, %fd28, %fd28;
	mov.f64 	%fd30, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd31, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd32, %fd31, %fd29, %fd30;
	mov.f64 	%fd33, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd34, %fd32, %fd29, %fd33;
	mov.f64 	%fd35, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd36, %fd34, %fd29, %fd35;
	mov.f64 	%fd37, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd38, %fd36, %fd29, %fd37;
	mov.f64 	%fd39, 0d3F624924923BE72D;
	fma.rn.f64 	%fd40, %fd38, %fd29, %fd39;
	mov.f64 	%fd41, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd42, %fd40, %fd29, %fd41;
	mov.f64 	%fd43, 0d3FB5555555555554;
	fma.rn.f64 	%fd44, %fd42, %fd29, %fd43;
	sub.f64 	%fd45, %fd26, %fd28;
	add.f64 	%fd46, %fd45, %fd45;
	neg.f64 	%fd47, %fd28;
	fma.rn.f64 	%fd48, %fd47, %fd26, %fd46;
	mul.f64 	%fd49, %fd25, %fd48;
	mul.f64 	%fd50, %fd29, %fd44;
	fma.rn.f64 	%fd51, %fd50, %fd28, %fd49;
	xor.b32  	%r33, %r44, -2147483648;
	mov.u32 	%r34, -2147483648;
	mov.u32 	%r35, 1127219200;
	mov.b64 	%fd52, {%r33, %r35};
	mov.b64 	%fd53, {%r34, %r35};
	sub.f64 	%fd54, %fd52, %fd53;
	mov.f64 	%fd55, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd56, %fd54, %fd55, %fd28;
	neg.f64 	%fd57, %fd54;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	sub.f64 	%fd59, %fd58, %fd28;
	sub.f64 	%fd60, %fd51, %fd59;
	mov.f64 	%fd61, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd62, %fd54, %fd61, %fd60;
	add.f64 	%fd90, %fd56, %fd62;
	bra.uni 	BB39_10;

BB39_5:
	mov.f64 	%fd17, 0d7FF0000000000000;
	fma.rn.f64 	%fd18, %fd88, %fd17, %fd17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd88;
	}
	mov.b32 	 %f1, %r26;
	setp.eq.f32	%p7, %f1, 0f00000000;
	selp.f64	%fd90, 0dFFF0000000000000, %fd18, %p7;

BB39_10:
	cvta.to.global.u64 	%rd6, %rd1;
	mul.f64 	%fd86, %fd90, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd1;
	}
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r37, %r39;
	mov.b64 	%fd87, {%r36, %r40};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd87;

BB39_11:
	ret;
}

	// .globl	vec_cbrt
.visible .entry vec_cbrt(
	.param .u32 vec_cbrt_param_0,
	.param .u64 vec_cbrt_param_1,
	.param .u64 vec_cbrt_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r13, [vec_cbrt_param_0];
	ld.param.u64 	%rd1, [vec_cbrt_param_1];
	ld.param.u64 	%rd2, [vec_cbrt_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB40_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r41, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r41, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB40_3;
	bra.uni 	BB40_2;

BB40_3:
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 0;
	setp.ne.s32	%p5, %r42, 0;
	@%p5 bra 	BB40_5;

	mov.b64 	%fd5, {%r40, %r41};
	mul.f64 	%fd6, %fd5, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd6;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd6;
	}
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 18;

BB40_5:
	add.s32 	%r25, %r42, -1022;
	cvt.rn.f32.s32	%f1, %r25;
	mul.f32 	%f2, %f1, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r26, %f2;
	mad.lo.s32 	%r27, %r26, -3145728, %r41;
	mov.b64 	%fd7, {%r40, %r27};
	cvt.rn.f32.f64	%f3, %fd7;
	lg2.approx.ftz.f32 	%f4, %f3;
	mul.f32 	%f5, %f4, 0f3EAAAAAB;
	ex2.approx.ftz.f32 	%f6, %f5;
	cvt.f64.f32	%fd8, %f6;
	mul.f64 	%fd9, %fd8, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd9;
	}
	add.s32 	%r30, %r29, 1048576;
	mov.b64 	%fd10, {%r28, %r30};
	fma.rn.f64 	%fd11, %fd10, %fd8, %fd7;
	rcp.approx.ftz.f64 	%fd12, %fd11;
	neg.f64 	%fd13, %fd11;
	mov.f64 	%fd14, 0d3FF0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd12, %fd14;
	fma.rn.f64 	%fd16, %fd15, %fd15, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd12, %fd12;
	neg.f64 	%fd18, %fd8;
	fma.rn.f64 	%fd19, %fd9, %fd18, %fd7;
	mul.f64 	%fd20, %fd17, %fd19;
	fma.rn.f64 	%fd21, %fd8, %fd20, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd21;
	}
	sub.s32 	%r33, %r26, %r43;
	shl.b32 	%r34, %r33, 20;
	add.s32 	%r35, %r32, %r34;
	mov.b64 	%fd22, {%r31, %r35};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd22;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd22;
	}
	and.b32  	%r38, %r3, -2147483648;
	or.b32  	%r39, %r37, %r38;
	mov.b64 	%fd23, {%r36, %r39};
	bra.uni 	BB40_6;

BB40_2:
	add.f64 	%fd23, %fd1, %fd1;

BB40_6:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd23;

BB40_7:
	ret;
}

	// .globl	vec_ceil
.visible .entry vec_ceil(
	.param .u32 vec_ceil_param_0,
	.param .u64 vec_ceil_param_1,
	.param .u64 vec_ceil_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_ceil_param_0];
	ld.param.u64 	%rd1, [vec_ceil_param_1];
	ld.param.u64 	%rd2, [vec_ceil_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB41_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rpi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB41_2:
	ret;
}

	// .globl	vec_cos
.visible .entry vec_cos(
	.param .u32 vec_cos_param_0,
	.param .u64 vec_cos_param_1,
	.param .u64 vec_cos_param_2
)
{
	.local .align 4 .b8 	__local_depot42[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<26>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<15>;


	mov.u64 	%SPL, __local_depot42;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r6, [vec_cos_param_0];
	ld.param.u64 	%rd2, [vec_cos_param_1];
	ld.param.u64 	%rd3, [vec_cos_param_2];
	add.u64 	%rd4, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r15;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB42_11;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd38, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd38;
	}
	and.b32  	%r17, %r16, 2147483647;
	setp.ne.s32	%p2, %r17, 2146435072;
	@%p2 bra 	BB42_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd38;
	}
	setp.ne.s32	%p3, %r18, 0;
	@%p3 bra 	BB42_4;

	mov.f64 	%fd14, 0d0000000000000000;
	mul.rn.f64 	%fd38, %fd38, %fd14;

BB42_4:
	mul.f64 	%fd15, %fd38, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r25, %fd15;
	st.local.u32 	[%rd1], %r25;
	cvt.rn.f64.s32	%fd16, %r25;
	neg.f64 	%fd17, %fd16;
	mov.f64 	%fd18, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd19, %fd17, %fd18, %fd38;
	mov.f64 	%fd20, 0d3C91A62633145C00;
	fma.rn.f64 	%fd21, %fd17, %fd20, %fd19;
	mov.f64 	%fd22, 0d397B839A252049C0;
	fma.rn.f64 	%fd39, %fd17, %fd22, %fd21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd38;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p4, %r20, 1105199104;
	@%p4 bra 	BB42_6;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd38;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd39, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r25, [%rd1];

BB42_6:
	add.s32 	%r5, %r25, 1;
	and.b32  	%r21, %r5, 1;
	shl.b32 	%r22, %r21, 3;
	setp.eq.s32	%p5, %r21, 0;
	selp.f64	%fd23, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r23, %r22, 1;
	mul.wide.s32 	%rd9, %r23, 8;
	mov.u64 	%rd10, __cudart_sin_cos_coeffs;
	add.s64 	%rd11, %rd10, %rd9;
	ld.const.f64 	%fd24, [%rd11];
	mul.rn.f64 	%fd7, %fd39, %fd39;
	fma.rn.f64 	%fd25, %fd23, %fd7, %fd24;
	ld.const.f64 	%fd26, [%rd11+8];
	fma.rn.f64 	%fd27, %fd25, %fd7, %fd26;
	ld.const.f64 	%fd28, [%rd11+16];
	fma.rn.f64 	%fd29, %fd27, %fd7, %fd28;
	ld.const.f64 	%fd30, [%rd11+24];
	fma.rn.f64 	%fd31, %fd29, %fd7, %fd30;
	ld.const.f64 	%fd32, [%rd11+32];
	fma.rn.f64 	%fd33, %fd31, %fd7, %fd32;
	ld.const.f64 	%fd34, [%rd11+40];
	fma.rn.f64 	%fd8, %fd33, %fd7, %fd34;
	fma.rn.f64 	%fd40, %fd8, %fd39, %fd39;
	@%p5 bra 	BB42_8;

	mov.f64 	%fd35, 0d3FF0000000000000;
	fma.rn.f64 	%fd40, %fd8, %fd7, %fd35;

BB42_8:
	and.b32  	%r24, %r5, 2;
	setp.eq.s32	%p6, %r24, 0;
	@%p6 bra 	BB42_10;

	mov.f64 	%fd36, 0d0000000000000000;
	mov.f64 	%fd37, 0dBFF0000000000000;
	fma.rn.f64 	%fd40, %fd40, %fd37, %fd36;

BB42_10:
	cvta.to.global.u64 	%rd12, %rd2;
	add.s64 	%rd14, %rd12, %rd6;
	st.global.f64 	[%rd14], %fd40;

BB42_11:
	ret;
}

	// .globl	vec_cosh
.visible .entry vec_cosh(
	.param .u32 vec_cosh_param_0,
	.param .u64 vec_cosh_param_1,
	.param .u64 vec_cosh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<46>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_cosh_param_0];
	ld.param.u64 	%rd1, [vec_cosh_param_1];
	ld.param.u64 	%rd2, [vec_cosh_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB43_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	and.b32  	%r13, %r12, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r14, %r13};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.u32	%p2, %r15, 1082536911;
	@%p2 bra 	BB43_3;
	bra.uni 	BB43_2;

BB43_3:
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd8, %fd2, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd8;
	}
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd2;
	mov.f64 	%fd13, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	mov.f64 	%fd15, 0d3E928AF3FCA213EA;
	mov.f64 	%fd16, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mov.f64 	%fd26, 0d3F81111111122322;
	fma.rn.f64 	%fd27, %fd25, %fd14, %fd26;
	mov.f64 	%fd28, 0d3FA55555555502A1;
	fma.rn.f64 	%fd29, %fd27, %fd14, %fd28;
	mov.f64 	%fd30, 0d3FC5555555555511;
	fma.rn.f64 	%fd31, %fd29, %fd14, %fd30;
	mov.f64 	%fd32, 0d3FE000000000000B;
	fma.rn.f64 	%fd33, %fd31, %fd14, %fd32;
	mov.f64 	%fd34, 0d3FF0000000000000;
	fma.rn.f64 	%fd35, %fd33, %fd14, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd14, %fd34;
	shl.b32 	%r17, %r16, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd36;
	}
	add.s32 	%r20, %r17, %r19;
	add.s32 	%r21, %r20, -2097152;
	mov.b64 	%fd37, {%r18, %r21};
	rcp.approx.ftz.f64 	%fd38, %fd37;
	neg.f64 	%fd39, %fd37;
	fma.rn.f64 	%fd40, %fd39, %fd38, %fd34;
	fma.rn.f64 	%fd41, %fd40, %fd40, %fd40;
	fma.rn.f64 	%fd42, %fd41, %fd38, %fd38;
	mov.f64 	%fd43, 0d3FB0000000000000;
	fma.rn.f64 	%fd45, %fd42, %fd43, %fd37;
	bra.uni 	BB43_4;

BB43_2:
	setp.gtu.f64	%p3, %fd1, 0d7FF0000000000000;
	selp.f64	%fd45, %fd1, 0d7FF0000000000000, %p3;

BB43_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	add.f64 	%fd44, %fd45, %fd45;
	st.global.f64 	[%rd8], %fd44;

BB43_5:
	ret;
}

	// .globl	vec_cospi
.visible .entry vec_cospi(
	.param .u32 vec_cospi_param_0,
	.param .u64 vec_cospi_param_1,
	.param .u64 vec_cospi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r3, [vec_cospi_param_0];
	ld.param.u64 	%rd1, [vec_cospi_param_1];
	ld.param.u64 	%rd2, [vec_cospi_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB44_8;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd35, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd35;
	}
	add.s32 	%r14, %r13, %r13;
	setp.lt.u32	%p2, %r14, -2038431743;
	@%p2 bra 	BB44_3;

	mov.f64 	%fd11, 0d0000000000000000;
	mul.rn.f64 	%fd35, %fd35, %fd11;

BB44_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd35;
	}
	add.s32 	%r16, %r15, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd35;
	}
	mov.b64 	%fd12, {%r17, %r16};
	cvt.rni.f64.f64	%fd13, %fd12;
	cvt.rzi.s64.f64	%rd6, %fd13;
	cvt.u32.u64	%r18, %rd6;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FE0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd35;
	mul.f64 	%fd17, %fd16, 0d3CA1A62633145C07;
	mov.f64 	%fd18, 0d400921FB54442D18;
	fma.rn.f64 	%fd19, %fd16, %fd18, %fd17;
	add.s32 	%r2, %r18, 1;
	and.b32  	%r19, %r2, 1;
	shl.b32 	%r20, %r19, 3;
	mul.rn.f64 	%fd4, %fd19, %fd19;
	setp.eq.s32	%p3, %r19, 0;
	selp.f64	%fd20, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p3;
	add.s32 	%r21, %r20, 1;
	mul.wide.s32 	%rd7, %r21, 8;
	mov.u64 	%rd8, __cudart_sin_cos_coeffs;
	add.s64 	%rd9, %rd8, %rd7;
	ld.const.f64 	%fd21, [%rd9];
	fma.rn.f64 	%fd22, %fd20, %fd4, %fd21;
	ld.const.f64 	%fd23, [%rd9+8];
	fma.rn.f64 	%fd24, %fd22, %fd4, %fd23;
	ld.const.f64 	%fd25, [%rd9+16];
	fma.rn.f64 	%fd26, %fd24, %fd4, %fd25;
	ld.const.f64 	%fd27, [%rd9+24];
	fma.rn.f64 	%fd28, %fd26, %fd4, %fd27;
	ld.const.f64 	%fd29, [%rd9+32];
	fma.rn.f64 	%fd30, %fd28, %fd4, %fd29;
	ld.const.f64 	%fd31, [%rd9+40];
	fma.rn.f64 	%fd5, %fd30, %fd4, %fd31;
	fma.rn.f64 	%fd36, %fd5, %fd19, %fd19;
	@%p3 bra 	BB44_5;

	mov.f64 	%fd32, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd5, %fd4, %fd32;

BB44_5:
	and.b32  	%r22, %r2, 2;
	setp.eq.s32	%p4, %r22, 0;
	@%p4 bra 	BB44_7;

	mov.f64 	%fd33, 0d0000000000000000;
	mov.f64 	%fd34, 0dBFF0000000000000;
	fma.rn.f64 	%fd36, %fd36, %fd34, %fd33;

BB44_7:
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd12, %rd10, %rd4;
	st.global.f64 	[%rd12], %fd36;

BB44_8:
	ret;
}

	// .globl	vec_erfc
.visible .entry vec_erfc(
	.param .u32 vec_erfc_param_0,
	.param .u64 vec_erfc_param_1,
	.param .u64 vec_erfc_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<123>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r5, [vec_erfc_param_0];
	ld.param.u64 	%rd2, [vec_erfc_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB45_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 2146435072;
	@%p2 bra 	BB45_3;
	bra.uni 	BB45_2;

BB45_3:
	setp.lt.s32	%p7, %r2, 0;
	mov.u32 	%r15, 0;
	mov.b64 	%fd7, {%r4, %r3};
	add.f64 	%fd8, %fd7, 0dC010000000000000;
	add.f64 	%fd9, %fd7, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd10, %fd9;
	neg.f64 	%fd11, %fd9;
	mov.f64 	%fd12, 0d3FF0000000000000;
	fma.rn.f64 	%fd13, %fd11, %fd10, %fd12;
	fma.rn.f64 	%fd14, %fd13, %fd13, %fd13;
	fma.rn.f64 	%fd15, %fd14, %fd10, %fd10;
	mul.f64 	%fd16, %fd8, %fd15;
	add.rn.f64 	%fd17, %fd16, %fd12;
	mov.f64 	%fd18, 0dC010000000000000;
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd7;
	neg.f64 	%fd20, %fd16;
	fma.rn.f64 	%fd21, %fd20, %fd7, %fd19;
	fma.rn.f64 	%fd22, %fd15, %fd21, %fd16;
	mov.f64 	%fd23, 0dBE44E1C6FD03D328;
	mov.f64 	%fd24, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	mov.f64 	%fd38, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd39, %fd37, %fd22, %fd38;
	mov.f64 	%fd40, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd41, %fd39, %fd22, %fd40;
	mov.f64 	%fd42, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd43, %fd41, %fd22, %fd42;
	mov.f64 	%fd44, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd45, %fd43, %fd22, %fd44;
	mov.f64 	%fd46, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd47, %fd45, %fd22, %fd46;
	mov.f64 	%fd48, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd49, %fd47, %fd22, %fd48;
	mov.f64 	%fd50, 0dBF9096238568E357;
	fma.rn.f64 	%fd51, %fd49, %fd22, %fd50;
	mov.f64 	%fd52, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd53, %fd51, %fd22, %fd52;
	mov.f64 	%fd54, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd55, %fd53, %fd22, %fd54;
	mov.f64 	%fd56, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd57, %fd55, %fd22, %fd56;
	mov.f64 	%fd58, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd59, %fd57, %fd22, %fd58;
	mov.f64 	%fd60, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd61, %fd59, %fd22, %fd60;
	mov.f64 	%fd62, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd63, %fd61, %fd22, %fd62;
	mov.f64 	%fd64, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd65, %fd63, %fd22, %fd64;
	mov.f64 	%fd66, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd67, %fd65, %fd22, %fd66;
	mov.f64 	%fd68, 0d4000000000000000;
	fma.rn.f64 	%fd69, %fd68, %fd7, %fd12;
	rcp.approx.ftz.f64 	%fd70, %fd69;
	neg.f64 	%fd71, %fd69;
	fma.rn.f64 	%fd72, %fd71, %fd70, %fd12;
	fma.rn.f64 	%fd73, %fd72, %fd72, %fd72;
	fma.rn.f64 	%fd74, %fd73, %fd70, %fd70;
	mul.f64 	%fd75, %fd67, %fd74;
	mul.f64 	%fd76, %fd75, 0dC000000000000000;
	fma.rn.f64 	%fd77, %fd7, %fd76, %fd67;
	neg.f64 	%fd78, %fd75;
	add.rn.f64 	%fd79, %fd77, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd74, %fd75;
	mul.f64 	%fd81, %fd7, %fd7;
	neg.f64 	%fd82, %fd81;
	mov.f64 	%fd83, 0d4338000000000000;
	mov.f64 	%fd84, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd85, %fd82, %fd84, %fd83;
	mov.f64 	%fd86, 0dC338000000000000;
	add.rn.f64 	%fd87, %fd85, %fd86;
	mov.f64 	%fd88, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd89, %fd87, %fd88, %fd82;
	mov.f64 	%fd90, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd91, %fd87, %fd90, %fd89;
	mov.f64 	%fd92, 0d3E928AF3FCA213EA;
	mov.f64 	%fd93, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0d3F81111111122322;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3FA55555555502A1;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	mov.f64 	%fd107, 0d3FC5555555555511;
	fma.rn.f64 	%fd108, %fd106, %fd91, %fd107;
	mov.f64 	%fd109, 0d3FE000000000000B;
	fma.rn.f64 	%fd110, %fd108, %fd91, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd91, %fd12;
	fma.rn.f64 	%fd112, %fd111, %fd91, %fd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd85;
	}
	shr.u32 	%r17, %r16, 31;
	add.s32 	%r18, %r16, %r17;
	shr.s32 	%r19, %r18, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd112;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd112;
	}
	shl.b32 	%r22, %r19, 20;
	add.s32 	%r23, %r21, %r22;
	mov.b64 	%fd113, {%r20, %r23};
	sub.s32 	%r24, %r16, %r19;
	shl.b32 	%r25, %r24, 20;
	add.s32 	%r26, %r25, 1072693248;
	mov.b64 	%fd114, {%r15, %r26};
	mul.f64 	%fd115, %fd113, %fd114;
	neg.f64 	%fd116, %fd7;
	fma.rn.f64 	%fd117, %fd116, %fd7, %fd81;
	fma.rn.f64 	%fd118, %fd115, %fd117, %fd115;
	mul.f64 	%fd119, %fd80, %fd118;
	setp.gt.u32	%p8, %r3, 1077624832;
	selp.f64	%fd120, 0d0000000000000000, %fd119, %p8;
	sub.f64 	%fd121, %fd68, %fd120;
	selp.f64	%fd122, %fd121, %fd120, %p7;
	bra.uni 	BB45_4;

BB45_2:
	setp.lt.s32	%p3, %r2, 0;
	setp.eq.s32	%p4, %r4, 0;
	setp.eq.s32	%p5, %r3, 2146435072;
	and.pred  	%p6, %p5, %p4;
	selp.f64	%fd5, 0d4000000000000000, 0d0000000000000000, %p3;
	add.f64 	%fd6, %fd1, %fd1;
	selp.f64	%fd122, %fd5, %fd6, %p6;

BB45_4:
	ld.param.u64 	%rd9, [vec_erfc_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd122;

BB45_5:
	ret;
}

	// .globl	vec_erfcinv
.visible .entry vec_erfcinv(
	.param .u32 vec_erfcinv_param_0,
	.param .u64 vec_erfcinv_param_1,
	.param .u64 vec_erfcinv_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<66>;
	.reg .f64 	%fd<268>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r13, [vec_erfcinv_param_0];
	ld.param.u64 	%rd2, [vec_erfcinv_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB46_15;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd19, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd19, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB46_13;
	bra.uni 	BB46_2;

BB46_13:
	mul.rn.f64 	%fd172, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd172;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd172;
	}
	shr.u32 	%r45, %r43, 20;
	and.b32  	%r46, %r45, 2046;
	add.s32 	%r47, %r46, 2147482626;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd173, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd174, {%r49, %r48};
	sub.f64 	%fd175, %fd173, %fd174;
	and.b32  	%r50, %r43, -2145386497;
	add.s32 	%r51, %r50, 1071644672;
	mov.b64 	%fd176, {%r44, %r51};
	add.f64 	%fd177, %fd176, 0dBFF0000000000000;
	add.f64 	%fd178, %fd176, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd179, %fd178;
	neg.f64 	%fd180, %fd178;
	mov.f64 	%fd181, 0d3FF0000000000000;
	fma.rn.f64 	%fd182, %fd180, %fd179, %fd181;
	fma.rn.f64 	%fd183, %fd182, %fd182, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd179, %fd179;
	mul.f64 	%fd185, %fd177, %fd184;
	mov.f64 	%fd186, 0dC000000000000000;
	fma.rn.f64 	%fd187, %fd186, %fd185, %fd177;
	neg.f64 	%fd188, %fd185;
	fma.rn.f64 	%fd189, %fd188, %fd177, %fd187;
	fma.rn.f64 	%fd190, %fd189, %fd184, %fd185;
	mul.f64 	%fd191, %fd190, %fd190;
	mov.f64 	%fd192, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd193, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd194, %fd193, %fd191, %fd192;
	mov.f64 	%fd195, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd196, %fd194, %fd191, %fd195;
	mov.f64 	%fd197, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd198, %fd196, %fd191, %fd197;
	mov.f64 	%fd199, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd200, %fd198, %fd191, %fd199;
	mov.f64 	%fd201, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd202, %fd200, %fd191, %fd201;
	mov.f64 	%fd203, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd204, %fd202, %fd191, %fd203;
	mov.f64 	%fd205, 0d3FC249249209112E;
	fma.rn.f64 	%fd206, %fd204, %fd191, %fd205;
	mov.f64 	%fd207, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd208, %fd206, %fd191, %fd207;
	mov.f64 	%fd209, 0d3FD5555555555535;
	fma.rn.f64 	%fd210, %fd208, %fd191, %fd209;
	mul.f64 	%fd211, %fd191, %fd210;
	fma.rn.f64 	%fd212, %fd211, %fd190, %fd190;
	add.f64 	%fd213, %fd212, %fd212;
	mov.f64 	%fd214, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd215, %fd175, %fd214, %fd213;
	mov.f64 	%fd216, 0dC009000000000000;
	sub.f64 	%fd217, %fd216, %fd215;
	mov.f64 	%fd218, 0dBC08DDF93324D327;
	mov.f64 	%fd219, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd220, %fd219, %fd217, %fd218;
	mov.f64 	%fd221, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd222, %fd220, %fd217, %fd221;
	mov.f64 	%fd223, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd224, %fd222, %fd217, %fd223;
	mov.f64 	%fd225, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd226, %fd224, %fd217, %fd225;
	mov.f64 	%fd227, 0d3C782E11898132E0;
	fma.rn.f64 	%fd228, %fd226, %fd217, %fd227;
	mov.f64 	%fd229, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd230, %fd228, %fd217, %fd229;
	mov.f64 	%fd231, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd232, %fd230, %fd217, %fd231;
	mov.f64 	%fd233, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd234, %fd232, %fd217, %fd233;
	mov.f64 	%fd235, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd236, %fd234, %fd217, %fd235;
	mov.f64 	%fd237, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd238, %fd236, %fd217, %fd237;
	mov.f64 	%fd239, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd240, %fd238, %fd217, %fd239;
	mov.f64 	%fd241, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd242, %fd240, %fd217, %fd241;
	mov.f64 	%fd243, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd244, %fd242, %fd217, %fd243;
	mov.f64 	%fd245, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd246, %fd244, %fd217, %fd245;
	mov.f64 	%fd247, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd248, %fd246, %fd217, %fd247;
	mov.f64 	%fd249, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd250, %fd248, %fd217, %fd249;
	mov.f64 	%fd251, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd252, %fd250, %fd217, %fd251;
	mov.f64 	%fd253, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd254, %fd252, %fd217, %fd253;
	mov.f64 	%fd255, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd256, %fd254, %fd217, %fd255;
	mov.f64 	%fd257, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd258, %fd256, %fd217, %fd257;
	mov.f64 	%fd259, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd260, %fd258, %fd217, %fd259;
	mov.f64 	%fd261, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd262, %fd260, %fd217, %fd261;
	fma.rn.f64 	%fd267, %fd262, %fd2, %fd262;
	bra.uni 	BB46_14;

BB46_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd263, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd263;
	}
	mov.b32 	 %f1, %r62;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd263;
	}
	@%p6 bra 	BB46_4;
	bra.uni 	BB46_3;

BB46_4:
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p7, %r62, 1048575;
	@%p7 bra 	BB46_6;

	mul.f64 	%fd263, %fd263, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd263;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd263;
	}
	mov.u32 	%r64, -1077;

BB46_6:
	add.s32 	%r32, %r62, -1;
	setp.lt.u32	%p8, %r32, 2146435071;
	@%p8 bra 	BB46_8;
	bra.uni 	BB46_7;

BB46_8:
	shr.u32 	%r34, %r62, 20;
	add.s32 	%r65, %r64, %r34;
	and.b32  	%r35, %r62, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd264, {%r63, %r36};
	setp.lt.s32	%p10, %r36, 1073127583;
	@%p10 bra 	BB46_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd264;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd264;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd264, {%r37, %r39};
	add.s32 	%r65, %r65, 1;

BB46_10:
	add.f64 	%fd106, %fd264, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd107, %fd106;
	neg.f64 	%fd108, %fd106;
	mov.f64 	%fd109, 0d3FF0000000000000;
	fma.rn.f64 	%fd110, %fd108, %fd107, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd110, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd107, %fd107;
	add.f64 	%fd113, %fd264, 0dBFF0000000000000;
	mul.f64 	%fd114, %fd113, %fd112;
	fma.rn.f64 	%fd115, %fd113, %fd112, %fd114;
	mul.f64 	%fd116, %fd115, %fd115;
	mov.f64 	%fd117, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd118, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	mov.f64 	%fd120, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd121, %fd119, %fd116, %fd120;
	mov.f64 	%fd122, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd123, %fd121, %fd116, %fd122;
	mov.f64 	%fd124, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd125, %fd123, %fd116, %fd124;
	mov.f64 	%fd126, 0d3F624924923BE72D;
	fma.rn.f64 	%fd127, %fd125, %fd116, %fd126;
	mov.f64 	%fd128, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd129, %fd127, %fd116, %fd128;
	mov.f64 	%fd130, 0d3FB5555555555554;
	fma.rn.f64 	%fd131, %fd129, %fd116, %fd130;
	sub.f64 	%fd132, %fd113, %fd115;
	add.f64 	%fd133, %fd132, %fd132;
	neg.f64 	%fd134, %fd115;
	fma.rn.f64 	%fd135, %fd134, %fd113, %fd133;
	mul.f64 	%fd136, %fd112, %fd135;
	mul.f64 	%fd137, %fd116, %fd131;
	fma.rn.f64 	%fd138, %fd137, %fd115, %fd136;
	xor.b32  	%r40, %r65, -2147483648;
	mov.u32 	%r41, -2147483648;
	mov.u32 	%r42, 1127219200;
	mov.b64 	%fd139, {%r40, %r42};
	mov.b64 	%fd140, {%r41, %r42};
	sub.f64 	%fd141, %fd139, %fd140;
	mov.f64 	%fd142, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd143, %fd141, %fd142, %fd115;
	neg.f64 	%fd144, %fd141;
	fma.rn.f64 	%fd145, %fd144, %fd142, %fd143;
	sub.f64 	%fd146, %fd145, %fd115;
	sub.f64 	%fd147, %fd138, %fd146;
	mov.f64 	%fd148, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd149, %fd141, %fd148, %fd147;
	add.f64 	%fd265, %fd143, %fd149;
	bra.uni 	BB46_11;

BB46_3:
	shr.u32 	%r23, %r62, 20;
	and.b32  	%r24, %r23, 2046;
	add.s32 	%r25, %r24, 2147482626;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd22, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd23, {%r27, %r26};
	sub.f64 	%fd24, %fd22, %fd23;
	and.b32  	%r28, %r62, -2145386497;
	add.s32 	%r29, %r28, 1071644672;
	mov.b64 	%fd25, {%r63, %r29};
	add.f64 	%fd26, %fd25, 0dBFF0000000000000;
	add.f64 	%fd27, %fd25, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd28, %fd27;
	neg.f64 	%fd29, %fd27;
	mov.f64 	%fd30, 0d3FF0000000000000;
	fma.rn.f64 	%fd31, %fd29, %fd28, %fd30;
	fma.rn.f64 	%fd32, %fd31, %fd31, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd28, %fd28;
	mul.f64 	%fd34, %fd26, %fd33;
	mov.f64 	%fd35, 0dC000000000000000;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd26;
	neg.f64 	%fd37, %fd34;
	fma.rn.f64 	%fd38, %fd37, %fd26, %fd36;
	fma.rn.f64 	%fd39, %fd38, %fd33, %fd34;
	mul.f64 	%fd40, %fd39, %fd39;
	mov.f64 	%fd41, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd42, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd43, %fd42, %fd40, %fd41;
	mov.f64 	%fd44, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd45, %fd43, %fd40, %fd44;
	mov.f64 	%fd46, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd47, %fd45, %fd40, %fd46;
	mov.f64 	%fd48, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd49, %fd47, %fd40, %fd48;
	mov.f64 	%fd50, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd51, %fd49, %fd40, %fd50;
	mov.f64 	%fd52, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd53, %fd51, %fd40, %fd52;
	mov.f64 	%fd54, 0d3FC249249209112E;
	fma.rn.f64 	%fd55, %fd53, %fd40, %fd54;
	mov.f64 	%fd56, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd57, %fd55, %fd40, %fd56;
	mov.f64 	%fd58, 0d3FD5555555555535;
	fma.rn.f64 	%fd59, %fd57, %fd40, %fd58;
	mul.f64 	%fd60, %fd40, %fd59;
	fma.rn.f64 	%fd61, %fd60, %fd39, %fd39;
	add.f64 	%fd62, %fd61, %fd61;
	mov.f64 	%fd63, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd64, %fd24, %fd63, %fd62;
	neg.f64 	%fd21, %fd64;
	// inline asm
	rsqrt.approx.ftz.f64 %fd20, %fd21;
	// inline asm
	mul.rn.f64 	%fd65, %fd20, %fd20;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd21, %fd66, %fd30;
	mov.f64 	%fd68, 0d3FE0000000000000;
	mov.f64 	%fd69, 0d3FD8000000000000;
	fma.rn.f64 	%fd70, %fd69, %fd67, %fd68;
	mul.rn.f64 	%fd71, %fd67, %fd20;
	fma.rn.f64 	%fd72, %fd70, %fd71, %fd20;
	mov.f64 	%fd73, 0d4000A0E7333839AA;
	mov.f64 	%fd74, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd75, %fd74, %fd72, %fd73;
	mov.f64 	%fd76, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd77, %fd75, %fd72, %fd76;
	mov.f64 	%fd78, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd79, %fd77, %fd72, %fd78;
	mov.f64 	%fd80, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd81, %fd79, %fd72, %fd80;
	mov.f64 	%fd82, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd83, %fd81, %fd72, %fd82;
	mov.f64 	%fd84, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd85, %fd83, %fd72, %fd84;
	mov.f64 	%fd86, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd87, %fd85, %fd72, %fd86;
	add.f64 	%fd88, %fd72, 0d4001E90FF51C2197;
	mov.f64 	%fd89, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd90, %fd88, %fd72, %fd89;
	mov.f64 	%fd91, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd92, %fd90, %fd72, %fd91;
	mov.f64 	%fd93, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd94, %fd92, %fd72, %fd93;
	mov.f64 	%fd95, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd96, %fd94, %fd72, %fd95;
	mov.f64 	%fd97, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd98, %fd96, %fd72, %fd97;
	mov.f64 	%fd99, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd100, %fd98, %fd72, %fd99;
	mov.f64 	%fd101, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd102, %fd100, %fd72, %fd101;
	mul.f64 	%fd103, %fd72, %fd102;
	div.rn.f64 	%fd266, %fd87, %fd103;
	bra.uni 	BB46_12;

BB46_7:
	mov.f64 	%fd104, 0d7FF0000000000000;
	fma.rn.f64 	%fd105, %fd263, %fd104, %fd104;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd263;
	}
	mov.b32 	 %f2, %r33;
	setp.eq.f32	%p9, %f2, 0f00000000;
	selp.f64	%fd265, 0dFFF0000000000000, %fd105, %p9;

BB46_11:
	neg.f64 	%fd150, %fd265;
	rsqrt.approx.f64 	%fd151, %fd150;
	mov.f64 	%fd152, 0d3FFA2013964E259C;
	mov.f64 	%fd153, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd154, %fd153, %fd151, %fd152;
	mov.f64 	%fd155, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd156, %fd154, %fd151, %fd155;
	mov.f64 	%fd157, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd158, %fd156, %fd151, %fd157;
	mov.f64 	%fd159, 0d3F311BD487102E94;
	fma.rn.f64 	%fd160, %fd158, %fd151, %fd159;
	add.f64 	%fd161, %fd151, 0d3FF59895C30BAA54;
	mov.f64 	%fd162, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd163, %fd161, %fd151, %fd162;
	mov.f64 	%fd164, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd165, %fd163, %fd151, %fd164;
	mov.f64 	%fd166, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd167, %fd165, %fd151, %fd166;
	mov.f64 	%fd168, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd169, %fd167, %fd151, %fd168;
	mul.f64 	%fd170, %fd151, %fd169;
	div.rn.f64 	%fd266, %fd160, %fd170;

BB46_12:
	neg.f64 	%fd171, %fd266;
	selp.f64	%fd267, %fd171, %fd266, %p5;

BB46_14:
	mov.u32 	%r61, %tid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r58, %ctaid.x;
	mov.u32 	%r57, %nctaid.x;
	mad.lo.s32 	%r56, %r59, %r60, %r61;
	mov.u32 	%r55, %tid.x;
	mov.u32 	%r54, %ntid.x;
	mad.lo.s32 	%r53, %r56, %r57, %r58;
	mad.lo.s32 	%r52, %r53, %r54, %r55;
	mul.wide.s32 	%rd10, %r52, 8;
	ld.param.u64 	%rd9, [vec_erfcinv_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd10;
	st.global.f64 	[%rd8], %fd267;

BB46_15:
	ret;
}

	// .globl	vec_erfcx
.visible .entry vec_erfcx(
	.param .u32 vec_erfcx_param_0,
	.param .u64 vec_erfcx_param_1,
	.param .u64 vec_erfcx_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<141>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [vec_erfcx_param_0];
	ld.param.u64 	%rd2, [vec_erfcx_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB47_10;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f2, %r2;
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p2, %f3, 0f40400000;
	@%p2 bra 	BB47_3;
	bra.uni 	BB47_2;

BB47_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd1;
	}
	and.b32  	%r17, %r2, 2147483647;
	mov.b64 	%fd27, {%r16, %r17};
	add.f64 	%fd28, %fd27, 0dC010000000000000;
	add.f64 	%fd29, %fd27, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd30, %fd29;
	neg.f64 	%fd31, %fd29;
	mov.f64 	%fd32, 0d3FF0000000000000;
	fma.rn.f64 	%fd33, %fd31, %fd30, %fd32;
	fma.rn.f64 	%fd34, %fd33, %fd33, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd30, %fd30;
	mul.f64 	%fd36, %fd28, %fd35;
	add.rn.f64 	%fd37, %fd36, %fd32;
	mov.f64 	%fd38, 0dC010000000000000;
	fma.rn.f64 	%fd39, %fd38, %fd37, %fd27;
	neg.f64 	%fd40, %fd36;
	fma.rn.f64 	%fd41, %fd40, %fd27, %fd39;
	fma.rn.f64 	%fd42, %fd35, %fd41, %fd36;
	mov.f64 	%fd43, 0dBE44E1C6FD03D328;
	mov.f64 	%fd44, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd45, %fd44, %fd42, %fd43;
	mov.f64 	%fd46, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd47, %fd45, %fd42, %fd46;
	mov.f64 	%fd48, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd49, %fd47, %fd42, %fd48;
	mov.f64 	%fd50, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd51, %fd49, %fd42, %fd50;
	mov.f64 	%fd52, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd53, %fd51, %fd42, %fd52;
	mov.f64 	%fd54, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd55, %fd53, %fd42, %fd54;
	mov.f64 	%fd56, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd57, %fd55, %fd42, %fd56;
	mov.f64 	%fd58, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd59, %fd57, %fd42, %fd58;
	mov.f64 	%fd60, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd61, %fd59, %fd42, %fd60;
	mov.f64 	%fd62, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd63, %fd61, %fd42, %fd62;
	mov.f64 	%fd64, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd65, %fd63, %fd42, %fd64;
	mov.f64 	%fd66, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd67, %fd65, %fd42, %fd66;
	mov.f64 	%fd68, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd69, %fd67, %fd42, %fd68;
	mov.f64 	%fd70, 0dBF9096238568E357;
	fma.rn.f64 	%fd71, %fd69, %fd42, %fd70;
	mov.f64 	%fd72, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd73, %fd71, %fd42, %fd72;
	mov.f64 	%fd74, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd75, %fd73, %fd42, %fd74;
	mov.f64 	%fd76, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd77, %fd75, %fd42, %fd76;
	mov.f64 	%fd78, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd79, %fd77, %fd42, %fd78;
	mov.f64 	%fd80, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd81, %fd79, %fd42, %fd80;
	mov.f64 	%fd82, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd83, %fd81, %fd42, %fd82;
	mov.f64 	%fd84, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd85, %fd83, %fd42, %fd84;
	mov.f64 	%fd86, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd87, %fd85, %fd42, %fd86;
	mov.f64 	%fd88, 0d4000000000000000;
	fma.rn.f64 	%fd89, %fd88, %fd27, %fd32;
	rcp.approx.ftz.f64 	%fd90, %fd89;
	neg.f64 	%fd91, %fd89;
	fma.rn.f64 	%fd92, %fd91, %fd90, %fd32;
	fma.rn.f64 	%fd93, %fd92, %fd92, %fd92;
	fma.rn.f64 	%fd94, %fd93, %fd90, %fd90;
	mul.f64 	%fd95, %fd87, %fd94;
	mul.f64 	%fd96, %fd95, 0dC000000000000000;
	fma.rn.f64 	%fd97, %fd27, %fd96, %fd87;
	neg.f64 	%fd98, %fd95;
	add.rn.f64 	%fd99, %fd97, %fd98;
	fma.rn.f64 	%fd138, %fd99, %fd94, %fd95;
	bra.uni 	BB47_4;

BB47_2:
	rcp.rn.f64 	%fd13, %fd1;
	mul.f64 	%fd14, %fd13, %fd13;
	mov.f64 	%fd15, 0d401A400000000000;
	mov.f64 	%fd16, 0dC03D880000000000;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0dBFFE000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3FE8000000000000;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0dBFE0000000000000;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mul.f64 	%fd26, %fd13, 0d3FE20DD750429B6D;
	mul.f64 	%fd138, %fd26, %fd25;

BB47_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd1;
	}
	setp.gt.s32	%p3, %r30, -1;
	@%p3 bra 	BB47_9;

	mul.f64 	%fd5, %fd1, %fd1;
	neg.f64 	%fd100, %fd5;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd100;
	mov.f64 	%fd101, 0d4338000000000000;
	mov.f64 	%fd102, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd103, %fd5, %fd102, %fd101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd103;
	}
	mov.f64 	%fd104, 0dC338000000000000;
	add.rn.f64 	%fd105, %fd103, %fd104;
	mov.f64 	%fd106, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd107, %fd105, %fd106, %fd5;
	mov.f64 	%fd108, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd109, %fd105, %fd108, %fd107;
	mov.f64 	%fd110, 0d3E928AF3FCA213EA;
	mov.f64 	%fd111, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd112, %fd111, %fd109, %fd110;
	mov.f64 	%fd113, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd114, %fd112, %fd109, %fd113;
	mov.f64 	%fd115, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd116, %fd114, %fd109, %fd115;
	mov.f64 	%fd117, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd118, %fd116, %fd109, %fd117;
	mov.f64 	%fd119, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd120, %fd118, %fd109, %fd119;
	mov.f64 	%fd121, 0d3F81111111122322;
	fma.rn.f64 	%fd122, %fd120, %fd109, %fd121;
	mov.f64 	%fd123, 0d3FA55555555502A1;
	fma.rn.f64 	%fd124, %fd122, %fd109, %fd123;
	mov.f64 	%fd125, 0d3FC5555555555511;
	fma.rn.f64 	%fd126, %fd124, %fd109, %fd125;
	mov.f64 	%fd127, 0d3FE000000000000B;
	fma.rn.f64 	%fd128, %fd126, %fd109, %fd127;
	mov.f64 	%fd129, 0d3FF0000000000000;
	fma.rn.f64 	%fd130, %fd128, %fd109, %fd129;
	fma.rn.f64 	%fd131, %fd130, %fd109, %fd129;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd131;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd131;
	}
	shl.b32 	%r18, %r3, 20;
	add.s32 	%r19, %r5, %r18;
	mov.b64 	%fd139, {%r4, %r19};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd5;
	}
	mov.b32 	 %f4, %r20;
	abs.f32 	%f1, %f4;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB47_8;

	setp.lt.f64	%p5, %fd5, 0d0000000000000000;
	add.f64 	%fd132, %fd5, 0d7FF0000000000000;
	selp.f64	%fd139, 0d0000000000000000, %fd132, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB47_8;

	shr.u32 	%r21, %r3, 31;
	add.s32 	%r22, %r3, %r21;
	shr.s32 	%r23, %r22, 1;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, %r5;
	mov.b64 	%fd133, {%r4, %r25};
	sub.s32 	%r26, %r3, %r23;
	shl.b32 	%r27, %r26, 20;
	add.s32 	%r28, %r27, 1072693248;
	mov.u32 	%r29, 0;
	mov.b64 	%fd134, {%r29, %r28};
	mul.f64 	%fd139, %fd133, %fd134;

BB47_8:
	add.f64 	%fd135, %fd139, %fd139;
	fma.rn.f64 	%fd136, %fd135, %fd6, %fd135;
	sub.f64 	%fd137, %fd136, %fd138;
	setp.eq.f64	%p7, %fd135, 0d7FF0000000000000;
	selp.f64	%fd138, %fd135, %fd137, %p7;

BB47_9:
	mov.u32 	%r40, %tid.y;
	mov.u32 	%r39, %ctaid.y;
	mov.u32 	%r38, %ntid.y;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r36, %nctaid.x;
	mad.lo.s32 	%r35, %r38, %r39, %r40;
	mov.u32 	%r34, %tid.x;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r32, %r35, %r36, %r37;
	mad.lo.s32 	%r31, %r32, %r33, %r34;
	mul.wide.s32 	%rd10, %r31, 8;
	ld.param.u64 	%rd9, [vec_erfcx_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd10;
	st.global.f64 	[%rd8], %fd138;

BB47_10:
	ret;
}

	// .globl	vec_erf
.visible .entry vec_erf(
	.param .u32 vec_erf_param_0,
	.param .u64 vec_erf_param_1,
	.param .u64 vec_erf_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<111>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r5, [vec_erf_param_0];
	ld.param.u64 	%rd2, [vec_erf_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB48_9;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 1072693248;
	@%p2 bra 	BB48_7;
	bra.uni 	BB48_2;

BB48_7:
	mul.f64 	%fd86, %fd1, %fd1;
	mov.f64 	%fd87, 0d3E4D5F4BB7A316F6;
	mov.f64 	%fd88, 0dBE0A83AA3B08FBC2;
	fma.rn.f64 	%fd89, %fd88, %fd86, %fd87;
	mov.f64 	%fd90, 0dBE85BDCE301B3CDF;
	fma.rn.f64 	%fd91, %fd89, %fd86, %fd90;
	mov.f64 	%fd92, 0d3EBB978FADB81BC9;
	fma.rn.f64 	%fd93, %fd91, %fd86, %fd92;
	mov.f64 	%fd94, 0dBEEF4C99D6AE5FB8;
	fma.rn.f64 	%fd95, %fd93, %fd86, %fd94;
	mov.f64 	%fd96, 0d3F1F9A2AF549012E;
	fma.rn.f64 	%fd97, %fd95, %fd86, %fd96;
	mov.f64 	%fd98, 0dBF4C02DAFC636A47;
	fma.rn.f64 	%fd99, %fd97, %fd86, %fd98;
	mov.f64 	%fd100, 0d3F7565BCCF619AC0;
	fma.rn.f64 	%fd101, %fd99, %fd86, %fd100;
	mov.f64 	%fd102, 0dBF9B82CE311E321A;
	fma.rn.f64 	%fd103, %fd101, %fd86, %fd102;
	mov.f64 	%fd104, 0d3FBCE2F21A04075C;
	fma.rn.f64 	%fd105, %fd103, %fd86, %fd104;
	mov.f64 	%fd106, 0dBFD812746B0379B4;
	fma.rn.f64 	%fd107, %fd105, %fd86, %fd106;
	mov.f64 	%fd108, 0d3FF20DD750429B6D;
	fma.rn.f64 	%fd109, %fd107, %fd86, %fd108;
	mul.f64 	%fd110, %fd1, %fd109;
	bra.uni 	BB48_8;

BB48_2:
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB48_6;
	bra.uni 	BB48_3;

BB48_6:
	mov.b64 	%fd8, {%r4, %r3};
	mov.f64 	%fd9, 0dBCF1384CE38C616A;
	mov.f64 	%fd10, 0d3C8B9C2B870030E8;
	fma.rn.f64 	%fd11, %fd10, %fd8, %fd9;
	mov.f64 	%fd12, 0d3D4458AE9746C2FD;
	fma.rn.f64 	%fd13, %fd11, %fd8, %fd12;
	mov.f64 	%fd14, 0dBD8E4A44D4F1AB56;
	fma.rn.f64 	%fd15, %fd13, %fd8, %fd14;
	mov.f64 	%fd16, 0d3DCFDF15265C58EE;
	fma.rn.f64 	%fd17, %fd15, %fd8, %fd16;
	mov.f64 	%fd18, 0dBE0933832F358D51;
	fma.rn.f64 	%fd19, %fd17, %fd8, %fd18;
	mov.f64 	%fd20, 0d3E3F136D3F719446;
	fma.rn.f64 	%fd21, %fd19, %fd8, %fd20;
	mov.f64 	%fd22, 0dBE6E94C2FE151B3B;
	fma.rn.f64 	%fd23, %fd21, %fd8, %fd22;
	mov.f64 	%fd24, 0d3E985A70310EE0A8;
	fma.rn.f64 	%fd25, %fd23, %fd8, %fd24;
	mov.f64 	%fd26, 0dBEBF944DA1520B74;
	fma.rn.f64 	%fd27, %fd25, %fd8, %fd26;
	mov.f64 	%fd28, 0d3EE09F503825C543;
	fma.rn.f64 	%fd29, %fd27, %fd8, %fd28;
	mov.f64 	%fd30, 0dBEFBEEFE9F949E59;
	fma.rn.f64 	%fd31, %fd29, %fd8, %fd30;
	mov.f64 	%fd32, 0d3F11D785C6E28857;
	fma.rn.f64 	%fd33, %fd31, %fd8, %fd32;
	mov.f64 	%fd34, 0dBF1D866B223048C7;
	fma.rn.f64 	%fd35, %fd33, %fd8, %fd34;
	mov.f64 	%fd36, 0d3EF258F0847E8908;
	fma.rn.f64 	%fd37, %fd35, %fd8, %fd36;
	mov.f64 	%fd38, 0d3F429CFC58DBB776;
	fma.rn.f64 	%fd39, %fd37, %fd8, %fd38;
	mov.f64 	%fd40, 0dBF5BE16D3F71F3C5;
	fma.rn.f64 	%fd41, %fd39, %fd8, %fd40;
	mov.f64 	%fd42, 0d3F2E8BDA60326B1A;
	fma.rn.f64 	%fd43, %fd41, %fd8, %fd42;
	mov.f64 	%fd44, 0d3F938FB20B0988A6;
	fma.rn.f64 	%fd45, %fd43, %fd8, %fd44;
	mov.f64 	%fd46, 0dBFBA4E3A80F64E33;
	fma.rn.f64 	%fd47, %fd45, %fd8, %fd46;
	mov.f64 	%fd48, 0dBFE45F3E88093928;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	mov.f64 	%fd50, 0dBFF20DD599CAEEA0;
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	mov.f64 	%fd52, 0dBE883BE1E31CE133;
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	mov.f64 	%fd54, 0d4338000000000000;
	mov.f64 	%fd55, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd56, %fd53, %fd55, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd56;
	}
	mov.f64 	%fd57, 0dC338000000000000;
	add.rn.f64 	%fd58, %fd56, %fd57;
	mov.f64 	%fd59, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd60, %fd58, %fd59, %fd53;
	mov.f64 	%fd61, 0d3E928AF3FCA213EA;
	mov.f64 	%fd62, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0d3F81111111122322;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0d3FA55555555502A1;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3FC5555555555511;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	mov.f64 	%fd78, 0d3FE000000000000B;
	fma.rn.f64 	%fd79, %fd77, %fd60, %fd78;
	mov.f64 	%fd80, 0d3FF0000000000000;
	fma.rn.f64 	%fd81, %fd79, %fd60, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd60, %fd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd82;
	}
	shl.b32 	%r21, %r19, 20;
	add.s32 	%r22, %r20, %r21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd82;
	}
	mov.b64 	%fd83, {%r23, %r22};
	sub.f64 	%fd84, %fd80, %fd83;
	setp.gt.u32	%p7, %r3, 1075294207;
	selp.f64	%fd85, 0d3FF0000000000000, %fd84, %p7;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd85;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd85;
	}
	and.b32  	%r26, %r2, -2147483648;
	or.b32  	%r27, %r25, %r26;
	mov.b64 	%fd110, {%r24, %r27};
	bra.uni 	BB48_8;

BB48_3:
	setp.eq.s32	%p4, %r3, 2146435072;
	setp.eq.s32	%p5, %r4, 0;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB48_5;
	bra.uni 	BB48_4;

BB48_5:
	mov.f64 	%fd7, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd7;
	}
	and.b32  	%r16, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd7;
	}
	or.b32  	%r18, %r17, %r16;
	mov.b64 	%fd110, {%r15, %r18};
	bra.uni 	BB48_8;

BB48_4:
	add.f64 	%fd110, %fd1, %fd1;

BB48_8:
	mov.u32 	%r37, %tid.y;
	mov.u32 	%r36, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r33, %nctaid.x;
	mad.lo.s32 	%r32, %r35, %r36, %r37;
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mad.lo.s32 	%r29, %r32, %r33, %r34;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	mul.wide.s32 	%rd10, %r28, 8;
	ld.param.u64 	%rd9, [vec_erf_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd10;
	st.global.f64 	[%rd8], %fd110;

BB48_9:
	ret;
}

	// .globl	vec_erfinv
.visible .entry vec_erfinv(
	.param .u32 vec_erfinv_param_0,
	.param .u64 vec_erfinv_param_1,
	.param .u64 vec_erfinv_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<34>;
	.reg .f64 	%fd<173>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_erfinv_param_0];
	ld.param.u64 	%rd2, [vec_erfinv_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB49_10;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	mov.b32 	 %f1, %r12;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FF00000;
	@%p2 bra 	BB49_4;
	bra.uni 	BB49_2;

BB49_4:
	neg.f64 	%fd10, %fd1;
	mov.f64 	%fd11, 0d3FF0000000000000;
	fma.rn.f64 	%fd12, %fd1, %fd10, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd12;
	}
	shr.u32 	%r15, %r13, 20;
	and.b32  	%r16, %r15, 2046;
	add.s32 	%r17, %r16, 2147482626;
	mov.u32 	%r18, 1127219200;
	mov.b64 	%fd13, {%r17, %r18};
	mov.u32 	%r19, -2147483648;
	mov.b64 	%fd14, {%r19, %r18};
	sub.f64 	%fd15, %fd13, %fd14;
	and.b32  	%r20, %r13, -2145386497;
	add.s32 	%r21, %r20, 1071644672;
	mov.b64 	%fd16, {%r14, %r21};
	add.f64 	%fd17, %fd16, 0dBFF0000000000000;
	add.f64 	%fd18, %fd16, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd19, %fd18;
	neg.f64 	%fd20, %fd18;
	fma.rn.f64 	%fd21, %fd20, %fd19, %fd11;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd19, %fd19;
	mul.f64 	%fd24, %fd17, %fd23;
	mov.f64 	%fd25, 0dC000000000000000;
	fma.rn.f64 	%fd26, %fd25, %fd24, %fd17;
	neg.f64 	%fd27, %fd24;
	fma.rn.f64 	%fd28, %fd27, %fd17, %fd26;
	fma.rn.f64 	%fd29, %fd28, %fd23, %fd24;
	mul.f64 	%fd30, %fd29, %fd29;
	mov.f64 	%fd31, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd32, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd33, %fd32, %fd30, %fd31;
	mov.f64 	%fd34, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd35, %fd33, %fd30, %fd34;
	mov.f64 	%fd36, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd37, %fd35, %fd30, %fd36;
	mov.f64 	%fd38, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd39, %fd37, %fd30, %fd38;
	mov.f64 	%fd40, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd41, %fd39, %fd30, %fd40;
	mov.f64 	%fd42, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd43, %fd41, %fd30, %fd42;
	mov.f64 	%fd44, 0d3FC249249209112E;
	fma.rn.f64 	%fd45, %fd43, %fd30, %fd44;
	mov.f64 	%fd46, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd47, %fd45, %fd30, %fd46;
	mov.f64 	%fd48, 0d3FD5555555555535;
	fma.rn.f64 	%fd49, %fd47, %fd30, %fd48;
	mul.f64 	%fd50, %fd30, %fd49;
	fma.rn.f64 	%fd51, %fd50, %fd29, %fd29;
	add.f64 	%fd52, %fd51, %fd51;
	mov.f64 	%fd53, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd4, %fd15, %fd53, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd4;
	}
	setp.lt.u32	%p5, %r22, -1072103424;
	@%p5 bra 	BB49_8;
	bra.uni 	BB49_5;

BB49_8:
	mov.f64 	%fd125, 0dC009000000000000;
	sub.f64 	%fd126, %fd125, %fd4;
	mov.f64 	%fd127, 0dBC08DDF93324D327;
	mov.f64 	%fd128, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd129, %fd128, %fd126, %fd127;
	mov.f64 	%fd130, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd131, %fd129, %fd126, %fd130;
	mov.f64 	%fd132, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd133, %fd131, %fd126, %fd132;
	mov.f64 	%fd134, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd135, %fd133, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C782E11898132E0;
	fma.rn.f64 	%fd137, %fd135, %fd126, %fd136;
	mov.f64 	%fd138, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd139, %fd137, %fd126, %fd138;
	mov.f64 	%fd140, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd141, %fd139, %fd126, %fd140;
	mov.f64 	%fd142, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd143, %fd141, %fd126, %fd142;
	mov.f64 	%fd144, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd145, %fd143, %fd126, %fd144;
	mov.f64 	%fd146, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd147, %fd145, %fd126, %fd146;
	mov.f64 	%fd148, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd149, %fd147, %fd126, %fd148;
	mov.f64 	%fd150, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd151, %fd149, %fd126, %fd150;
	mov.f64 	%fd152, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd153, %fd151, %fd126, %fd152;
	mov.f64 	%fd154, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd155, %fd153, %fd126, %fd154;
	mov.f64 	%fd156, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd157, %fd155, %fd126, %fd156;
	mov.f64 	%fd158, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd159, %fd157, %fd126, %fd158;
	mov.f64 	%fd160, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd161, %fd159, %fd126, %fd160;
	mov.f64 	%fd162, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd163, %fd161, %fd126, %fd162;
	mov.f64 	%fd164, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd165, %fd163, %fd126, %fd164;
	mov.f64 	%fd166, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd167, %fd165, %fd126, %fd166;
	mov.f64 	%fd168, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd169, %fd167, %fd126, %fd168;
	mov.f64 	%fd170, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd172, %fd169, %fd126, %fd170;
	bra.uni 	BB49_9;

BB49_2:
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p3, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd172, %fd1;
	@%p3 bra 	BB49_9;

	setp.eq.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd172, 0d7FF0000000000000, 0dFFF8000000000000, %p4;
	bra.uni 	BB49_9;

BB49_5:
	neg.f64 	%fd54, %fd4;
	sqrt.rn.f64 	%fd5, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd5;
	}
	setp.lt.s32	%p6, %r23, 1074790400;
	@%p6 bra 	BB49_7;
	bra.uni 	BB49_6;

BB49_7:
	add.f64 	%fd88, %fd5, 0dC00A000000000000;
	mov.f64 	%fd89, 0d3E785CBE52878635;
	mov.f64 	%fd90, 0d3E23040F87DBD932;
	fma.rn.f64 	%fd91, %fd90, %fd88, %fd89;
	mov.f64 	%fd92, 0dBE92777453DD3955;
	fma.rn.f64 	%fd93, %fd91, %fd88, %fd92;
	mov.f64 	%fd94, 0d3E5395ABCD554C6C;
	fma.rn.f64 	%fd95, %fd93, %fd88, %fd94;
	mov.f64 	%fd96, 0d3EB936388A3790AD;
	fma.rn.f64 	%fd97, %fd95, %fd88, %fd96;
	mov.f64 	%fd98, 0dBED0D5DB812B5083;
	fma.rn.f64 	%fd99, %fd97, %fd88, %fd98;
	mov.f64 	%fd100, 0d3EC8860CD5D652F6;
	fma.rn.f64 	%fd101, %fd99, %fd88, %fd100;
	mov.f64 	%fd102, 0d3EEA29A0CACDFB23;
	fma.rn.f64 	%fd103, %fd101, %fd88, %fd102;
	mov.f64 	%fd104, 0dBF08CEF1F80281F2;
	fma.rn.f64 	%fd105, %fd103, %fd88, %fd104;
	mov.f64 	%fd106, 0d3F11E684D0B9188A;
	fma.rn.f64 	%fd107, %fd105, %fd88, %fd106;
	mov.f64 	%fd108, 0d3EF932CD54C8A222;
	fma.rn.f64 	%fd109, %fd107, %fd88, %fd108;
	mov.f64 	%fd110, 0dBF37448A89EF8AA3;
	fma.rn.f64 	%fd111, %fd109, %fd88, %fd110;
	mov.f64 	%fd112, 0d3F4F3CC55AD40C25;
	fma.rn.f64 	%fd113, %fd111, %fd88, %fd112;
	mov.f64 	%fd114, 0dBF5BA924132F38B1;
	fma.rn.f64 	%fd115, %fd113, %fd88, %fd114;
	mov.f64 	%fd116, 0d3F6468EECA533CF8;
	fma.rn.f64 	%fd117, %fd115, %fd88, %fd116;
	mov.f64 	%fd118, 0dBF6EBADABB891BBD;
	fma.rn.f64 	%fd119, %fd117, %fd88, %fd118;
	mov.f64 	%fd120, 0d3F75FFCFE5B76AFC;
	fma.rn.f64 	%fd121, %fd119, %fd88, %fd120;
	mov.f64 	%fd122, 0d3FF0158A6D641D39;
	fma.rn.f64 	%fd123, %fd121, %fd88, %fd122;
	mov.f64 	%fd124, 0d4008ABCC380D5A48;
	fma.rn.f64 	%fd172, %fd123, %fd88, %fd124;
	bra.uni 	BB49_9;

BB49_6:
	add.f64 	%fd55, %fd5, 0dC014000000000000;
	mov.f64 	%fd56, 0dBDF18FEEC0E38727;
	mov.f64 	%fd57, 0dBDBDCEC3A7785389;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3E19E6BF2DDA45E3;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBE30468FB24E2F5F;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3E405AC6A8FBA182;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0dBE50102E495FB9C0;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3E5F4C20E1334AF8;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0dBE722D220FDF9C3E;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3E8EBC8BB824CB54;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0dBEB0A8D40EA372CC;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3ED2FBD29D093D2B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0dBEF4A3497E1E0FAC;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3F13EBF4EB00938F;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mov.f64 	%fd81, 0dBF2C2F36A8FC5D53;
	fma.rn.f64 	%fd82, %fd80, %fd55, %fd81;
	mov.f64 	%fd83, 0dBF222EA5DF04047C;
	fma.rn.f64 	%fd84, %fd82, %fd55, %fd83;
	mov.f64 	%fd85, 0d3FF02A30D1FBA0DC;
	fma.rn.f64 	%fd86, %fd84, %fd55, %fd85;
	mov.f64 	%fd87, 0d4013664DDD1AD7FB;
	fma.rn.f64 	%fd172, %fd86, %fd55, %fd87;

BB49_9:
	mov.u32 	%r33, %tid.y;
	mov.u32 	%r32, %ctaid.y;
	mov.u32 	%r31, %ntid.y;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r29, %nctaid.x;
	mad.lo.s32 	%r28, %r31, %r32, %r33;
	mov.u32 	%r27, %tid.x;
	mov.u32 	%r26, %ntid.x;
	mad.lo.s32 	%r25, %r28, %r29, %r30;
	mad.lo.s32 	%r24, %r25, %r26, %r27;
	mul.wide.s32 	%rd10, %r24, 8;
	ld.param.u64 	%rd9, [vec_erfinv_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd10;
	mul.f64 	%fd171, %fd1, %fd172;
	st.global.f64 	[%rd8], %fd171;

BB49_10:
	ret;
}

	// .globl	vec_exp10
.visible .entry vec_exp10(
	.param .u32 vec_exp10_param_0,
	.param .u64 vec_exp10_param_1,
	.param .u64 vec_exp10_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<46>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r6, [vec_exp10_param_0];
	ld.param.u64 	%rd1, [vec_exp10_param_1];
	ld.param.u64 	%rd2, [vec_exp10_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB50_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d400A934F0979A371;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd8;
	}
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFD34413509F79FF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0d3C49DC1DA994FD21;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	mul.f64 	%fd15, %fd14, 0dBCAF48AD494EA3E9;
	mov.f64 	%fd16, 0d40026BB1BBB55516;
	fma.rn.f64 	%fd17, %fd14, %fd16, %fd15;
	mov.f64 	%fd18, 0d3E928AF3FCA213EA;
	mov.f64 	%fd19, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F81111111122322;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3FA55555555502A1;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555511;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FE000000000000B;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd17, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd39;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd39;
	}
	shl.b32 	%r16, %r2, 20;
	add.s32 	%r17, %r4, %r16;
	mov.b64 	%fd45, {%r3, %r17};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f40733A71;
	@%p2 bra 	BB50_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd40, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd41, %fd1;
	setp.gtu.f64	%p4, %fd41, 0d7FF0000000000000;
	add.f64 	%fd42, %fd1, %fd1;
	selp.f64	%fd45, %fd42, %fd40, %p4;
	setp.geu.f32	%p5, %f1, 0f407439B8;
	@%p5 bra 	BB50_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd43, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd44, {%r26, %r25};
	mul.f64 	%fd45, %fd43, %fd44;

BB50_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd45;

BB50_5:
	ret;
}

	// .globl	vec_exp2
.visible .entry vec_exp2(
	.param .u32 vec_exp2_param_0,
	.param .u64 vec_exp2_param_1,
	.param .u64 vec_exp2_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r6, [vec_exp2_param_0];
	ld.param.u64 	%rd1, [vec_exp2_param_1];
	ld.param.u64 	%rd2, [vec_exp2_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB51_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	mov.f64 	%fd6, 0d4338000000000000;
	add.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0dC338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd7;
	}
	sub.f64 	%fd10, %fd1, %fd9;
	mul.f64 	%fd11, %fd10, 0d3C7ABC9E3B39803F;
	mov.f64 	%fd12, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd10, %fd12, %fd11;
	mov.f64 	%fd14, 0d3E928AF3FCA213EA;
	mov.f64 	%fd15, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd16, %fd15, %fd13, %fd14;
	mov.f64 	%fd17, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd18, %fd16, %fd13, %fd17;
	mov.f64 	%fd19, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd20, %fd18, %fd13, %fd19;
	mov.f64 	%fd21, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd22, %fd20, %fd13, %fd21;
	mov.f64 	%fd23, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd24, %fd22, %fd13, %fd23;
	mov.f64 	%fd25, 0d3F81111111122322;
	fma.rn.f64 	%fd26, %fd24, %fd13, %fd25;
	mov.f64 	%fd27, 0d3FA55555555502A1;
	fma.rn.f64 	%fd28, %fd26, %fd13, %fd27;
	mov.f64 	%fd29, 0d3FC5555555555511;
	fma.rn.f64 	%fd30, %fd28, %fd13, %fd29;
	mov.f64 	%fd31, 0d3FE000000000000B;
	fma.rn.f64 	%fd32, %fd30, %fd13, %fd31;
	mov.f64 	%fd33, 0d3FF0000000000000;
	fma.rn.f64 	%fd34, %fd32, %fd13, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd13, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd35;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd35;
	}
	shl.b32 	%r16, %r2, 20;
	add.s32 	%r17, %r4, %r16;
	mov.b64 	%fd41, {%r3, %r17};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f408FF000;
	@%p2 bra 	BB51_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd36, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd37, %fd1;
	setp.gtu.f64	%p4, %fd37, 0d7FF0000000000000;
	add.f64 	%fd38, %fd1, %fd1;
	selp.f64	%fd41, %fd38, %fd36, %p4;
	setp.geu.f32	%p5, %f1, 0f4090CC00;
	@%p5 bra 	BB51_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd39, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd40, {%r26, %r25};
	mul.f64 	%fd41, %fd39, %fd40;

BB51_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd41;

BB51_5:
	ret;
}

	// .globl	vec_exp
.visible .entry vec_exp(
	.param .u32 vec_exp_param_0,
	.param .u64 vec_exp_param_1,
	.param .u64 vec_exp_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r5, [vec_exp_param_0];
	ld.param.u64 	%rd1, [vec_exp_param_1];
	ld.param.u64 	%rd2, [vec_exp_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB52_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd8;
	}
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	mov.f64 	%fd15, 0d3E928AF3FCA213EA;
	mov.f64 	%fd16, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mov.f64 	%fd26, 0d3F81111111122322;
	fma.rn.f64 	%fd27, %fd25, %fd14, %fd26;
	mov.f64 	%fd28, 0d3FA55555555502A1;
	fma.rn.f64 	%fd29, %fd27, %fd14, %fd28;
	mov.f64 	%fd30, 0d3FC5555555555511;
	fma.rn.f64 	%fd31, %fd29, %fd14, %fd30;
	mov.f64 	%fd32, 0d3FE000000000000B;
	fma.rn.f64 	%fd33, %fd31, %fd14, %fd32;
	mov.f64 	%fd34, 0d3FF0000000000000;
	fma.rn.f64 	%fd35, %fd33, %fd14, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd14, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd36;
	}
	shl.b32 	%r15, %r2, 20;
	add.s32 	%r16, %r4, %r15;
	mov.b64 	%fd40, {%r3, %r16};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd1;
	}
	mov.b32 	 %f2, %r17;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f4086232B;
	@%p2 bra 	BB52_4;

	setp.lt.f64	%p3, %fd1, 0d0000000000000000;
	add.f64 	%fd37, %fd1, 0d7FF0000000000000;
	selp.f64	%fd40, 0d0000000000000000, %fd37, %p3;
	setp.geu.f32	%p4, %f1, 0f40874800;
	@%p4 bra 	BB52_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd38, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd39, {%r26, %r25};
	mul.f64 	%fd40, %fd38, %fd39;

BB52_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd40;

BB52_5:
	ret;
}

	// .globl	vec_expm1
.visible .entry vec_expm1(
	.param .u32 vec_expm1_param_0,
	.param .u64 vec_expm1_param_1,
	.param .u64 vec_expm1_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<48>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r3, [vec_expm1_param_0];
	ld.param.u64 	%rd1, [vec_expm1_param_1];
	ld.param.u64 	%rd2, [vec_expm1_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB53_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.lt.u32	%p2, %r2, 1082535491;
	setp.lt.s32	%p3, %r2, -1068859392;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB53_3;
	bra.uni 	BB53_2;

BB53_3:
	mov.f64 	%fd8, 0d4338000000000000;
	mov.f64 	%fd9, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd10, %fd1, %fd9, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd10;
	}
	mov.f64 	%fd11, 0dC338000000000000;
	add.rn.f64 	%fd12, %fd10, %fd11;
	mov.f64 	%fd13, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd14, %fd12, %fd13, %fd1;
	mov.f64 	%fd15, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd16, %fd12, %fd15, %fd14;
	add.s32 	%r14, %r2, %r2;
	setp.lt.u32	%p7, %r14, 2142496327;
	selp.b32	%r15, 0, %r13, %p7;
	mov.u32 	%r16, 0;
	selp.f64	%fd17, %fd1, %fd16, %p7;
	mov.f64 	%fd18, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd19, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3F8111111110F74D;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FA555555555554D;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FC5555555555557;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FE0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	mul.f64 	%fd39, %fd17, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd17, %fd17;
	setp.eq.s32	%p8, %r15, 1024;
	selp.b32	%r17, -1, 0, %p8;
	add.s32 	%r18, %r17, %r15;
	shl.b32 	%r19, %r18, 20;
	add.s32 	%r20, %r19, 1072693248;
	mov.u32 	%r21, 1072693248;
	mov.b64 	%fd41, {%r16, %r20};
	mov.b64 	%fd42, {%r16, %r21};
	sub.f64 	%fd43, %fd41, %fd42;
	fma.rn.f64 	%fd44, %fd40, %fd41, %fd43;
	add.f64 	%fd45, %fd44, %fd44;
	setp.eq.s32	%p9, %r14, 0;
	selp.f64	%fd46, %fd45, %fd44, %p8;
	selp.f64	%fd47, %fd17, %fd46, %p9;
	bra.uni 	BB53_4;

BB53_2:
	setp.lt.s32	%p5, %r2, 0;
	abs.f64 	%fd5, %fd1;
	setp.gtu.f64	%p6, %fd5, 0d7FF0000000000000;
	selp.f64	%fd6, 0dBFF0000000000000, 0d7FF0000000000000, %p5;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd47, %fd7, %fd6, %p6;

BB53_4:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd47;

BB53_5:
	ret;
}

	// .globl	vec_fabs
.visible .entry vec_fabs(
	.param .u32 vec_fabs_param_0,
	.param .u64 vec_fabs_param_1,
	.param .u64 vec_fabs_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_fabs_param_0];
	ld.param.u64 	%rd1, [vec_fabs_param_1];
	ld.param.u64 	%rd2, [vec_fabs_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB54_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB54_2:
	ret;
}

	// .globl	vec_floor
.visible .entry vec_floor(
	.param .u32 vec_floor_param_0,
	.param .u64 vec_floor_param_1,
	.param .u64 vec_floor_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_floor_param_0];
	ld.param.u64 	%rd1, [vec_floor_param_1];
	ld.param.u64 	%rd2, [vec_floor_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB55_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rmi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB55_2:
	ret;
}

	// .globl	vec_j0
.visible .entry vec_j0(
	.param .u32 vec_j0_param_0,
	.param .u64 vec_j0_param_1,
	.param .u64 vec_j0_param_2
)
{
	.local .align 4 .b8 	__local_depot56[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<216>;
	.reg .b64 	%rd<20>;


	mov.u64 	%SPL, __local_depot56;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r9, [vec_j0_param_0];
	ld.param.u64 	%rd3, [vec_j0_param_1];
	ld.param.u64 	%rd4, [vec_j0_param_2];
	add.u64 	%rd1, %SPL, 0;
	add.u64 	%rd2, %SPL, 4;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB56_22;

	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r1, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd25, [%rd9];
	abs.f64 	%fd1, %fd25;
	setp.gtu.f64	%p2, %fd1, 0d400FB319F277BBE5;
	@%p2 bra 	BB56_3;
	bra.uni 	BB56_2;

BB56_3:
	setp.gtu.f64	%p3, %fd1, 0d401C58FD1A62F5EC;
	@%p3 bra 	BB56_5;
	bra.uni 	BB56_4;

BB56_5:
	setp.gtu.f64	%p4, %fd1, 0d402471FCB6A7A8C0;
	@%p4 bra 	BB56_7;
	bra.uni 	BB56_6;

BB56_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd1;
	}
	and.b32  	%r20, %r19, 2147483647;
	setp.ne.s32	%p5, %r20, 2146435072;
	@%p5 bra 	BB56_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd1;
	}
	setp.eq.s32	%p6, %r21, 0;
	mov.f64 	%fd215, 0d0000000000000000;
	@%p6 bra 	BB56_21;

BB56_9:
	rcp.approx.ftz.f64 	%fd132, %fd1;
	neg.f64 	%fd133, %fd1;
	mov.f64 	%fd134, 0d3FF0000000000000;
	fma.rn.f64 	%fd135, %fd133, %fd132, %fd134;
	fma.rn.f64 	%fd136, %fd135, %fd135, %fd135;
	fma.rn.f64 	%fd137, %fd136, %fd132, %fd132;
	mul.f64 	%fd138, %fd137, %fd137;
	mov.f64 	%fd139, 0d409927467A655012;
	mov.f64 	%fd140, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd141, %fd140, %fd138, %fd139;
	mov.f64 	%fd142, 0dC05751787E247BD4;
	fma.rn.f64 	%fd143, %fd141, %fd138, %fd142;
	mov.f64 	%fd144, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd145, %fd143, %fd138, %fd144;
	mov.f64 	%fd146, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd147, %fd145, %fd138, %fd146;
	mov.f64 	%fd148, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd149, %fd147, %fd138, %fd148;
	mov.f64 	%fd150, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd151, %fd149, %fd138, %fd150;
	mov.f64 	%fd152, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd153, %fd151, %fd138, %fd152;
	mov.f64 	%fd154, 0d410ECD4523B12B84;
	mov.f64 	%fd155, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd156, %fd155, %fd138, %fd154;
	mov.f64 	%fd157, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd158, %fd156, %fd138, %fd157;
	mov.f64 	%fd159, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd160, %fd158, %fd138, %fd159;
	mov.f64 	%fd161, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd162, %fd160, %fd138, %fd161;
	mov.f64 	%fd163, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd164, %fd162, %fd138, %fd163;
	mov.f64 	%fd165, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd166, %fd164, %fd138, %fd165;
	mov.f64 	%fd167, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd168, %fd166, %fd138, %fd167;
	mov.f64 	%fd169, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd170, %fd168, %fd138, %fd169;
	fma.rn.f64 	%fd5, %fd170, %fd137, %fd1;
	rsqrt.approx.f64 	%fd171, %fd1;
	mul.f64 	%fd172, %fd171, 0d3FE9884533D43651;
	mul.f64 	%fd6, %fd153, %fd172;
	mul.f64 	%fd173, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r34, %fd173;
	st.local.u32 	[%rd2], %r34;
	cvt.rn.f64.s32	%fd174, %r34;
	neg.f64 	%fd175, %fd174;
	mov.f64 	%fd176, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd177, %fd175, %fd176, %fd5;
	mov.f64 	%fd178, 0d3C91A62633145C00;
	fma.rn.f64 	%fd179, %fd175, %fd178, %fd177;
	mov.f64 	%fd180, 0d397B839A252049C0;
	fma.rn.f64 	%fd210, %fd175, %fd180, %fd179;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd5;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p7, %r23, 1105199104;
	@%p7 bra 	BB56_11;

	add.u64 	%rd19, %SP, 4;
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd210, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r34, [%rd2];

BB56_11:
	and.b32  	%r24, %r34, 3;
	cvt.rn.f64.s32	%fd181, %r24;
	add.f64 	%fd182, %fd210, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd211, %fd181, 0d3FF921FB54442D18, %fd182;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd211;
	}
	and.b32  	%r26, %r25, 2147483647;
	setp.ne.s32	%p8, %r26, 2146435072;
	@%p8 bra 	BB56_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd211;
	}
	setp.ne.s32	%p9, %r27, 0;
	@%p9 bra 	BB56_14;

	mov.f64 	%fd183, 0d0000000000000000;
	mul.rn.f64 	%fd211, %fd211, %fd183;

BB56_14:
	mov.f64 	%fd209, 0d397B839A252049C0;
	mov.f64 	%fd208, 0d3C91A62633145C00;
	mov.f64 	%fd207, 0d3FF921FB54442D18;
	mul.f64 	%fd184, %fd211, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r35, %fd184;
	st.local.u32 	[%rd1], %r35;
	cvt.rn.f64.s32	%fd185, %r35;
	neg.f64 	%fd186, %fd185;
	fma.rn.f64 	%fd188, %fd186, %fd207, %fd211;
	fma.rn.f64 	%fd190, %fd186, %fd208, %fd188;
	fma.rn.f64 	%fd212, %fd186, %fd209, %fd190;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd211;
	}
	and.b32  	%r29, %r28, 2145386496;
	setp.lt.u32	%p10, %r29, 1105199104;
	@%p10 bra 	BB56_16;

	add.u64 	%rd18, %SP, 0;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd211;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd18;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd212, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r35, [%rd1];

BB56_16:
	add.s32 	%r8, %r35, 1;
	and.b32  	%r30, %r8, 1;
	shl.b32 	%r31, %r30, 3;
	setp.eq.s32	%p11, %r30, 0;
	selp.f64	%fd192, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p11;
	add.s32 	%r32, %r31, 1;
	mul.wide.s32 	%rd12, %r32, 8;
	mov.u64 	%rd13, __cudart_sin_cos_coeffs;
	add.s64 	%rd14, %rd13, %rd12;
	ld.const.f64 	%fd193, [%rd14];
	mul.rn.f64 	%fd16, %fd212, %fd212;
	fma.rn.f64 	%fd194, %fd192, %fd16, %fd193;
	ld.const.f64 	%fd195, [%rd14+8];
	fma.rn.f64 	%fd196, %fd194, %fd16, %fd195;
	ld.const.f64 	%fd197, [%rd14+16];
	fma.rn.f64 	%fd198, %fd196, %fd16, %fd197;
	ld.const.f64 	%fd199, [%rd14+24];
	fma.rn.f64 	%fd200, %fd198, %fd16, %fd199;
	ld.const.f64 	%fd201, [%rd14+32];
	fma.rn.f64 	%fd202, %fd200, %fd16, %fd201;
	ld.const.f64 	%fd203, [%rd14+40];
	fma.rn.f64 	%fd17, %fd202, %fd16, %fd203;
	fma.rn.f64 	%fd213, %fd17, %fd212, %fd212;
	@%p11 bra 	BB56_18;

	fma.rn.f64 	%fd213, %fd17, %fd16, %fd134;

BB56_18:
	and.b32  	%r33, %r8, 2;
	setp.eq.s32	%p12, %r33, 0;
	@%p12 bra 	BB56_20;

	mov.f64 	%fd205, 0d0000000000000000;
	mov.f64 	%fd206, 0dBFF0000000000000;
	fma.rn.f64 	%fd213, %fd213, %fd206, %fd205;

BB56_20:
	mul.f64 	%fd215, %fd6, %fd213;
	bra.uni 	BB56_21;

BB56_2:
	add.f64 	%fd26, %fd1, 0dC0033D152E971B40;
	add.f64 	%fd27, %fd26, 0d3CA0F539D7DA258E;
	mov.f64 	%fd28, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd29, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	mov.f64 	%fd41, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd42, %fd40, %fd27, %fd41;
	mov.f64 	%fd43, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd44, %fd42, %fd27, %fd43;
	mov.f64 	%fd45, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd46, %fd44, %fd27, %fd45;
	mov.f64 	%fd47, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd48, %fd46, %fd27, %fd47;
	mov.f64 	%fd49, 0d3F315382BA096A62;
	fma.rn.f64 	%fd50, %fd48, %fd27, %fd49;
	mov.f64 	%fd51, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd52, %fd50, %fd27, %fd51;
	mov.f64 	%fd53, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd54, %fd52, %fd27, %fd53;
	mov.f64 	%fd55, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd56, %fd54, %fd27, %fd55;
	mov.f64 	%fd57, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd58, %fd56, %fd27, %fd57;
	mov.f64 	%fd59, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd60, %fd58, %fd27, %fd59;
	mul.f64 	%fd215, %fd27, %fd60;
	bra.uni 	BB56_21;

BB56_4:
	add.f64 	%fd61, %fd1, 0dC016148F5B2C2E45;
	add.f64 	%fd62, %fd61, 0dBC975054CD60A517;
	mov.f64 	%fd63, 0d3CF83FD1F333EB61;
	mov.f64 	%fd64, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBD7846076D004627;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0dBF0863F481A37337;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mov.f64 	%fd88, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd89, %fd87, %fd62, %fd88;
	mov.f64 	%fd90, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd91, %fd89, %fd62, %fd90;
	mov.f64 	%fd92, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd93, %fd91, %fd62, %fd92;
	mov.f64 	%fd94, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd95, %fd93, %fd62, %fd94;
	mul.f64 	%fd215, %fd62, %fd95;
	bra.uni 	BB56_21;

BB56_6:
	add.f64 	%fd96, %fd1, 0dC0214EB56CCCDECA;
	add.f64 	%fd97, %fd96, 0d3CB51970714C7C25;
	mov.f64 	%fd98, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd99, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd100, %fd99, %fd97, %fd98;
	mov.f64 	%fd101, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd102, %fd100, %fd97, %fd101;
	mov.f64 	%fd103, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd104, %fd102, %fd97, %fd103;
	mov.f64 	%fd105, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd106, %fd104, %fd97, %fd105;
	mov.f64 	%fd107, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd108, %fd106, %fd97, %fd107;
	mov.f64 	%fd109, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd110, %fd108, %fd97, %fd109;
	mov.f64 	%fd111, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd112, %fd110, %fd97, %fd111;
	mov.f64 	%fd113, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd114, %fd112, %fd97, %fd113;
	mov.f64 	%fd115, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd116, %fd114, %fd97, %fd115;
	mov.f64 	%fd117, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd118, %fd116, %fd97, %fd117;
	mov.f64 	%fd119, 0d3F1D79605276949A;
	fma.rn.f64 	%fd120, %fd118, %fd97, %fd119;
	mov.f64 	%fd121, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd122, %fd120, %fd97, %fd121;
	mov.f64 	%fd123, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd124, %fd122, %fd97, %fd123;
	mov.f64 	%fd125, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd126, %fd124, %fd97, %fd125;
	mov.f64 	%fd127, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd128, %fd126, %fd97, %fd127;
	mov.f64 	%fd129, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd130, %fd128, %fd97, %fd129;
	mul.f64 	%fd215, %fd97, %fd130;

BB56_21:
	cvta.to.global.u64 	%rd15, %rd3;
	add.s64 	%rd17, %rd15, %rd8;
	st.global.f64 	[%rd17], %fd215;

BB56_22:
	ret;
}

	// .globl	vec_j1
.visible .entry vec_j1(
	.param .u32 vec_j1_param_0,
	.param .u64 vec_j1_param_1,
	.param .u64 vec_j1_param_2
)
{
	.local .align 4 .b8 	__local_depot57[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<215>;
	.reg .b64 	%rd<20>;


	mov.u64 	%SPL, __local_depot57;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r9, [vec_j1_param_0];
	ld.param.u64 	%rd3, [vec_j1_param_1];
	ld.param.u64 	%rd4, [vec_j1_param_2];
	add.u64 	%rd1, %SPL, 0;
	add.u64 	%rd2, %SPL, 4;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB57_22;

	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r1, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d400353AABAD7B784;
	@%p2 bra 	BB57_3;
	bra.uni 	BB57_2;

BB57_3:
	setp.gtu.f64	%p3, %fd2, 0d4015B1D0574614EA;
	@%p3 bra 	BB57_5;
	bra.uni 	BB57_4;

BB57_5:
	setp.gtu.f64	%p4, %fd2, 0d40213065E54C1AA9;
	@%p4 bra 	BB57_7;
	bra.uni 	BB57_6;

BB57_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd2;
	}
	and.b32  	%r20, %r19, 2147483647;
	setp.ne.s32	%p5, %r20, 2146435072;
	@%p5 bra 	BB57_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd2;
	}
	setp.eq.s32	%p6, %r21, 0;
	mov.f64 	%fd214, 0d0000000000000000;
	@%p6 bra 	BB57_21;

BB57_9:
	rcp.approx.ftz.f64 	%fd124, %fd2;
	neg.f64 	%fd125, %fd2;
	mov.f64 	%fd126, 0d3FF0000000000000;
	fma.rn.f64 	%fd127, %fd125, %fd124, %fd126;
	fma.rn.f64 	%fd128, %fd127, %fd127, %fd127;
	fma.rn.f64 	%fd129, %fd128, %fd124, %fd124;
	mul.f64 	%fd130, %fd129, %fd129;
	mov.f64 	%fd131, 0dC099C06322A3F8BE;
	mov.f64 	%fd132, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd133, %fd132, %fd130, %fd131;
	mov.f64 	%fd134, 0d405B89354DA77324;
	fma.rn.f64 	%fd135, %fd133, %fd130, %fd134;
	mov.f64 	%fd136, 0dC01E352294653188;
	fma.rn.f64 	%fd137, %fd135, %fd130, %fd136;
	mov.f64 	%fd138, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd139, %fd137, %fd130, %fd138;
	mov.f64 	%fd140, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd141, %fd139, %fd130, %fd140;
	mov.f64 	%fd142, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd143, %fd141, %fd130, %fd142;
	mov.f64 	%fd144, 0d3FF00000000068CC;
	fma.rn.f64 	%fd145, %fd143, %fd130, %fd144;
	mov.f64 	%fd146, 0d415A30AC6857BEE0;
	mov.f64 	%fd147, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd148, %fd147, %fd130, %fd146;
	mov.f64 	%fd149, 0dC11764222AD7C910;
	fma.rn.f64 	%fd150, %fd148, %fd130, %fd149;
	mov.f64 	%fd151, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd152, %fd150, %fd130, %fd151;
	mov.f64 	%fd153, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd154, %fd152, %fd130, %fd153;
	mov.f64 	%fd155, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd156, %fd154, %fd130, %fd155;
	mov.f64 	%fd157, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd158, %fd156, %fd130, %fd157;
	mov.f64 	%fd159, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd160, %fd158, %fd130, %fd159;
	mov.f64 	%fd161, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd162, %fd160, %fd130, %fd161;
	mov.f64 	%fd163, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd164, %fd162, %fd130, %fd163;
	fma.rn.f64 	%fd6, %fd164, %fd129, %fd2;
	rsqrt.approx.f64 	%fd165, %fd2;
	mul.f64 	%fd166, %fd165, 0d3FE9884533D43651;
	mul.f64 	%fd7, %fd145, %fd166;
	mul.f64 	%fd167, %fd6, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r34, %fd167;
	st.local.u32 	[%rd2], %r34;
	cvt.rn.f64.s32	%fd168, %r34;
	neg.f64 	%fd169, %fd168;
	mov.f64 	%fd170, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd171, %fd169, %fd170, %fd6;
	mov.f64 	%fd172, 0d3C91A62633145C00;
	fma.rn.f64 	%fd173, %fd169, %fd172, %fd171;
	mov.f64 	%fd174, 0d397B839A252049C0;
	fma.rn.f64 	%fd209, %fd169, %fd174, %fd173;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd6;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p7, %r23, 1105199104;
	@%p7 bra 	BB57_11;

	add.u64 	%rd19, %SP, 4;
	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd6;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd209, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r34, [%rd2];

BB57_11:
	and.b32  	%r24, %r34, 3;
	cvt.rn.f64.s32	%fd175, %r24;
	add.f64 	%fd176, %fd209, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd210, %fd175, 0d3FF921FB54442D18, %fd176;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd210;
	}
	and.b32  	%r26, %r25, 2147483647;
	setp.ne.s32	%p8, %r26, 2146435072;
	@%p8 bra 	BB57_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r27, %temp}, %fd210;
	}
	setp.ne.s32	%p9, %r27, 0;
	@%p9 bra 	BB57_14;

	mov.f64 	%fd177, 0d0000000000000000;
	mul.rn.f64 	%fd210, %fd210, %fd177;

BB57_14:
	mov.f64 	%fd207, 0d397B839A252049C0;
	mov.f64 	%fd206, 0d3C91A62633145C00;
	mov.f64 	%fd205, 0d3FF921FB54442D18;
	mul.f64 	%fd178, %fd210, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r35, %fd178;
	st.local.u32 	[%rd1], %r35;
	cvt.rn.f64.s32	%fd179, %r35;
	neg.f64 	%fd180, %fd179;
	fma.rn.f64 	%fd182, %fd180, %fd205, %fd210;
	fma.rn.f64 	%fd184, %fd180, %fd206, %fd182;
	fma.rn.f64 	%fd211, %fd180, %fd207, %fd184;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd210;
	}
	and.b32  	%r29, %r28, 2145386496;
	setp.lt.u32	%p10, %r29, 1105199104;
	@%p10 bra 	BB57_16;

	add.u64 	%rd18, %SP, 0;
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd210;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd18;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd211, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r35, [%rd1];

BB57_16:
	add.s32 	%r8, %r35, 1;
	and.b32  	%r30, %r8, 1;
	shl.b32 	%r31, %r30, 3;
	setp.eq.s32	%p11, %r30, 0;
	selp.f64	%fd186, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p11;
	add.s32 	%r32, %r31, 1;
	mul.wide.s32 	%rd12, %r32, 8;
	mov.u64 	%rd13, __cudart_sin_cos_coeffs;
	add.s64 	%rd14, %rd13, %rd12;
	ld.const.f64 	%fd187, [%rd14];
	mul.rn.f64 	%fd17, %fd211, %fd211;
	fma.rn.f64 	%fd188, %fd186, %fd17, %fd187;
	ld.const.f64 	%fd189, [%rd14+8];
	fma.rn.f64 	%fd190, %fd188, %fd17, %fd189;
	ld.const.f64 	%fd191, [%rd14+16];
	fma.rn.f64 	%fd192, %fd190, %fd17, %fd191;
	ld.const.f64 	%fd193, [%rd14+24];
	fma.rn.f64 	%fd194, %fd192, %fd17, %fd193;
	ld.const.f64 	%fd195, [%rd14+32];
	fma.rn.f64 	%fd196, %fd194, %fd17, %fd195;
	ld.const.f64 	%fd197, [%rd14+40];
	fma.rn.f64 	%fd18, %fd196, %fd17, %fd197;
	fma.rn.f64 	%fd212, %fd18, %fd211, %fd211;
	@%p11 bra 	BB57_18;

	mov.f64 	%fd208, 0d3FF0000000000000;
	fma.rn.f64 	%fd212, %fd18, %fd17, %fd208;

BB57_18:
	and.b32  	%r33, %r8, 2;
	setp.eq.s32	%p12, %r33, 0;
	@%p12 bra 	BB57_20;

	mov.f64 	%fd199, 0d0000000000000000;
	mov.f64 	%fd200, 0dBFF0000000000000;
	fma.rn.f64 	%fd212, %fd212, %fd200, %fd199;

BB57_20:
	mul.f64 	%fd214, %fd7, %fd212;
	bra.uni 	BB57_21;

BB57_2:
	mov.f64 	%fd26, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd27, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd28, %fd27, %fd2, %fd26;
	mov.f64 	%fd29, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd30, %fd28, %fd2, %fd29;
	mov.f64 	%fd31, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd32, %fd30, %fd2, %fd31;
	mov.f64 	%fd33, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F65555555506982;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0dBFB0000000000065;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mov.f64 	%fd55, 0d3FE0000000000000;
	fma.rn.f64 	%fd56, %fd54, %fd2, %fd55;
	mul.f64 	%fd214, %fd2, %fd56;
	bra.uni 	BB57_21;

BB57_4:
	add.f64 	%fd57, %fd2, 0dC00EA75575AF6F09;
	add.f64 	%fd58, %fd57, 0d3CA60155A9D1B256;
	mov.f64 	%fd59, 0d3D41011A1DF02DAD;
	mov.f64 	%fd60, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd61, %fd60, %fd58, %fd59;
	mov.f64 	%fd62, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	mov.f64 	%fd64, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	mov.f64 	%fd66, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	mov.f64 	%fd68, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	mov.f64 	%fd70, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	mov.f64 	%fd72, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd73, %fd71, %fd58, %fd72;
	mov.f64 	%fd74, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd75, %fd73, %fd58, %fd74;
	mov.f64 	%fd76, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd77, %fd75, %fd58, %fd76;
	mov.f64 	%fd78, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd79, %fd77, %fd58, %fd78;
	mov.f64 	%fd80, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd81, %fd79, %fd58, %fd80;
	mov.f64 	%fd82, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd83, %fd81, %fd58, %fd82;
	mov.f64 	%fd84, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd85, %fd83, %fd58, %fd84;
	mov.f64 	%fd86, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd87, %fd85, %fd58, %fd86;
	mov.f64 	%fd88, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd89, %fd87, %fd58, %fd88;
	mul.f64 	%fd214, %fd58, %fd89;
	bra.uni 	BB57_21;

BB57_6:
	add.f64 	%fd90, %fd2, 0dC01C0FF5F3B47250;
	add.f64 	%fd91, %fd90, 0d3C9B226D9D243827;
	mov.f64 	%fd92, 0dBD40E8363DB649A9;
	mov.f64 	%fd93, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0dBE34462EED732A23;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	mov.f64 	%fd107, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd108, %fd106, %fd91, %fd107;
	mov.f64 	%fd109, 0dBF0796A751B32693;
	fma.rn.f64 	%fd110, %fd108, %fd91, %fd109;
	mov.f64 	%fd111, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd112, %fd110, %fd91, %fd111;
	mov.f64 	%fd113, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd114, %fd112, %fd91, %fd113;
	mov.f64 	%fd115, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd116, %fd114, %fd91, %fd115;
	mov.f64 	%fd117, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd118, %fd116, %fd91, %fd117;
	mov.f64 	%fd119, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd120, %fd118, %fd91, %fd119;
	mov.f64 	%fd121, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd122, %fd120, %fd91, %fd121;
	mul.f64 	%fd214, %fd91, %fd122;

BB57_21:
	neg.f64 	%fd201, %fd214;
	setp.lt.f64	%p13, %fd1, 0d0000000000000000;
	selp.f64	%fd202, %fd201, %fd214, %p13;
	mul.f64 	%fd203, %fd1, 0d3FE0000000000000;
	setp.lt.f64	%p14, %fd2, 0d39B4484BFEEBC2A0;
	selp.f64	%fd204, %fd203, %fd202, %p14;
	cvta.to.global.u64 	%rd15, %rd3;
	add.s64 	%rd17, %rd15, %rd8;
	st.global.f64 	[%rd17], %fd204;

BB57_22:
	ret;
}

	// .globl	vec_lgamma
.visible .entry vec_lgamma(
	.param .u32 vec_lgamma_param_0,
	.param .u64 vec_lgamma_param_1,
	.param .u64 vec_lgamma_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<45>;
	.reg .f64 	%fd<107>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r14, [vec_lgamma_param_0];
	ld.param.u64 	%rd1, [vec_lgamma_param_1];
	ld.param.u64 	%rd2, [vec_lgamma_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %tid.y;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %nctaid.x;
	mov.u32 	%r21, %ctaid.x;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %ntid.x;
	mad.lo.s32 	%r1, %r22, %r23, %r15;
	setp.ge.s32	%p1, %r1, %r14;
	@%p1 bra 	BB58_21;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd102, %fd1;
	setp.gtu.f64	%p2, %fd102, 0d7FF0000000000000;
	@%p2 bra 	BB58_19;
	bra.uni 	BB58_2;

BB58_19:
	add.f64 	%fd106, %fd1, %fd1;
	bra.uni 	BB58_20;

BB58_2:
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd102;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd3, [retval0+0];
	
	//{
	}// Callseq End 7
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd1;
	}
	setp.gt.s32	%p3, %r24, -1;
	@%p3 bra 	BB58_3;
	bra.uni 	BB58_4;

BB58_3:
	mov.f64 	%fd106, %fd3;
	bra.uni 	BB58_20;

BB58_4:
	cvt.rzi.f64.f64	%fd25, %fd102;
	setp.eq.f64	%p4, %fd102, %fd25;
	mov.f64 	%fd106, 0d7FF0000000000000;
	@%p4 bra 	BB58_20;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd102;
	}
	setp.lt.s32	%p5, %r2, 1006632960;
	@%p5 bra 	BB58_11;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd102;
	}
	add.s32 	%r26, %r2, 1048576;
	mov.b64 	%fd26, {%r25, %r26};
	cvt.rni.f64.f64	%fd27, %fd26;
	cvt.rzi.s64.f64	%rd6, %fd27;
	cvt.u32.u64	%r3, %rd6;
	neg.f64 	%fd28, %fd27;
	mov.f64 	%fd29, 0d3FE0000000000000;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd102;
	mul.f64 	%fd31, %fd30, 0d3CA1A62633145C07;
	mov.f64 	%fd32, 0d400921FB54442D18;
	fma.rn.f64 	%fd33, %fd30, %fd32, %fd31;
	and.b64  	%rd7, %rd6, 1;
	mul.rn.f64 	%fd4, %fd33, %fd33;
	setp.eq.b64	%p6, %rd7, 1;
	not.pred 	%p7, %p6;
	selp.f64	%fd34, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p7;
	shl.b64 	%rd8, %rd7, 6;
	mov.u64 	%rd9, __cudart_sin_cos_coeffs;
	add.s64 	%rd10, %rd8, %rd9;
	ld.const.f64 	%fd35, [%rd10+8];
	fma.rn.f64 	%fd36, %fd34, %fd4, %fd35;
	ld.const.f64 	%fd37, [%rd10+16];
	fma.rn.f64 	%fd38, %fd36, %fd4, %fd37;
	ld.const.f64 	%fd39, [%rd10+24];
	fma.rn.f64 	%fd40, %fd38, %fd4, %fd39;
	ld.const.f64 	%fd41, [%rd10+32];
	fma.rn.f64 	%fd42, %fd40, %fd4, %fd41;
	ld.const.f64 	%fd43, [%rd10+40];
	fma.rn.f64 	%fd44, %fd42, %fd4, %fd43;
	ld.const.f64 	%fd45, [%rd10+48];
	fma.rn.f64 	%fd5, %fd44, %fd4, %fd45;
	fma.rn.f64 	%fd100, %fd5, %fd33, %fd33;
	@%p7 bra 	BB58_8;

	mov.f64 	%fd46, 0d3FF0000000000000;
	fma.rn.f64 	%fd100, %fd5, %fd4, %fd46;

BB58_8:
	and.b32  	%r27, %r3, 2;
	setp.eq.s32	%p8, %r27, 0;
	@%p8 bra 	BB58_10;

	mov.f64 	%fd47, 0d0000000000000000;
	mov.f64 	%fd48, 0dBFF0000000000000;
	fma.rn.f64 	%fd100, %fd100, %fd48, %fd47;

BB58_10:
	abs.f64 	%fd49, %fd100;
	mul.f64 	%fd50, %fd102, %fd49;
	div.rn.f64 	%fd102, %fd32, %fd50;

BB58_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd102;
	}
	mov.u32 	%r43, -1023;
	setp.gt.s32	%p9, %r41, 1048575;
	@%p9 bra 	BB58_13;

	mul.f64 	%fd102, %fd102, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd102;
	}
	mov.u32 	%r43, -1077;

BB58_13:
	add.s32 	%r30, %r41, -1;
	setp.lt.u32	%p10, %r30, 2146435071;
	@%p10 bra 	BB58_15;
	bra.uni 	BB58_14;

BB58_15:
	shr.u32 	%r32, %r41, 20;
	add.s32 	%r44, %r43, %r32;
	and.b32  	%r33, %r41, -2146435073;
	or.b32  	%r34, %r33, 1072693248;
	mov.b64 	%fd104, {%r42, %r34};
	setp.lt.s32	%p12, %r34, 1073127583;
	@%p12 bra 	BB58_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd104;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd104;
	}
	add.s32 	%r37, %r36, -1048576;
	mov.b64 	%fd104, {%r35, %r37};
	add.s32 	%r44, %r44, 1;

BB58_17:
	add.f64 	%fd54, %fd104, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd55, %fd54;
	neg.f64 	%fd56, %fd54;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd55, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd55, %fd55;
	add.f64 	%fd61, %fd104, 0dBFF0000000000000;
	mul.f64 	%fd62, %fd61, %fd60;
	fma.rn.f64 	%fd63, %fd61, %fd60, %fd62;
	mul.f64 	%fd64, %fd63, %fd63;
	mov.f64 	%fd65, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd66, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd67, %fd66, %fd64, %fd65;
	mov.f64 	%fd68, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd69, %fd67, %fd64, %fd68;
	mov.f64 	%fd70, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd71, %fd69, %fd64, %fd70;
	mov.f64 	%fd72, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd73, %fd71, %fd64, %fd72;
	mov.f64 	%fd74, 0d3F624924923BE72D;
	fma.rn.f64 	%fd75, %fd73, %fd64, %fd74;
	mov.f64 	%fd76, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd77, %fd75, %fd64, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555554;
	fma.rn.f64 	%fd79, %fd77, %fd64, %fd78;
	sub.f64 	%fd80, %fd61, %fd63;
	add.f64 	%fd81, %fd80, %fd80;
	neg.f64 	%fd82, %fd63;
	fma.rn.f64 	%fd83, %fd82, %fd61, %fd81;
	mul.f64 	%fd84, %fd60, %fd83;
	mul.f64 	%fd85, %fd64, %fd79;
	fma.rn.f64 	%fd86, %fd85, %fd63, %fd84;
	xor.b32  	%r38, %r44, -2147483648;
	mov.u32 	%r39, -2147483648;
	mov.u32 	%r40, 1127219200;
	mov.b64 	%fd87, {%r38, %r40};
	mov.b64 	%fd88, {%r39, %r40};
	sub.f64 	%fd89, %fd87, %fd88;
	mov.f64 	%fd90, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd91, %fd89, %fd90, %fd63;
	neg.f64 	%fd92, %fd89;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	sub.f64 	%fd94, %fd93, %fd63;
	sub.f64 	%fd95, %fd86, %fd94;
	mov.f64 	%fd96, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd97, %fd89, %fd96, %fd95;
	add.f64 	%fd105, %fd91, %fd97;
	bra.uni 	BB58_18;

BB58_14:
	mov.f64 	%fd52, 0d7FF0000000000000;
	fma.rn.f64 	%fd53, %fd102, %fd52, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd102;
	}
	mov.b32 	 %f1, %r31;
	setp.eq.f32	%p11, %f1, 0f00000000;
	selp.f64	%fd105, 0dFFF0000000000000, %fd53, %p11;

BB58_18:
	sub.f64 	%fd98, %fd105, %fd3;
	neg.f64 	%fd99, %fd105;
	selp.f64	%fd106, %fd99, %fd98, %p5;

BB58_20:
	cvta.to.global.u64 	%rd11, %rd1;
	add.s64 	%rd13, %rd11, %rd4;
	st.global.f64 	[%rd13], %fd106;

BB58_21:
	ret;
}

	// .globl	vec_log10
.visible .entry vec_log10(
	.param .u32 vec_log10_param_0,
	.param .u64 vec_log10_param_1,
	.param .u64 vec_log10_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r12, [vec_log10_param_0];
	ld.param.u64 	%rd1, [vec_log10_param_1];
	ld.param.u64 	%rd2, [vec_log10_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB59_9;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd59, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB59_3;

	mul.f64 	%fd59, %fd59, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1077;

BB59_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB59_5;
	bra.uni 	BB59_4;

BB59_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB59_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB59_7:
	add.f64 	%fd12, %fd60, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd13, %fd12;
	neg.f64 	%fd14, %fd12;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd13, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd13, %fd13;
	add.f64 	%fd19, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd45, {%r32, %r34};
	mov.b64 	%fd46, {%r33, %r34};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd61, %fd49, %fd55;
	bra.uni 	BB59_8;

BB59_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd59, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd59;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd11, %p4;

BB59_8:
	cvta.to.global.u64 	%rd6, %rd1;
	mul.f64 	%fd56, %fd61, 0d3C695355BAAAFAD3;
	mov.f64 	%fd57, 0d3FDBCB7B1526E50E;
	fma.rn.f64 	%fd58, %fd61, %fd57, %fd56;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd58;

BB59_9:
	ret;
}

	// .globl	vec_log1p
.visible .entry vec_log1p(
	.param .u32 vec_log1p_param_0,
	.param .u64 vec_log1p_param_1,
	.param .u64 vec_log1p_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<40>;
	.reg .f64 	%fd<84>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r12, [vec_log1p_param_0];
	ld.param.u64 	%rd1, [vec_log1p_param_1];
	ld.param.u64 	%rd2, [vec_log1p_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB60_11;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd1;
	}
	setp.lt.u32	%p2, %r22, 1071994197;
	setp.lt.s32	%p3, %r22, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB60_9;
	bra.uni 	BB60_2;

BB60_9:
	add.f64 	%fd58, %fd1, 0d4000000000000000;
	div.rn.f64 	%fd59, %fd1, %fd58;
	mul.f64 	%fd60, %fd1, %fd59;
	neg.f64 	%fd61, %fd60;
	sub.f64 	%fd62, %fd1, %fd60;
	mul.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd65, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3F899999999D70C4;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3FB5555555555462;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	mul.f64 	%fd79, %fd63, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd62, %fd61;
	add.f64 	%fd83, %fd1, %fd80;
	bra.uni 	BB60_10;

BB60_2:
	add.f64 	%fd81, %fd1, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd81;
	}
	mov.u32 	%r38, -1023;
	setp.gt.s32	%p5, %r36, 1048575;
	@%p5 bra 	BB60_4;

	mul.f64 	%fd81, %fd81, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd81;
	}
	mov.u32 	%r38, -1077;

BB60_4:
	add.s32 	%r25, %r36, -1;
	setp.lt.u32	%p6, %r25, 2146435071;
	@%p6 bra 	BB60_6;
	bra.uni 	BB60_5;

BB60_6:
	shr.u32 	%r27, %r36, 20;
	add.s32 	%r39, %r38, %r27;
	and.b32  	%r28, %r36, -2146435073;
	or.b32  	%r29, %r28, 1072693248;
	mov.b64 	%fd82, {%r37, %r29};
	setp.lt.s32	%p8, %r29, 1073127583;
	@%p8 bra 	BB60_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd82;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd82;
	}
	add.s32 	%r32, %r31, -1048576;
	mov.b64 	%fd82, {%r30, %r32};
	add.s32 	%r39, %r39, 1;

BB60_8:
	add.f64 	%fd14, %fd82, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd15, %fd14;
	neg.f64 	%fd16, %fd14;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd16, %fd15, %fd17;
	fma.rn.f64 	%fd19, %fd18, %fd18, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd15, %fd15;
	add.f64 	%fd21, %fd82, 0dBFF0000000000000;
	mul.f64 	%fd22, %fd21, %fd20;
	fma.rn.f64 	%fd23, %fd21, %fd20, %fd22;
	mul.f64 	%fd24, %fd23, %fd23;
	mov.f64 	%fd25, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd26, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F624924923BE72D;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3FB5555555555554;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	sub.f64 	%fd40, %fd21, %fd23;
	add.f64 	%fd41, %fd40, %fd40;
	neg.f64 	%fd42, %fd23;
	fma.rn.f64 	%fd43, %fd42, %fd21, %fd41;
	mul.f64 	%fd44, %fd20, %fd43;
	mul.f64 	%fd45, %fd24, %fd39;
	fma.rn.f64 	%fd46, %fd45, %fd23, %fd44;
	xor.b32  	%r33, %r39, -2147483648;
	mov.u32 	%r34, -2147483648;
	mov.u32 	%r35, 1127219200;
	mov.b64 	%fd47, {%r33, %r35};
	mov.b64 	%fd48, {%r34, %r35};
	sub.f64 	%fd49, %fd47, %fd48;
	mov.f64 	%fd50, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd51, %fd49, %fd50, %fd23;
	neg.f64 	%fd52, %fd49;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	sub.f64 	%fd54, %fd53, %fd23;
	sub.f64 	%fd55, %fd46, %fd54;
	mov.f64 	%fd56, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd57, %fd49, %fd56, %fd55;
	add.f64 	%fd83, %fd51, %fd57;
	bra.uni 	BB60_10;

BB60_5:
	mov.f64 	%fd12, 0d7FF0000000000000;
	fma.rn.f64 	%fd13, %fd81, %fd12, %fd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd81;
	}
	mov.b32 	 %f1, %r26;
	setp.eq.f32	%p7, %f1, 0f00000000;
	selp.f64	%fd83, 0dFFF0000000000000, %fd13, %p7;

BB60_10:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd83;

BB60_11:
	ret;
}

	// .globl	vec_log2
.visible .entry vec_log2(
	.param .u32 vec_log2_param_0,
	.param .u64 vec_log2_param_1,
	.param .u64 vec_log2_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r12, [vec_log2_param_0];
	ld.param.u64 	%rd1, [vec_log2_param_1];
	ld.param.u64 	%rd2, [vec_log2_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB61_9;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd59, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB61_3;

	mul.f64 	%fd59, %fd59, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1077;

BB61_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB61_5;
	bra.uni 	BB61_4;

BB61_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB61_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB61_7:
	add.f64 	%fd12, %fd60, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd13, %fd12;
	neg.f64 	%fd14, %fd12;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd13, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd13, %fd13;
	add.f64 	%fd19, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd45, {%r32, %r34};
	mov.b64 	%fd46, {%r33, %r34};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd61, %fd49, %fd55;
	bra.uni 	BB61_8;

BB61_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd59, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd59;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd11, %p4;

BB61_8:
	cvta.to.global.u64 	%rd6, %rd1;
	mul.f64 	%fd56, %fd61, 0d3C7777D0FFDA0D24;
	mov.f64 	%fd57, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd58, %fd61, %fd57, %fd56;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd58;

BB61_9:
	ret;
}

	// .globl	vec_logb
.visible .entry vec_logb(
	.param .u32 vec_logb_param_0,
	.param .u64 vec_logb_param_1,
	.param .u64 vec_logb_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r3, [vec_logb_param_0];
	ld.param.u64 	%rd1, [vec_logb_param_1];
	ld.param.u64 	%rd2, [vec_logb_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB62_10;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d7FF0000000000000;
	@%p2 bra 	BB62_8;
	bra.uni 	BB62_2;

BB62_8:
	add.f64 	%fd8, %fd1, %fd1;
	bra.uni 	BB62_9;

BB62_2:
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	@%p3 bra 	BB62_3;
	bra.uni 	BB62_4;

BB62_3:
	mov.f64 	%fd8, %fd2;
	bra.uni 	BB62_9;

BB62_4:
	setp.eq.f64	%p4, %fd2, 0d0000000000000000;
	mov.f64 	%fd8, 0dFFF0000000000000;
	@%p4 bra 	BB62_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd2;
	}
	setp.gt.u32	%p5, %r2, 1048575;
	@%p5 bra 	BB62_7;
	bra.uni 	BB62_6;

BB62_7:
	shr.u32 	%r17, %r2, 20;
	add.s32 	%r18, %r17, -1023;
	cvt.rn.f64.s32	%fd8, %r18;
	bra.uni 	BB62_9;

BB62_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd2;
	}
	cvt.u64.u32	%rd6, %r13;
	cvt.u64.u32	%rd7, %r2;
	shl.b64 	%rd8, %rd7, 32;
	or.b64  	%rd9, %rd8, %rd6;
	clz.b64 	%r14, %rd9;
	mov.u32 	%r15, -1011;
	sub.s32 	%r16, %r15, %r14;
	cvt.rn.f64.s32	%fd8, %r16;

BB62_9:
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd12, %rd10, %rd4;
	st.global.f64 	[%rd12], %fd8;

BB62_10:
	ret;
}

	// .globl	vec_log
.visible .entry vec_log(
	.param .u32 vec_log_param_0,
	.param .u64 vec_log_param_1,
	.param .u64 vec_log_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<59>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r12, [vec_log_param_0];
	ld.param.u64 	%rd1, [vec_log_param_1];
	ld.param.u64 	%rd2, [vec_log_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB63_9;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd56, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd56;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB63_3;

	mul.f64 	%fd56, %fd56, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd56;
	}
	mov.u32 	%r37, -1077;

BB63_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB63_5;
	bra.uni 	BB63_4;

BB63_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd57, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB63_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd57;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd57;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd57, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB63_7:
	add.f64 	%fd12, %fd57, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd13, %fd12;
	neg.f64 	%fd14, %fd12;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd13, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd13, %fd13;
	add.f64 	%fd19, %fd57, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd45, {%r32, %r34};
	mov.b64 	%fd46, {%r33, %r34};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd58, %fd49, %fd55;
	bra.uni 	BB63_8;

BB63_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd56, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd56;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd58, 0dFFF0000000000000, %fd11, %p4;

BB63_8:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd58;

BB63_9:
	ret;
}

	// .globl	vec_normcdf
.visible .entry vec_normcdf(
	.param .u32 vec_normcdf_param_0,
	.param .u64 vec_normcdf_param_1,
	.param .u64 vec_normcdf_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<158>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r5, [vec_normcdf_param_0];
	ld.param.u64 	%rd2, [vec_normcdf_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB64_9;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd155, [%rd5];
	abs.f64 	%fd12, %fd155;
	setp.leu.f64	%p2, %fd12, 0d4043400000000000;
	@%p2 bra 	BB64_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd155;
	}
	and.b32  	%r16, %r15, -2147483648;
	mov.f64 	%fd13, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd13;
	}
	and.b32  	%r18, %r17, 2147483647;
	or.b32  	%r19, %r18, %r16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd13;
	}
	mov.b64 	%fd155, {%r20, %r19};

BB64_3:
	mov.f64 	%fd14, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd4, %fd155, %fd14;
	neg.f64 	%fd15, %fd4;
	fma.rn.f64 	%fd16, %fd155, %fd14, %fd15;
	mov.f64 	%fd17, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd5, %fd155, %fd17, %fd16;
	add.rn.f64 	%fd6, %fd4, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd6;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd6;
	}
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB64_5;
	bra.uni 	BB64_4;

BB64_5:
	setp.lt.s32	%p8, %r2, 0;
	mov.u32 	%r21, 0;
	mov.b64 	%fd20, {%r4, %r3};
	add.f64 	%fd21, %fd20, 0dC010000000000000;
	add.f64 	%fd22, %fd20, 0d4010000000000000;
	rcp.approx.ftz.f64 	%fd23, %fd22;
	neg.f64 	%fd24, %fd22;
	mov.f64 	%fd25, 0d3FF0000000000000;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	fma.rn.f64 	%fd27, %fd26, %fd26, %fd26;
	fma.rn.f64 	%fd28, %fd27, %fd23, %fd23;
	mul.f64 	%fd29, %fd21, %fd28;
	add.rn.f64 	%fd30, %fd29, %fd25;
	mov.f64 	%fd31, 0dC010000000000000;
	fma.rn.f64 	%fd32, %fd31, %fd30, %fd20;
	neg.f64 	%fd33, %fd29;
	fma.rn.f64 	%fd34, %fd33, %fd20, %fd32;
	fma.rn.f64 	%fd35, %fd28, %fd34, %fd29;
	mov.f64 	%fd36, 0dBE44E1C6FD03D328;
	mov.f64 	%fd37, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd38, %fd37, %fd35, %fd36;
	mov.f64 	%fd39, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd40, %fd38, %fd35, %fd39;
	mov.f64 	%fd41, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd42, %fd40, %fd35, %fd41;
	mov.f64 	%fd43, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd44, %fd42, %fd35, %fd43;
	mov.f64 	%fd45, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd46, %fd44, %fd35, %fd45;
	mov.f64 	%fd47, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd48, %fd46, %fd35, %fd47;
	mov.f64 	%fd49, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd50, %fd48, %fd35, %fd49;
	mov.f64 	%fd51, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd52, %fd50, %fd35, %fd51;
	mov.f64 	%fd53, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd54, %fd52, %fd35, %fd53;
	mov.f64 	%fd55, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd56, %fd54, %fd35, %fd55;
	mov.f64 	%fd57, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd58, %fd56, %fd35, %fd57;
	mov.f64 	%fd59, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd60, %fd58, %fd35, %fd59;
	mov.f64 	%fd61, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd62, %fd60, %fd35, %fd61;
	mov.f64 	%fd63, 0dBF9096238568E357;
	fma.rn.f64 	%fd64, %fd62, %fd35, %fd63;
	mov.f64 	%fd65, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd66, %fd64, %fd35, %fd65;
	mov.f64 	%fd67, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd68, %fd66, %fd35, %fd67;
	mov.f64 	%fd69, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd70, %fd68, %fd35, %fd69;
	mov.f64 	%fd71, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd72, %fd70, %fd35, %fd71;
	mov.f64 	%fd73, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd74, %fd72, %fd35, %fd73;
	mov.f64 	%fd75, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd76, %fd74, %fd35, %fd75;
	mov.f64 	%fd77, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd78, %fd76, %fd35, %fd77;
	mov.f64 	%fd79, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd80, %fd78, %fd35, %fd79;
	mov.f64 	%fd81, 0d4000000000000000;
	fma.rn.f64 	%fd82, %fd81, %fd20, %fd25;
	rcp.approx.ftz.f64 	%fd83, %fd82;
	neg.f64 	%fd84, %fd82;
	fma.rn.f64 	%fd85, %fd84, %fd83, %fd25;
	fma.rn.f64 	%fd86, %fd85, %fd85, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd83, %fd83;
	mul.f64 	%fd88, %fd80, %fd87;
	mul.f64 	%fd89, %fd88, 0dC000000000000000;
	fma.rn.f64 	%fd90, %fd20, %fd89, %fd80;
	neg.f64 	%fd91, %fd88;
	add.rn.f64 	%fd92, %fd90, %fd91;
	fma.rn.f64 	%fd93, %fd92, %fd87, %fd88;
	mul.f64 	%fd94, %fd20, %fd20;
	neg.f64 	%fd95, %fd94;
	mov.f64 	%fd96, 0d4338000000000000;
	mov.f64 	%fd97, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd98, %fd95, %fd97, %fd96;
	mov.f64 	%fd99, 0dC338000000000000;
	add.rn.f64 	%fd100, %fd98, %fd99;
	mov.f64 	%fd101, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd102, %fd100, %fd101, %fd95;
	mov.f64 	%fd103, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd104, %fd100, %fd103, %fd102;
	mov.f64 	%fd105, 0d3E928AF3FCA213EA;
	mov.f64 	%fd106, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd107, %fd106, %fd104, %fd105;
	mov.f64 	%fd108, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd109, %fd107, %fd104, %fd108;
	mov.f64 	%fd110, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd111, %fd109, %fd104, %fd110;
	mov.f64 	%fd112, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd113, %fd111, %fd104, %fd112;
	mov.f64 	%fd114, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd115, %fd113, %fd104, %fd114;
	mov.f64 	%fd116, 0d3F81111111122322;
	fma.rn.f64 	%fd117, %fd115, %fd104, %fd116;
	mov.f64 	%fd118, 0d3FA55555555502A1;
	fma.rn.f64 	%fd119, %fd117, %fd104, %fd118;
	mov.f64 	%fd120, 0d3FC5555555555511;
	fma.rn.f64 	%fd121, %fd119, %fd104, %fd120;
	mov.f64 	%fd122, 0d3FE000000000000B;
	fma.rn.f64 	%fd123, %fd121, %fd104, %fd122;
	fma.rn.f64 	%fd124, %fd123, %fd104, %fd25;
	fma.rn.f64 	%fd125, %fd124, %fd104, %fd25;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd98;
	}
	shr.u32 	%r23, %r22, 31;
	add.s32 	%r24, %r22, %r23;
	shr.s32 	%r25, %r24, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd125;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd125;
	}
	shl.b32 	%r28, %r25, 20;
	add.s32 	%r29, %r27, %r28;
	mov.b64 	%fd126, {%r26, %r29};
	sub.s32 	%r30, %r22, %r25;
	shl.b32 	%r31, %r30, 20;
	add.s32 	%r32, %r31, 1072693248;
	mov.b64 	%fd127, {%r21, %r32};
	mul.f64 	%fd128, %fd126, %fd127;
	neg.f64 	%fd129, %fd20;
	fma.rn.f64 	%fd130, %fd129, %fd20, %fd94;
	fma.rn.f64 	%fd131, %fd128, %fd130, %fd128;
	mul.f64 	%fd132, %fd93, %fd131;
	setp.gt.u32	%p9, %r3, 1077624832;
	selp.f64	%fd133, 0d0000000000000000, %fd132, %p9;
	sub.f64 	%fd134, %fd81, %fd133;
	selp.f64	%fd156, %fd134, %fd133, %p8;
	bra.uni 	BB64_6;

BB64_4:
	setp.lt.s32	%p4, %r2, 0;
	setp.eq.s32	%p5, %r4, 0;
	setp.eq.s32	%p6, %r3, 2146435072;
	and.pred  	%p7, %p6, %p5;
	selp.f64	%fd18, 0d4000000000000000, 0d0000000000000000, %p4;
	add.f64 	%fd19, %fd6, %fd6;
	selp.f64	%fd156, %fd18, %fd19, %p7;

BB64_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd155;
	}
	setp.lt.u32	%p10, %r33, -1074790399;
	@%p10 bra 	BB64_8;

	mov.f64 	%fd154, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd153, %fd155, %fd154;
	neg.f64 	%fd152, %fd153;
	fma.rn.f64 	%fd151, %fd155, %fd154, %fd152;
	mov.f64 	%fd150, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd149, %fd155, %fd150, %fd151;
	add.rn.f64 	%fd148, %fd153, %fd149;
	mov.f64 	%fd147, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd146, %fd155, %fd147;
	neg.f64 	%fd145, %fd146;
	fma.rn.f64 	%fd144, %fd155, %fd147, %fd145;
	mov.f64 	%fd143, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd142, %fd155, %fd143, %fd144;
	mov.f64 	%fd141, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd140, %fd155, %fd141;
	sub.f64 	%fd135, %fd140, %fd148;
	add.rn.f64 	%fd136, %fd135, %fd142;
	mul.f64 	%fd137, %fd148, 0dC000000000000000;
	mul.f64 	%fd138, %fd137, %fd156;
	fma.rn.f64 	%fd156, %fd138, %fd136, %fd156;

BB64_8:
	mov.u32 	%r43, %tid.y;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r39, %nctaid.x;
	mad.lo.s32 	%r38, %r41, %r42, %r43;
	mov.u32 	%r37, %tid.x;
	mov.u32 	%r36, %ntid.x;
	mad.lo.s32 	%r35, %r38, %r39, %r40;
	mad.lo.s32 	%r34, %r35, %r36, %r37;
	mul.wide.s32 	%rd10, %r34, 8;
	ld.param.u64 	%rd9, [vec_normcdf_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	add.s64 	%rd8, %rd6, %rd10;
	mul.f64 	%fd139, %fd156, 0d3FE0000000000000;
	st.global.f64 	[%rd8], %fd139;

BB64_9:
	ret;
}

	// .globl	vec_normcdfinv
.visible .entry vec_normcdfinv(
	.param .u32 vec_normcdfinv_param_0,
	.param .u64 vec_normcdfinv_param_1,
	.param .u64 vec_normcdfinv_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<66>;
	.reg .f64 	%fd<274>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r13, [vec_normcdfinv_param_0];
	ld.param.u64 	%rd2, [vec_normcdfinv_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB65_15;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd19, [%rd5];
	add.f64 	%fd1, %fd19, %fd19;
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd20, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd20, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB65_13;
	bra.uni 	BB65_2;

BB65_13:
	mul.rn.f64 	%fd173, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd173;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd173;
	}
	shr.u32 	%r45, %r43, 20;
	and.b32  	%r46, %r45, 2046;
	add.s32 	%r47, %r46, 2147482626;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd174, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd175, {%r49, %r48};
	sub.f64 	%fd176, %fd174, %fd175;
	and.b32  	%r50, %r43, -2145386497;
	add.s32 	%r51, %r50, 1071644672;
	mov.b64 	%fd177, {%r44, %r51};
	add.f64 	%fd178, %fd177, 0dBFF0000000000000;
	add.f64 	%fd179, %fd177, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd180, %fd179;
	neg.f64 	%fd181, %fd179;
	mov.f64 	%fd182, 0d3FF0000000000000;
	fma.rn.f64 	%fd183, %fd181, %fd180, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd183, %fd183;
	fma.rn.f64 	%fd185, %fd184, %fd180, %fd180;
	mul.f64 	%fd186, %fd178, %fd185;
	mov.f64 	%fd187, 0dC000000000000000;
	fma.rn.f64 	%fd188, %fd187, %fd186, %fd178;
	neg.f64 	%fd189, %fd186;
	fma.rn.f64 	%fd190, %fd189, %fd178, %fd188;
	fma.rn.f64 	%fd191, %fd190, %fd185, %fd186;
	mul.f64 	%fd192, %fd191, %fd191;
	mov.f64 	%fd193, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd194, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd195, %fd194, %fd192, %fd193;
	mov.f64 	%fd196, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd197, %fd195, %fd192, %fd196;
	mov.f64 	%fd198, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd199, %fd197, %fd192, %fd198;
	mov.f64 	%fd200, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd201, %fd199, %fd192, %fd200;
	mov.f64 	%fd202, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd203, %fd201, %fd192, %fd202;
	mov.f64 	%fd204, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd205, %fd203, %fd192, %fd204;
	mov.f64 	%fd206, 0d3FC249249209112E;
	fma.rn.f64 	%fd207, %fd205, %fd192, %fd206;
	mov.f64 	%fd208, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd209, %fd207, %fd192, %fd208;
	mov.f64 	%fd210, 0d3FD5555555555535;
	fma.rn.f64 	%fd211, %fd209, %fd192, %fd210;
	mul.f64 	%fd212, %fd192, %fd211;
	fma.rn.f64 	%fd213, %fd212, %fd191, %fd191;
	add.f64 	%fd214, %fd213, %fd213;
	mov.f64 	%fd215, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd216, %fd176, %fd215, %fd214;
	mov.f64 	%fd217, 0dC009000000000000;
	sub.f64 	%fd218, %fd217, %fd216;
	mov.f64 	%fd219, 0dBC08DDF93324D327;
	mov.f64 	%fd220, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd221, %fd220, %fd218, %fd219;
	mov.f64 	%fd222, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd223, %fd221, %fd218, %fd222;
	mov.f64 	%fd224, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd225, %fd223, %fd218, %fd224;
	mov.f64 	%fd226, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd227, %fd225, %fd218, %fd226;
	mov.f64 	%fd228, 0d3C782E11898132E0;
	fma.rn.f64 	%fd229, %fd227, %fd218, %fd228;
	mov.f64 	%fd230, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd231, %fd229, %fd218, %fd230;
	mov.f64 	%fd232, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd233, %fd231, %fd218, %fd232;
	mov.f64 	%fd234, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd235, %fd233, %fd218, %fd234;
	mov.f64 	%fd236, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd237, %fd235, %fd218, %fd236;
	mov.f64 	%fd238, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd239, %fd237, %fd218, %fd238;
	mov.f64 	%fd240, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd241, %fd239, %fd218, %fd240;
	mov.f64 	%fd242, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd243, %fd241, %fd218, %fd242;
	mov.f64 	%fd244, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd245, %fd243, %fd218, %fd244;
	mov.f64 	%fd246, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd247, %fd245, %fd218, %fd246;
	mov.f64 	%fd248, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd249, %fd247, %fd218, %fd248;
	mov.f64 	%fd250, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd251, %fd249, %fd218, %fd250;
	mov.f64 	%fd252, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd253, %fd251, %fd218, %fd252;
	mov.f64 	%fd254, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd255, %fd253, %fd218, %fd254;
	mov.f64 	%fd256, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd257, %fd255, %fd218, %fd256;
	mov.f64 	%fd258, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd259, %fd257, %fd218, %fd258;
	mov.f64 	%fd260, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd261, %fd259, %fd218, %fd260;
	mov.f64 	%fd262, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd263, %fd261, %fd218, %fd262;
	fma.rn.f64 	%fd273, %fd263, %fd2, %fd263;
	bra.uni 	BB65_14;

BB65_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd269, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd269;
	}
	mov.b32 	 %f1, %r62;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd269;
	}
	@%p6 bra 	BB65_4;
	bra.uni 	BB65_3;

BB65_4:
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p7, %r62, 1048575;
	@%p7 bra 	BB65_6;

	mul.f64 	%fd269, %fd269, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd269;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd269;
	}
	mov.u32 	%r64, -1077;

BB65_6:
	add.s32 	%r32, %r62, -1;
	setp.lt.u32	%p8, %r32, 2146435071;
	@%p8 bra 	BB65_8;
	bra.uni 	BB65_7;

BB65_8:
	shr.u32 	%r34, %r62, 20;
	add.s32 	%r65, %r64, %r34;
	and.b32  	%r35, %r62, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd270, {%r63, %r36};
	setp.lt.s32	%p10, %r36, 1073127583;
	@%p10 bra 	BB65_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd270;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd270;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd270, {%r37, %r39};
	add.s32 	%r65, %r65, 1;

BB65_10:
	add.f64 	%fd107, %fd270, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd108, %fd107;
	neg.f64 	%fd109, %fd107;
	mov.f64 	%fd110, 0d3FF0000000000000;
	fma.rn.f64 	%fd111, %fd109, %fd108, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd111, %fd111;
	fma.rn.f64 	%fd113, %fd112, %fd108, %fd108;
	add.f64 	%fd114, %fd270, 0dBFF0000000000000;
	mul.f64 	%fd115, %fd114, %fd113;
	fma.rn.f64 	%fd116, %fd114, %fd113, %fd115;
	mul.f64 	%fd117, %fd116, %fd116;
	mov.f64 	%fd118, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd119, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd120, %fd119, %fd117, %fd118;
	mov.f64 	%fd121, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd122, %fd120, %fd117, %fd121;
	mov.f64 	%fd123, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd124, %fd122, %fd117, %fd123;
	mov.f64 	%fd125, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd126, %fd124, %fd117, %fd125;
	mov.f64 	%fd127, 0d3F624924923BE72D;
	fma.rn.f64 	%fd128, %fd126, %fd117, %fd127;
	mov.f64 	%fd129, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd130, %fd128, %fd117, %fd129;
	mov.f64 	%fd131, 0d3FB5555555555554;
	fma.rn.f64 	%fd132, %fd130, %fd117, %fd131;
	sub.f64 	%fd133, %fd114, %fd116;
	add.f64 	%fd134, %fd133, %fd133;
	neg.f64 	%fd135, %fd116;
	fma.rn.f64 	%fd136, %fd135, %fd114, %fd134;
	mul.f64 	%fd137, %fd113, %fd136;
	mul.f64 	%fd138, %fd117, %fd132;
	fma.rn.f64 	%fd139, %fd138, %fd116, %fd137;
	xor.b32  	%r40, %r65, -2147483648;
	mov.u32 	%r41, -2147483648;
	mov.u32 	%r42, 1127219200;
	mov.b64 	%fd140, {%r40, %r42};
	mov.b64 	%fd141, {%r41, %r42};
	sub.f64 	%fd142, %fd140, %fd141;
	mov.f64 	%fd143, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd144, %fd142, %fd143, %fd116;
	neg.f64 	%fd145, %fd142;
	fma.rn.f64 	%fd146, %fd145, %fd143, %fd144;
	sub.f64 	%fd147, %fd146, %fd116;
	sub.f64 	%fd148, %fd139, %fd147;
	mov.f64 	%fd149, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd150, %fd142, %fd149, %fd148;
	add.f64 	%fd271, %fd144, %fd150;
	bra.uni 	BB65_11;

BB65_3:
	shr.u32 	%r23, %r62, 20;
	and.b32  	%r24, %r23, 2046;
	add.s32 	%r25, %r24, 2147482626;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd23, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd24, {%r27, %r26};
	sub.f64 	%fd25, %fd23, %fd24;
	and.b32  	%r28, %r62, -2145386497;
	add.s32 	%r29, %r28, 1071644672;
	mov.b64 	%fd26, {%r63, %r29};
	add.f64 	%fd27, %fd26, 0dBFF0000000000000;
	add.f64 	%fd28, %fd26, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd29, %fd28;
	neg.f64 	%fd30, %fd28;
	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd32, %fd30, %fd29, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd32, %fd32;
	fma.rn.f64 	%fd34, %fd33, %fd29, %fd29;
	mul.f64 	%fd35, %fd27, %fd34;
	mov.f64 	%fd36, 0dC000000000000000;
	fma.rn.f64 	%fd37, %fd36, %fd35, %fd27;
	neg.f64 	%fd38, %fd35;
	fma.rn.f64 	%fd39, %fd38, %fd27, %fd37;
	fma.rn.f64 	%fd40, %fd39, %fd34, %fd35;
	mul.f64 	%fd41, %fd40, %fd40;
	mov.f64 	%fd42, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd43, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd44, %fd43, %fd41, %fd42;
	mov.f64 	%fd45, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd46, %fd44, %fd41, %fd45;
	mov.f64 	%fd47, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd48, %fd46, %fd41, %fd47;
	mov.f64 	%fd49, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd50, %fd48, %fd41, %fd49;
	mov.f64 	%fd51, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd52, %fd50, %fd41, %fd51;
	mov.f64 	%fd53, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd54, %fd52, %fd41, %fd53;
	mov.f64 	%fd55, 0d3FC249249209112E;
	fma.rn.f64 	%fd56, %fd54, %fd41, %fd55;
	mov.f64 	%fd57, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd58, %fd56, %fd41, %fd57;
	mov.f64 	%fd59, 0d3FD5555555555535;
	fma.rn.f64 	%fd60, %fd58, %fd41, %fd59;
	mul.f64 	%fd61, %fd41, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd40, %fd40;
	add.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd65, %fd25, %fd64, %fd63;
	neg.f64 	%fd22, %fd65;
	// inline asm
	rsqrt.approx.ftz.f64 %fd21, %fd22;
	// inline asm
	mul.rn.f64 	%fd66, %fd21, %fd21;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd68, %fd22, %fd67, %fd31;
	mov.f64 	%fd69, 0d3FE0000000000000;
	mov.f64 	%fd70, 0d3FD8000000000000;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	mul.rn.f64 	%fd72, %fd68, %fd21;
	fma.rn.f64 	%fd73, %fd71, %fd72, %fd21;
	mov.f64 	%fd74, 0d4000A0E7333839AA;
	mov.f64 	%fd75, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd76, %fd75, %fd73, %fd74;
	mov.f64 	%fd77, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd78, %fd76, %fd73, %fd77;
	mov.f64 	%fd79, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd80, %fd78, %fd73, %fd79;
	mov.f64 	%fd81, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd82, %fd80, %fd73, %fd81;
	mov.f64 	%fd83, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd84, %fd82, %fd73, %fd83;
	mov.f64 	%fd85, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd86, %fd84, %fd73, %fd85;
	mov.f64 	%fd87, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd88, %fd86, %fd73, %fd87;
	add.f64 	%fd89, %fd73, 0d4001E90FF51C2197;
	mov.f64 	%fd90, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd91, %fd89, %fd73, %fd90;
	mov.f64 	%fd92, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd93, %fd91, %fd73, %fd92;
	mov.f64 	%fd94, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd95, %fd93, %fd73, %fd94;
	mov.f64 	%fd96, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd97, %fd95, %fd73, %fd96;
	mov.f64 	%fd98, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd99, %fd97, %fd73, %fd98;
	mov.f64 	%fd100, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd101, %fd99, %fd73, %fd100;
	mov.f64 	%fd102, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd103, %fd101, %fd73, %fd102;
	mul.f64 	%fd104, %fd73, %fd103;
	div.rn.f64 	%fd272, %fd88, %fd104;
	bra.uni 	BB65_12;

BB65_7:
	mov.f64 	%fd105, 0d7FF0000000000000;
	fma.rn.f64 	%fd106, %fd269, %fd105, %fd105;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd269;
	}
	mov.b32 	 %f2, %r33;
	setp.eq.f32	%p9, %f2, 0f00000000;
	selp.f64	%fd271, 0dFFF0000000000000, %fd106, %p9;

BB65_11:
	neg.f64 	%fd151, %fd271;
	rsqrt.approx.f64 	%fd152, %fd151;
	mov.f64 	%fd153, 0d3FFA2013964E259C;
	mov.f64 	%fd154, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd155, %fd154, %fd152, %fd153;
	mov.f64 	%fd156, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd157, %fd155, %fd152, %fd156;
	mov.f64 	%fd158, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd159, %fd157, %fd152, %fd158;
	mov.f64 	%fd160, 0d3F311BD487102E94;
	fma.rn.f64 	%fd161, %fd159, %fd152, %fd160;
	add.f64 	%fd162, %fd152, 0d3FF59895C30BAA54;
	mov.f64 	%fd163, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd164, %fd162, %fd152, %fd163;
	mov.f64 	%fd165, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd166, %fd164, %fd152, %fd165;
	mov.f64 	%fd167, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd168, %fd166, %fd152, %fd167;
	mov.f64 	%fd169, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd170, %fd168, %fd152, %fd169;
	mul.f64 	%fd171, %fd152, %fd170;
	div.rn.f64 	%fd272, %fd161, %fd171;

BB65_12:
	neg.f64 	%fd172, %fd272;
	selp.f64	%fd273, %fd172, %fd272, %p5;

BB65_14:
	mov.u32 	%r61, %tid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r58, %ctaid.x;
	mov.u32 	%r57, %nctaid.x;
	mad.lo.s32 	%r56, %r59, %r60, %r61;
	mov.u32 	%r55, %tid.x;
	mov.u32 	%r54, %ntid.x;
	mad.lo.s32 	%r53, %r56, %r57, %r58;
	mad.lo.s32 	%r52, %r53, %r54, %r55;
	mul.wide.s32 	%rd10, %r52, 8;
	ld.param.u64 	%rd9, [vec_normcdfinv_param_1];
	cvta.to.global.u64 	%rd6, %rd9;
	mul.f64 	%fd264, %fd273, 0dBCA21165F626CDD5;
	mov.f64 	%fd265, 0dBFF6A09E667F3BCC;
	fma.rn.f64 	%fd266, %fd265, %fd273, %fd264;
	mov.f64 	%fd267, 0d0000000000000000;
	add.rn.f64 	%fd268, %fd266, %fd267;
	add.s64 	%rd8, %rd6, %rd10;
	st.global.f64 	[%rd8], %fd268;

BB65_15:
	ret;
}

	// .globl	vec_rcbrt
.visible .entry vec_rcbrt(
	.param .u32 vec_rcbrt_param_0,
	.param .u64 vec_rcbrt_param_1,
	.param .u64 vec_rcbrt_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<31>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r13, [vec_rcbrt_param_0];
	ld.param.u64 	%rd1, [vec_rcbrt_param_1];
	ld.param.u64 	%rd2, [vec_rcbrt_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB66_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r39, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r39, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB66_3;
	bra.uni 	BB66_2;

BB66_3:
	shr.u32 	%r40, %r39, 20;
	mov.u32 	%r41, 0;
	setp.ne.s32	%p6, %r40, 0;
	@%p6 bra 	BB66_5;

	mov.b64 	%fd8, {%r38, %r39};
	mul.f64 	%fd9, %fd8, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd9;
	}
	shr.u32 	%r40, %r39, 20;
	mov.u32 	%r41, 18;

BB66_5:
	add.s32 	%r26, %r40, -1022;
	cvt.rn.f32.s32	%f1, %r26;
	mul.f32 	%f2, %f1, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r27, %f2;
	mad.lo.s32 	%r28, %r27, -3145728, %r39;
	mov.b64 	%fd10, {%r38, %r28};
	cvt.rn.f32.f64	%f3, %fd10;
	lg2.approx.ftz.f32 	%f4, %f3;
	mul.f32 	%f5, %f4, 0fBEAAAAAB;
	ex2.approx.ftz.f32 	%f6, %f5;
	cvt.f64.f32	%fd11, %f6;
	mul.rn.f64 	%fd12, %fd11, %fd11;
	mul.rn.f64 	%fd13, %fd10, %fd11;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd12, %fd14, %fd15;
	fma.rn.f64 	%fd17, %fd10, %fd11, %fd14;
	neg.f64 	%fd18, %fd17;
	fma.rn.f64 	%fd19, %fd12, %fd18, %fd16;
	neg.f64 	%fd20, %fd12;
	fma.rn.f64 	%fd21, %fd11, %fd11, %fd20;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd13, %fd22, %fd19;
	mov.f64 	%fd24, 0d3FD5555555555555;
	mov.f64 	%fd25, 0d3FCC71C71C71C71C;
	fma.rn.f64 	%fd26, %fd25, %fd23, %fd24;
	mul.rn.f64 	%fd27, %fd23, %fd11;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd28;
	}
	sub.s32 	%r31, %r41, %r27;
	shl.b32 	%r32, %r31, 20;
	add.s32 	%r33, %r30, %r32;
	mov.b64 	%fd30, {%r29, %r33};
	bra.uni 	BB66_6;

BB66_2:
	xor.b32  	%r23, %r39, 2146435072;
	mov.b64 	%fd5, {%r38, %r23};
	abs.f64 	%fd6, %fd1;
	setp.gtu.f64	%p5, %fd6, 0d7FF0000000000000;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd30, %fd7, %fd5, %p5;

BB66_6:
	cvta.to.global.u64 	%rd6, %rd1;
	and.b32  	%r34, %r3, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd30;
	}
	or.b32  	%r36, %r35, %r34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd30;
	}
	mov.b64 	%fd29, {%r37, %r36};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd29;

BB66_7:
	ret;
}

	// .globl	vec_rint
.visible .entry vec_rint(
	.param .u32 vec_rint_param_0,
	.param .u64 vec_rint_param_1,
	.param .u64 vec_rint_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_rint_param_0];
	ld.param.u64 	%rd1, [vec_rint_param_1];
	ld.param.u64 	%rd2, [vec_rint_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB67_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rni.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB67_2:
	ret;
}

	// .globl	vec_round
.visible .entry vec_round(
	.param .u32 vec_round_param_0,
	.param .u64 vec_round_param_1,
	.param .u64 vec_round_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_round_param_0];
	ld.param.u64 	%rd1, [vec_round_param_1];
	ld.param.u64 	%rd2, [vec_round_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB68_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd8, [%rd5];
	abs.f64 	%fd2, %fd8;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB68_3;

	add.f64 	%fd5, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd8;
	}
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r13, %r15;
	mov.b64 	%fd8, {%r12, %r16};

BB68_3:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd8;

BB68_4:
	ret;
}

	// .globl	vec_rsqrt
.visible .entry vec_rsqrt(
	.param .u32 vec_rsqrt_param_0,
	.param .u64 vec_rsqrt_param_1,
	.param .u64 vec_rsqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_rsqrt_param_0];
	ld.param.u64 	%rd1, [vec_rsqrt_param_1];
	ld.param.u64 	%rd2, [vec_rsqrt_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB69_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	rsqrt.approx.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB69_2:
	ret;
}

	// .globl	vec_sin
.visible .entry vec_sin(
	.param .u32 vec_sin_param_0,
	.param .u64 vec_sin_param_1,
	.param .u64 vec_sin_param_2
)
{
	.local .align 4 .b8 	__local_depot70[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<15>;


	mov.u64 	%SPL, __local_depot70;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r5, [vec_sin_param_0];
	ld.param.u64 	%rd2, [vec_sin_param_1];
	ld.param.u64 	%rd3, [vec_sin_param_2];
	add.u64 	%rd4, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB70_11;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd38, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd38;
	}
	and.b32  	%r16, %r15, 2147483647;
	setp.ne.s32	%p2, %r16, 2146435072;
	@%p2 bra 	BB70_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd38;
	}
	setp.ne.s32	%p3, %r17, 0;
	@%p3 bra 	BB70_4;

	mov.f64 	%fd14, 0d0000000000000000;
	mul.rn.f64 	%fd38, %fd38, %fd14;

BB70_4:
	mul.f64 	%fd15, %fd38, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r24, %fd15;
	st.local.u32 	[%rd1], %r24;
	cvt.rn.f64.s32	%fd16, %r24;
	neg.f64 	%fd17, %fd16;
	mov.f64 	%fd18, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd19, %fd17, %fd18, %fd38;
	mov.f64 	%fd20, 0d3C91A62633145C00;
	fma.rn.f64 	%fd21, %fd17, %fd20, %fd19;
	mov.f64 	%fd22, 0d397B839A252049C0;
	fma.rn.f64 	%fd39, %fd17, %fd22, %fd21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd38;
	}
	and.b32  	%r19, %r18, 2145386496;
	setp.lt.u32	%p4, %r19, 1105199104;
	@%p4 bra 	BB70_6;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd38;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd39, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r24, [%rd1];

BB70_6:
	and.b32  	%r20, %r24, 1;
	shl.b32 	%r21, %r20, 3;
	setp.eq.s32	%p5, %r20, 0;
	selp.f64	%fd23, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r22, %r21, 1;
	mul.wide.s32 	%rd9, %r22, 8;
	mov.u64 	%rd10, __cudart_sin_cos_coeffs;
	add.s64 	%rd11, %rd10, %rd9;
	ld.const.f64 	%fd24, [%rd11];
	mul.rn.f64 	%fd7, %fd39, %fd39;
	fma.rn.f64 	%fd25, %fd23, %fd7, %fd24;
	ld.const.f64 	%fd26, [%rd11+8];
	fma.rn.f64 	%fd27, %fd25, %fd7, %fd26;
	ld.const.f64 	%fd28, [%rd11+16];
	fma.rn.f64 	%fd29, %fd27, %fd7, %fd28;
	ld.const.f64 	%fd30, [%rd11+24];
	fma.rn.f64 	%fd31, %fd29, %fd7, %fd30;
	ld.const.f64 	%fd32, [%rd11+32];
	fma.rn.f64 	%fd33, %fd31, %fd7, %fd32;
	ld.const.f64 	%fd34, [%rd11+40];
	fma.rn.f64 	%fd8, %fd33, %fd7, %fd34;
	fma.rn.f64 	%fd40, %fd8, %fd39, %fd39;
	@%p5 bra 	BB70_8;

	mov.f64 	%fd35, 0d3FF0000000000000;
	fma.rn.f64 	%fd40, %fd8, %fd7, %fd35;

BB70_8:
	and.b32  	%r23, %r24, 2;
	setp.eq.s32	%p6, %r23, 0;
	@%p6 bra 	BB70_10;

	mov.f64 	%fd36, 0d0000000000000000;
	mov.f64 	%fd37, 0dBFF0000000000000;
	fma.rn.f64 	%fd40, %fd40, %fd37, %fd36;

BB70_10:
	cvta.to.global.u64 	%rd12, %rd2;
	add.s64 	%rd14, %rd12, %rd6;
	st.global.f64 	[%rd14], %fd40;

BB70_11:
	ret;
}

	// .globl	vec_sinh
.visible .entry vec_sinh(
	.param .u32 vec_sinh_param_0,
	.param .u64 vec_sinh_param_1,
	.param .u64 vec_sinh_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<68>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r3, [vec_sinh_param_0];
	ld.param.u64 	%rd1, [vec_sinh_param_1];
	ld.param.u64 	%rd2, [vec_sinh_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB71_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd5, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd5;
	}
	and.b32  	%r13, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd5;
	}
	mov.b64 	%fd1, {%r14, %r13};
	setp.lt.u32	%p2, %r13, 1072693248;
	@%p2 bra 	BB71_3;
	bra.uni 	BB71_2;

BB71_3:
	mul.f64 	%fd51, %fd1, %fd1;
	mov.f64 	%fd52, 0d3DE611A561D87DEF;
	mov.f64 	%fd53, 0d3D6B4C75AB274C53;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3E5AE64671B18F5C;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0d3EC71DE3A465B1E4;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F2A01A01A02899D;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0d3F811111111110A6;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3FC5555555555556;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mul.f64 	%fd65, %fd51, %fd64;
	fma.rn.f64 	%fd67, %fd65, %fd1, %fd1;
	bra.uni 	BB71_4;

BB71_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd1;
	}
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd8;
	}
	add.s32 	%r17, %r16, -1;
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	add.s32 	%r18, %r15, %r15;
	setp.lt.u32	%p3, %r18, 2142496327;
	selp.b32	%r19, 0, %r17, %p3;
	mov.u32 	%r20, 0;
	selp.f64	%fd15, %fd1, %fd14, %p3;
	mov.f64 	%fd16, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd17, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd18, %fd17, %fd15, %fd16;
	mov.f64 	%fd19, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd20, %fd18, %fd15, %fd19;
	mov.f64 	%fd21, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd22, %fd20, %fd15, %fd21;
	mov.f64 	%fd23, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd24, %fd22, %fd15, %fd23;
	mov.f64 	%fd25, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd26, %fd24, %fd15, %fd25;
	mov.f64 	%fd27, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd28, %fd26, %fd15, %fd27;
	mov.f64 	%fd29, 0d3F8111111110F74D;
	fma.rn.f64 	%fd30, %fd28, %fd15, %fd29;
	mov.f64 	%fd31, 0d3FA555555555554D;
	fma.rn.f64 	%fd32, %fd30, %fd15, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555557;
	fma.rn.f64 	%fd34, %fd32, %fd15, %fd33;
	mov.f64 	%fd35, 0d3FE0000000000000;
	fma.rn.f64 	%fd36, %fd34, %fd15, %fd35;
	mul.f64 	%fd37, %fd15, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd15, %fd15;
	setp.eq.s32	%p4, %r19, 1024;
	selp.b32	%r21, -1, 0, %p4;
	add.s32 	%r22, %r21, %r19;
	shl.b32 	%r23, %r22, 20;
	add.s32 	%r24, %r23, 1072693248;
	mov.b64 	%fd39, {%r20, %r24};
	mov.u32 	%r25, 1071644672;
	mov.b64 	%fd40, {%r20, %r25};
	sub.f64 	%fd41, %fd39, %fd40;
	fma.rn.f64 	%fd42, %fd38, %fd39, %fd41;
	add.f64 	%fd43, %fd42, %fd42;
	selp.f64	%fd44, %fd43, %fd42, %p4;
	setp.eq.s32	%p5, %r18, 0;
	selp.f64	%fd45, %fd15, %fd44, %p5;
	mov.f64 	%fd46, 0d3FF0000000000000;
	mov.f64 	%fd47, 0d4000000000000000;
	fma.rn.f64 	%fd48, %fd47, %fd45, %fd46;
	div.rn.f64 	%fd49, %fd45, %fd48;
	add.f64 	%fd50, %fd49, %fd45;
	setp.ltu.f64	%p6, %fd1, 0d408633CE8FB9F87E;
	selp.f64	%fd67, %fd50, 0d7FF0000000000000, %p6;

BB71_4:
	cvta.to.global.u64 	%rd6, %rd1;
	and.b32  	%r26, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd67;
	}
	or.b32  	%r28, %r27, %r26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd67;
	}
	mov.b64 	%fd66, {%r29, %r28};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd66;

BB71_5:
	ret;
}

	// .globl	vec_sinpi
.visible .entry vec_sinpi(
	.param .u32 vec_sinpi_param_0,
	.param .u64 vec_sinpi_param_1,
	.param .u64 vec_sinpi_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<39>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r3, [vec_sinpi_param_0];
	ld.param.u64 	%rd1, [vec_sinpi_param_1];
	ld.param.u64 	%rd2, [vec_sinpi_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB72_8;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	add.s32 	%r15, %r14, 1048576;
	mov.b64 	%fd11, {%r13, %r15};
	cvt.rni.f64.f64	%fd12, %fd11;
	cvt.rzi.s64.f64	%rd6, %fd12;
	cvt.u32.u64	%r2, %rd6;
	neg.f64 	%fd13, %fd12;
	mov.f64 	%fd14, 0d3FE0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd1;
	mul.f64 	%fd16, %fd15, 0d3CA1A62633145C07;
	mov.f64 	%fd17, 0d400921FB54442D18;
	fma.rn.f64 	%fd18, %fd15, %fd17, %fd16;
	and.b64  	%rd7, %rd6, 1;
	mul.rn.f64 	%fd2, %fd18, %fd18;
	setp.eq.b64	%p2, %rd7, 1;
	not.pred 	%p3, %p2;
	selp.f64	%fd19, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p3;
	shl.b64 	%rd8, %rd7, 6;
	mov.u64 	%rd9, __cudart_sin_cos_coeffs;
	add.s64 	%rd10, %rd8, %rd9;
	ld.const.f64 	%fd20, [%rd10+8];
	fma.rn.f64 	%fd21, %fd19, %fd2, %fd20;
	ld.const.f64 	%fd22, [%rd10+16];
	fma.rn.f64 	%fd23, %fd21, %fd2, %fd22;
	ld.const.f64 	%fd24, [%rd10+24];
	fma.rn.f64 	%fd25, %fd23, %fd2, %fd24;
	ld.const.f64 	%fd26, [%rd10+32];
	fma.rn.f64 	%fd27, %fd25, %fd2, %fd26;
	ld.const.f64 	%fd28, [%rd10+40];
	fma.rn.f64 	%fd29, %fd27, %fd2, %fd28;
	ld.const.f64 	%fd30, [%rd10+48];
	fma.rn.f64 	%fd3, %fd29, %fd2, %fd30;
	fma.rn.f64 	%fd38, %fd3, %fd18, %fd18;
	@%p3 bra 	BB72_3;

	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd3, %fd2, %fd31;

BB72_3:
	and.b32  	%r16, %r2, 2;
	setp.eq.s32	%p4, %r16, 0;
	@%p4 bra 	BB72_5;

	mov.f64 	%fd32, 0d0000000000000000;
	mov.f64 	%fd33, 0dBFF0000000000000;
	fma.rn.f64 	%fd38, %fd38, %fd33, %fd32;

BB72_5:
	cvt.rzi.f64.f64	%fd34, %fd1;
	setp.neu.f64	%p5, %fd34, %fd1;
	@%p5 bra 	BB72_7;

	mov.f64 	%fd35, 0d0000000000000000;
	mul.rn.f64 	%fd38, %fd1, %fd35;

BB72_7:
	cvta.to.global.u64 	%rd11, %rd1;
	add.s64 	%rd13, %rd11, %rd4;
	st.global.f64 	[%rd13], %fd38;

BB72_8:
	ret;
}

	// .globl	vec_sqrt
.visible .entry vec_sqrt(
	.param .u32 vec_sqrt_param_0,
	.param .u64 vec_sqrt_param_1,
	.param .u64 vec_sqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_sqrt_param_0];
	ld.param.u64 	%rd1, [vec_sqrt_param_1];
	ld.param.u64 	%rd2, [vec_sqrt_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB73_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	sqrt.rn.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB73_2:
	ret;
}

	// .globl	vec_tan
.visible .entry vec_tan(
	.param .u32 vec_tan_param_0,
	.param .u64 vec_tan_param_1,
	.param .u64 vec_tan_param_2
)
{
	.local .align 4 .b8 	__local_depot74[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<12>;


	mov.u64 	%SPL, __local_depot74;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r5, [vec_tan_param_0];
	ld.param.u64 	%rd2, [vec_tan_param_1];
	ld.param.u64 	%rd3, [vec_tan_param_2];
	add.u64 	%rd4, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB74_9;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd62, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd62;
	}
	and.b32  	%r16, %r15, 2147483647;
	setp.ne.s32	%p2, %r16, 2146435072;
	@%p2 bra 	BB74_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd62;
	}
	setp.ne.s32	%p3, %r17, 0;
	@%p3 bra 	BB74_4;

	mov.f64 	%fd11, 0d0000000000000000;
	mul.rn.f64 	%fd62, %fd62, %fd11;

BB74_4:
	mul.f64 	%fd12, %fd62, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r21, %fd12;
	st.local.u32 	[%rd1], %r21;
	cvt.rn.f64.s32	%fd13, %r21;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd62;
	mov.f64 	%fd17, 0d3C91A62633145C00;
	fma.rn.f64 	%fd18, %fd14, %fd17, %fd16;
	mov.f64 	%fd19, 0d397B839A252049C0;
	fma.rn.f64 	%fd63, %fd14, %fd19, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd62;
	}
	and.b32  	%r19, %r18, 2145386496;
	setp.lt.u32	%p4, %r19, 1105199104;
	@%p4 bra 	BB74_6;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd62;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd63, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r21, [%rd1];

BB74_6:
	mul.f64 	%fd20, %fd63, %fd63;
	mov.f64 	%fd21, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd22, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd23, %fd22, %fd20, %fd21;
	mov.f64 	%fd24, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd25, %fd23, %fd20, %fd24;
	mov.f64 	%fd26, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd27, %fd25, %fd20, %fd26;
	mov.f64 	%fd28, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd29, %fd27, %fd20, %fd28;
	mov.f64 	%fd30, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd31, %fd29, %fd20, %fd30;
	mov.f64 	%fd32, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd33, %fd31, %fd20, %fd32;
	mov.f64 	%fd34, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd35, %fd33, %fd20, %fd34;
	mov.f64 	%fd36, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd37, %fd35, %fd20, %fd36;
	mov.f64 	%fd38, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd39, %fd37, %fd20, %fd38;
	mov.f64 	%fd40, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd41, %fd39, %fd20, %fd40;
	mov.f64 	%fd42, 0d3F9664F485D25660;
	fma.rn.f64 	%fd43, %fd41, %fd20, %fd42;
	mov.f64 	%fd44, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd45, %fd43, %fd20, %fd44;
	mov.f64 	%fd46, 0d3FC11111111105D2;
	fma.rn.f64 	%fd47, %fd45, %fd20, %fd46;
	mov.f64 	%fd48, 0d3FD555555555555E;
	fma.rn.f64 	%fd49, %fd47, %fd20, %fd48;
	mul.f64 	%fd7, %fd20, %fd49;
	fma.rn.f64 	%fd64, %fd7, %fd63, %fd63;
	and.b32  	%r20, %r21, 1;
	setp.eq.b32	%p5, %r20, 1;
	@!%p5 bra 	BB74_8;
	bra.uni 	BB74_7;

BB74_7:
	sub.f64 	%fd50, %fd64, %fd63;
	neg.f64 	%fd51, %fd50;
	fma.rn.f64 	%fd52, %fd7, %fd63, %fd51;
	neg.f64 	%fd53, %fd64;
	rcp.approx.ftz.f64 	%fd54, %fd64;
	mov.f64 	%fd55, 0d3FF0000000000000;
	fma.rn.f64 	%fd56, %fd53, %fd54, %fd55;
	fma.rn.f64 	%fd57, %fd56, %fd56, %fd56;
	fma.rn.f64 	%fd58, %fd57, %fd54, %fd54;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd64, %fd59, %fd55;
	fma.rn.f64 	%fd61, %fd59, %fd52, %fd60;
	fma.rn.f64 	%fd64, %fd61, %fd59, %fd59;

BB74_8:
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd11, %rd9, %rd6;
	st.global.f64 	[%rd11], %fd64;

BB74_9:
	ret;
}

	// .globl	vec_tanh
.visible .entry vec_tanh(
	.param .u32 vec_tanh_param_0,
	.param .u64 vec_tanh_param_1,
	.param .u64 vec_tanh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<74>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r4, [vec_tanh_param_0];
	ld.param.u64 	%rd1, [vec_tanh_param_1];
	ld.param.u64 	%rd2, [vec_tanh_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB75_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r14, %r3};
	setp.ltu.f64	%p2, %fd2, 0d3FE1C7A398201CD6;
	@%p2 bra 	BB75_3;
	bra.uni 	BB75_2;

BB75_3:
	mul.f64 	%fd51, %fd1, %fd1;
	mov.f64 	%fd52, 0dBF2B9093D89F0E23;
	mov.f64 	%fd53, 0d3F0ABFFC9B5786C4;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3F42FA2744C30B61;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0dBF57CF3B9C1E491D;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F6D6C61D450119A;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0dBF8226DDD44294F5;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3F9664F45C2B04A6;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mov.f64 	%fd65, 0dBFABA1BA1AD70754;
	fma.rn.f64 	%fd66, %fd64, %fd51, %fd65;
	mov.f64 	%fd67, 0d3FC111111110295E;
	fma.rn.f64 	%fd68, %fd66, %fd51, %fd67;
	mov.f64 	%fd69, 0dBFD555555555549F;
	fma.rn.f64 	%fd70, %fd68, %fd51, %fd69;
	mul.f64 	%fd71, %fd51, %fd70;
	fma.rn.f64 	%fd73, %fd71, %fd1, %fd1;
	bra.uni 	BB75_4;

BB75_2:
	add.f64 	%fd6, %fd2, %fd2;
	mov.f64 	%fd7, 0d4338000000000000;
	mov.f64 	%fd8, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd9, %fd6, %fd8, %fd7;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd9;
	}
	mov.f64 	%fd10, 0dC338000000000000;
	add.rn.f64 	%fd11, %fd9, %fd10;
	mov.f64 	%fd12, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd6;
	mov.f64 	%fd14, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd15, %fd11, %fd14, %fd13;
	mov.f64 	%fd16, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd17, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd18, %fd17, %fd15, %fd16;
	mov.f64 	%fd19, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd20, %fd18, %fd15, %fd19;
	mov.f64 	%fd21, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd22, %fd20, %fd15, %fd21;
	mov.f64 	%fd23, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd24, %fd22, %fd15, %fd23;
	mov.f64 	%fd25, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd26, %fd24, %fd15, %fd25;
	mov.f64 	%fd27, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd28, %fd26, %fd15, %fd27;
	mov.f64 	%fd29, 0d3F8111111110F74D;
	fma.rn.f64 	%fd30, %fd28, %fd15, %fd29;
	mov.f64 	%fd31, 0d3FA555555555554D;
	fma.rn.f64 	%fd32, %fd30, %fd15, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555557;
	fma.rn.f64 	%fd34, %fd32, %fd15, %fd33;
	mov.f64 	%fd35, 0d3FE0000000000000;
	fma.rn.f64 	%fd36, %fd34, %fd15, %fd35;
	mul.f64 	%fd37, %fd15, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd15, %fd15;
	shl.b32 	%r16, %r15, 20;
	add.s32 	%r17, %r16, 1072693248;
	mov.u32 	%r18, 0;
	mov.b64 	%fd39, {%r18, %r17};
	fma.rn.f64 	%fd40, %fd38, %fd39, %fd39;
	add.f64 	%fd41, %fd40, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd42, %fd41;
	neg.f64 	%fd43, %fd41;
	mov.f64 	%fd44, 0d3FF0000000000000;
	fma.rn.f64 	%fd45, %fd43, %fd42, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd42, %fd42;
	neg.f64 	%fd48, %fd47;
	mov.f64 	%fd49, 0d4000000000000000;
	fma.rn.f64 	%fd50, %fd49, %fd48, %fd44;
	setp.gt.u32	%p3, %r3, 1077936127;
	selp.f64	%fd73, 0d3FF0000000000000, %fd50, %p3;

BB75_4:
	cvta.to.global.u64 	%rd6, %rd1;
	and.b32  	%r19, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd73;
	}
	or.b32  	%r21, %r20, %r19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd73;
	}
	mov.b64 	%fd72, {%r22, %r21};
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f64 	[%rd8], %fd72;

BB75_5:
	ret;
}

	// .globl	vec_tgamma
.visible .entry vec_tgamma(
	.param .u32 vec_tgamma_param_0,
	.param .u64 vec_tgamma_param_1,
	.param .u64 vec_tgamma_param_2
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<98>;
	.reg .f64 	%fd<421>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r24, [vec_tgamma_param_0];
	ld.param.u64 	%rd2, [vec_tgamma_param_2];
	mov.u32 	%r25, %tid.x;
	mov.u32 	%r26, %ntid.y;
	mov.u32 	%r27, %ctaid.y;
	mov.u32 	%r28, %tid.y;
	mad.lo.s32 	%r29, %r26, %r27, %r28;
	mov.u32 	%r30, %nctaid.x;
	mov.u32 	%r31, %ctaid.x;
	mad.lo.s32 	%r32, %r29, %r30, %r31;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r34, %r32, %r33, %r25;
	setp.ge.s32	%p1, %r34, %r24;
	@%p1 bra 	BB76_45;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r34, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	@%p2 bra 	BB76_18;
	bra.uni 	BB76_2;

BB76_18:
	setp.lt.f64	%p15, %fd1, 0d0000000000000000;
	@%p15 bra 	BB76_20;
	bra.uni 	BB76_19;

BB76_20:
	cvt.rzi.f64.f64	%fd265, %fd1;
	setp.eq.f64	%p16, %fd265, %fd1;
	mov.f64 	%fd420, 0dFFF8000000000000;
	@%p16 bra 	BB76_44;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd1;
	}
	setp.lt.u32	%p17, %r17, -1072693248;
	@%p17 bra 	BB76_34;
	bra.uni 	BB76_22;

BB76_34:
	setp.lt.u32	%p26, %r17, -1072955392;
	@%p26 bra 	BB76_35;

	fma.rn.f64 	%fd414, %fd1, %fd1, %fd1;
	add.f64 	%fd1, %fd1, 0d3FF0000000000000;
	bra.uni 	BB76_37;

BB76_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd1;
	}
	setp.lt.s32	%p3, %r94, 1074790400;
	@%p3 bra 	BB76_11;
	bra.uni 	BB76_3;

BB76_11:
	mov.f64 	%fd404, 0d3FF0000000000000;
	setp.lt.s32	%p10, %r94, 1074528256;
	mov.f64 	%fd405, %fd1;
	@%p10 bra 	BB76_13;

	mov.f64 	%fd225, 0dBFF0000000000000;
	mov.f64 	%fd226, 0d3FF0000000000000;
	fma.rn.f64 	%fd404, %fd1, %fd226, %fd225;
	add.f64 	%fd405, %fd1, 0dBFF0000000000000;

BB76_13:
	setp.lt.s32	%p11, %r94, 1074003968;
	@%p11 bra 	BB76_15;

	neg.f64 	%fd227, %fd404;
	fma.rn.f64 	%fd404, %fd405, %fd404, %fd227;
	add.f64 	%fd405, %fd405, 0dBFF0000000000000;

BB76_15:
	setp.lt.s32	%p12, %r94, 1073217536;
	@%p12 bra 	BB76_17;

	neg.f64 	%fd228, %fd404;
	fma.rn.f64 	%fd404, %fd405, %fd404, %fd228;
	add.f64 	%fd405, %fd405, 0dBFF0000000000000;

BB76_17:
	add.f64 	%fd229, %fd405, 0dBFF0000000000000;
	setp.gt.s32	%p13, %r94, 1071644671;
	selp.f64	%fd230, %fd229, %fd405, %p13;
	mov.f64 	%fd231, 0dBE8AF7049AE8A594;
	mov.f64 	%fd232, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd233, %fd232, %fd230, %fd231;
	mov.f64 	%fd234, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd235, %fd233, %fd230, %fd234;
	mov.f64 	%fd236, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd237, %fd235, %fd230, %fd236;
	mov.f64 	%fd238, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd239, %fd237, %fd230, %fd238;
	mov.f64 	%fd240, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd241, %fd239, %fd230, %fd240;
	mov.f64 	%fd242, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd243, %fd241, %fd230, %fd242;
	mov.f64 	%fd244, 0dBF531711365A3E26;
	fma.rn.f64 	%fd245, %fd243, %fd230, %fd244;
	mov.f64 	%fd246, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd247, %fd245, %fd230, %fd246;
	mov.f64 	%fd248, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd249, %fd247, %fd230, %fd248;
	mov.f64 	%fd250, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd251, %fd249, %fd230, %fd250;
	mov.f64 	%fd252, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd253, %fd251, %fd230, %fd252;
	mov.f64 	%fd254, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd255, %fd253, %fd230, %fd254;
	mov.f64 	%fd256, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd257, %fd255, %fd230, %fd256;
	mov.f64 	%fd258, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd259, %fd257, %fd230, %fd258;
	mov.f64 	%fd260, 0d3FF0000000000000;
	fma.rn.f64 	%fd261, %fd259, %fd230, %fd260;
	mul.f64 	%fd262, %fd1, %fd261;
	setp.lt.s32	%p14, %r94, 1071644672;
	selp.f64	%fd263, %fd262, %fd261, %p14;
	div.rn.f64 	%fd420, %fd404, %fd263;
	bra.uni 	BB76_44;

BB76_19:
	add.f64 	%fd420, %fd1, %fd1;
	bra.uni 	BB76_44;

BB76_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd1;
	}
	shr.u32 	%r95, %r94, 20;
	setp.ne.s32	%p4, %r95, 0;
	@%p4 bra 	BB76_5;

	mul.f64 	%fd62, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd62;
	}
	shr.u32 	%r44, %r94, 20;
	add.s32 	%r95, %r44, -54;

BB76_5:
	add.s32 	%r96, %r95, -1023;
	and.b32  	%r45, %r94, -2146435073;
	or.b32  	%r46, %r45, 1072693248;
	mov.b64 	%fd400, {%r93, %r46};
	setp.lt.u32	%p5, %r46, 1073127583;
	@%p5 bra 	BB76_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd400;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd400;
	}
	add.s32 	%r49, %r48, -1048576;
	mov.b64 	%fd400, {%r47, %r49};
	add.s32 	%r96, %r95, -1022;

BB76_7:
	add.f64 	%fd63, %fd400, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd64, %fd63;
	neg.f64 	%fd65, %fd63;
	mov.f64 	%fd66, 0d3FF0000000000000;
	fma.rn.f64 	%fd67, %fd65, %fd64, %fd66;
	fma.rn.f64 	%fd68, %fd67, %fd67, %fd67;
	fma.rn.f64 	%fd69, %fd68, %fd64, %fd64;
	add.f64 	%fd70, %fd400, 0dBFF0000000000000;
	mul.f64 	%fd71, %fd70, %fd69;
	fma.rn.f64 	%fd72, %fd70, %fd69, %fd71;
	mul.f64 	%fd73, %fd72, %fd72;
	mov.f64 	%fd74, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd75, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd76, %fd75, %fd73, %fd74;
	mov.f64 	%fd77, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd78, %fd76, %fd73, %fd77;
	mov.f64 	%fd79, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd80, %fd78, %fd73, %fd79;
	mov.f64 	%fd81, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd82, %fd80, %fd73, %fd81;
	mov.f64 	%fd83, 0d3F6249249242B910;
	fma.rn.f64 	%fd84, %fd82, %fd73, %fd83;
	mov.f64 	%fd85, 0d3F89999999999DFB;
	fma.rn.f64 	%fd86, %fd84, %fd73, %fd85;
	sub.f64 	%fd87, %fd70, %fd72;
	add.f64 	%fd88, %fd87, %fd87;
	neg.f64 	%fd89, %fd72;
	fma.rn.f64 	%fd90, %fd89, %fd70, %fd88;
	mul.f64 	%fd91, %fd69, %fd90;
	fma.rn.f64 	%fd92, %fd73, %fd86, 0d3FB5555555555555;
	mov.f64 	%fd93, 0d3FB5555555555555;
	sub.f64 	%fd94, %fd93, %fd92;
	fma.rn.f64 	%fd95, %fd73, %fd86, %fd94;
	add.f64 	%fd96, %fd95, 0d0000000000000000;
	add.f64 	%fd97, %fd96, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd98, %fd92, %fd97;
	sub.f64 	%fd99, %fd92, %fd98;
	add.f64 	%fd100, %fd97, %fd99;
	mul.rn.f64 	%fd101, %fd72, %fd72;
	neg.f64 	%fd102, %fd101;
	fma.rn.f64 	%fd103, %fd72, %fd72, %fd102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd91;
	}
	add.s32 	%r52, %r51, 1048576;
	mov.b64 	%fd104, {%r50, %r52};
	fma.rn.f64 	%fd105, %fd72, %fd104, %fd103;
	mul.rn.f64 	%fd106, %fd101, %fd72;
	neg.f64 	%fd107, %fd106;
	fma.rn.f64 	%fd108, %fd101, %fd72, %fd107;
	fma.rn.f64 	%fd109, %fd101, %fd91, %fd108;
	fma.rn.f64 	%fd110, %fd105, %fd72, %fd109;
	mul.rn.f64 	%fd111, %fd98, %fd106;
	neg.f64 	%fd112, %fd111;
	fma.rn.f64 	%fd113, %fd98, %fd106, %fd112;
	fma.rn.f64 	%fd114, %fd98, %fd110, %fd113;
	fma.rn.f64 	%fd115, %fd100, %fd106, %fd114;
	add.f64 	%fd116, %fd111, %fd115;
	sub.f64 	%fd117, %fd111, %fd116;
	add.f64 	%fd118, %fd115, %fd117;
	add.f64 	%fd119, %fd72, %fd116;
	sub.f64 	%fd120, %fd72, %fd119;
	add.f64 	%fd121, %fd116, %fd120;
	add.f64 	%fd122, %fd118, %fd121;
	add.f64 	%fd123, %fd91, %fd122;
	add.f64 	%fd124, %fd119, %fd123;
	sub.f64 	%fd125, %fd119, %fd124;
	add.f64 	%fd126, %fd123, %fd125;
	xor.b32  	%r53, %r96, -2147483648;
	mov.u32 	%r54, -2147483648;
	mov.u32 	%r55, 1127219200;
	mov.b64 	%fd127, {%r53, %r55};
	mov.b64 	%fd128, {%r54, %r55};
	sub.f64 	%fd129, %fd127, %fd128;
	mov.f64 	%fd130, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd131, %fd129, %fd130, %fd124;
	neg.f64 	%fd132, %fd129;
	fma.rn.f64 	%fd133, %fd132, %fd130, %fd131;
	sub.f64 	%fd134, %fd133, %fd124;
	sub.f64 	%fd135, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd137, %fd129, %fd136, %fd135;
	add.f64 	%fd138, %fd131, %fd137;
	sub.f64 	%fd139, %fd131, %fd138;
	add.f64 	%fd140, %fd137, %fd139;
	add.f64 	%fd141, %fd1, 0dBFE0000000000000;
	mul.rn.f64 	%fd142, %fd138, %fd141;
	neg.f64 	%fd143, %fd142;
	fma.rn.f64 	%fd144, %fd138, %fd141, %fd143;
	fma.rn.f64 	%fd145, %fd140, %fd141, %fd144;
	add.f64 	%fd146, %fd142, %fd145;
	sub.f64 	%fd147, %fd142, %fd146;
	add.f64 	%fd148, %fd145, %fd147;
	sub.f64 	%fd149, %fd146, %fd1;
	sub.f64 	%fd150, %fd146, %fd149;
	sub.f64 	%fd151, %fd150, %fd1;
	add.f64 	%fd152, %fd148, %fd151;
	add.f64 	%fd5, %fd149, %fd152;
	sub.f64 	%fd153, %fd149, %fd5;
	add.f64 	%fd6, %fd152, %fd153;
	mov.f64 	%fd154, 0d4338000000000000;
	mov.f64 	%fd155, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd156, %fd5, %fd155, %fd154;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd156;
	}
	mov.f64 	%fd157, 0dC338000000000000;
	add.rn.f64 	%fd158, %fd156, %fd157;
	mov.f64 	%fd159, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd160, %fd158, %fd159, %fd5;
	mov.f64 	%fd161, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd162, %fd158, %fd161, %fd160;
	mov.f64 	%fd163, 0d3E928AF3FCA213EA;
	mov.f64 	%fd164, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd165, %fd164, %fd162, %fd163;
	mov.f64 	%fd166, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd167, %fd165, %fd162, %fd166;
	mov.f64 	%fd168, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd169, %fd167, %fd162, %fd168;
	mov.f64 	%fd170, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd171, %fd169, %fd162, %fd170;
	mov.f64 	%fd172, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd173, %fd171, %fd162, %fd172;
	mov.f64 	%fd174, 0d3F81111111122322;
	fma.rn.f64 	%fd175, %fd173, %fd162, %fd174;
	mov.f64 	%fd176, 0d3FA55555555502A1;
	fma.rn.f64 	%fd177, %fd175, %fd162, %fd176;
	mov.f64 	%fd178, 0d3FC5555555555511;
	fma.rn.f64 	%fd179, %fd177, %fd162, %fd178;
	mov.f64 	%fd180, 0d3FE000000000000B;
	fma.rn.f64 	%fd181, %fd179, %fd162, %fd180;
	fma.rn.f64 	%fd182, %fd181, %fd162, %fd66;
	fma.rn.f64 	%fd183, %fd182, %fd162, %fd66;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd183;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd183;
	}
	shl.b32 	%r56, %r14, 20;
	add.s32 	%r57, %r16, %r56;
	mov.b64 	%fd401, {%r15, %r57};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd5;
	}
	mov.b32 	 %f2, %r58;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p6, %f1, 0f4086232B;
	@%p6 bra 	BB76_10;

	setp.lt.f64	%p7, %fd5, 0d0000000000000000;
	add.f64 	%fd184, %fd5, 0d7FF0000000000000;
	selp.f64	%fd401, 0d0000000000000000, %fd184, %p7;
	setp.geu.f32	%p8, %f1, 0f40874800;
	@%p8 bra 	BB76_10;

	mov.f64 	%fd399, 0d4338000000000000;
	mov.f64 	%fd398, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd397, %fd5, %fd398, %fd399;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd397;
	}
	shr.u32 	%r59, %r92, 31;
	add.s32 	%r60, %r92, %r59;
	shr.s32 	%r61, %r60, 1;
	shl.b32 	%r62, %r61, 20;
	add.s32 	%r63, %r62, %r16;
	mov.b64 	%fd185, {%r15, %r63};
	sub.s32 	%r64, %r92, %r61;
	shl.b32 	%r65, %r64, 20;
	add.s32 	%r66, %r65, 1072693248;
	mov.u32 	%r67, 0;
	mov.b64 	%fd186, {%r67, %r66};
	mul.f64 	%fd401, %fd185, %fd186;

BB76_10:
	fma.rn.f64 	%fd187, %fd6, %fd401, %fd401;
	mul.f64 	%fd188, %fd187, 0dBCAA6A0D6F814637;
	mov.f64 	%fd189, 0d40040D931FF62706;
	fma.rn.f64 	%fd190, %fd187, %fd189, %fd188;
	neg.f64 	%fd191, %fd1;
	rcp.approx.ftz.f64 	%fd192, %fd1;
	fma.rn.f64 	%fd194, %fd191, %fd192, %fd66;
	fma.rn.f64 	%fd195, %fd194, %fd194, %fd194;
	fma.rn.f64 	%fd196, %fd195, %fd192, %fd192;
	mov.f64 	%fd197, 0d3F73C25DA81303D5;
	mov.f64 	%fd198, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd199, %fd198, %fd196, %fd197;
	mov.f64 	%fd200, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd201, %fd199, %fd196, %fd200;
	mov.f64 	%fd202, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd203, %fd201, %fd196, %fd202;
	mov.f64 	%fd204, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd205, %fd203, %fd196, %fd204;
	mov.f64 	%fd206, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd207, %fd205, %fd196, %fd206;
	mov.f64 	%fd208, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd209, %fd207, %fd196, %fd208;
	mov.f64 	%fd210, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd211, %fd209, %fd196, %fd210;
	mov.f64 	%fd212, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd213, %fd211, %fd196, %fd212;
	mov.f64 	%fd214, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd215, %fd213, %fd196, %fd214;
	mov.f64 	%fd216, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd217, %fd215, %fd196, %fd216;
	mov.f64 	%fd218, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd219, %fd217, %fd196, %fd218;
	mov.f64 	%fd220, 0d3FB5555555555556;
	fma.rn.f64 	%fd221, %fd219, %fd196, %fd220;
	mul.f64 	%fd222, %fd196, %fd221;
	fma.rn.f64 	%fd223, %fd222, %fd190, %fd190;
	setp.ltu.f64	%p9, %fd1, 0d406573FAE561F648;
	selp.f64	%fd420, %fd223, 0d7FF0000000000000, %p9;

BB76_44:
	mov.u32 	%r91, %tid.y;
	mov.u32 	%r90, %ctaid.y;
	mov.u32 	%r89, %ntid.y;
	mov.u32 	%r88, %ctaid.x;
	mov.u32 	%r87, %nctaid.x;
	mad.lo.s32 	%r86, %r89, %r90, %r91;
	mov.u32 	%r85, %tid.x;
	mov.u32 	%r84, %ntid.x;
	mad.lo.s32 	%r83, %r86, %r87, %r88;
	mad.lo.s32 	%r82, %r83, %r84, %r85;
	mul.wide.s32 	%rd15, %r82, 8;
	ld.param.u64 	%rd14, [vec_tgamma_param_1];
	cvta.to.global.u64 	%rd11, %rd14;
	add.s64 	%rd13, %rd11, %rd15;
	st.global.f64 	[%rd13], %fd420;

BB76_45:
	ret;

BB76_22:
	setp.lt.u32	%p18, %r17, -1066983424;
	@%p18 bra 	BB76_24;
	bra.uni 	BB76_23;

BB76_24:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd1;
	}
	add.s32 	%r69, %r17, 1048576;
	mov.b64 	%fd270, {%r68, %r69};
	cvt.rni.f64.f64	%fd271, %fd270;
	cvt.rzi.s64.f64	%rd6, %fd271;
	cvt.u32.u64	%r18, %rd6;
	neg.f64 	%fd272, %fd271;
	mov.f64 	%fd273, 0d3FE0000000000000;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd1;
	mul.f64 	%fd275, %fd274, 0d3CA1A62633145C07;
	mov.f64 	%fd276, 0d400921FB54442D18;
	fma.rn.f64 	%fd277, %fd274, %fd276, %fd275;
	and.b64  	%rd7, %rd6, 1;
	mul.rn.f64 	%fd27, %fd277, %fd277;
	setp.eq.b64	%p20, %rd7, 1;
	not.pred 	%p21, %p20;
	selp.f64	%fd278, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p21;
	shl.b64 	%rd8, %rd7, 6;
	mov.u64 	%rd9, __cudart_sin_cos_coeffs;
	add.s64 	%rd10, %rd8, %rd9;
	ld.const.f64 	%fd279, [%rd10+8];
	fma.rn.f64 	%fd280, %fd278, %fd27, %fd279;
	ld.const.f64 	%fd281, [%rd10+16];
	fma.rn.f64 	%fd282, %fd280, %fd27, %fd281;
	ld.const.f64 	%fd283, [%rd10+24];
	fma.rn.f64 	%fd284, %fd282, %fd27, %fd283;
	ld.const.f64 	%fd285, [%rd10+32];
	fma.rn.f64 	%fd286, %fd284, %fd27, %fd285;
	ld.const.f64 	%fd287, [%rd10+40];
	fma.rn.f64 	%fd288, %fd286, %fd27, %fd287;
	ld.const.f64 	%fd289, [%rd10+48];
	fma.rn.f64 	%fd28, %fd288, %fd27, %fd289;
	fma.rn.f64 	%fd408, %fd28, %fd277, %fd277;
	@%p21 bra 	BB76_26;

	mov.f64 	%fd290, 0d3FF0000000000000;
	fma.rn.f64 	%fd408, %fd28, %fd27, %fd290;

BB76_26:
	and.b32  	%r70, %r18, 2;
	setp.eq.s32	%p22, %r70, 0;
	@%p22 bra 	BB76_28;

	mov.f64 	%fd291, 0d0000000000000000;
	mov.f64 	%fd292, 0dBFF0000000000000;
	fma.rn.f64 	%fd408, %fd408, %fd292, %fd291;

BB76_28:
	abs.f64 	%fd34, %fd1;
	rcp.approx.ftz.f64 	%fd293, %fd34;
	neg.f64 	%fd294, %fd34;
	mov.f64 	%fd295, 0d3FF0000000000000;
	fma.rn.f64 	%fd296, %fd294, %fd293, %fd295;
	fma.rn.f64 	%fd297, %fd296, %fd296, %fd296;
	fma.rn.f64 	%fd298, %fd297, %fd293, %fd293;
	mov.f64 	%fd299, 0d3F73C25DA81303D5;
	mov.f64 	%fd300, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd301, %fd300, %fd298, %fd299;
	mov.f64 	%fd302, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd303, %fd301, %fd298, %fd302;
	mov.f64 	%fd304, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd305, %fd303, %fd298, %fd304;
	mov.f64 	%fd306, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd307, %fd305, %fd298, %fd306;
	mov.f64 	%fd308, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd309, %fd307, %fd298, %fd308;
	mov.f64 	%fd310, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd311, %fd309, %fd298, %fd310;
	mov.f64 	%fd312, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd313, %fd311, %fd298, %fd312;
	mov.f64 	%fd314, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd315, %fd313, %fd298, %fd314;
	mov.f64 	%fd316, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd317, %fd315, %fd298, %fd316;
	mov.f64 	%fd318, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd319, %fd317, %fd298, %fd318;
	mov.f64 	%fd320, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd321, %fd319, %fd298, %fd320;
	mov.f64 	%fd322, 0d3FB5555555555556;
	fma.rn.f64 	%fd323, %fd321, %fd298, %fd322;
	mul.f64 	%fd324, %fd298, %fd323;
	fma.rn.f64 	%fd35, %fd324, %fd408, %fd408;
	mov.f64 	%fd325, 0d4338000000000000;
	mov.f64 	%fd326, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd327, %fd34, %fd326, %fd325;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd327;
	}
	mov.f64 	%fd328, 0dC338000000000000;
	add.rn.f64 	%fd329, %fd327, %fd328;
	mov.f64 	%fd330, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd331, %fd329, %fd330, %fd34;
	mov.f64 	%fd332, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd333, %fd329, %fd332, %fd331;
	mov.f64 	%fd334, 0d3E928AF3FCA213EA;
	mov.f64 	%fd335, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd336, %fd335, %fd333, %fd334;
	mov.f64 	%fd337, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd338, %fd336, %fd333, %fd337;
	mov.f64 	%fd339, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd340, %fd338, %fd333, %fd339;
	mov.f64 	%fd341, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd342, %fd340, %fd333, %fd341;
	mov.f64 	%fd343, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd344, %fd342, %fd333, %fd343;
	mov.f64 	%fd345, 0d3F81111111122322;
	fma.rn.f64 	%fd346, %fd344, %fd333, %fd345;
	mov.f64 	%fd347, 0d3FA55555555502A1;
	fma.rn.f64 	%fd348, %fd346, %fd333, %fd347;
	mov.f64 	%fd349, 0d3FC5555555555511;
	fma.rn.f64 	%fd350, %fd348, %fd333, %fd349;
	mov.f64 	%fd351, 0d3FE000000000000B;
	fma.rn.f64 	%fd352, %fd350, %fd333, %fd351;
	fma.rn.f64 	%fd353, %fd352, %fd333, %fd295;
	fma.rn.f64 	%fd410, %fd353, %fd333, %fd295;
	abs.s32 	%r71, %r19;
	setp.lt.s32	%p23, %r71, 1023;
	@%p23 bra 	BB76_30;
	bra.uni 	BB76_29;

BB76_30:
	shl.b32 	%r77, %r19, 20;
	add.s32 	%r97, %r77, 1072693248;
	bra.uni 	BB76_31;

BB76_35:
	mov.f64 	%fd414, %fd1;

BB76_37:
	setp.lt.u32	%p27, %r17, -1073479680;
	@%p27 bra 	BB76_39;

	fma.rn.f64 	%fd414, %fd1, %fd414, %fd414;
	add.f64 	%fd1, %fd1, 0d3FF0000000000000;

BB76_39:
	setp.lt.u32	%p28, %r17, -1074266112;
	@%p28 bra 	BB76_41;

	fma.rn.f64 	%fd414, %fd1, %fd414, %fd414;
	add.f64 	%fd1, %fd1, 0d3FF0000000000000;

BB76_41:
	setp.lt.u32	%p29, %r17, -1075838976;
	@%p29 bra 	BB76_43;

	fma.rn.f64 	%fd414, %fd1, %fd414, %fd414;
	add.f64 	%fd1, %fd1, 0d3FF0000000000000;

BB76_43:
	mov.f64 	%fd365, 0dBE8AF7049AE8A594;
	mov.f64 	%fd366, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd367, %fd366, %fd1, %fd365;
	mov.f64 	%fd368, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd369, %fd367, %fd1, %fd368;
	mov.f64 	%fd370, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd371, %fd369, %fd1, %fd370;
	mov.f64 	%fd372, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd373, %fd371, %fd1, %fd372;
	mov.f64 	%fd374, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd375, %fd373, %fd1, %fd374;
	mov.f64 	%fd376, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd377, %fd375, %fd1, %fd376;
	mov.f64 	%fd378, 0dBF531711365A3E26;
	fma.rn.f64 	%fd379, %fd377, %fd1, %fd378;
	mov.f64 	%fd380, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd381, %fd379, %fd1, %fd380;
	mov.f64 	%fd382, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd383, %fd381, %fd1, %fd382;
	mov.f64 	%fd384, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd385, %fd383, %fd1, %fd384;
	mov.f64 	%fd386, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd387, %fd385, %fd1, %fd386;
	mov.f64 	%fd388, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd389, %fd387, %fd1, %fd388;
	mov.f64 	%fd390, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd391, %fd389, %fd1, %fd390;
	mov.f64 	%fd392, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd393, %fd391, %fd1, %fd392;
	mov.f64 	%fd394, 0d3FF0000000000000;
	fma.rn.f64 	%fd395, %fd393, %fd1, %fd394;
	mul.f64 	%fd396, %fd414, %fd395;
	rcp.rn.f64 	%fd420, %fd396;
	bra.uni 	BB76_44;

BB76_23:
	cvt.rmi.f64.f64	%fd266, %fd1;
	mul.f64 	%fd267, %fd266, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd268, %fd267;
	fma.rn.f64 	%fd269, %fd268, 0dC000000000000000, %fd266;
	setp.eq.f64	%p19, %fd269, 0d3FF0000000000000;
	selp.f64	%fd420, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB76_44;

BB76_29:
	add.s32 	%r72, %r19, 2046;
	shl.b32 	%r73, %r72, 19;
	and.b32  	%r74, %r73, -1048576;
	shl.b32 	%r75, %r72, 20;
	sub.s32 	%r97, %r75, %r74;
	mov.u32 	%r76, 0;
	mov.b64 	%fd354, {%r76, %r74};
	mul.f64 	%fd410, %fd410, %fd354;

BB76_31:
	mov.u32 	%r78, 0;
	mov.b64 	%fd355, {%r78, %r97};
	mul.f64 	%fd356, %fd410, %fd355;
	mul.f64 	%fd357, %fd356, 0dBC9A6A0D6F814637;
	mov.f64 	%fd358, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd359, %fd356, %fd358, %fd357;
	mul.f64 	%fd360, %fd34, %fd35;
	div.rn.f64 	%fd39, %fd359, %fd360;
	add.f64 	%fd411, %fd34, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd34;
	}
	setp.lt.s32	%p24, %r23, 1080157184;
	@%p24 bra 	BB76_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd411;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd411;
	}
	add.s32 	%r81, %r80, -1048576;
	mov.b64 	%fd411, {%r79, %r81};

BB76_33:
	neg.f64 	%fd361, %fd411;
	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd34;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd361;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd362, [retval0+0];
	
	//{
	}// Callseq End 10
	mul.f64 	%fd363, %fd39, %fd362;
	setp.gt.s32	%p25, %r23, 1080157183;
	selp.f64	%fd364, %fd363, %fd39, %p25;
	mul.f64 	%fd420, %fd362, %fd364;
	bra.uni 	BB76_44;
}

	// .globl	vec_trunc
.visible .entry vec_trunc(
	.param .u32 vec_trunc_param_0,
	.param .u64 vec_trunc_param_1,
	.param .u64 vec_trunc_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_trunc_param_0];
	ld.param.u64 	%rd1, [vec_trunc_param_1];
	ld.param.u64 	%rd2, [vec_trunc_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB77_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rzi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB77_2:
	ret;
}

	// .globl	vec_y0
.visible .entry vec_y0(
	.param .u32 vec_y0_param_0,
	.param .u64 vec_y0_param_1,
	.param .u64 vec_y0_param_2
)
{
	.local .align 4 .b8 	__local_depot78[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<97>;
	.reg .f64 	%fd<541>;
	.reg .b64 	%rd<31>;


	mov.u64 	%SPL, __local_depot78;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r26, [vec_y0_param_0];
	ld.param.u64 	%rd1, [vec_y0_param_1];
	ld.param.u64 	%rd2, [vec_y0_param_2];
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r30, %r27, %r28, %r29;
	mov.u32 	%r31, %nctaid.x;
	mov.u32 	%r32, %ctaid.x;
	mad.lo.s32 	%r33, %r30, %r31, %r32;
	mov.u32 	%r34, %ntid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r1, %r33, %r34, %r35;
	setp.ge.s32	%p1, %r1, %r26;
	@%p1 bra 	BB78_51;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d3FE97F4A8F9D3F28;
	@%p2 bra 	BB78_30;
	bra.uni 	BB78_2;

BB78_30:
	setp.gtu.f64	%p18, %fd2, 0d4000347C4AB37B18;
	@%p18 bra 	BB78_32;
	bra.uni 	BB78_31;

BB78_32:
	setp.gtu.f64	%p19, %fd2, 0d40161663B5D9A628;
	@%p19 bra 	BB78_34;
	bra.uni 	BB78_33;

BB78_34:
	setp.gtu.f64	%p20, %fd2, 0d40214EF30C0C06ED;
	@%p20 bra 	BB78_36;
	bra.uni 	BB78_35;

BB78_36:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd2;
	}
	and.b32  	%r65, %r64, 2147483647;
	setp.ne.s32	%p21, %r65, 2146435072;
	@%p21 bra 	BB78_38;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd2;
	}
	setp.eq.s32	%p22, %r66, 0;
	mov.f64 	%fd540, 0d0000000000000000;
	@%p22 bra 	BB78_50;

BB78_38:
	rcp.approx.ftz.f64 	%fd440, %fd2;
	neg.f64 	%fd441, %fd2;
	mov.f64 	%fd442, 0d3FF0000000000000;
	fma.rn.f64 	%fd443, %fd441, %fd440, %fd442;
	fma.rn.f64 	%fd444, %fd443, %fd443, %fd443;
	fma.rn.f64 	%fd445, %fd444, %fd440, %fd440;
	mul.f64 	%fd446, %fd445, %fd445;
	mov.f64 	%fd447, 0d4093F56A049CDDE7;
	mov.f64 	%fd448, 0dC0C5E91E6AC3AD03;
	fma.rn.f64 	%fd449, %fd448, %fd446, %fd447;
	mov.f64 	%fd450, 0dC05572D39DFB8433;
	fma.rn.f64 	%fd451, %fd449, %fd446, %fd450;
	mov.f64 	%fd452, 0d4016A6041CAA59E5;
	fma.rn.f64 	%fd453, %fd451, %fd446, %fd452;
	mov.f64 	%fd454, 0dBFE155E3A0493880;
	fma.rn.f64 	%fd455, %fd453, %fd446, %fd454;
	mov.f64 	%fd456, 0d3FBA7FB92F417F7F;
	fma.rn.f64 	%fd457, %fd455, %fd446, %fd456;
	mov.f64 	%fd458, 0dBFAFFFFFB12E32F5;
	fma.rn.f64 	%fd459, %fd457, %fd446, %fd458;
	mov.f64 	%fd460, 0d3FEFFFFFFFFECED5;
	fma.rn.f64 	%fd461, %fd459, %fd446, %fd460;
	mov.f64 	%fd462, 0dC15709C79AAC5813;
	mov.f64 	%fd463, 0d418A86A64BE101DC;
	fma.rn.f64 	%fd464, %fd463, %fd446, %fd462;
	mov.f64 	%fd465, 0d41142A31C980A287;
	fma.rn.f64 	%fd466, %fd464, %fd446, %fd465;
	mov.f64 	%fd467, 0dC0C9CBE68930485D;
	fma.rn.f64 	%fd468, %fd466, %fd446, %fd467;
	mov.f64 	%fd469, 0d407F583E14E8A4E8;
	fma.rn.f64 	%fd470, %fd468, %fd446, %fd469;
	mov.f64 	%fd471, 0dC0374A629C650680;
	fma.rn.f64 	%fd472, %fd470, %fd446, %fd471;
	mov.f64 	%fd473, 0d3FFA32A7AF17FAE9;
	fma.rn.f64 	%fd474, %fd472, %fd446, %fd473;
	mov.f64 	%fd475, 0dBFCAD32497785CD6;
	fma.rn.f64 	%fd476, %fd474, %fd446, %fd475;
	mov.f64 	%fd477, 0d3FB0AAAA9FB75F7B;
	fma.rn.f64 	%fd478, %fd476, %fd446, %fd477;
	mov.f64 	%fd479, 0dBFBFFFFFFFFE320F;
	fma.rn.f64 	%fd480, %fd478, %fd446, %fd479;
	fma.rn.f64 	%fd40, %fd480, %fd445, %fd2;
	rsqrt.approx.f64 	%fd481, %fd2;
	mul.f64 	%fd482, %fd481, 0d3FE9884533D43651;
	mul.f64 	%fd41, %fd461, %fd482;
	mul.f64 	%fd483, %fd40, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r95, %fd483;
	add.u64 	%rd17, %SP, 0;
	add.u64 	%rd18, %SPL, 0;
	st.local.u32 	[%rd18], %r95;
	cvt.rn.f64.s32	%fd484, %r95;
	neg.f64 	%fd485, %fd484;
	mov.f64 	%fd486, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd487, %fd485, %fd486, %fd40;
	mov.f64 	%fd488, 0d3C91A62633145C00;
	fma.rn.f64 	%fd489, %fd485, %fd488, %fd487;
	mov.f64 	%fd490, 0d397B839A252049C0;
	fma.rn.f64 	%fd535, %fd485, %fd490, %fd489;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd40;
	}
	and.b32  	%r68, %r67, 2145386496;
	setp.lt.u32	%p23, %r68, 1105199104;
	@%p23 bra 	BB78_40;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd40;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd535, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r95, [%rd18];

BB78_40:
	and.b32  	%r69, %r95, 3;
	cvt.rn.f64.s32	%fd491, %r69;
	add.f64 	%fd492, %fd535, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd536, %fd491, 0d3FF921FB54442D18, %fd492;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd536;
	}
	and.b32  	%r71, %r70, 2147483647;
	setp.ne.s32	%p24, %r71, 2146435072;
	@%p24 bra 	BB78_43;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r72, %temp}, %fd536;
	}
	setp.ne.s32	%p25, %r72, 0;
	@%p25 bra 	BB78_43;

	mov.f64 	%fd493, 0d0000000000000000;
	mul.rn.f64 	%fd536, %fd536, %fd493;

BB78_43:
	mov.f64 	%fd524, 0d397B839A252049C0;
	mov.f64 	%fd523, 0d3C91A62633145C00;
	mov.f64 	%fd522, 0d3FF921FB54442D18;
	mul.f64 	%fd494, %fd536, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r96, %fd494;
	st.local.u32 	[%rd18], %r96;
	cvt.rn.f64.s32	%fd495, %r96;
	neg.f64 	%fd496, %fd495;
	fma.rn.f64 	%fd498, %fd496, %fd522, %fd536;
	fma.rn.f64 	%fd500, %fd496, %fd523, %fd498;
	fma.rn.f64 	%fd537, %fd496, %fd524, %fd500;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd536;
	}
	and.b32  	%r74, %r73, 2145386496;
	setp.lt.u32	%p26, %r74, 1105199104;
	@%p26 bra 	BB78_45;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd536;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd537, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r96, [%rd18];

BB78_45:
	add.s32 	%r25, %r96, 1;
	and.b32  	%r75, %r25, 1;
	shl.b32 	%r76, %r75, 3;
	setp.eq.s32	%p27, %r75, 0;
	selp.f64	%fd502, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p27;
	add.s32 	%r77, %r76, 1;
	mul.wide.s32 	%rd25, %r77, 8;
	mov.u64 	%rd26, __cudart_sin_cos_coeffs;
	add.s64 	%rd27, %rd26, %rd25;
	ld.const.f64 	%fd503, [%rd27];
	mul.rn.f64 	%fd51, %fd537, %fd537;
	fma.rn.f64 	%fd504, %fd502, %fd51, %fd503;
	ld.const.f64 	%fd505, [%rd27+8];
	fma.rn.f64 	%fd506, %fd504, %fd51, %fd505;
	ld.const.f64 	%fd507, [%rd27+16];
	fma.rn.f64 	%fd508, %fd506, %fd51, %fd507;
	ld.const.f64 	%fd509, [%rd27+24];
	fma.rn.f64 	%fd510, %fd508, %fd51, %fd509;
	ld.const.f64 	%fd511, [%rd27+32];
	fma.rn.f64 	%fd512, %fd510, %fd51, %fd511;
	ld.const.f64 	%fd513, [%rd27+40];
	fma.rn.f64 	%fd52, %fd512, %fd51, %fd513;
	fma.rn.f64 	%fd538, %fd52, %fd537, %fd537;
	@%p27 bra 	BB78_47;

	mov.f64 	%fd525, 0d3FF0000000000000;
	fma.rn.f64 	%fd538, %fd52, %fd51, %fd525;

BB78_47:
	and.b32  	%r78, %r25, 2;
	setp.eq.s32	%p28, %r78, 0;
	@%p28 bra 	BB78_49;

	mov.f64 	%fd515, 0d0000000000000000;
	mov.f64 	%fd516, 0dBFF0000000000000;
	fma.rn.f64 	%fd538, %fd538, %fd516, %fd515;

BB78_49:
	mul.f64 	%fd540, %fd41, %fd538;
	bra.uni 	BB78_50;

BB78_2:
	mul.f64 	%fd60, %fd2, %fd2;
	mov.f64 	%fd61, 0dBD13098C51C18514;
	mov.f64 	%fd62, 0d3C8EFBD0A1B77C65;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0d3D923102D2F5F2F5;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0dBE0A5F2DEE7D526E;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3E7BB77E758B38AF;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0dBEE3D1A206EC4F36;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0d3F4183DCD3ED6294;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0dBF903921CF04F123;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3FC5DB69D7753176;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	add.f64 	%fd78, %fd60, 0dBFDBA96740000000;
	add.f64 	%fd79, %fd78, 0d3E15A30C80000000;
	mul.f64 	%fd3, %fd79, %fd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r90, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd2;
	}
	mov.u32 	%r91, -1023;
	setp.gt.s32	%p3, %r89, 1048575;
	mov.f64 	%fd526, %fd2;
	@%p3 bra 	BB78_4;

	mul.f64 	%fd526, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd526;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r90, %temp}, %fd526;
	}
	mov.u32 	%r91, -1077;

BB78_4:
	add.s32 	%r38, %r89, -1;
	setp.lt.u32	%p4, %r38, 2146435071;
	@%p4 bra 	BB78_6;
	bra.uni 	BB78_5;

BB78_6:
	shr.u32 	%r40, %r89, 20;
	add.s32 	%r92, %r91, %r40;
	and.b32  	%r41, %r89, -2146435073;
	or.b32  	%r42, %r41, 1072693248;
	mov.b64 	%fd527, {%r90, %r42};
	setp.lt.s32	%p6, %r42, 1073127583;
	@%p6 bra 	BB78_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd527;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd527;
	}
	add.s32 	%r45, %r44, -1048576;
	mov.b64 	%fd527, {%r43, %r45};
	add.s32 	%r92, %r92, 1;

BB78_8:
	add.f64 	%fd82, %fd527, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd83, %fd82;
	neg.f64 	%fd84, %fd82;
	mov.f64 	%fd85, 0d3FF0000000000000;
	fma.rn.f64 	%fd86, %fd84, %fd83, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd83, %fd83;
	add.f64 	%fd89, %fd527, 0dBFF0000000000000;
	mul.f64 	%fd90, %fd89, %fd88;
	fma.rn.f64 	%fd91, %fd89, %fd88, %fd90;
	mul.f64 	%fd92, %fd91, %fd91;
	mov.f64 	%fd93, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd94, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F624924923BE72D;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	mov.f64 	%fd106, 0d3FB5555555555554;
	fma.rn.f64 	%fd107, %fd105, %fd92, %fd106;
	sub.f64 	%fd108, %fd89, %fd91;
	add.f64 	%fd109, %fd108, %fd108;
	neg.f64 	%fd110, %fd91;
	fma.rn.f64 	%fd111, %fd110, %fd89, %fd109;
	mul.f64 	%fd112, %fd88, %fd111;
	mul.f64 	%fd113, %fd92, %fd107;
	fma.rn.f64 	%fd114, %fd113, %fd91, %fd112;
	xor.b32  	%r46, %r92, -2147483648;
	mov.u32 	%r47, -2147483648;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd115, {%r46, %r48};
	mov.b64 	%fd116, {%r47, %r48};
	sub.f64 	%fd117, %fd115, %fd116;
	mov.f64 	%fd118, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd119, %fd117, %fd118, %fd91;
	neg.f64 	%fd120, %fd117;
	fma.rn.f64 	%fd121, %fd120, %fd118, %fd119;
	sub.f64 	%fd122, %fd121, %fd91;
	sub.f64 	%fd123, %fd114, %fd122;
	mov.f64 	%fd124, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd125, %fd117, %fd124, %fd123;
	add.f64 	%fd528, %fd119, %fd125;
	bra.uni 	BB78_9;

BB78_31:
	add.f64 	%fd308, %fd2, 0dBFEC982EB8D417EA;
	add.f64 	%fd309, %fd308, 0dBC7EA9D270347F83;
	mov.f64 	%fd310, 0d3F3D054B05D3C52D;
	mov.f64 	%fd311, 0dBF01630132D75FC3;
	fma.rn.f64 	%fd312, %fd311, %fd309, %fd310;
	mov.f64 	%fd313, 0dBF66DAC0B314B2E5;
	fma.rn.f64 	%fd314, %fd312, %fd309, %fd313;
	mov.f64 	%fd315, 0d3F86A5D1DE76263F;
	fma.rn.f64 	%fd316, %fd314, %fd309, %fd315;
	mov.f64 	%fd317, 0dBF9FD16652824592;
	fma.rn.f64 	%fd318, %fd316, %fd309, %fd317;
	mov.f64 	%fd319, 0d3FB0F69A9CC79FBD;
	fma.rn.f64 	%fd320, %fd318, %fd309, %fd319;
	mov.f64 	%fd321, 0dBFBCCE40EF15583E;
	fma.rn.f64 	%fd322, %fd320, %fd309, %fd321;
	mov.f64 	%fd323, 0d3FC446B11780E4FC;
	fma.rn.f64 	%fd324, %fd322, %fd309, %fd323;
	mov.f64 	%fd325, 0dBFC89AE7E19621F7;
	fma.rn.f64 	%fd326, %fd324, %fd309, %fd325;
	mov.f64 	%fd327, 0d3FCACBA1B38EF7B8;
	fma.rn.f64 	%fd328, %fd326, %fd309, %fd327;
	mov.f64 	%fd329, 0dBFCB4166A03BBFA5;
	fma.rn.f64 	%fd330, %fd328, %fd309, %fd329;
	mov.f64 	%fd331, 0d3FCACCA4D5D4889A;
	fma.rn.f64 	%fd332, %fd330, %fd309, %fd331;
	mov.f64 	%fd333, 0dBFCA1455932B9392;
	fma.rn.f64 	%fd334, %fd332, %fd309, %fd333;
	mov.f64 	%fd335, 0d3FC96D8DB8D844EC;
	fma.rn.f64 	%fd336, %fd334, %fd309, %fd335;
	mov.f64 	%fd337, 0dBFC8F7FB77522EDF;
	fma.rn.f64 	%fd338, %fd336, %fd309, %fd337;
	mov.f64 	%fd339, 0d3FC8C0926ABC9AB0;
	fma.rn.f64 	%fd340, %fd338, %fd309, %fd339;
	mov.f64 	%fd341, 0dBFC8D35B8FEA468C;
	fma.rn.f64 	%fd342, %fd340, %fd309, %fd341;
	mov.f64 	%fd343, 0d3FC9424B8A0C8F94;
	fma.rn.f64 	%fd344, %fd342, %fd309, %fd343;
	mov.f64 	%fd345, 0dBFCA396A7F3403EF;
	fma.rn.f64 	%fd346, %fd344, %fd309, %fd345;
	mov.f64 	%fd347, 0d3FCC068086C37055;
	fma.rn.f64 	%fd348, %fd346, %fd309, %fd347;
	mov.f64 	%fd349, 0dBFCCF18E6A4C5C4E;
	fma.rn.f64 	%fd350, %fd348, %fd309, %fd349;
	mov.f64 	%fd351, 0d3FCC3B1338AF4239;
	fma.rn.f64 	%fd352, %fd350, %fd309, %fd351;
	mov.f64 	%fd353, 0dBFDF7E38A46D70DB;
	fma.rn.f64 	%fd354, %fd352, %fd309, %fd353;
	mov.f64 	%fd355, 0d3FEC24371844B88A;
	fma.rn.f64 	%fd356, %fd354, %fd309, %fd355;
	mul.f64 	%fd540, %fd309, %fd356;
	bra.uni 	BB78_50;

BB78_5:
	mov.f64 	%fd80, 0d7FF0000000000000;
	fma.rn.f64 	%fd81, %fd526, %fd80, %fd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd526;
	}
	mov.b32 	 %f1, %r39;
	setp.eq.f32	%p5, %f1, 0f00000000;
	selp.f64	%fd528, 0dFFF0000000000000, %fd81, %p5;

BB78_9:
	abs.f64 	%fd12, %fd2;
	setp.gtu.f64	%p7, %fd12, 0d400FB319F277BBE5;
	@%p7 bra 	BB78_11;
	bra.uni 	BB78_10;

BB78_11:
	setp.gtu.f64	%p8, %fd12, 0d401C58FD1A62F5EC;
	@%p8 bra 	BB78_13;
	bra.uni 	BB78_12;

BB78_13:
	setp.gtu.f64	%p9, %fd12, 0d402471FCB6A7A8C0;
	@%p9 bra 	BB78_15;
	bra.uni 	BB78_14;

BB78_15:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd12;
	}
	and.b32  	%r50, %r49, 2147483647;
	setp.ne.s32	%p10, %r50, 2146435072;
	@%p10 bra 	BB78_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd12;
	}
	setp.eq.s32	%p11, %r51, 0;
	mov.f64 	%fd534, 0d0000000000000000;
	@%p11 bra 	BB78_29;

BB78_17:
	rcp.approx.ftz.f64 	%fd232, %fd12;
	neg.f64 	%fd233, %fd12;
	mov.f64 	%fd234, 0d3FF0000000000000;
	fma.rn.f64 	%fd235, %fd233, %fd232, %fd234;
	fma.rn.f64 	%fd236, %fd235, %fd235, %fd235;
	fma.rn.f64 	%fd237, %fd236, %fd232, %fd232;
	mul.f64 	%fd238, %fd237, %fd237;
	mov.f64 	%fd239, 0d409927467A655012;
	mov.f64 	%fd240, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd241, %fd240, %fd238, %fd239;
	mov.f64 	%fd242, 0dC05751787E247BD4;
	fma.rn.f64 	%fd243, %fd241, %fd238, %fd242;
	mov.f64 	%fd244, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd245, %fd243, %fd238, %fd244;
	mov.f64 	%fd246, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd247, %fd245, %fd238, %fd246;
	mov.f64 	%fd248, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd249, %fd247, %fd238, %fd248;
	mov.f64 	%fd250, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd251, %fd249, %fd238, %fd250;
	mov.f64 	%fd252, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd253, %fd251, %fd238, %fd252;
	mov.f64 	%fd254, 0d410ECD4523B12B84;
	mov.f64 	%fd255, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd256, %fd255, %fd238, %fd254;
	mov.f64 	%fd257, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd258, %fd256, %fd238, %fd257;
	mov.f64 	%fd259, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd260, %fd258, %fd238, %fd259;
	mov.f64 	%fd261, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd262, %fd260, %fd238, %fd261;
	mov.f64 	%fd263, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd264, %fd262, %fd238, %fd263;
	mov.f64 	%fd265, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd266, %fd264, %fd238, %fd265;
	mov.f64 	%fd267, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd268, %fd266, %fd238, %fd267;
	mov.f64 	%fd269, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd270, %fd268, %fd238, %fd269;
	fma.rn.f64 	%fd16, %fd270, %fd237, %fd12;
	rsqrt.approx.f64 	%fd271, %fd12;
	mul.f64 	%fd272, %fd271, 0d3FE9884533D43651;
	mul.f64 	%fd17, %fd253, %fd272;
	mul.f64 	%fd273, %fd16, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r93, %fd273;
	add.u64 	%rd6, %SP, 0;
	add.u64 	%rd7, %SPL, 0;
	st.local.u32 	[%rd7], %r93;
	cvt.rn.f64.s32	%fd274, %r93;
	neg.f64 	%fd275, %fd274;
	mov.f64 	%fd276, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd277, %fd275, %fd276, %fd16;
	mov.f64 	%fd278, 0d3C91A62633145C00;
	fma.rn.f64 	%fd279, %fd275, %fd278, %fd277;
	mov.f64 	%fd280, 0d397B839A252049C0;
	fma.rn.f64 	%fd529, %fd275, %fd280, %fd279;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd16;
	}
	and.b32  	%r53, %r52, 2145386496;
	setp.lt.u32	%p12, %r53, 1105199104;
	@%p12 bra 	BB78_19;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd16;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd6;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd529, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r93, [%rd7];

BB78_19:
	and.b32  	%r54, %r93, 3;
	cvt.rn.f64.s32	%fd281, %r54;
	add.f64 	%fd282, %fd529, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd530, %fd281, 0d3FF921FB54442D18, %fd282;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd530;
	}
	and.b32  	%r56, %r55, 2147483647;
	setp.ne.s32	%p13, %r56, 2146435072;
	@%p13 bra 	BB78_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r57, %temp}, %fd530;
	}
	setp.ne.s32	%p14, %r57, 0;
	@%p14 bra 	BB78_22;

	mov.f64 	%fd283, 0d0000000000000000;
	mul.rn.f64 	%fd530, %fd530, %fd283;

BB78_22:
	mov.f64 	%fd520, 0d397B839A252049C0;
	mov.f64 	%fd519, 0d3C91A62633145C00;
	mov.f64 	%fd518, 0d3FF921FB54442D18;
	mul.f64 	%fd284, %fd530, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r94, %fd284;
	st.local.u32 	[%rd7], %r94;
	cvt.rn.f64.s32	%fd285, %r94;
	neg.f64 	%fd286, %fd285;
	fma.rn.f64 	%fd288, %fd286, %fd518, %fd530;
	fma.rn.f64 	%fd290, %fd286, %fd519, %fd288;
	fma.rn.f64 	%fd531, %fd286, %fd520, %fd290;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd530;
	}
	and.b32  	%r59, %r58, 2145386496;
	setp.lt.u32	%p15, %r59, 1105199104;
	@%p15 bra 	BB78_24;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd530;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd6;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd531, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r94, [%rd7];

BB78_24:
	add.s32 	%r18, %r94, 1;
	and.b32  	%r60, %r18, 1;
	shl.b32 	%r61, %r60, 3;
	setp.eq.s32	%p16, %r60, 0;
	selp.f64	%fd292, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p16;
	add.s32 	%r62, %r61, 1;
	mul.wide.s32 	%rd14, %r62, 8;
	mov.u64 	%rd15, __cudart_sin_cos_coeffs;
	add.s64 	%rd16, %rd15, %rd14;
	ld.const.f64 	%fd293, [%rd16];
	mul.rn.f64 	%fd27, %fd531, %fd531;
	fma.rn.f64 	%fd294, %fd292, %fd27, %fd293;
	ld.const.f64 	%fd295, [%rd16+8];
	fma.rn.f64 	%fd296, %fd294, %fd27, %fd295;
	ld.const.f64 	%fd297, [%rd16+16];
	fma.rn.f64 	%fd298, %fd296, %fd27, %fd297;
	ld.const.f64 	%fd299, [%rd16+24];
	fma.rn.f64 	%fd300, %fd298, %fd27, %fd299;
	ld.const.f64 	%fd301, [%rd16+32];
	fma.rn.f64 	%fd302, %fd300, %fd27, %fd301;
	ld.const.f64 	%fd303, [%rd16+40];
	fma.rn.f64 	%fd28, %fd302, %fd27, %fd303;
	fma.rn.f64 	%fd532, %fd28, %fd531, %fd531;
	@%p16 bra 	BB78_26;

	mov.f64 	%fd521, 0d3FF0000000000000;
	fma.rn.f64 	%fd532, %fd28, %fd27, %fd521;

BB78_26:
	and.b32  	%r63, %r18, 2;
	setp.eq.s32	%p17, %r63, 0;
	@%p17 bra 	BB78_28;

	mov.f64 	%fd305, 0d0000000000000000;
	mov.f64 	%fd306, 0dBFF0000000000000;
	fma.rn.f64 	%fd532, %fd532, %fd306, %fd305;

BB78_28:
	mul.f64 	%fd534, %fd17, %fd532;
	bra.uni 	BB78_29;

BB78_10:
	add.f64 	%fd126, %fd12, 0dC0033D152E971B40;
	add.f64 	%fd127, %fd126, 0d3CA0F539D7DA258E;
	mov.f64 	%fd128, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd129, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd130, %fd129, %fd127, %fd128;
	mov.f64 	%fd131, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd132, %fd130, %fd127, %fd131;
	mov.f64 	%fd133, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd134, %fd132, %fd127, %fd133;
	mov.f64 	%fd135, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd136, %fd134, %fd127, %fd135;
	mov.f64 	%fd137, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd138, %fd136, %fd127, %fd137;
	mov.f64 	%fd139, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd140, %fd138, %fd127, %fd139;
	mov.f64 	%fd141, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd142, %fd140, %fd127, %fd141;
	mov.f64 	%fd143, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd144, %fd142, %fd127, %fd143;
	mov.f64 	%fd145, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd146, %fd144, %fd127, %fd145;
	mov.f64 	%fd147, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd148, %fd146, %fd127, %fd147;
	mov.f64 	%fd149, 0d3F315382BA096A62;
	fma.rn.f64 	%fd150, %fd148, %fd127, %fd149;
	mov.f64 	%fd151, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd152, %fd150, %fd127, %fd151;
	mov.f64 	%fd153, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd154, %fd152, %fd127, %fd153;
	mov.f64 	%fd155, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd156, %fd154, %fd127, %fd155;
	mov.f64 	%fd157, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd158, %fd156, %fd127, %fd157;
	mov.f64 	%fd159, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd160, %fd158, %fd127, %fd159;
	mul.f64 	%fd534, %fd127, %fd160;
	bra.uni 	BB78_29;

BB78_33:
	add.f64 	%fd357, %fd2, 0dC00FA9534D98569C;
	add.f64 	%fd358, %fd357, 0d3C9F06AE7804384E;
	mov.f64 	%fd359, 0dBCD2434958151AC7;
	mov.f64 	%fd360, 0dBCDAEA62AC8BDA68;
	fma.rn.f64 	%fd361, %fd360, %fd358, %fd359;
	mov.f64 	%fd362, 0d3D11C24A40D33FE1;
	fma.rn.f64 	%fd363, %fd361, %fd358, %fd362;
	mov.f64 	%fd364, 0d3D237CD62FA08CA4;
	fma.rn.f64 	%fd365, %fd363, %fd358, %fd364;
	mov.f64 	%fd366, 0dBD43902E0298C52A;
	fma.rn.f64 	%fd367, %fd365, %fd358, %fd366;
	mov.f64 	%fd368, 0dBD1DDAAD11CAB40F;
	fma.rn.f64 	%fd369, %fd367, %fd358, %fd368;
	mov.f64 	%fd370, 0dBD5209D9F06D7DE4;
	fma.rn.f64 	%fd371, %fd369, %fd358, %fd370;
	mov.f64 	%fd372, 0d3D8BB9F464468E1A;
	fma.rn.f64 	%fd373, %fd371, %fd358, %fd372;
	mov.f64 	%fd374, 0dBDA8F67B07D1B440;
	fma.rn.f64 	%fd375, %fd373, %fd358, %fd374;
	mov.f64 	%fd376, 0d3DC7C8D60F9EAECF;
	fma.rn.f64 	%fd377, %fd375, %fd358, %fd376;
	mov.f64 	%fd378, 0dBDE9703405B49A8D;
	fma.rn.f64 	%fd379, %fd377, %fd358, %fd378;
	mov.f64 	%fd380, 0d3E0A6B64E76417E4;
	fma.rn.f64 	%fd381, %fd379, %fd358, %fd380;
	mov.f64 	%fd382, 0dBE2F6B5AFB2F1359;
	fma.rn.f64 	%fd383, %fd381, %fd358, %fd382;
	mov.f64 	%fd384, 0d3E54526B71C21EC1;
	fma.rn.f64 	%fd385, %fd383, %fd358, %fd384;
	mov.f64 	%fd386, 0dBE5776DBCBBC8E1D;
	fma.rn.f64 	%fd387, %fd385, %fd358, %fd386;
	mov.f64 	%fd388, 0dBE93B211FC2DF90E;
	fma.rn.f64 	%fd389, %fd387, %fd358, %fd388;
	mov.f64 	%fd390, 0dBED486372E8562DC;
	fma.rn.f64 	%fd391, %fd389, %fd358, %fd390;
	mov.f64 	%fd392, 0d3F0AB2C1FBC3A254;
	fma.rn.f64 	%fd393, %fd391, %fd358, %fd392;
	mov.f64 	%fd394, 0d3F299827653353B8;
	fma.rn.f64 	%fd395, %fd393, %fd358, %fd394;
	mov.f64 	%fd396, 0dBF61E32BC4ED7084;
	fma.rn.f64 	%fd397, %fd395, %fd358, %fd396;
	mov.f64 	%fd398, 0dBF7C116FDC599A09;
	fma.rn.f64 	%fd399, %fd397, %fd358, %fd398;
	mov.f64 	%fd400, 0d3FADF6D59BF50C77;
	fma.rn.f64 	%fd401, %fd399, %fd358, %fd400;
	mov.f64 	%fd402, 0d3FAA09C92903680B;
	fma.rn.f64 	%fd403, %fd401, %fd358, %fd402;
	mov.f64 	%fd404, 0dBFD9C34256A12A0B;
	fma.rn.f64 	%fd405, %fd403, %fd358, %fd404;
	mul.f64 	%fd540, %fd358, %fd405;
	bra.uni 	BB78_50;

BB78_12:
	add.f64 	%fd161, %fd12, 0dC016148F5B2C2E45;
	add.f64 	%fd162, %fd161, 0dBC975054CD60A517;
	mov.f64 	%fd163, 0d3CF83FD1F333EB61;
	mov.f64 	%fd164, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd165, %fd164, %fd162, %fd163;
	mov.f64 	%fd166, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd167, %fd165, %fd162, %fd166;
	mov.f64 	%fd168, 0dBD7846076D004627;
	fma.rn.f64 	%fd169, %fd167, %fd162, %fd168;
	mov.f64 	%fd170, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd171, %fd169, %fd162, %fd170;
	mov.f64 	%fd172, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd173, %fd171, %fd162, %fd172;
	mov.f64 	%fd174, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd175, %fd173, %fd162, %fd174;
	mov.f64 	%fd176, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd177, %fd175, %fd162, %fd176;
	mov.f64 	%fd178, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd179, %fd177, %fd162, %fd178;
	mov.f64 	%fd180, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd181, %fd179, %fd162, %fd180;
	mov.f64 	%fd182, 0dBF0863F481A37337;
	fma.rn.f64 	%fd183, %fd181, %fd162, %fd182;
	mov.f64 	%fd184, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd185, %fd183, %fd162, %fd184;
	mov.f64 	%fd186, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd187, %fd185, %fd162, %fd186;
	mov.f64 	%fd188, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd189, %fd187, %fd162, %fd188;
	mov.f64 	%fd190, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd191, %fd189, %fd162, %fd190;
	mov.f64 	%fd192, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd193, %fd191, %fd162, %fd192;
	mov.f64 	%fd194, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd195, %fd193, %fd162, %fd194;
	mul.f64 	%fd534, %fd162, %fd195;
	bra.uni 	BB78_29;

BB78_35:
	add.f64 	%fd406, %fd2, 0dC01C581DC4E72103;
	add.f64 	%fd407, %fd406, 0d3C99774A495F56CF;
	mov.f64 	%fd408, 0dBD3F443BB4F53D75;
	mov.f64 	%fd409, 0d3CF1CB3ABA718B8E;
	fma.rn.f64 	%fd410, %fd409, %fd407, %fd408;
	mov.f64 	%fd411, 0dBD770F737BD6A786;
	fma.rn.f64 	%fd412, %fd410, %fd407, %fd411;
	mov.f64 	%fd413, 0d3DBF0E9A20459E14;
	fma.rn.f64 	%fd414, %fd412, %fd407, %fd413;
	mov.f64 	%fd415, 0d3DEFA6B137D5E108;
	fma.rn.f64 	%fd416, %fd414, %fd407, %fd415;
	mov.f64 	%fd417, 0dBE344296729FB7FA;
	fma.rn.f64 	%fd418, %fd416, %fd407, %fd417;
	mov.f64 	%fd419, 0dBE60A2813A80DFAA;
	fma.rn.f64 	%fd420, %fd418, %fd407, %fd419;
	mov.f64 	%fd421, 0d3EA34AA737A83EB4;
	fma.rn.f64 	%fd422, %fd420, %fd407, %fd421;
	mov.f64 	%fd423, 0d3EC6A9227332D03C;
	fma.rn.f64 	%fd424, %fd422, %fd407, %fd423;
	mov.f64 	%fd425, 0dBF08177E4F93C81E;
	fma.rn.f64 	%fd426, %fd424, %fd407, %fd425;
	mov.f64 	%fd427, 0dBF226DD71E391775;
	fma.rn.f64 	%fd428, %fd426, %fd407, %fd427;
	mov.f64 	%fd429, 0d3F61D35E85FD7B22;
	fma.rn.f64 	%fd430, %fd428, %fd407, %fd429;
	mov.f64 	%fd431, 0d3F6B2F14A955285C;
	fma.rn.f64 	%fd432, %fd430, %fd407, %fd431;
	mov.f64 	%fd433, 0dBFA8969C64CBF388;
	fma.rn.f64 	%fd434, %fd432, %fd407, %fd433;
	mov.f64 	%fd435, 0dBF95AEF611FC4D5A;
	fma.rn.f64 	%fd436, %fd434, %fd407, %fd435;
	mov.f64 	%fd437, 0d3FD334CCA0697A5A;
	fma.rn.f64 	%fd438, %fd436, %fd407, %fd437;
	mul.f64 	%fd540, %fd407, %fd438;
	bra.uni 	BB78_50;

BB78_14:
	add.f64 	%fd196, %fd12, 0dC0214EB56CCCDECA;
	add.f64 	%fd197, %fd196, 0d3CB51970714C7C25;
	mov.f64 	%fd198, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd199, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd200, %fd199, %fd197, %fd198;
	mov.f64 	%fd201, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd202, %fd200, %fd197, %fd201;
	mov.f64 	%fd203, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd204, %fd202, %fd197, %fd203;
	mov.f64 	%fd205, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd206, %fd204, %fd197, %fd205;
	mov.f64 	%fd207, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd208, %fd206, %fd197, %fd207;
	mov.f64 	%fd209, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd210, %fd208, %fd197, %fd209;
	mov.f64 	%fd211, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd212, %fd210, %fd197, %fd211;
	mov.f64 	%fd213, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd214, %fd212, %fd197, %fd213;
	mov.f64 	%fd215, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd216, %fd214, %fd197, %fd215;
	mov.f64 	%fd217, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd218, %fd216, %fd197, %fd217;
	mov.f64 	%fd219, 0d3F1D79605276949A;
	fma.rn.f64 	%fd220, %fd218, %fd197, %fd219;
	mov.f64 	%fd221, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd222, %fd220, %fd197, %fd221;
	mov.f64 	%fd223, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd224, %fd222, %fd197, %fd223;
	mov.f64 	%fd225, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd226, %fd224, %fd197, %fd225;
	mov.f64 	%fd227, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd228, %fd226, %fd197, %fd227;
	mov.f64 	%fd229, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd230, %fd228, %fd197, %fd229;
	mul.f64 	%fd534, %fd197, %fd230;

BB78_29:
	mul.f64 	%fd307, %fd528, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd540, %fd307, %fd534, %fd3;

BB78_50:
	setp.lt.f64	%p29, %fd1, 0d0000000000000000;
	selp.f64	%fd517, 0dFFF8000000000000, %fd540, %p29;
	cvta.to.global.u64 	%rd28, %rd1;
	add.s64 	%rd30, %rd28, %rd4;
	st.global.f64 	[%rd30], %fd517;

BB78_51:
	ret;
}

	// .globl	vec_y1
.visible .entry vec_y1(
	.param .u32 vec_y1_param_0,
	.param .u64 vec_y1_param_1,
	.param .u64 vec_y1_param_2
)
{
	.local .align 4 .b8 	__local_depot79[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<107>;
	.reg .f64 	%fd<536>;
	.reg .b64 	%rd<33>;


	mov.u64 	%SPL, __local_depot79;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r25, [vec_y1_param_0];
	ld.param.u64 	%rd1, [vec_y1_param_1];
	ld.param.u64 	%rd2, [vec_y1_param_2];
	mov.u32 	%r26, %ntid.y;
	mov.u32 	%r27, %ctaid.y;
	mov.u32 	%r28, %tid.y;
	mad.lo.s32 	%r29, %r26, %r27, %r28;
	mov.u32 	%r30, %nctaid.x;
	mov.u32 	%r31, %ctaid.x;
	mad.lo.s32 	%r32, %r29, %r30, %r31;
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r35, %r32, %r33, %r34;
	setp.ge.s32	%p1, %r35, %r25;
	@%p1 bra 	BB79_55;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r35, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p2, %fd2, 0d000730D67819E8D2;
	@%p2 bra 	BB79_51;
	bra.uni 	BB79_2;

BB79_51:
	mov.f64 	%fd510, 0dBFE45F306DC9C883;
	div.rn.f64 	%fd534, %fd510, %fd2;
	bra.uni 	BB79_52;

BB79_2:
	setp.gtu.f64	%p3, %fd2, 0d3FF4C6F208132576;
	@%p3 bra 	BB79_31;
	bra.uni 	BB79_3;

BB79_31:
	setp.gtu.f64	%p21, %fd2, 0d4009B510EC2ADC83;
	@%p21 bra 	BB79_33;
	bra.uni 	BB79_32;

BB79_33:
	setp.gtu.f64	%p22, %fd2, 0d401C0D26D5A541CB;
	@%p22 bra 	BB79_35;
	bra.uni 	BB79_34;

BB79_35:
	setp.gtu.f64	%p23, %fd2, 0d4022585C739ACDDD;
	@%p23 bra 	BB79_37;
	bra.uni 	BB79_36;

BB79_37:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r74}, %fd2;
	}
	and.b32  	%r75, %r74, 2147483647;
	setp.ne.s32	%p24, %r75, 2146435072;
	@%p24 bra 	BB79_39;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r76, %temp}, %fd2;
	}
	setp.eq.s32	%p25, %r76, 0;
	mov.f64 	%fd534, 0d0000000000000000;
	@%p25 bra 	BB79_52;

BB79_39:
	rcp.approx.ftz.f64 	%fd433, %fd2;
	neg.f64 	%fd434, %fd2;
	mov.f64 	%fd435, 0d3FF0000000000000;
	fma.rn.f64 	%fd436, %fd434, %fd433, %fd435;
	fma.rn.f64 	%fd437, %fd436, %fd436, %fd436;
	fma.rn.f64 	%fd438, %fd437, %fd433, %fd433;
	mul.f64 	%fd439, %fd438, %fd438;
	mov.f64 	%fd440, 0dC09C26E89385D5B1;
	mov.f64 	%fd441, 0d40D13DB326ECEBFE;
	fma.rn.f64 	%fd442, %fd441, %fd439, %fd440;
	mov.f64 	%fd443, 0d405C6AB923C6F55E;
	fma.rn.f64 	%fd444, %fd442, %fd439, %fd443;
	mov.f64 	%fd445, 0dC01E61EAF3BD2FA1;
	fma.rn.f64 	%fd446, %fd444, %fd439, %fd445;
	mov.f64 	%fd447, 0d3FE9BF15D9B97DD1;
	fma.rn.f64 	%fd448, %fd446, %fd439, %fd447;
	mov.f64 	%fd449, 0dBFC8BFECF93D7D19;
	fma.rn.f64 	%fd450, %fd448, %fd439, %fd449;
	mov.f64 	%fd451, 0d3FC7FFFFF756AA6C;
	fma.rn.f64 	%fd452, %fd450, %fd439, %fd451;
	mov.f64 	%fd453, 0d3FF0000000003646;
	fma.rn.f64 	%fd454, %fd452, %fd439, %fd453;
	mov.f64 	%fd455, 0d416024E99BA46E7B;
	mov.f64 	%fd456, 0dC1943281A050209C;
	fma.rn.f64 	%fd457, %fd456, %fd439, %fd455;
	mov.f64 	%fd458, 0dC11A6875D7DFBD65;
	fma.rn.f64 	%fd459, %fd457, %fd439, %fd458;
	mov.f64 	%fd460, 0d40D032C041790233;
	fma.rn.f64 	%fd461, %fd459, %fd439, %fd460;
	mov.f64 	%fd462, 0dC0839F895BC22946;
	fma.rn.f64 	%fd463, %fd461, %fd439, %fd462;
	mov.f64 	%fd464, 0d403E77CC78ECD2D8;
	fma.rn.f64 	%fd465, %fd463, %fd439, %fd464;
	mov.f64 	%fd466, 0dC002F368D0117BE9;
	fma.rn.f64 	%fd467, %fd465, %fd439, %fd466;
	mov.f64 	%fd468, 0d3FD7BCC786009A25;
	fma.rn.f64 	%fd469, %fd467, %fd439, %fd468;
	mov.f64 	%fd470, 0dBFC4FFFFFC51BC7A;
	fma.rn.f64 	%fd471, %fd469, %fd439, %fd470;
	mov.f64 	%fd472, 0d3FD7FFFFFFFFB5EA;
	fma.rn.f64 	%fd473, %fd471, %fd439, %fd472;
	fma.rn.f64 	%fd40, %fd473, %fd438, %fd2;
	rsqrt.approx.f64 	%fd474, %fd2;
	mul.f64 	%fd475, %fd474, 0d3FE9884533D43651;
	mul.f64 	%fd41, %fd454, %fd475;
	mul.f64 	%fd476, %fd40, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r105, %fd476;
	add.u64 	%rd17, %SP, 0;
	add.u64 	%rd18, %SPL, 0;
	st.local.u32 	[%rd18], %r105;
	cvt.rn.f64.s32	%fd477, %r105;
	neg.f64 	%fd478, %fd477;
	mov.f64 	%fd479, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd480, %fd478, %fd479, %fd40;
	mov.f64 	%fd481, 0d3C91A62633145C00;
	fma.rn.f64 	%fd482, %fd478, %fd481, %fd480;
	mov.f64 	%fd483, 0d397B839A252049C0;
	fma.rn.f64 	%fd529, %fd478, %fd483, %fd482;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd40;
	}
	and.b32  	%r78, %r77, 2145386496;
	setp.lt.u32	%p26, %r78, 1105199104;
	@%p26 bra 	BB79_41;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd40;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd529, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r105, [%rd18];

BB79_41:
	and.b32  	%r79, %r105, 3;
	cvt.rn.f64.s32	%fd484, %r79;
	add.f64 	%fd485, %fd529, 0dC00F6A7A2955385E;
	fma.rn.f64 	%fd530, %fd484, 0d3FF921FB54442D18, %fd485;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd530;
	}
	and.b32  	%r81, %r80, 2147483647;
	setp.ne.s32	%p27, %r81, 2146435072;
	@%p27 bra 	BB79_44;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r82, %temp}, %fd530;
	}
	setp.ne.s32	%p28, %r82, 0;
	@%p28 bra 	BB79_44;

	mov.f64 	%fd486, 0d0000000000000000;
	mul.rn.f64 	%fd530, %fd530, %fd486;

BB79_44:
	mov.f64 	%fd518, 0d397B839A252049C0;
	mov.f64 	%fd517, 0d3C91A62633145C00;
	mov.f64 	%fd516, 0d3FF921FB54442D18;
	mul.f64 	%fd487, %fd530, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r106, %fd487;
	st.local.u32 	[%rd18], %r106;
	cvt.rn.f64.s32	%fd488, %r106;
	neg.f64 	%fd489, %fd488;
	fma.rn.f64 	%fd491, %fd489, %fd516, %fd530;
	fma.rn.f64 	%fd493, %fd489, %fd517, %fd491;
	fma.rn.f64 	%fd531, %fd489, %fd518, %fd493;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd530;
	}
	and.b32  	%r84, %r83, 2145386496;
	setp.lt.u32	%p29, %r84, 1105199104;
	@%p29 bra 	BB79_46;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd530;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd531, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r106, [%rd18];

BB79_46:
	add.s32 	%r24, %r106, 1;
	and.b32  	%r85, %r24, 1;
	shl.b32 	%r86, %r85, 3;
	setp.eq.s32	%p30, %r85, 0;
	selp.f64	%fd495, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p30;
	add.s32 	%r87, %r86, 1;
	mul.wide.s32 	%rd25, %r87, 8;
	mov.u64 	%rd26, __cudart_sin_cos_coeffs;
	add.s64 	%rd27, %rd26, %rd25;
	ld.const.f64 	%fd496, [%rd27];
	mul.rn.f64 	%fd51, %fd531, %fd531;
	fma.rn.f64 	%fd497, %fd495, %fd51, %fd496;
	ld.const.f64 	%fd498, [%rd27+8];
	fma.rn.f64 	%fd499, %fd497, %fd51, %fd498;
	ld.const.f64 	%fd500, [%rd27+16];
	fma.rn.f64 	%fd501, %fd499, %fd51, %fd500;
	ld.const.f64 	%fd502, [%rd27+24];
	fma.rn.f64 	%fd503, %fd501, %fd51, %fd502;
	ld.const.f64 	%fd504, [%rd27+32];
	fma.rn.f64 	%fd505, %fd503, %fd51, %fd504;
	ld.const.f64 	%fd506, [%rd27+40];
	fma.rn.f64 	%fd52, %fd505, %fd51, %fd506;
	fma.rn.f64 	%fd532, %fd52, %fd531, %fd531;
	@%p30 bra 	BB79_48;

	mov.f64 	%fd519, 0d3FF0000000000000;
	fma.rn.f64 	%fd532, %fd52, %fd51, %fd519;

BB79_48:
	and.b32  	%r88, %r24, 2;
	setp.eq.s32	%p31, %r88, 0;
	@%p31 bra 	BB79_50;

	mov.f64 	%fd508, 0d0000000000000000;
	mov.f64 	%fd509, 0dBFF0000000000000;
	fma.rn.f64 	%fd532, %fd532, %fd509, %fd508;

BB79_50:
	mul.f64 	%fd534, %fd41, %fd532;
	bra.uni 	BB79_52;

BB79_3:
	mul.f64 	%fd63, %fd2, %fd2;
	mov.f64 	%fd64, 0dBDCF0B5B1FB7B95E;
	mov.f64 	%fd65, 0d3D5249F90687428C;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3E432E589311FA14;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0dBEB0A780AA4A92E9;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F12C7DBFFCAEC2B;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0dBF6835B97894BA4A;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3FABD3975C75B4A3;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0dBFC91866143CBC8A;
	fma.rn.f64 	%fd3, %fd76, %fd63, %fd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r100, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r99}, %fd2;
	}
	mov.u32 	%r101, -1023;
	setp.gt.s32	%p4, %r99, 1048575;
	mov.f64 	%fd520, %fd2;
	@%p4 bra 	BB79_5;

	mul.f64 	%fd520, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r99}, %fd520;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r100, %temp}, %fd520;
	}
	mov.u32 	%r101, -1077;

BB79_5:
	add.s32 	%r48, %r99, -1;
	setp.lt.u32	%p5, %r48, 2146435071;
	@%p5 bra 	BB79_7;
	bra.uni 	BB79_6;

BB79_7:
	shr.u32 	%r50, %r99, 20;
	add.s32 	%r102, %r101, %r50;
	and.b32  	%r51, %r99, -2146435073;
	or.b32  	%r52, %r51, 1072693248;
	mov.b64 	%fd521, {%r100, %r52};
	setp.lt.s32	%p7, %r52, 1073127583;
	@%p7 bra 	BB79_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd521;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd521;
	}
	add.s32 	%r55, %r54, -1048576;
	mov.b64 	%fd521, {%r53, %r55};
	add.s32 	%r102, %r102, 1;

BB79_9:
	add.f64 	%fd80, %fd521, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd81, %fd80;
	neg.f64 	%fd82, %fd80;
	mov.f64 	%fd83, 0d3FF0000000000000;
	fma.rn.f64 	%fd84, %fd82, %fd81, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd84, %fd84;
	fma.rn.f64 	%fd86, %fd85, %fd81, %fd81;
	add.f64 	%fd87, %fd521, 0dBFF0000000000000;
	mul.f64 	%fd88, %fd87, %fd86;
	fma.rn.f64 	%fd89, %fd87, %fd86, %fd88;
	mul.f64 	%fd90, %fd89, %fd89;
	mov.f64 	%fd91, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd92, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	mov.f64 	%fd94, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd95, %fd93, %fd90, %fd94;
	mov.f64 	%fd96, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd97, %fd95, %fd90, %fd96;
	mov.f64 	%fd98, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd99, %fd97, %fd90, %fd98;
	mov.f64 	%fd100, 0d3F624924923BE72D;
	fma.rn.f64 	%fd101, %fd99, %fd90, %fd100;
	mov.f64 	%fd102, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd103, %fd101, %fd90, %fd102;
	mov.f64 	%fd104, 0d3FB5555555555554;
	fma.rn.f64 	%fd105, %fd103, %fd90, %fd104;
	sub.f64 	%fd106, %fd87, %fd89;
	add.f64 	%fd107, %fd106, %fd106;
	neg.f64 	%fd108, %fd89;
	fma.rn.f64 	%fd109, %fd108, %fd87, %fd107;
	mul.f64 	%fd110, %fd86, %fd109;
	mul.f64 	%fd111, %fd90, %fd105;
	fma.rn.f64 	%fd112, %fd111, %fd89, %fd110;
	xor.b32  	%r56, %r102, -2147483648;
	mov.u32 	%r57, -2147483648;
	mov.u32 	%r58, 1127219200;
	mov.b64 	%fd113, {%r56, %r58};
	mov.b64 	%fd114, {%r57, %r58};
	sub.f64 	%fd115, %fd113, %fd114;
	mov.f64 	%fd116, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd117, %fd115, %fd116, %fd89;
	neg.f64 	%fd118, %fd115;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	sub.f64 	%fd120, %fd119, %fd89;
	sub.f64 	%fd121, %fd112, %fd120;
	mov.f64 	%fd122, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd123, %fd115, %fd122, %fd121;
	add.f64 	%fd522, %fd117, %fd123;
	bra.uni 	BB79_10;

BB79_32:
	add.f64 	%fd307, %fd2, 0dC00193BED4DFF243;
	add.f64 	%fd308, %fd307, 0d3C8BD1E50D219BFD;
	mov.f64 	%fd309, 0d3E4833AAE4D8B975;
	mov.f64 	%fd310, 0dBE2B87B0BE2AA150;
	fma.rn.f64 	%fd311, %fd310, %fd308, %fd309;
	mov.f64 	%fd312, 0dBE44E279B423E68F;
	fma.rn.f64 	%fd313, %fd311, %fd308, %fd312;
	mov.f64 	%fd314, 0d3E129DC6A747EB4F;
	fma.rn.f64 	%fd315, %fd313, %fd308, %fd314;
	mov.f64 	%fd316, 0dBE61D15534496CD8;
	fma.rn.f64 	%fd317, %fd315, %fd308, %fd316;
	mov.f64 	%fd318, 0d3E7EEC8D48FECE00;
	fma.rn.f64 	%fd319, %fd317, %fd308, %fd318;
	mov.f64 	%fd320, 0dBE8D1180AF70A134;
	fma.rn.f64 	%fd321, %fd319, %fd308, %fd320;
	mov.f64 	%fd322, 0d3E9C8386A0EA1388;
	fma.rn.f64 	%fd323, %fd321, %fd308, %fd322;
	mov.f64 	%fd324, 0dBEB01A014E7F3250;
	fma.rn.f64 	%fd325, %fd323, %fd308, %fd324;
	mov.f64 	%fd326, 0d3EC1FB752010A320;
	fma.rn.f64 	%fd327, %fd325, %fd308, %fd326;
	mov.f64 	%fd328, 0dBED3AA0AFF4E332B;
	fma.rn.f64 	%fd329, %fd327, %fd308, %fd328;
	mov.f64 	%fd330, 0d3EE584A6C77F6700;
	fma.rn.f64 	%fd331, %fd329, %fd308, %fd330;
	mov.f64 	%fd332, 0dBEF794C520FC2EBB;
	fma.rn.f64 	%fd333, %fd331, %fd308, %fd332;
	mov.f64 	%fd334, 0d3F09D18D2D35CC71;
	fma.rn.f64 	%fd335, %fd333, %fd308, %fd334;
	mov.f64 	%fd336, 0dBF1C3FB7315C4599;
	fma.rn.f64 	%fd337, %fd335, %fd308, %fd336;
	mov.f64 	%fd338, 0d3F2EEA7ADECCE927;
	fma.rn.f64 	%fd339, %fd337, %fd308, %fd338;
	mov.f64 	%fd340, 0dBF40B2D85257446F;
	fma.rn.f64 	%fd341, %fd339, %fd308, %fd340;
	mov.f64 	%fd342, 0d3F517AB4B1FE5D5B;
	fma.rn.f64 	%fd343, %fd341, %fd308, %fd342;
	mov.f64 	%fd344, 0dBF65429DC6516C0D;
	fma.rn.f64 	%fd345, %fd343, %fd308, %fd344;
	mov.f64 	%fd346, 0d3F7E671C7D0B090B;
	fma.rn.f64 	%fd347, %fd345, %fd308, %fd346;
	mov.f64 	%fd348, 0dBF73A6DEC36FB27C;
	fma.rn.f64 	%fd349, %fd347, %fd308, %fd348;
	mov.f64 	%fd350, 0dBFA0D2AF4E931FD1;
	fma.rn.f64 	%fd351, %fd349, %fd308, %fd350;
	mov.f64 	%fd352, 0dBFBE56F82217B964;
	fma.rn.f64 	%fd353, %fd351, %fd308, %fd352;
	mov.f64 	%fd354, 0d3FE0AA48442F014B;
	fma.rn.f64 	%fd355, %fd353, %fd308, %fd354;
	mul.f64 	%fd534, %fd308, %fd355;
	bra.uni 	BB79_52;

BB79_6:
	mov.f64 	%fd78, 0d7FF0000000000000;
	fma.rn.f64 	%fd79, %fd520, %fd78, %fd78;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd520;
	}
	mov.b32 	 %f1, %r49;
	setp.eq.f32	%p6, %f1, 0f00000000;
	selp.f64	%fd522, 0dFFF0000000000000, %fd79, %p6;

BB79_10:
	abs.f64 	%fd12, %fd2;
	setp.gtu.f64	%p8, %fd12, 0d400353AABAD7B784;
	@%p8 bra 	BB79_12;
	bra.uni 	BB79_11;

BB79_12:
	setp.gtu.f64	%p9, %fd12, 0d4015B1D0574614EA;
	@%p9 bra 	BB79_14;
	bra.uni 	BB79_13;

BB79_14:
	setp.gtu.f64	%p10, %fd12, 0d40213065E54C1AA9;
	@%p10 bra 	BB79_16;
	bra.uni 	BB79_15;

BB79_16:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd12;
	}
	and.b32  	%r60, %r59, 2147483647;
	setp.ne.s32	%p11, %r60, 2146435072;
	@%p11 bra 	BB79_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd12;
	}
	setp.eq.s32	%p12, %r61, 0;
	mov.f64 	%fd528, 0d0000000000000000;
	@%p12 bra 	BB79_30;

BB79_18:
	rcp.approx.ftz.f64 	%fd222, %fd12;
	neg.f64 	%fd223, %fd12;
	mov.f64 	%fd224, 0d3FF0000000000000;
	fma.rn.f64 	%fd225, %fd223, %fd222, %fd224;
	fma.rn.f64 	%fd226, %fd225, %fd225, %fd225;
	fma.rn.f64 	%fd227, %fd226, %fd222, %fd222;
	mul.f64 	%fd228, %fd227, %fd227;
	mov.f64 	%fd229, 0dC099C06322A3F8BE;
	mov.f64 	%fd230, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd231, %fd230, %fd228, %fd229;
	mov.f64 	%fd232, 0d405B89354DA77324;
	fma.rn.f64 	%fd233, %fd231, %fd228, %fd232;
	mov.f64 	%fd234, 0dC01E352294653188;
	fma.rn.f64 	%fd235, %fd233, %fd228, %fd234;
	mov.f64 	%fd236, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd237, %fd235, %fd228, %fd236;
	mov.f64 	%fd238, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd239, %fd237, %fd228, %fd238;
	mov.f64 	%fd240, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd241, %fd239, %fd228, %fd240;
	mov.f64 	%fd242, 0d3FF00000000068CC;
	fma.rn.f64 	%fd243, %fd241, %fd228, %fd242;
	mov.f64 	%fd244, 0d415A30AC6857BEE0;
	mov.f64 	%fd245, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd246, %fd245, %fd228, %fd244;
	mov.f64 	%fd247, 0dC11764222AD7C910;
	fma.rn.f64 	%fd248, %fd246, %fd228, %fd247;
	mov.f64 	%fd249, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd250, %fd248, %fd228, %fd249;
	mov.f64 	%fd251, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd252, %fd250, %fd228, %fd251;
	mov.f64 	%fd253, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd254, %fd252, %fd228, %fd253;
	mov.f64 	%fd255, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd256, %fd254, %fd228, %fd255;
	mov.f64 	%fd257, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd258, %fd256, %fd228, %fd257;
	mov.f64 	%fd259, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd260, %fd258, %fd228, %fd259;
	mov.f64 	%fd261, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd262, %fd260, %fd228, %fd261;
	fma.rn.f64 	%fd16, %fd262, %fd227, %fd12;
	rsqrt.approx.f64 	%fd263, %fd12;
	mul.f64 	%fd264, %fd263, 0d3FE9884533D43651;
	mul.f64 	%fd17, %fd243, %fd264;
	mul.f64 	%fd265, %fd16, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r103, %fd265;
	add.u64 	%rd7, %SPL, 0;
	st.local.u32 	[%rd7], %r103;
	cvt.rn.f64.s32	%fd266, %r103;
	neg.f64 	%fd267, %fd266;
	mov.f64 	%fd268, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd269, %fd267, %fd268, %fd16;
	mov.f64 	%fd270, 0d3C91A62633145C00;
	fma.rn.f64 	%fd271, %fd267, %fd270, %fd269;
	mov.f64 	%fd272, 0d397B839A252049C0;
	fma.rn.f64 	%fd523, %fd267, %fd272, %fd271;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd16;
	}
	and.b32  	%r63, %r62, 2145386496;
	setp.lt.u32	%p13, %r63, 1105199104;
	@%p13 bra 	BB79_20;

	add.u64 	%rd32, %SP, 0;
	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd16;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd523, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r103, [%rd7];

BB79_20:
	and.b32  	%r64, %r103, 3;
	cvt.rn.f64.s32	%fd273, %r64;
	add.f64 	%fd274, %fd523, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd524, %fd273, 0d3FF921FB54442D18, %fd274;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd524;
	}
	and.b32  	%r66, %r65, 2147483647;
	setp.ne.s32	%p14, %r66, 2146435072;
	@%p14 bra 	BB79_23;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd524;
	}
	setp.ne.s32	%p15, %r67, 0;
	@%p15 bra 	BB79_23;

	mov.f64 	%fd275, 0d0000000000000000;
	mul.rn.f64 	%fd524, %fd524, %fd275;

BB79_23:
	mov.f64 	%fd513, 0d397B839A252049C0;
	mov.f64 	%fd512, 0d3C91A62633145C00;
	mov.f64 	%fd511, 0d3FF921FB54442D18;
	mul.f64 	%fd276, %fd524, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r104, %fd276;
	st.local.u32 	[%rd7], %r104;
	cvt.rn.f64.s32	%fd277, %r104;
	neg.f64 	%fd278, %fd277;
	fma.rn.f64 	%fd280, %fd278, %fd511, %fd524;
	fma.rn.f64 	%fd282, %fd278, %fd512, %fd280;
	fma.rn.f64 	%fd525, %fd278, %fd513, %fd282;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd524;
	}
	and.b32  	%r69, %r68, 2145386496;
	setp.lt.u32	%p16, %r69, 1105199104;
	@%p16 bra 	BB79_25;

	add.u64 	%rd31, %SP, 0;
	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd524;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd31;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd525, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r104, [%rd7];

BB79_25:
	add.s32 	%r17, %r104, 1;
	and.b32  	%r70, %r17, 1;
	shl.b32 	%r71, %r70, 3;
	setp.eq.s32	%p17, %r70, 0;
	selp.f64	%fd284, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p17;
	add.s32 	%r72, %r71, 1;
	mul.wide.s32 	%rd14, %r72, 8;
	mov.u64 	%rd15, __cudart_sin_cos_coeffs;
	add.s64 	%rd16, %rd15, %rd14;
	ld.const.f64 	%fd285, [%rd16];
	mul.rn.f64 	%fd27, %fd525, %fd525;
	fma.rn.f64 	%fd286, %fd284, %fd27, %fd285;
	ld.const.f64 	%fd287, [%rd16+8];
	fma.rn.f64 	%fd288, %fd286, %fd27, %fd287;
	ld.const.f64 	%fd289, [%rd16+16];
	fma.rn.f64 	%fd290, %fd288, %fd27, %fd289;
	ld.const.f64 	%fd291, [%rd16+24];
	fma.rn.f64 	%fd292, %fd290, %fd27, %fd291;
	ld.const.f64 	%fd293, [%rd16+32];
	fma.rn.f64 	%fd294, %fd292, %fd27, %fd293;
	ld.const.f64 	%fd295, [%rd16+40];
	fma.rn.f64 	%fd28, %fd294, %fd27, %fd295;
	fma.rn.f64 	%fd526, %fd28, %fd525, %fd525;
	@%p17 bra 	BB79_27;

	mov.f64 	%fd514, 0d3FF0000000000000;
	fma.rn.f64 	%fd526, %fd28, %fd27, %fd514;

BB79_27:
	and.b32  	%r73, %r17, 2;
	setp.eq.s32	%p18, %r73, 0;
	@%p18 bra 	BB79_29;

	mov.f64 	%fd297, 0d0000000000000000;
	mov.f64 	%fd298, 0dBFF0000000000000;
	fma.rn.f64 	%fd526, %fd526, %fd298, %fd297;

BB79_29:
	mul.f64 	%fd528, %fd17, %fd526;
	bra.uni 	BB79_30;

BB79_11:
	mov.f64 	%fd124, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd125, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd126, %fd125, %fd12, %fd124;
	mov.f64 	%fd127, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd128, %fd126, %fd12, %fd127;
	mov.f64 	%fd129, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd130, %fd128, %fd12, %fd129;
	mov.f64 	%fd131, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd132, %fd130, %fd12, %fd131;
	mov.f64 	%fd133, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd134, %fd132, %fd12, %fd133;
	mov.f64 	%fd135, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd136, %fd134, %fd12, %fd135;
	mov.f64 	%fd137, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd138, %fd136, %fd12, %fd137;
	mov.f64 	%fd139, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd140, %fd138, %fd12, %fd139;
	mov.f64 	%fd141, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd142, %fd140, %fd12, %fd141;
	mov.f64 	%fd143, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd144, %fd142, %fd12, %fd143;
	mov.f64 	%fd145, 0d3F65555555506982;
	fma.rn.f64 	%fd146, %fd144, %fd12, %fd145;
	mov.f64 	%fd147, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd148, %fd146, %fd12, %fd147;
	mov.f64 	%fd149, 0dBFB0000000000065;
	fma.rn.f64 	%fd150, %fd148, %fd12, %fd149;
	mov.f64 	%fd151, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd152, %fd150, %fd12, %fd151;
	mov.f64 	%fd153, 0d3FE0000000000000;
	fma.rn.f64 	%fd154, %fd152, %fd12, %fd153;
	mul.f64 	%fd528, %fd12, %fd154;
	bra.uni 	BB79_30;

BB79_34:
	add.f64 	%fd356, %fd2, 0dC015B7FE4E87B02E;
	add.f64 	%fd357, %fd356, 0dBCBDFE7BAC228E8C;
	mov.f64 	%fd358, 0d3CC69A30996793E2;
	mov.f64 	%fd359, 0d3CBA3C76069F1D8C;
	fma.rn.f64 	%fd360, %fd359, %fd357, %fd358;
	mov.f64 	%fd361, 0dBCDDD8432FE756E7;
	fma.rn.f64 	%fd362, %fd360, %fd357, %fd361;
	mov.f64 	%fd363, 0dBD143158EEE220F7;
	fma.rn.f64 	%fd364, %fd362, %fd357, %fd363;
	mov.f64 	%fd365, 0d3D28D44491230F5A;
	fma.rn.f64 	%fd366, %fd364, %fd357, %fd365;
	mov.f64 	%fd367, 0dBD438842EAF4EDBC;
	fma.rn.f64 	%fd368, %fd366, %fd357, %fd367;
	mov.f64 	%fd369, 0d3D74958DAFBFAF5A;
	fma.rn.f64 	%fd370, %fd368, %fd357, %fd369;
	mov.f64 	%fd371, 0dBD9449A60E664848;
	fma.rn.f64 	%fd372, %fd370, %fd357, %fd371;
	mov.f64 	%fd373, 0d3D838BC8CD594A76;
	fma.rn.f64 	%fd374, %fd372, %fd357, %fd373;
	mov.f64 	%fd375, 0dBDFA798002141323;
	fma.rn.f64 	%fd376, %fd374, %fd357, %fd375;
	mov.f64 	%fd377, 0d3E380B4198956AAA;
	fma.rn.f64 	%fd378, %fd376, %fd357, %fd377;
	mov.f64 	%fd379, 0d3E5B62B5F21BACD4;
	fma.rn.f64 	%fd380, %fd378, %fd357, %fd379;
	mov.f64 	%fd381, 0dBEA255E729FB6AAE;
	fma.rn.f64 	%fd382, %fd380, %fd357, %fd381;
	mov.f64 	%fd383, 0dBEC80618F6BAE5AA;
	fma.rn.f64 	%fd384, %fd382, %fd357, %fd383;
	mov.f64 	%fd385, 0d3F085B940F8E8D36;
	fma.rn.f64 	%fd386, %fd384, %fd357, %fd385;
	mov.f64 	%fd387, 0d3F2337C7E10E14E8;
	fma.rn.f64 	%fd388, %fd386, %fd357, %fd387;
	mov.f64 	%fd389, 0dBF61BE6DB99332CA;
	fma.rn.f64 	%fd390, %fd388, %fd357, %fd389;
	mov.f64 	%fd391, 0dBF710A329E2BE9B8;
	fma.rn.f64 	%fd392, %fd390, %fd357, %fd391;
	mov.f64 	%fd393, 0d3FAA15D92DFE3FCF;
	fma.rn.f64 	%fd394, %fd392, %fd357, %fd393;
	mov.f64 	%fd395, 0d3FA00B9F8571C9BE;
	fma.rn.f64 	%fd396, %fd394, %fd357, %fd395;
	mov.f64 	%fd397, 0dBFD5C7C556F0C19A;
	fma.rn.f64 	%fd398, %fd396, %fd357, %fd397;
	mul.f64 	%fd534, %fd357, %fd398;
	bra.uni 	BB79_52;

BB79_13:
	add.f64 	%fd155, %fd12, 0dC00EA75575AF6F09;
	add.f64 	%fd156, %fd155, 0d3CA60155A9D1B256;
	mov.f64 	%fd157, 0d3D41011A1DF02DAD;
	mov.f64 	%fd158, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd159, %fd158, %fd156, %fd157;
	mov.f64 	%fd160, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd161, %fd159, %fd156, %fd160;
	mov.f64 	%fd162, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd163, %fd161, %fd156, %fd162;
	mov.f64 	%fd164, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd165, %fd163, %fd156, %fd164;
	mov.f64 	%fd166, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd167, %fd165, %fd156, %fd166;
	mov.f64 	%fd168, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd169, %fd167, %fd156, %fd168;
	mov.f64 	%fd170, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd171, %fd169, %fd156, %fd170;
	mov.f64 	%fd172, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd173, %fd171, %fd156, %fd172;
	mov.f64 	%fd174, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd175, %fd173, %fd156, %fd174;
	mov.f64 	%fd176, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd177, %fd175, %fd156, %fd176;
	mov.f64 	%fd178, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd179, %fd177, %fd156, %fd178;
	mov.f64 	%fd180, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd181, %fd179, %fd156, %fd180;
	mov.f64 	%fd182, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd183, %fd181, %fd156, %fd182;
	mov.f64 	%fd184, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd185, %fd183, %fd156, %fd184;
	mov.f64 	%fd186, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd187, %fd185, %fd156, %fd186;
	mul.f64 	%fd528, %fd156, %fd187;
	bra.uni 	BB79_30;

BB79_36:
	add.f64 	%fd399, %fd2, 0dC0213127AE6169B4;
	add.f64 	%fd400, %fd399, 0dBCB479CC068D9046;
	mov.f64 	%fd401, 0dBD43515F67644276;
	mov.f64 	%fd402, 0d3CB09CCC22945996;
	fma.rn.f64 	%fd403, %fd402, %fd400, %fd401;
	mov.f64 	%fd404, 0dBD72C5B978E9F5C7;
	fma.rn.f64 	%fd405, %fd403, %fd400, %fd404;
	mov.f64 	%fd406, 0d3DBEC1151613913C;
	fma.rn.f64 	%fd407, %fd405, %fd400, %fd406;
	mov.f64 	%fd408, 0d3DE9E38D13C4A824;
	fma.rn.f64 	%fd409, %fd407, %fd400, %fd408;
	mov.f64 	%fd410, 0dBE341E75E1088EB5;
	fma.rn.f64 	%fd411, %fd409, %fd400, %fd410;
	mov.f64 	%fd412, 0dBE5A384EBB13CFE1;
	fma.rn.f64 	%fd413, %fd411, %fd400, %fd412;
	mov.f64 	%fd414, 0d3EA2BECB27F8C8F8;
	fma.rn.f64 	%fd415, %fd413, %fd400, %fd414;
	mov.f64 	%fd416, 0d3EC176E72B989FD8;
	fma.rn.f64 	%fd417, %fd415, %fd400, %fd416;
	mov.f64 	%fd418, 0dBF06F7BAB102F822;
	fma.rn.f64 	%fd419, %fd417, %fd400, %fd418;
	mov.f64 	%fd420, 0dBF1B50D7E1D278E1;
	fma.rn.f64 	%fd421, %fd419, %fd400, %fd420;
	mov.f64 	%fd422, 0d3F607A678D60004F;
	fma.rn.f64 	%fd423, %fd421, %fd400, %fd422;
	mov.f64 	%fd424, 0d3F63CED2A2E69115;
	fma.rn.f64 	%fd425, %fd423, %fd400, %fd424;
	mov.f64 	%fd426, 0dBFA6395DFE49FCD4;
	fma.rn.f64 	%fd427, %fd425, %fd400, %fd426;
	mov.f64 	%fd428, 0dBF902B3933CF21B1;
	fma.rn.f64 	%fd429, %fd427, %fd400, %fd428;
	mov.f64 	%fd430, 0d3FD15F993FCEAB5C;
	fma.rn.f64 	%fd431, %fd429, %fd400, %fd430;
	mul.f64 	%fd534, %fd400, %fd431;
	bra.uni 	BB79_52;

BB79_15:
	add.f64 	%fd188, %fd12, 0dC01C0FF5F3B47250;
	add.f64 	%fd189, %fd188, 0d3C9B226D9D243827;
	mov.f64 	%fd190, 0dBD40E8363DB649A9;
	mov.f64 	%fd191, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd192, %fd191, %fd189, %fd190;
	mov.f64 	%fd193, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd194, %fd192, %fd189, %fd193;
	mov.f64 	%fd195, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd196, %fd194, %fd189, %fd195;
	mov.f64 	%fd197, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd198, %fd196, %fd189, %fd197;
	mov.f64 	%fd199, 0dBE34462EED732A23;
	fma.rn.f64 	%fd200, %fd198, %fd189, %fd199;
	mov.f64 	%fd201, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd202, %fd200, %fd189, %fd201;
	mov.f64 	%fd203, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd204, %fd202, %fd189, %fd203;
	mov.f64 	%fd205, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd206, %fd204, %fd189, %fd205;
	mov.f64 	%fd207, 0dBF0796A751B32693;
	fma.rn.f64 	%fd208, %fd206, %fd189, %fd207;
	mov.f64 	%fd209, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd210, %fd208, %fd189, %fd209;
	mov.f64 	%fd211, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd212, %fd210, %fd189, %fd211;
	mov.f64 	%fd213, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd214, %fd212, %fd189, %fd213;
	mov.f64 	%fd215, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd216, %fd214, %fd189, %fd215;
	mov.f64 	%fd217, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd218, %fd216, %fd189, %fd217;
	mov.f64 	%fd219, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd220, %fd218, %fd189, %fd219;
	mul.f64 	%fd528, %fd189, %fd220;

BB79_30:
	abs.f64 	%fd515, %fd1;
	neg.f64 	%fd299, %fd528;
	setp.lt.f64	%p19, %fd515, 0d0000000000000000;
	selp.f64	%fd300, %fd299, %fd528, %p19;
	mul.f64 	%fd301, %fd515, 0d3FE0000000000000;
	setp.lt.f64	%p20, %fd12, 0d39B4484BFEEBC2A0;
	selp.f64	%fd302, %fd301, %fd300, %p20;
	mov.f64 	%fd303, 0dBFF0000000000000;
	div.rn.f64 	%fd304, %fd303, %fd515;
	fma.rn.f64 	%fd305, %fd522, %fd302, %fd304;
	mul.f64 	%fd306, %fd305, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd534, %fd515, %fd3, %fd306;

BB79_52:
	setp.gtu.f64	%p32, %fd1, 0d0000000000000000;
	@%p32 bra 	BB79_54;

	setp.eq.f64	%p33, %fd1, 0d0000000000000000;
	selp.f64	%fd534, 0dFFF0000000000000, 0dFFF8000000000000, %p33;

BB79_54:
	cvta.to.global.u64 	%rd28, %rd1;
	add.s64 	%rd30, %rd28, %rd4;
	st.global.f64 	[%rd30], %fd534;

BB79_55:
	ret;
}

	// .globl	vec_copysign
.visible .entry vec_copysign(
	.param .u32 vec_copysign_param_0,
	.param .u64 vec_copysign_param_1,
	.param .u64 vec_copysign_param_2,
	.param .u64 vec_copysign_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_copysign_param_0];
	ld.param.u64 	%rd1, [vec_copysign_param_1];
	ld.param.u64 	%rd2, [vec_copysign_param_2];
	ld.param.u64 	%rd3, [vec_copysign_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB80_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd2;
	}
	and.b32  	%r15, %r12, -2147483648;
	and.b32  	%r16, %r14, 2147483647;
	or.b32  	%r17, %r16, %r15;
	mov.b64 	%fd3, {%r13, %r17};
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB80_2:
	ret;
}

	// .globl	vec_fdim
.visible .entry vec_fdim(
	.param .u32 vec_fdim_param_0,
	.param .u64 vec_fdim_param_1,
	.param .u64 vec_fdim_param_2,
	.param .u64 vec_fdim_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fdim_param_0];
	ld.param.u64 	%rd1, [vec_fdim_param_1];
	ld.param.u64 	%rd2, [vec_fdim_param_2];
	ld.param.u64 	%rd3, [vec_fdim_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB81_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd4, %fd3, 0d0000000000000000, %p2;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd4;

BB81_2:
	ret;
}

	// .globl	vec_fdivide
.visible .entry vec_fdivide(
	.param .u32 vec_fdivide_param_0,
	.param .u64 vec_fdivide_param_1,
	.param .u64 vec_fdivide_param_2,
	.param .u64 vec_fdivide_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fdivide_param_0];
	ld.param.u64 	%rd1, [vec_fdivide_param_1];
	ld.param.u64 	%rd2, [vec_fdivide_param_2];
	ld.param.u64 	%rd3, [vec_fdivide_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB82_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB82_2:
	ret;
}

	// .globl	vec_fmax
.visible .entry vec_fmax(
	.param .u32 vec_fmax_param_0,
	.param .u64 vec_fmax_param_1,
	.param .u64 vec_fmax_param_2,
	.param .u64 vec_fmax_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fmax_param_0];
	ld.param.u64 	%rd1, [vec_fmax_param_1];
	ld.param.u64 	%rd2, [vec_fmax_param_2];
	ld.param.u64 	%rd3, [vec_fmax_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB83_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	max.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB83_2:
	ret;
}

	// .globl	vec_fmin
.visible .entry vec_fmin(
	.param .u32 vec_fmin_param_0,
	.param .u64 vec_fmin_param_1,
	.param .u64 vec_fmin_param_2,
	.param .u64 vec_fmin_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fmin_param_0];
	ld.param.u64 	%rd1, [vec_fmin_param_1];
	ld.param.u64 	%rd2, [vec_fmin_param_2];
	ld.param.u64 	%rd3, [vec_fmin_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB84_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	min.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB84_2:
	ret;
}

	// .globl	vec_fmod
.visible .entry vec_fmod(
	.param .u32 vec_fmod_param_0,
	.param .u64 vec_fmod_param_1,
	.param .u64 vec_fmod_param_2,
	.param .u64 vec_fmod_param_3
)
{
	.reg .pred 	%p<55>;
	.reg .b32 	%r<135>;
	.reg .f64 	%fd<54>;
	.reg .b64 	%rd<142>;


	ld.param.u32 	%r28, [vec_fmod_param_0];
	ld.param.u64 	%rd22, [vec_fmod_param_2];
	ld.param.u64 	%rd23, [vec_fmod_param_3];
	mov.u32 	%r29, %tid.x;
	mov.u32 	%r30, %ntid.y;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r32, %tid.y;
	mad.lo.s32 	%r33, %r30, %r31, %r32;
	mov.u32 	%r34, %nctaid.x;
	mov.u32 	%r35, %ctaid.x;
	mad.lo.s32 	%r36, %r33, %r34, %r35;
	mov.u32 	%r37, %ntid.x;
	mad.lo.s32 	%r1, %r36, %r37, %r29;
	setp.ge.s32	%p1, %r1, %r28;
	@%p1 bra 	BB85_31;

	cvta.to.global.u64 	%rd24, %rd22;
	mul.wide.s32 	%rd25, %r1, 8;
	add.s64 	%rd26, %rd24, %rd25;
	cvta.to.global.u64 	%rd27, %rd23;
	add.s64 	%rd28, %rd27, %rd25;
	ld.global.f64 	%fd1, [%rd26];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r38, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd28];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd2;
	}
	and.b32  	%r41, %r40, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd2;
	}
	mov.b64 	%fd50, {%r39, %r38};
	mov.b64 	%fd51, {%r42, %r41};
	setp.gt.u32	%p2, %r38, 2146435071;
	setp.gt.u32	%p3, %r41, 2146435071;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB85_27;
	bra.uni 	BB85_2;

BB85_27:
	setp.gtu.f64	%p51, %fd50, 0d7FF0000000000000;
	setp.gtu.f64	%p52, %fd51, 0d7FF0000000000000;
	or.pred  	%p53, %p51, %p52;
	@%p53 bra 	BB85_29;
	bra.uni 	BB85_28;

BB85_29:
	add.f64 	%fd53, %fd1, %fd2;
	bra.uni 	BB85_30;

BB85_2:
	setp.eq.f64	%p5, %fd51, 0d0000000000000000;
	mov.f64 	%fd53, 0dFFF8000000000000;
	@%p5 bra 	BB85_30;

	setp.ltu.f64	%p6, %fd50, %fd51;
	mov.f64 	%fd53, %fd1;
	@%p6 bra 	BB85_30;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd50;
	}
	shr.u32 	%r126, %r43, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd51;
	}
	shr.u32 	%r127, %r44, 20;
	setp.ne.s32	%p7, %r126, 0;
	@%p7 bra 	BB85_6;

	mul.f64 	%fd50, %fd50, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd50;
	}
	shr.u32 	%r46, %r45, 20;
	add.s32 	%r126, %r46, -54;

BB85_6:
	setp.ne.s32	%p8, %r127, 0;
	@%p8 bra 	BB85_8;

	mul.f64 	%fd51, %fd51, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd51;
	}
	shr.u32 	%r48, %r47, 20;
	add.s32 	%r127, %r48, -54;

BB85_8:
	mov.b64 	 %rd30, %fd50;
	and.b64  	%rd31, %rd30, 4503599627370495;
	or.b64  	%rd8, %rd31, 4503599627370496;
	mov.b64 	 %rd32, %fd51;
	and.b64  	%rd33, %rd32, 4503599627370495;
	or.b64  	%rd2, %rd33, 4503599627370496;
	sub.s32 	%r17, %r126, %r127;
	not.b32 	%r49, %r126;
	mov.u32 	%r50, -1;
	add.s32 	%r51, %r127, %r49;
	max.s32 	%r52, %r51, %r50;
	add.s32 	%r53, %r126, 2;
	sub.s32 	%r54, %r53, %r127;
	add.s32 	%r10, %r54, %r52;
	and.b32  	%r11, %r10, 3;
	setp.eq.s32	%p9, %r11, 0;
	mov.u64 	%rd141, 0;
	@%p9 bra 	BB85_14;

	setp.eq.s32	%p10, %r11, 1;
	@%p10 bra 	BB85_13;

	setp.eq.s32	%p11, %r11, 2;
	@%p11 bra 	BB85_12;

	sub.s64 	%rd34, %rd8, %rd2;
	mov.b64 	 %fd16, %rd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd16;
	}
	setp.lt.s32	%p12, %r55, 0;
	selp.b64	%rd35, %rd8, %rd34, %p12;
	add.s64 	%rd8, %rd35, %rd35;
	add.s32 	%r17, %r17, -1;

BB85_12:
	sub.s64 	%rd36, %rd8, %rd2;
	mov.b64 	 %fd17, %rd36;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd17;
	}
	setp.lt.s32	%p13, %r56, 0;
	selp.b64	%rd37, %rd8, %rd36, %p13;
	add.s64 	%rd8, %rd37, %rd37;
	add.s32 	%r17, %r17, -1;

BB85_13:
	sub.s64 	%rd38, %rd8, %rd2;
	mov.b64 	 %fd18, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd18;
	}
	setp.lt.s32	%p14, %r57, 0;
	selp.b64	%rd39, %rd8, %rd38, %p14;
	add.s64 	%rd8, %rd39, %rd39;
	add.s32 	%r17, %r17, -1;
	mov.u64 	%rd141, %rd8;

BB85_14:
	setp.lt.u32	%p15, %r10, 4;
	@%p15 bra 	BB85_24;

	mov.u32 	%r58, 2;
	sub.s32 	%r59, %r58, %r17;
	max.s32 	%r61, %r59, %r50;
	add.s32 	%r62, %r17, %r61;
	add.s32 	%r63, %r62, 1;
	shr.u32 	%r64, %r63, 2;
	add.s32 	%r18, %r64, 1;
	and.b32  	%r19, %r18, 3;
	setp.eq.s32	%p16, %r19, 0;
	mov.u64 	%rd141, 0;
	@%p16 bra 	BB85_21;

	setp.eq.s32	%p17, %r19, 1;
	@%p17 bra 	BB85_20;

	setp.eq.s32	%p18, %r19, 2;
	@%p18 bra 	BB85_19;

	sub.s64 	%rd41, %rd8, %rd2;
	mov.b64 	 %fd19, %rd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd19;
	}
	setp.lt.s32	%p19, %r65, 0;
	selp.b64	%rd42, %rd8, %rd41, %p19;
	add.s64 	%rd43, %rd42, %rd42;
	sub.s64 	%rd44, %rd43, %rd2;
	mov.b64 	 %fd20, %rd44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd20;
	}
	setp.lt.s32	%p20, %r66, 0;
	selp.b64	%rd45, %rd43, %rd44, %p20;
	add.s64 	%rd46, %rd45, %rd45;
	sub.s64 	%rd47, %rd46, %rd2;
	mov.b64 	 %fd21, %rd47;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd21;
	}
	setp.lt.s32	%p21, %r67, 0;
	selp.b64	%rd48, %rd46, %rd47, %p21;
	add.s64 	%rd49, %rd48, %rd48;
	sub.s64 	%rd50, %rd49, %rd2;
	mov.b64 	 %fd22, %rd50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd22;
	}
	setp.lt.s32	%p22, %r68, 0;
	selp.b64	%rd51, %rd49, %rd50, %p22;
	add.s64 	%rd8, %rd51, %rd51;
	add.s32 	%r17, %r17, -4;

BB85_19:
	sub.s64 	%rd52, %rd8, %rd2;
	mov.b64 	 %fd23, %rd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd23;
	}
	setp.lt.s32	%p23, %r69, 0;
	selp.b64	%rd53, %rd8, %rd52, %p23;
	add.s64 	%rd54, %rd53, %rd53;
	sub.s64 	%rd55, %rd54, %rd2;
	mov.b64 	 %fd24, %rd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd24;
	}
	setp.lt.s32	%p24, %r70, 0;
	selp.b64	%rd56, %rd54, %rd55, %p24;
	add.s64 	%rd57, %rd56, %rd56;
	sub.s64 	%rd58, %rd57, %rd2;
	mov.b64 	 %fd25, %rd58;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd25;
	}
	setp.lt.s32	%p25, %r71, 0;
	selp.b64	%rd59, %rd57, %rd58, %p25;
	add.s64 	%rd60, %rd59, %rd59;
	sub.s64 	%rd61, %rd60, %rd2;
	mov.b64 	 %fd26, %rd61;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r72}, %fd26;
	}
	setp.lt.s32	%p26, %r72, 0;
	selp.b64	%rd62, %rd60, %rd61, %p26;
	add.s64 	%rd8, %rd62, %rd62;
	add.s32 	%r17, %r17, -4;

BB85_20:
	sub.s64 	%rd63, %rd8, %rd2;
	mov.b64 	 %fd27, %rd63;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd27;
	}
	setp.lt.s32	%p27, %r73, 0;
	selp.b64	%rd64, %rd8, %rd63, %p27;
	add.s64 	%rd65, %rd64, %rd64;
	sub.s64 	%rd66, %rd65, %rd2;
	mov.b64 	 %fd28, %rd66;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r74}, %fd28;
	}
	setp.lt.s32	%p28, %r74, 0;
	selp.b64	%rd67, %rd65, %rd66, %p28;
	add.s64 	%rd68, %rd67, %rd67;
	sub.s64 	%rd69, %rd68, %rd2;
	mov.b64 	 %fd29, %rd69;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd29;
	}
	setp.lt.s32	%p29, %r75, 0;
	selp.b64	%rd70, %rd68, %rd69, %p29;
	add.s64 	%rd71, %rd70, %rd70;
	sub.s64 	%rd72, %rd71, %rd2;
	mov.b64 	 %fd30, %rd72;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r76}, %fd30;
	}
	setp.lt.s32	%p30, %r76, 0;
	selp.b64	%rd73, %rd71, %rd72, %p30;
	add.s64 	%rd8, %rd73, %rd73;
	add.s32 	%r17, %r17, -4;
	mov.u64 	%rd141, %rd8;

BB85_21:
	setp.lt.u32	%p31, %r18, 4;
	@%p31 bra 	BB85_24;

	mov.u64 	%rd141, %rd8;

BB85_23:
	sub.s64 	%rd74, %rd141, %rd2;
	mov.b64 	 %fd31, %rd74;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd31;
	}
	setp.lt.s32	%p32, %r77, 0;
	selp.b64	%rd75, %rd141, %rd74, %p32;
	add.s64 	%rd76, %rd75, %rd75;
	sub.s64 	%rd77, %rd76, %rd2;
	mov.b64 	 %fd32, %rd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd32;
	}
	setp.lt.s32	%p33, %r78, 0;
	selp.b64	%rd78, %rd76, %rd77, %p33;
	add.s64 	%rd79, %rd78, %rd78;
	sub.s64 	%rd80, %rd79, %rd2;
	mov.b64 	 %fd33, %rd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd33;
	}
	setp.lt.s32	%p34, %r79, 0;
	selp.b64	%rd81, %rd79, %rd80, %p34;
	add.s64 	%rd82, %rd81, %rd81;
	sub.s64 	%rd83, %rd82, %rd2;
	mov.b64 	 %fd34, %rd83;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd34;
	}
	setp.lt.s32	%p35, %r80, 0;
	selp.b64	%rd84, %rd82, %rd83, %p35;
	add.s64 	%rd85, %rd84, %rd84;
	sub.s64 	%rd86, %rd85, %rd2;
	mov.b64 	 %fd35, %rd86;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r81}, %fd35;
	}
	setp.lt.s32	%p36, %r81, 0;
	selp.b64	%rd87, %rd85, %rd86, %p36;
	add.s64 	%rd88, %rd87, %rd87;
	sub.s64 	%rd89, %rd88, %rd2;
	mov.b64 	 %fd36, %rd89;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r82}, %fd36;
	}
	setp.lt.s32	%p37, %r82, 0;
	selp.b64	%rd90, %rd88, %rd89, %p37;
	add.s64 	%rd91, %rd90, %rd90;
	sub.s64 	%rd92, %rd91, %rd2;
	mov.b64 	 %fd37, %rd92;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd37;
	}
	setp.lt.s32	%p38, %r83, 0;
	selp.b64	%rd93, %rd91, %rd92, %p38;
	add.s64 	%rd94, %rd93, %rd93;
	sub.s64 	%rd95, %rd94, %rd2;
	mov.b64 	 %fd38, %rd95;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r84}, %fd38;
	}
	setp.lt.s32	%p39, %r84, 0;
	selp.b64	%rd96, %rd94, %rd95, %p39;
	add.s64 	%rd97, %rd96, %rd96;
	sub.s64 	%rd98, %rd97, %rd2;
	mov.b64 	 %fd39, %rd98;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd39;
	}
	setp.lt.s32	%p40, %r85, 0;
	selp.b64	%rd99, %rd97, %rd98, %p40;
	add.s64 	%rd100, %rd99, %rd99;
	sub.s64 	%rd101, %rd100, %rd2;
	mov.b64 	 %fd40, %rd101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd40;
	}
	setp.lt.s32	%p41, %r86, 0;
	selp.b64	%rd102, %rd100, %rd101, %p41;
	add.s64 	%rd103, %rd102, %rd102;
	sub.s64 	%rd104, %rd103, %rd2;
	mov.b64 	 %fd41, %rd104;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %fd41;
	}
	setp.lt.s32	%p42, %r87, 0;
	selp.b64	%rd105, %rd103, %rd104, %p42;
	add.s64 	%rd106, %rd105, %rd105;
	sub.s64 	%rd107, %rd106, %rd2;
	mov.b64 	 %fd42, %rd107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r88}, %fd42;
	}
	setp.lt.s32	%p43, %r88, 0;
	selp.b64	%rd108, %rd106, %rd107, %p43;
	add.s64 	%rd109, %rd108, %rd108;
	sub.s64 	%rd110, %rd109, %rd2;
	mov.b64 	 %fd43, %rd110;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd43;
	}
	setp.lt.s32	%p44, %r89, 0;
	selp.b64	%rd111, %rd109, %rd110, %p44;
	add.s64 	%rd112, %rd111, %rd111;
	sub.s64 	%rd113, %rd112, %rd2;
	mov.b64 	 %fd44, %rd113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r90}, %fd44;
	}
	setp.lt.s32	%p45, %r90, 0;
	selp.b64	%rd114, %rd112, %rd113, %p45;
	add.s64 	%rd115, %rd114, %rd114;
	sub.s64 	%rd116, %rd115, %rd2;
	mov.b64 	 %fd45, %rd116;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r91}, %fd45;
	}
	setp.lt.s32	%p46, %r91, 0;
	selp.b64	%rd117, %rd115, %rd116, %p46;
	add.s64 	%rd118, %rd117, %rd117;
	sub.s64 	%rd119, %rd118, %rd2;
	mov.b64 	 %fd46, %rd119;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r92}, %fd46;
	}
	setp.lt.s32	%p47, %r92, 0;
	selp.b64	%rd120, %rd118, %rd119, %p47;
	add.s64 	%rd141, %rd120, %rd120;
	add.s32 	%r27, %r17, -16;
	add.s32 	%r93, %r17, -15;
	setp.gt.s32	%p48, %r93, 0;
	mov.u32 	%r17, %r27;
	@%p48 bra 	BB85_23;

BB85_24:
	shr.u64 	%rd20, %rd141, 1;
	setp.eq.s64	%p49, %rd20, 0;
	mov.f64 	%fd52, 0d0000000000000000;
	@%p49 bra 	BB85_26;

	mov.b64 	 %fd48, %rd20;
	mul.f64 	%fd49, %fd48, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd49;
	}
	shr.u32 	%r95, %r94, 20;
	mov.u32 	%r96, 55;
	sub.s32 	%r97, %r96, %r95;
	sub.s32 	%r98, %r127, %r97;
	shl.b64 	%rd121, %rd20, %r97;
	setp.lt.s32	%p50, %r98, 1;
	mov.u32 	%r99, 1;
	sub.s32 	%r100, %r99, %r98;
	shr.u64 	%rd122, %rd121, %r100;
	add.s32 	%r101, %r98, 4095;
	cvt.u64.u32	%rd123, %r101;
	shl.b64 	%rd124, %rd123, 52;
	add.s64 	%rd125, %rd124, %rd121;
	selp.b64	%rd126, %rd122, %rd125, %p50;
	mov.b64 	 %fd52, %rd126;

BB85_26:
	and.b32  	%r102, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %fd52;
	}
	or.b32  	%r104, %r103, %r102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd52;
	}
	mov.b64 	%fd53, {%r105, %r104};
	bra.uni 	BB85_30;

BB85_28:
	setp.eq.f64	%p54, %fd50, 0d7FF0000000000000;
	selp.f64	%fd53, 0dFFF8000000000000, %fd1, %p54;

BB85_30:
	mov.u32 	%r125, %tid.y;
	mov.u32 	%r124, %ctaid.y;
	mov.u32 	%r123, %ntid.y;
	mov.u32 	%r122, %ctaid.x;
	mov.u32 	%r121, %nctaid.x;
	mad.lo.s32 	%r120, %r123, %r124, %r125;
	mov.u32 	%r119, %tid.x;
	mov.u32 	%r118, %ntid.x;
	mad.lo.s32 	%r117, %r120, %r121, %r122;
	mad.lo.s32 	%r116, %r117, %r118, %r119;
	mul.wide.s32 	%rd131, %r116, 8;
	ld.param.u64 	%rd130, [vec_fmod_param_1];
	cvta.to.global.u64 	%rd127, %rd130;
	add.s64 	%rd129, %rd127, %rd131;
	st.global.f64 	[%rd129], %fd53;

BB85_31:
	ret;
}

	// .globl	vec_hypot
.visible .entry vec_hypot(
	.param .u32 vec_hypot_param_0,
	.param .u64 vec_hypot_param_1,
	.param .u64 vec_hypot_param_2,
	.param .u64 vec_hypot_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r2, [vec_hypot_param_0];
	ld.param.u64 	%rd1, [vec_hypot_param_1];
	ld.param.u64 	%rd2, [vec_hypot_param_2];
	ld.param.u64 	%rd3, [vec_hypot_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB86_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd3, [%rd6];
	abs.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd8];
	abs.f64 	%fd6, %fd5;
	mov.b64 	 %rd9, %fd6;
	mov.b64 	 %rd10, %fd4;
	min.u64 	%rd11, %rd9, %rd10;
	mov.b64 	 %fd7, %rd11;
	max.u64 	%rd12, %rd10, %rd9;
	mov.b64 	 %fd8, %rd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd8;
	}
	and.b32  	%r13, %r12, -4194304;
	mov.u32 	%r14, 2144337920;
	sub.s32 	%r15, %r14, %r13;
	mov.u32 	%r16, 0;
	mov.b64 	%fd9, {%r16, %r15};
	mul.f64 	%fd10, %fd7, %fd9;
	mul.f64 	%fd11, %fd8, %fd9;
	mul.f64 	%fd12, %fd10, %fd10;
	fma.rn.f64 	%fd13, %fd11, %fd11, %fd12;
	mov.f64 	%fd14, 0d7FEFFFFFFFFFFFFF;
	min.f64 	%fd2, %fd13, %fd14;
	// inline asm
	rsqrt.approx.ftz.f64 %fd1, %fd2;
	// inline asm
	mul.rn.f64 	%fd15, %fd1, %fd1;
	neg.f64 	%fd16, %fd15;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd2, %fd16, %fd17;
	mov.f64 	%fd19, 0d3FE0000000000000;
	mov.f64 	%fd20, 0d3FD8000000000000;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mul.rn.f64 	%fd22, %fd18, %fd1;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd1;
	mul.f64 	%fd24, %fd13, %fd23;
	add.s32 	%r17, %r13, 1048576;
	mov.b64 	%fd25, {%r16, %r17};
	mul.f64 	%fd26, %fd24, %fd25;
	setp.eq.f64	%p2, %fd7, 0d0000000000000000;
	selp.f64	%fd27, %fd8, %fd26, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd7;
	}
	mov.f64 	%fd28, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd28;
	}
	setp.lt.u32	%p3, %r18, %r19;
	selp.f64	%fd29, %fd27, %fd7, %p3;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd5;
	st.global.f64 	[%rd14], %fd29;

BB86_2:
	ret;
}

	// .globl	vec_nextafter
.visible .entry vec_nextafter(
	.param .u32 vec_nextafter_param_0,
	.param .u64 vec_nextafter_param_1,
	.param .u64 vec_nextafter_param_2,
	.param .u64 vec_nextafter_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<25>;


	ld.param.u32 	%r2, [vec_nextafter_param_0];
	ld.param.u64 	%rd2, [vec_nextafter_param_1];
	ld.param.u64 	%rd3, [vec_nextafter_param_2];
	ld.param.u64 	%rd4, [vec_nextafter_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB87_9;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	mov.b64 	 %rd1, %fd1;
	ld.global.f64 	%fd10, [%rd9];
	abs.f64 	%fd7, %fd1;
	setp.gtu.f64	%p2, %fd7, 0d7FF0000000000000;
	@%p2 bra 	BB87_7;

	abs.f64 	%fd8, %fd10;
	setp.gtu.f64	%p3, %fd8, 0d7FF0000000000000;
	@%p3 bra 	BB87_7;
	bra.uni 	BB87_3;

BB87_7:
	add.f64 	%fd10, %fd1, %fd10;

BB87_8:
	cvta.to.global.u64 	%rd22, %rd2;
	add.s64 	%rd24, %rd22, %rd6;
	st.global.f64 	[%rd24], %fd10;

BB87_9:
	ret;

BB87_3:
	mov.b64 	 %rd10, %fd10;
	or.b64  	%rd11, %rd10, %rd1;
	and.b64  	%rd12, %rd11, 9223372036854775807;
	setp.eq.s64	%p4, %rd12, 0;
	@%p4 bra 	BB87_8;

	shl.b64 	%rd13, %rd1, 1;
	setp.eq.s64	%p5, %rd13, 0;
	@%p5 bra 	BB87_6;
	bra.uni 	BB87_5;

BB87_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd10;
	}
	and.b32  	%r13, %r12, -2147483648;
	mov.f64 	%fd9, 0d0000000000000001;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd9;
	}
	or.b32  	%r15, %r14, %r13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd9;
	}
	mov.b64 	%fd10, {%r16, %r15};
	bra.uni 	BB87_8;

BB87_5:
	setp.lt.f64	%p6, %fd1, %fd10;
	setp.lt.f64	%p7, %fd1, 0d0000000000000000;
	and.pred  	%p8, %p6, %p7;
	selp.s64	%rd14, -1, 0, %p8;
	add.s64 	%rd15, %rd14, %rd1;
	setp.gt.f64	%p9, %fd1, 0d0000000000000000;
	and.pred  	%p10, %p6, %p9;
	selp.u64	%rd16, 1, 0, %p10;
	add.s64 	%rd17, %rd15, %rd16;
	setp.gt.f64	%p11, %fd1, %fd10;
	and.pred  	%p12, %p11, %p7;
	selp.u64	%rd18, 1, 0, %p12;
	add.s64 	%rd19, %rd17, %rd18;
	and.pred  	%p13, %p11, %p9;
	selp.s64	%rd20, -1, 0, %p13;
	add.s64 	%rd21, %rd19, %rd20;
	mov.b64 	 %fd10, %rd21;
	bra.uni 	BB87_8;
}

	// .globl	vec_pow
.visible .entry vec_pow(
	.param .u32 vec_pow_param_0,
	.param .u64 vec_pow_param_1,
	.param .u64 vec_pow_param_2,
	.param .u64 vec_pow_param_3
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<40>;
	.reg .f64 	%fd<20>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r4, [vec_pow_param_0];
	ld.param.u64 	%rd2, [vec_pow_param_1];
	ld.param.u64 	%rd3, [vec_pow_param_2];
	ld.param.u64 	%rd4, [vec_pow_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p2, %r1, %r4;
	@%p2 bra 	BB88_19;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd9];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	bfe.u32 	%r14, %r3, 20, 11;
	add.s32 	%r15, %r14, -1012;
	mov.b64 	 %rd10, %fd2;
	shl.b64 	%rd1, %rd10, %r15;
	setp.eq.s64	%p3, %rd1, -9223372036854775808;
	abs.f64 	%fd3, %fd1;
	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd2;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd9, [retval0+0];
	
	//{
	}// Callseq End 19
	setp.lt.s32	%p4, %r2, 0;
	and.pred  	%p1, %p4, %p3;
	@!%p1 bra 	BB88_3;
	bra.uni 	BB88_2;

BB88_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd9;
	}
	xor.b32  	%r17, %r16, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd9;
	}
	mov.b64 	%fd9, {%r18, %r17};

BB88_3:
	setp.eq.f64	%p5, %fd1, 0d0000000000000000;
	@%p5 bra 	BB88_6;
	bra.uni 	BB88_4;

BB88_6:
	selp.b32	%r19, %r2, 0, %p3;
	mov.u32 	%r20, 0;
	or.b32  	%r21, %r19, 2146435072;
	setp.lt.s32	%p9, %r3, 0;
	selp.b32	%r22, %r21, %r19, %p9;
	mov.b64 	%fd9, {%r20, %r22};
	bra.uni 	BB88_7;

BB88_4:
	setp.gt.s32	%p6, %r2, -1;
	@%p6 bra 	BB88_7;

	cvt.rzi.f64.f64	%fd14, %fd2;
	setp.neu.f64	%p7, %fd14, %fd2;
	selp.f64	%fd9, 0dFFF8000000000000, %fd9, %p7;

BB88_7:
	add.f64 	%fd19, %fd1, %fd2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd19;
	}
	and.b32  	%r24, %r23, 2146435072;
	setp.ne.s32	%p10, %r24, 2146435072;
	@%p10 bra 	BB88_8;

	setp.gtu.f64	%p11, %fd3, 0d7FF0000000000000;
	@%p11 bra 	BB88_18;

	abs.f64 	%fd15, %fd2;
	setp.gtu.f64	%p12, %fd15, 0d7FF0000000000000;
	@%p12 bra 	BB88_18;

	and.b32  	%r25, %r3, 2147483647;
	setp.ne.s32	%p13, %r25, 2146435072;
	@%p13 bra 	BB88_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r26, %temp}, %fd2;
	}
	setp.eq.s32	%p14, %r26, 0;
	@%p14 bra 	BB88_17;

BB88_13:
	and.b32  	%r27, %r2, 2147483647;
	setp.ne.s32	%p15, %r27, 2146435072;
	@%p15 bra 	BB88_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd1;
	}
	setp.ne.s32	%p16, %r28, 0;
	mov.f64 	%fd19, %fd9;
	@%p16 bra 	BB88_18;

	shr.s32 	%r29, %r3, 31;
	and.b32  	%r30, %r29, -2146435072;
	add.s32 	%r31, %r30, 2146435072;
	or.b32  	%r32, %r31, -2147483648;
	selp.b32	%r33, %r32, %r31, %p1;
	mov.u32 	%r34, 0;
	mov.b64 	%fd19, {%r34, %r33};
	bra.uni 	BB88_18;

BB88_8:
	mov.f64 	%fd19, %fd9;

BB88_18:
	setp.eq.f64	%p20, %fd2, 0d0000000000000000;
	setp.eq.f64	%p21, %fd1, 0d3FF0000000000000;
	or.pred  	%p22, %p21, %p20;
	selp.f64	%fd16, 0d3FF0000000000000, %fd19, %p22;
	cvta.to.global.u64 	%rd11, %rd2;
	add.s64 	%rd13, %rd11, %rd6;
	st.global.f64 	[%rd13], %fd16;

BB88_19:
	ret;

BB88_14:
	mov.f64 	%fd19, %fd9;
	bra.uni 	BB88_18;

BB88_17:
	setp.gt.f64	%p17, %fd3, 0d3FF0000000000000;
	selp.b32	%r35, 2146435072, 0, %p17;
	mov.u32 	%r36, 0;
	xor.b32  	%r37, %r35, 2146435072;
	setp.lt.s32	%p18, %r3, 0;
	selp.b32	%r38, %r37, %r35, %p18;
	setp.eq.f64	%p19, %fd1, 0dBFF0000000000000;
	selp.b32	%r39, 1072693248, %r38, %p19;
	mov.b64 	%fd19, {%r36, %r39};
	bra.uni 	BB88_18;
}

	// .globl	vec_remainder
.visible .entry vec_remainder(
	.param .u32 vec_remainder_param_0,
	.param .u64 vec_remainder_param_1,
	.param .u64 vec_remainder_param_2,
	.param .u64 vec_remainder_param_3
)
{
	.reg .pred 	%p<62>;
	.reg .b32 	%r<149>;
	.reg .f64 	%fd<57>;
	.reg .b64 	%rd<142>;


	ld.param.u32 	%r35, [vec_remainder_param_0];
	ld.param.u64 	%rd22, [vec_remainder_param_2];
	ld.param.u64 	%rd23, [vec_remainder_param_3];
	mov.u32 	%r36, %tid.x;
	mov.u32 	%r37, %ntid.y;
	mov.u32 	%r38, %ctaid.y;
	mov.u32 	%r39, %tid.y;
	mad.lo.s32 	%r40, %r37, %r38, %r39;
	mov.u32 	%r41, %nctaid.x;
	mov.u32 	%r42, %ctaid.x;
	mad.lo.s32 	%r43, %r40, %r41, %r42;
	mov.u32 	%r44, %ntid.x;
	mad.lo.s32 	%r1, %r43, %r44, %r36;
	setp.ge.s32	%p3, %r1, %r35;
	@%p3 bra 	BB89_33;

	cvta.to.global.u64 	%rd24, %rd22;
	mul.wide.s32 	%rd25, %r1, 8;
	add.s64 	%rd26, %rd24, %rd25;
	cvta.to.global.u64 	%rd27, %rd23;
	add.s64 	%rd28, %rd27, %rd25;
	ld.global.f64 	%fd1, [%rd26];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r45, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd28];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd2;
	}
	and.b32  	%r48, %r47, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd2;
	}
	mov.b64 	%fd3, {%r46, %r45};
	mov.b64 	%fd4, {%r49, %r48};
	setp.gt.u32	%p4, %r45, 2146435071;
	setp.gt.u32	%p5, %r48, 2146435071;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB89_29;
	bra.uni 	BB89_2;

BB89_29:
	setp.gtu.f64	%p57, %fd3, 0d7FF0000000000000;
	setp.gtu.f64	%p58, %fd4, 0d7FF0000000000000;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	BB89_31;
	bra.uni 	BB89_30;

BB89_31:
	add.f64 	%fd56, %fd1, %fd2;
	bra.uni 	BB89_32;

BB89_2:
	setp.eq.f64	%p7, %fd4, 0d0000000000000000;
	mov.f64 	%fd56, 0dFFF8000000000000;
	@%p7 bra 	BB89_32;

	setp.ltu.f64	%p8, %fd3, %fd4;
	mov.u32 	%r147, 1;
	@%p8 bra 	BB89_26;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd3;
	}
	shr.u32 	%r136, %r51, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd4;
	}
	shr.u32 	%r137, %r52, 20;
	setp.ne.s32	%p9, %r136, 0;
	@%p9 bra 	BB89_6;

	mul.f64 	%fd3, %fd3, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd3;
	}
	shr.u32 	%r54, %r53, 20;
	add.s32 	%r136, %r54, -54;

BB89_6:
	setp.ne.s32	%p10, %r137, 0;
	mov.f64 	%fd54, %fd4;
	@%p10 bra 	BB89_8;

	mul.f64 	%fd54, %fd4, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd54;
	}
	shr.u32 	%r56, %r55, 20;
	add.s32 	%r137, %r56, -54;

BB89_8:
	mov.b64 	 %rd30, %fd3;
	and.b64  	%rd31, %rd30, 4503599627370495;
	or.b64  	%rd8, %rd31, 4503599627370496;
	mov.b64 	 %rd32, %fd54;
	and.b64  	%rd33, %rd32, 4503599627370495;
	or.b64  	%rd2, %rd33, 4503599627370496;
	sub.s32 	%r18, %r136, %r137;
	not.b32 	%r58, %r136;
	mov.u32 	%r59, -1;
	add.s32 	%r60, %r137, %r58;
	max.s32 	%r61, %r60, %r59;
	add.s32 	%r62, %r136, 2;
	sub.s32 	%r63, %r62, %r137;
	add.s32 	%r10, %r63, %r61;
	and.b32  	%r11, %r10, 3;
	setp.eq.s32	%p11, %r11, 0;
	mov.u32 	%r147, 0;
	mov.u64 	%rd141, 0;
	@%p11 bra 	BB89_14;

	setp.eq.s32	%p12, %r11, 1;
	@%p12 bra 	BB89_13;

	setp.eq.s32	%p13, %r11, 2;
	@%p13 bra 	BB89_12;

	sub.s64 	%rd34, %rd8, %rd2;
	mov.b64 	 %fd17, %rd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd17;
	}
	setp.gt.s32	%p14, %r64, -1;
	selp.b64	%rd35, %rd34, %rd8, %p14;
	add.s64 	%rd8, %rd35, %rd35;
	add.s32 	%r18, %r18, -1;

BB89_12:
	sub.s64 	%rd36, %rd8, %rd2;
	mov.b64 	 %fd18, %rd36;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd18;
	}
	setp.gt.s32	%p15, %r65, -1;
	selp.b64	%rd37, %rd36, %rd8, %p15;
	add.s64 	%rd8, %rd37, %rd37;
	add.s32 	%r18, %r18, -1;

BB89_13:
	sub.s64 	%rd38, %rd8, %rd2;
	mov.b64 	 %fd19, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd19;
	}
	and.b32  	%r147, %r66, -2147483648;
	setp.eq.s32	%p16, %r147, 0;
	selp.b64	%rd39, %rd38, %rd8, %p16;
	add.s64 	%rd8, %rd39, %rd39;
	add.s32 	%r18, %r18, -1;
	mov.u64 	%rd141, %rd8;

BB89_14:
	setp.lt.u32	%p17, %r10, 4;
	@%p17 bra 	BB89_24;

	mov.u32 	%r68, 2;
	sub.s32 	%r69, %r68, %r18;
	max.s32 	%r71, %r69, %r59;
	add.s32 	%r72, %r18, %r71;
	add.s32 	%r73, %r72, 1;
	shr.u32 	%r74, %r73, 2;
	add.s32 	%r20, %r74, 1;
	and.b32  	%r21, %r20, 3;
	setp.eq.s32	%p18, %r21, 0;
	mov.u32 	%r147, 0;
	mov.u64 	%rd141, 0;
	@%p18 bra 	BB89_21;

	setp.eq.s32	%p19, %r21, 1;
	@%p19 bra 	BB89_20;

	setp.eq.s32	%p20, %r21, 2;
	@%p20 bra 	BB89_19;

	sub.s64 	%rd41, %rd8, %rd2;
	mov.b64 	 %fd20, %rd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd20;
	}
	setp.gt.s32	%p21, %r75, -1;
	selp.b64	%rd42, %rd41, %rd8, %p21;
	add.s64 	%rd43, %rd42, %rd42;
	sub.s64 	%rd44, %rd43, %rd2;
	mov.b64 	 %fd21, %rd44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r76}, %fd21;
	}
	setp.gt.s32	%p22, %r76, -1;
	selp.b64	%rd45, %rd44, %rd43, %p22;
	add.s64 	%rd46, %rd45, %rd45;
	sub.s64 	%rd47, %rd46, %rd2;
	mov.b64 	 %fd22, %rd47;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd22;
	}
	setp.gt.s32	%p23, %r77, -1;
	selp.b64	%rd48, %rd47, %rd46, %p23;
	add.s64 	%rd49, %rd48, %rd48;
	sub.s64 	%rd50, %rd49, %rd2;
	mov.b64 	 %fd23, %rd50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd23;
	}
	setp.gt.s32	%p24, %r78, -1;
	selp.b64	%rd51, %rd50, %rd49, %p24;
	add.s64 	%rd8, %rd51, %rd51;
	add.s32 	%r18, %r18, -4;

BB89_19:
	sub.s64 	%rd52, %rd8, %rd2;
	mov.b64 	 %fd24, %rd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd24;
	}
	setp.gt.s32	%p25, %r79, -1;
	selp.b64	%rd53, %rd52, %rd8, %p25;
	add.s64 	%rd54, %rd53, %rd53;
	sub.s64 	%rd55, %rd54, %rd2;
	mov.b64 	 %fd25, %rd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd25;
	}
	setp.gt.s32	%p26, %r80, -1;
	selp.b64	%rd56, %rd55, %rd54, %p26;
	add.s64 	%rd57, %rd56, %rd56;
	sub.s64 	%rd58, %rd57, %rd2;
	mov.b64 	 %fd26, %rd58;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r81}, %fd26;
	}
	setp.gt.s32	%p27, %r81, -1;
	selp.b64	%rd59, %rd58, %rd57, %p27;
	add.s64 	%rd60, %rd59, %rd59;
	sub.s64 	%rd61, %rd60, %rd2;
	mov.b64 	 %fd27, %rd61;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r82}, %fd27;
	}
	setp.gt.s32	%p28, %r82, -1;
	selp.b64	%rd62, %rd61, %rd60, %p28;
	add.s64 	%rd8, %rd62, %rd62;
	add.s32 	%r18, %r18, -4;

BB89_20:
	sub.s64 	%rd63, %rd8, %rd2;
	mov.b64 	 %fd28, %rd63;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd28;
	}
	setp.gt.s32	%p29, %r83, -1;
	selp.b64	%rd64, %rd63, %rd8, %p29;
	add.s64 	%rd65, %rd64, %rd64;
	sub.s64 	%rd66, %rd65, %rd2;
	mov.b64 	 %fd29, %rd66;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r84}, %fd29;
	}
	setp.gt.s32	%p30, %r84, -1;
	selp.b64	%rd67, %rd66, %rd65, %p30;
	add.s64 	%rd68, %rd67, %rd67;
	sub.s64 	%rd69, %rd68, %rd2;
	mov.b64 	 %fd30, %rd69;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd30;
	}
	setp.gt.s32	%p31, %r85, -1;
	selp.b64	%rd70, %rd69, %rd68, %p31;
	add.s64 	%rd71, %rd70, %rd70;
	sub.s64 	%rd72, %rd71, %rd2;
	mov.b64 	 %fd31, %rd72;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd31;
	}
	and.b32  	%r147, %r86, -2147483648;
	setp.eq.s32	%p32, %r147, 0;
	selp.b64	%rd73, %rd72, %rd71, %p32;
	add.s64 	%rd8, %rd73, %rd73;
	add.s32 	%r18, %r18, -4;
	mov.u64 	%rd141, %rd8;

BB89_21:
	setp.lt.u32	%p33, %r20, 4;
	@%p33 bra 	BB89_24;

	mov.u64 	%rd141, %rd8;

BB89_23:
	sub.s64 	%rd74, %rd141, %rd2;
	mov.b64 	 %fd32, %rd74;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %fd32;
	}
	setp.gt.s32	%p34, %r87, -1;
	selp.b64	%rd75, %rd74, %rd141, %p34;
	add.s64 	%rd76, %rd75, %rd75;
	sub.s64 	%rd77, %rd76, %rd2;
	mov.b64 	 %fd33, %rd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r88}, %fd33;
	}
	setp.gt.s32	%p35, %r88, -1;
	selp.b64	%rd78, %rd77, %rd76, %p35;
	add.s64 	%rd79, %rd78, %rd78;
	sub.s64 	%rd80, %rd79, %rd2;
	mov.b64 	 %fd34, %rd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd34;
	}
	setp.gt.s32	%p36, %r89, -1;
	selp.b64	%rd81, %rd80, %rd79, %p36;
	add.s64 	%rd82, %rd81, %rd81;
	sub.s64 	%rd83, %rd82, %rd2;
	mov.b64 	 %fd35, %rd83;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r90}, %fd35;
	}
	setp.gt.s32	%p37, %r90, -1;
	selp.b64	%rd84, %rd83, %rd82, %p37;
	add.s64 	%rd85, %rd84, %rd84;
	sub.s64 	%rd86, %rd85, %rd2;
	mov.b64 	 %fd36, %rd86;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r91}, %fd36;
	}
	setp.gt.s32	%p38, %r91, -1;
	selp.b64	%rd87, %rd86, %rd85, %p38;
	add.s64 	%rd88, %rd87, %rd87;
	sub.s64 	%rd89, %rd88, %rd2;
	mov.b64 	 %fd37, %rd89;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r92}, %fd37;
	}
	setp.gt.s32	%p39, %r92, -1;
	selp.b64	%rd90, %rd89, %rd88, %p39;
	add.s64 	%rd91, %rd90, %rd90;
	sub.s64 	%rd92, %rd91, %rd2;
	mov.b64 	 %fd38, %rd92;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd38;
	}
	setp.gt.s32	%p40, %r93, -1;
	selp.b64	%rd93, %rd92, %rd91, %p40;
	add.s64 	%rd94, %rd93, %rd93;
	sub.s64 	%rd95, %rd94, %rd2;
	mov.b64 	 %fd39, %rd95;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd39;
	}
	setp.gt.s32	%p41, %r94, -1;
	selp.b64	%rd96, %rd95, %rd94, %p41;
	add.s64 	%rd97, %rd96, %rd96;
	sub.s64 	%rd98, %rd97, %rd2;
	mov.b64 	 %fd40, %rd98;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r95}, %fd40;
	}
	setp.gt.s32	%p42, %r95, -1;
	selp.b64	%rd99, %rd98, %rd97, %p42;
	add.s64 	%rd100, %rd99, %rd99;
	sub.s64 	%rd101, %rd100, %rd2;
	mov.b64 	 %fd41, %rd101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r96}, %fd41;
	}
	setp.gt.s32	%p43, %r96, -1;
	selp.b64	%rd102, %rd101, %rd100, %p43;
	add.s64 	%rd103, %rd102, %rd102;
	sub.s64 	%rd104, %rd103, %rd2;
	mov.b64 	 %fd42, %rd104;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r97}, %fd42;
	}
	setp.gt.s32	%p44, %r97, -1;
	selp.b64	%rd105, %rd104, %rd103, %p44;
	add.s64 	%rd106, %rd105, %rd105;
	sub.s64 	%rd107, %rd106, %rd2;
	mov.b64 	 %fd43, %rd107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r98}, %fd43;
	}
	setp.gt.s32	%p45, %r98, -1;
	selp.b64	%rd108, %rd107, %rd106, %p45;
	add.s64 	%rd109, %rd108, %rd108;
	sub.s64 	%rd110, %rd109, %rd2;
	mov.b64 	 %fd44, %rd110;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r99}, %fd44;
	}
	setp.gt.s32	%p46, %r99, -1;
	selp.b64	%rd111, %rd110, %rd109, %p46;
	add.s64 	%rd112, %rd111, %rd111;
	sub.s64 	%rd113, %rd112, %rd2;
	mov.b64 	 %fd45, %rd113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r100}, %fd45;
	}
	setp.gt.s32	%p47, %r100, -1;
	selp.b64	%rd114, %rd113, %rd112, %p47;
	add.s64 	%rd115, %rd114, %rd114;
	sub.s64 	%rd116, %rd115, %rd2;
	mov.b64 	 %fd46, %rd116;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r101}, %fd46;
	}
	setp.gt.s32	%p48, %r101, -1;
	selp.b64	%rd117, %rd116, %rd115, %p48;
	add.s64 	%rd118, %rd117, %rd117;
	sub.s64 	%rd119, %rd118, %rd2;
	mov.b64 	 %fd47, %rd119;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r102}, %fd47;
	}
	and.b32  	%r147, %r102, -2147483648;
	setp.eq.s32	%p49, %r147, 0;
	selp.b64	%rd120, %rd119, %rd118, %p49;
	add.s64 	%rd141, %rd120, %rd120;
	add.s32 	%r32, %r18, -16;
	add.s32 	%r103, %r18, -15;
	setp.gt.s32	%p50, %r103, 0;
	mov.u32 	%r18, %r32;
	@%p50 bra 	BB89_23;

BB89_24:
	shr.u64 	%rd20, %rd141, 1;
	setp.eq.s64	%p51, %rd20, 0;
	mov.f64 	%fd3, 0d0000000000000000;
	@%p51 bra 	BB89_26;

	mov.b64 	 %fd49, %rd20;
	mul.f64 	%fd50, %fd49, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd50;
	}
	shr.u32 	%r105, %r104, 20;
	mov.u32 	%r106, 55;
	sub.s32 	%r107, %r106, %r105;
	sub.s32 	%r108, %r137, %r107;
	shl.b64 	%rd121, %rd20, %r107;
	setp.lt.s32	%p52, %r108, 1;
	mov.u32 	%r109, 1;
	sub.s32 	%r110, %r109, %r108;
	shr.u64 	%rd122, %rd121, %r110;
	add.s32 	%r111, %r108, 4095;
	cvt.u64.u32	%rd123, %r111;
	shl.b64 	%rd124, %rd123, 52;
	add.s64 	%rd125, %rd124, %rd121;
	selp.b64	%rd126, %rd122, %rd125, %p52;
	mov.b64 	 %fd3, %rd126;

BB89_26:
	add.f64 	%fd11, %fd3, %fd3;
	setp.gt.f64	%p54, %fd11, %fd4;
	mov.pred 	%p61, -1;
	@%p54 bra 	BB89_28;

	setp.eq.f64	%p55, %fd11, %fd4;
	setp.eq.s32	%p56, %r147, 0;
	and.pred  	%p61, %p55, %p56;

BB89_28:
	sub.f64 	%fd51, %fd3, %fd4;
	selp.f64	%fd52, %fd51, %fd3, %p61;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd52;
	}
	and.b32  	%r113, %r2, -2147483648;
	xor.b32  	%r114, %r112, %r113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r115, %temp}, %fd52;
	}
	mov.b64 	%fd56, {%r115, %r114};
	bra.uni 	BB89_32;

BB89_30:
	setp.eq.f64	%p60, %fd3, 0d7FF0000000000000;
	selp.f64	%fd56, 0dFFF8000000000000, %fd1, %p60;

BB89_32:
	mov.u32 	%r135, %tid.y;
	mov.u32 	%r134, %ctaid.y;
	mov.u32 	%r133, %ntid.y;
	mov.u32 	%r132, %ctaid.x;
	mov.u32 	%r131, %nctaid.x;
	mad.lo.s32 	%r130, %r133, %r134, %r135;
	mov.u32 	%r129, %tid.x;
	mov.u32 	%r128, %ntid.x;
	mad.lo.s32 	%r127, %r130, %r131, %r132;
	mad.lo.s32 	%r126, %r127, %r128, %r129;
	mul.wide.s32 	%rd131, %r126, 8;
	ld.param.u64 	%rd130, [vec_remainder_param_1];
	cvta.to.global.u64 	%rd127, %rd130;
	add.s64 	%rd129, %rd127, %rd131;
	st.global.f64 	[%rd129], %fd56;

BB89_33:
	ret;
}

	// .globl	vec_testkernel
.visible .entry vec_testkernel(
	.param .u32 vec_testkernel_param_0,
	.param .u64 vec_testkernel_param_1,
	.param .u64 vec_testkernel_param_2,
	.param .u64 vec_testkernel_param_3
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd4, [vec_testkernel_param_1];
	ld.param.u64 	%rd5, [vec_testkernel_param_2];
	ld.param.u64 	%rd6, [vec_testkernel_param_3];
	cvta.to.global.u64 	%rd7, %rd4;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r16;
	cvta.to.global.u64 	%rd8, %rd5;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd1, %rd8, %rd9;
	cvta.to.global.u64 	%rd10, %rd6;
	add.s64 	%rd2, %rd10, %rd9;
	abs.s32 	%r17, %r1;
	and.b32  	%r18, %r17, 1;
	setp.eq.b32	%p1, %r18, 1;
	not.pred 	%p2, %p1;
	mov.u32 	%r20, 0;
	selp.f64	%fd1, 0d3FF0000000000000, 0d400921F9F01B866E, %p2;
	shr.u32 	%r2, %r17, 1;
	add.s64 	%rd3, %rd7, %rd9;

BB90_1:
	ld.global.f64 	%fd11, [%rd2];
	ld.global.f64 	%fd12, [%rd1];
	mul.f64 	%fd2, %fd12, %fd11;
	setp.eq.s32	%p3, %r2, 0;
	mov.f64 	%fd17, 0d400921F9F01B866E;
	mov.u32 	%r21, %r2;
	mov.f64 	%fd16, %fd1;
	@%p3 bra 	BB90_3;

BB90_2:
	and.b32  	%r19, %r21, 1;
	setp.eq.b32	%p4, %r19, 1;
	not.pred 	%p5, %p4;
	mul.f64 	%fd17, %fd17, %fd17;
	mul.f64 	%fd13, %fd16, %fd17;
	selp.f64	%fd16, %fd16, %fd13, %p5;
	shr.u32 	%r21, %r21, 1;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB90_2;

BB90_3:
	setp.gt.s32	%p7, %r1, -1;
	@%p7 bra 	BB90_5;

	rcp.rn.f64 	%fd16, %fd16;

BB90_5:
	sqrt.rn.f64 	%fd14, %fd16;
	add.f64 	%fd15, %fd2, %fd14;
	st.global.f64 	[%rd3], %fd15;
	add.s32 	%r20, %r20, 1;
	setp.lt.s32	%p8, %r20, 100;
	@%p8 bra 	BB90_1;

	ret;
}

	// .globl	vec_computePSF_phase
.visible .entry vec_computePSF_phase(
	.param .u32 vec_computePSF_phase_param_0,
	.param .u64 vec_computePSF_phase_param_1,
	.param .u64 vec_computePSF_phase_param_2,
	.param .u64 vec_computePSF_phase_param_3,
	.param .u64 vec_computePSF_phase_param_4,
	.param .u64 vec_computePSF_phase_param_5,
	.param .u64 vec_computePSF_phase_param_6,
	.param .u64 vec_computePSF_phase_param_7,
	.param .f64 vec_computePSF_phase_param_8,
	.param .f64 vec_computePSF_phase_param_9,
	.param .f64 vec_computePSF_phase_param_10
)
{
	.local .align 4 .b8 	__local_depot91[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<57>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<41>;


	mov.u64 	%SPL, __local_depot91;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r10, [vec_computePSF_phase_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_phase_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_phase_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_phase_param_3];
	ld.param.u64 	%rd4, [vec_computePSF_phase_param_4];
	ld.param.u64 	%rd5, [vec_computePSF_phase_param_5];
	ld.param.u64 	%rd6, [vec_computePSF_phase_param_6];
	ld.param.u64 	%rd7, [vec_computePSF_phase_param_7];
	ld.param.f64 	%fd28, [vec_computePSF_phase_param_8];
	ld.param.f64 	%fd29, [vec_computePSF_phase_param_9];
	ld.param.f64 	%fd30, [vec_computePSF_phase_param_10];
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %nctaid.x;
	mov.u32 	%r16, %ctaid.x;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r19;
	setp.ge.s32	%p1, %r1, %r10;
	@%p1 bra 	BB91_20;

	cvta.to.global.u64 	%rd8, %rd6;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r1, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd31, [%rd11];
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f64 	%fd32, [%rd13];
	mul.f64 	%fd33, %fd32, %fd29;
	fma.rn.f64 	%fd34, %fd31, %fd28, %fd33;
	cvta.to.global.u64 	%rd14, %rd5;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.f64 	%fd35, [%rd15];
	fma.rn.f64 	%fd36, %fd35, %fd30, %fd34;
	cvta.to.global.u64 	%rd16, %rd7;
	add.s64 	%rd17, %rd16, %rd10;
	ld.global.f64 	%fd37, [%rd17];
	add.f64 	%fd92, %fd37, %fd36;
	add.s64 	%rd18, %rd8, %rd10;
	ld.global.f64 	%fd2, [%rd18];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd92;
	}
	and.b32  	%r2, %r20, 2147483647;
	setp.ne.s32	%p2, %r2, 2146435072;
	mov.f64 	%fd88, %fd92;
	@%p2 bra 	BB91_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd92;
	}
	setp.ne.s32	%p3, %r21, 0;
	mov.f64 	%fd88, %fd92;
	@%p3 bra 	BB91_4;

	mov.f64 	%fd38, 0d0000000000000000;
	mul.rn.f64 	%fd88, %fd92, %fd38;

BB91_4:
	mul.f64 	%fd39, %fd88, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r55, %fd39;
	add.u64 	%rd19, %SP, 4;
	add.u64 	%rd20, %SPL, 4;
	st.local.u32 	[%rd20], %r55;
	cvt.rn.f64.s32	%fd40, %r55;
	neg.f64 	%fd41, %fd40;
	mov.f64 	%fd42, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd43, %fd41, %fd42, %fd88;
	mov.f64 	%fd44, 0d3C91A62633145C00;
	fma.rn.f64 	%fd45, %fd41, %fd44, %fd43;
	mov.f64 	%fd46, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd41, %fd46, %fd45;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd88;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p4, %r23, 1105199104;
	@%p4 bra 	BB91_6;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd88;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r55, [%rd20];

BB91_6:
	add.s32 	%r6, %r55, 1;
	and.b32  	%r24, %r6, 1;
	shl.b32 	%r25, %r24, 3;
	setp.eq.s32	%p5, %r24, 0;
	selp.f64	%fd47, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r26, %r25, 1;
	mul.wide.s32 	%rd23, %r26, 8;
	mov.u64 	%rd24, __cudart_sin_cos_coeffs;
	add.s64 	%rd25, %rd24, %rd23;
	ld.const.f64 	%fd48, [%rd25];
	mul.rn.f64 	%fd8, %fd89, %fd89;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	ld.const.f64 	%fd50, [%rd25+8];
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	ld.const.f64 	%fd52, [%rd25+16];
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	ld.const.f64 	%fd54, [%rd25+24];
	fma.rn.f64 	%fd55, %fd53, %fd8, %fd54;
	ld.const.f64 	%fd56, [%rd25+32];
	fma.rn.f64 	%fd57, %fd55, %fd8, %fd56;
	ld.const.f64 	%fd58, [%rd25+40];
	fma.rn.f64 	%fd9, %fd57, %fd8, %fd58;
	fma.rn.f64 	%fd90, %fd9, %fd89, %fd89;
	@%p5 bra 	BB91_8;

	mov.f64 	%fd59, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd9, %fd8, %fd59;

BB91_8:
	and.b32  	%r27, %r6, 2;
	setp.eq.s32	%p6, %r27, 0;
	@%p6 bra 	BB91_10;

	mov.f64 	%fd60, 0d0000000000000000;
	mov.f64 	%fd61, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd61, %fd60;

BB91_10:
	cvta.to.global.u64 	%rd26, %rd1;
	add.s64 	%rd28, %rd26, %rd10;
	mul.f64 	%fd62, %fd2, %fd90;
	st.global.f64 	[%rd28], %fd62;
	ld.global.f64 	%fd15, [%rd18];
	@%p2 bra 	BB91_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd92;
	}
	setp.ne.s32	%p8, %r38, 0;
	@%p8 bra 	BB91_13;

	mov.f64 	%fd63, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd92, %fd63;

BB91_13:
	mul.f64 	%fd64, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r56, %fd64;
	add.u64 	%rd31, %SP, 0;
	add.u64 	%rd32, %SPL, 0;
	st.local.u32 	[%rd32], %r56;
	cvt.rn.f64.s32	%fd65, %r56;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd68, %fd66, %fd42, %fd92;
	fma.rn.f64 	%fd70, %fd66, %fd44, %fd68;
	fma.rn.f64 	%fd93, %fd66, %fd46, %fd70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd92;
	}
	and.b32  	%r40, %r39, 2145386496;
	setp.lt.u32	%p9, %r40, 1105199104;
	@%p9 bra 	BB91_15;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd31;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd93, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r56, [%rd32];

BB91_15:
	and.b32  	%r41, %r56, 1;
	shl.b32 	%r42, %r41, 3;
	setp.eq.s32	%p10, %r41, 0;
	selp.f64	%fd72, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	add.s32 	%r43, %r42, 1;
	mul.wide.s32 	%rd35, %r43, 8;
	add.s64 	%rd37, %rd24, %rd35;
	ld.const.f64 	%fd73, [%rd37];
	mul.rn.f64 	%fd21, %fd93, %fd93;
	fma.rn.f64 	%fd74, %fd72, %fd21, %fd73;
	ld.const.f64 	%fd75, [%rd37+8];
	fma.rn.f64 	%fd76, %fd74, %fd21, %fd75;
	ld.const.f64 	%fd77, [%rd37+16];
	fma.rn.f64 	%fd78, %fd76, %fd21, %fd77;
	ld.const.f64 	%fd79, [%rd37+24];
	fma.rn.f64 	%fd80, %fd78, %fd21, %fd79;
	ld.const.f64 	%fd81, [%rd37+32];
	fma.rn.f64 	%fd82, %fd80, %fd21, %fd81;
	ld.const.f64 	%fd83, [%rd37+40];
	fma.rn.f64 	%fd22, %fd82, %fd21, %fd83;
	fma.rn.f64 	%fd94, %fd22, %fd93, %fd93;
	@%p10 bra 	BB91_17;

	mov.f64 	%fd84, 0d3FF0000000000000;
	fma.rn.f64 	%fd94, %fd22, %fd21, %fd84;

BB91_17:
	and.b32  	%r44, %r56, 2;
	setp.eq.s32	%p11, %r44, 0;
	@%p11 bra 	BB91_19;

	mov.f64 	%fd85, 0d0000000000000000;
	mov.f64 	%fd86, 0dBFF0000000000000;
	fma.rn.f64 	%fd94, %fd94, %fd86, %fd85;

BB91_19:
	cvta.to.global.u64 	%rd38, %rd2;
	add.s64 	%rd40, %rd38, %rd10;
	mul.f64 	%fd87, %fd15, %fd94;
	st.global.f64 	[%rd40], %fd87;

BB91_20:
	ret;
}

	// .globl	vec_computePSF_phaseN
.visible .entry vec_computePSF_phaseN(
	.param .u32 vec_computePSF_phaseN_param_0,
	.param .u64 vec_computePSF_phaseN_param_1,
	.param .u64 vec_computePSF_phaseN_param_2,
	.param .u64 vec_computePSF_phaseN_param_3,
	.param .u64 vec_computePSF_phaseN_param_4,
	.param .u64 vec_computePSF_phaseN_param_5,
	.param .f64 vec_computePSF_phaseN_param_6,
	.param .f64 vec_computePSF_phaseN_param_7,
	.param .f64 vec_computePSF_phaseN_param_8,
	.param .u64 vec_computePSF_phaseN_param_9,
	.param .u64 vec_computePSF_phaseN_param_10,
	.param .u64 vec_computePSF_phaseN_param_11
)
{
	.local .align 4 .b8 	__local_depot92[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<59>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<49>;


	mov.u64 	%SPL, __local_depot92;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r10, [vec_computePSF_phaseN_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_phaseN_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_phaseN_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_phaseN_param_3];
	ld.param.u64 	%rd4, [vec_computePSF_phaseN_param_4];
	ld.param.u64 	%rd5, [vec_computePSF_phaseN_param_5];
	ld.param.f64 	%fd28, [vec_computePSF_phaseN_param_6];
	ld.param.f64 	%fd29, [vec_computePSF_phaseN_param_7];
	ld.param.f64 	%fd30, [vec_computePSF_phaseN_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_phaseN_param_9];
	ld.param.u64 	%rd7, [vec_computePSF_phaseN_param_10];
	ld.param.u64 	%rd8, [vec_computePSF_phaseN_param_11];
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %nctaid.x;
	mov.u32 	%r16, %ctaid.x;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r19;
	setp.ge.s32	%p1, %r1, %r10;
	@%p1 bra 	BB92_20;

	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r1, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f64 	%fd31, [%rd12];
	cvta.to.global.u64 	%rd13, %rd2;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.f64 	%fd32, [%rd14];
	mul.f64 	%fd33, %fd32, %fd29;
	fma.rn.f64 	%fd34, %fd31, %fd28, %fd33;
	cvta.to.global.u64 	%rd15, %rd3;
	add.s64 	%rd16, %rd15, %rd11;
	ld.global.f64 	%fd35, [%rd16];
	fma.rn.f64 	%fd36, %fd35, %fd30, %fd34;
	cvta.to.global.u64 	%rd17, %rd5;
	add.s64 	%rd18, %rd17, %rd11;
	ld.global.f64 	%fd37, [%rd18];
	add.f64 	%fd92, %fd37, %fd36;
	add.s64 	%rd19, %rd9, %rd11;
	ld.global.f64 	%fd2, [%rd19];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd92;
	}
	and.b32  	%r2, %r20, 2147483647;
	setp.ne.s32	%p2, %r2, 2146435072;
	mov.f64 	%fd88, %fd92;
	@%p2 bra 	BB92_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd92;
	}
	setp.ne.s32	%p3, %r21, 0;
	mov.f64 	%fd88, %fd92;
	@%p3 bra 	BB92_4;

	mov.f64 	%fd38, 0d0000000000000000;
	mul.rn.f64 	%fd88, %fd92, %fd38;

BB92_4:
	mul.f64 	%fd39, %fd88, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r57, %fd39;
	add.u64 	%rd20, %SP, 4;
	add.u64 	%rd21, %SPL, 4;
	st.local.u32 	[%rd21], %r57;
	cvt.rn.f64.s32	%fd40, %r57;
	neg.f64 	%fd41, %fd40;
	mov.f64 	%fd42, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd43, %fd41, %fd42, %fd88;
	mov.f64 	%fd44, 0d3C91A62633145C00;
	fma.rn.f64 	%fd45, %fd41, %fd44, %fd43;
	mov.f64 	%fd46, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd41, %fd46, %fd45;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd88;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p4, %r23, 1105199104;
	@%p4 bra 	BB92_6;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd88;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd20;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r57, [%rd21];

BB92_6:
	add.s32 	%r6, %r57, 1;
	and.b32  	%r24, %r6, 1;
	shl.b32 	%r25, %r24, 3;
	setp.eq.s32	%p5, %r24, 0;
	selp.f64	%fd47, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r26, %r25, 1;
	mul.wide.s32 	%rd24, %r26, 8;
	mov.u64 	%rd25, __cudart_sin_cos_coeffs;
	add.s64 	%rd26, %rd25, %rd24;
	ld.const.f64 	%fd48, [%rd26];
	mul.rn.f64 	%fd8, %fd89, %fd89;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	ld.const.f64 	%fd50, [%rd26+8];
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	ld.const.f64 	%fd52, [%rd26+16];
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	ld.const.f64 	%fd54, [%rd26+24];
	fma.rn.f64 	%fd55, %fd53, %fd8, %fd54;
	ld.const.f64 	%fd56, [%rd26+32];
	fma.rn.f64 	%fd57, %fd55, %fd8, %fd56;
	ld.const.f64 	%fd58, [%rd26+40];
	fma.rn.f64 	%fd9, %fd57, %fd8, %fd58;
	fma.rn.f64 	%fd90, %fd9, %fd89, %fd89;
	@%p5 bra 	BB92_8;

	mov.f64 	%fd59, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd9, %fd8, %fd59;

BB92_8:
	and.b32  	%r27, %r6, 2;
	setp.eq.s32	%p6, %r27, 0;
	@%p6 bra 	BB92_10;

	mov.f64 	%fd60, 0d0000000000000000;
	mov.f64 	%fd61, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd61, %fd60;

BB92_10:
	cvta.to.global.u64 	%rd27, %rd6;
	mul.wide.s32 	%rd28, %r1, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.u32 	%r38, [%rd29];
	cvta.to.global.u64 	%rd30, %rd8;
	mul.wide.s32 	%rd31, %r38, 8;
	add.s64 	%rd32, %rd30, %rd31;
	mul.f64 	%fd62, %fd2, %fd90;
	st.global.f64 	[%rd32], %fd62;
	ld.global.f64 	%fd15, [%rd19];
	@%p2 bra 	BB92_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd92;
	}
	setp.ne.s32	%p8, %r39, 0;
	@%p8 bra 	BB92_13;

	mov.f64 	%fd63, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd92, %fd63;

BB92_13:
	mul.f64 	%fd64, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r58, %fd64;
	add.u64 	%rd36, %SP, 0;
	add.u64 	%rd37, %SPL, 0;
	st.local.u32 	[%rd37], %r58;
	cvt.rn.f64.s32	%fd65, %r58;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd68, %fd66, %fd42, %fd92;
	fma.rn.f64 	%fd70, %fd66, %fd44, %fd68;
	fma.rn.f64 	%fd93, %fd66, %fd46, %fd70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd92;
	}
	and.b32  	%r41, %r40, 2145386496;
	setp.lt.u32	%p9, %r41, 1105199104;
	@%p9 bra 	BB92_15;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd93, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r58, [%rd37];

BB92_15:
	and.b32  	%r42, %r58, 1;
	shl.b32 	%r43, %r42, 3;
	setp.eq.s32	%p10, %r42, 0;
	selp.f64	%fd72, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	add.s32 	%r44, %r43, 1;
	mul.wide.s32 	%rd40, %r44, 8;
	add.s64 	%rd42, %rd25, %rd40;
	ld.const.f64 	%fd73, [%rd42];
	mul.rn.f64 	%fd21, %fd93, %fd93;
	fma.rn.f64 	%fd74, %fd72, %fd21, %fd73;
	ld.const.f64 	%fd75, [%rd42+8];
	fma.rn.f64 	%fd76, %fd74, %fd21, %fd75;
	ld.const.f64 	%fd77, [%rd42+16];
	fma.rn.f64 	%fd78, %fd76, %fd21, %fd77;
	ld.const.f64 	%fd79, [%rd42+24];
	fma.rn.f64 	%fd80, %fd78, %fd21, %fd79;
	ld.const.f64 	%fd81, [%rd42+32];
	fma.rn.f64 	%fd82, %fd80, %fd21, %fd81;
	ld.const.f64 	%fd83, [%rd42+40];
	fma.rn.f64 	%fd22, %fd82, %fd21, %fd83;
	fma.rn.f64 	%fd94, %fd22, %fd93, %fd93;
	@%p10 bra 	BB92_17;

	mov.f64 	%fd84, 0d3FF0000000000000;
	fma.rn.f64 	%fd94, %fd22, %fd21, %fd84;

BB92_17:
	and.b32  	%r45, %r58, 2;
	setp.eq.s32	%p11, %r45, 0;
	@%p11 bra 	BB92_19;

	mov.f64 	%fd85, 0d0000000000000000;
	mov.f64 	%fd86, 0dBFF0000000000000;
	fma.rn.f64 	%fd94, %fd94, %fd86, %fd85;

BB92_19:
	cvta.to.global.u64 	%rd43, %rd7;
	add.s64 	%rd45, %rd43, %rd28;
	ld.global.u32 	%r56, [%rd45];
	mul.wide.s32 	%rd47, %r56, 8;
	add.s64 	%rd48, %rd30, %rd47;
	mul.f64 	%fd87, %fd15, %fd94;
	st.global.f64 	[%rd48], %fd87;

BB92_20:
	ret;
}

	// .globl	vec_computePSF_phaseNwithOil
.visible .entry vec_computePSF_phaseNwithOil(
	.param .u32 vec_computePSF_phaseNwithOil_param_0,
	.param .u64 vec_computePSF_phaseNwithOil_param_1,
	.param .u64 vec_computePSF_phaseNwithOil_param_2,
	.param .u64 vec_computePSF_phaseNwithOil_param_3,
	.param .u64 vec_computePSF_phaseNwithOil_param_4,
	.param .u64 vec_computePSF_phaseNwithOil_param_5,
	.param .u64 vec_computePSF_phaseNwithOil_param_6,
	.param .u64 vec_computePSF_phaseNwithOil_param_7,
	.param .u64 vec_computePSF_phaseNwithOil_param_8,
	.param .f64 vec_computePSF_phaseNwithOil_param_9,
	.param .f64 vec_computePSF_phaseNwithOil_param_10,
	.param .f64 vec_computePSF_phaseNwithOil_param_11,
	.param .f64 vec_computePSF_phaseNwithOil_param_12,
	.param .u64 vec_computePSF_phaseNwithOil_param_13,
	.param .u64 vec_computePSF_phaseNwithOil_param_14,
	.param .u64 vec_computePSF_phaseNwithOil_param_15
)
{
	.local .align 4 .b8 	__local_depot93[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<51>;
	.reg .f64 	%fd<106>;
	.reg .b64 	%rd<56>;


	mov.u64 	%SPL, __local_depot93;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r9, [vec_computePSF_phaseNwithOil_param_0];
	ld.param.u64 	%rd3, [vec_computePSF_phaseNwithOil_param_1];
	ld.param.u64 	%rd4, [vec_computePSF_phaseNwithOil_param_2];
	ld.param.u64 	%rd5, [vec_computePSF_phaseNwithOil_param_3];
	ld.param.u64 	%rd6, [vec_computePSF_phaseNwithOil_param_4];
	ld.param.u64 	%rd7, [vec_computePSF_phaseNwithOil_param_5];
	ld.param.u64 	%rd8, [vec_computePSF_phaseNwithOil_param_6];
	ld.param.u64 	%rd9, [vec_computePSF_phaseNwithOil_param_7];
	ld.param.u64 	%rd10, [vec_computePSF_phaseNwithOil_param_8];
	ld.param.f64 	%fd29, [vec_computePSF_phaseNwithOil_param_9];
	ld.param.f64 	%fd30, [vec_computePSF_phaseNwithOil_param_10];
	ld.param.f64 	%fd31, [vec_computePSF_phaseNwithOil_param_11];
	ld.param.f64 	%fd32, [vec_computePSF_phaseNwithOil_param_12];
	ld.param.u64 	%rd11, [vec_computePSF_phaseNwithOil_param_13];
	ld.param.u64 	%rd12, [vec_computePSF_phaseNwithOil_param_14];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNwithOil_param_15];
	add.u64 	%rd14, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	add.u64 	%rd15, %SP, 4;
	add.u64 	%rd2, %SPL, 4;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB93_20;

	cvta.to.global.u64 	%rd16, %rd5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.s32 	%rd18, %r1, 8;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.f64 	%fd33, [%rd19];
	cvta.to.global.u64 	%rd20, %rd4;
	add.s64 	%rd21, %rd20, %rd18;
	ld.global.f64 	%fd34, [%rd21];
	mul.f64 	%fd35, %fd34, %fd30;
	fma.rn.f64 	%fd36, %fd33, %fd29, %fd35;
	cvta.to.global.u64 	%rd22, %rd10;
	add.s64 	%rd23, %rd22, %rd18;
	ld.global.f64 	%fd37, [%rd23];
	add.f64 	%fd38, %fd37, %fd36;
	add.s64 	%rd24, %rd16, %rd18;
	ld.global.f64 	%fd39, [%rd24];
	mul.f64 	%fd40, %fd39, %fd31;
	add.f64 	%fd41, %fd38, %fd40;
	cvta.to.global.u64 	%rd25, %rd7;
	add.s64 	%rd26, %rd25, %rd18;
	ld.global.f64 	%fd42, [%rd26];
	mul.f64 	%fd43, %fd42, %fd32;
	sub.f64 	%fd98, %fd41, %fd43;
	cvta.to.global.u64 	%rd27, %rd6;
	add.s64 	%rd28, %rd27, %rd18;
	ld.global.f64 	%fd44, [%rd28];
	fma.rn.f64 	%fd45, %fd40, %fd44, %fd38;
	cvta.to.global.u64 	%rd29, %rd8;
	add.s64 	%rd30, %rd29, %rd18;
	ld.global.f64 	%fd46, [%rd30];
	mul.f64 	%fd47, %fd43, %fd46;
	sub.f64 	%fd102, %fd45, %fd47;
	cvta.to.global.u64 	%rd31, %rd9;
	add.s64 	%rd32, %rd31, %rd18;
	ld.global.f64 	%fd3, [%rd32];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd98;
	}
	and.b32  	%r20, %r19, 2147483647;
	setp.ne.s32	%p2, %r20, 2146435072;
	@%p2 bra 	BB93_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd98;
	}
	setp.ne.s32	%p3, %r21, 0;
	@%p3 bra 	BB93_4;

	mov.f64 	%fd48, 0d0000000000000000;
	mul.rn.f64 	%fd98, %fd98, %fd48;

BB93_4:
	mul.f64 	%fd49, %fd98, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r49, %fd49;
	st.local.u32 	[%rd2], %r49;
	cvt.rn.f64.s32	%fd50, %r49;
	neg.f64 	%fd51, %fd50;
	mov.f64 	%fd52, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd53, %fd51, %fd52, %fd98;
	mov.f64 	%fd54, 0d3C91A62633145C00;
	fma.rn.f64 	%fd55, %fd51, %fd54, %fd53;
	mov.f64 	%fd56, 0d397B839A252049C0;
	fma.rn.f64 	%fd99, %fd51, %fd56, %fd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd98;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p4, %r23, 1105199104;
	@%p4 bra 	BB93_6;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd98;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd15;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd99, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r49, [%rd2];

BB93_6:
	add.s32 	%r5, %r49, 1;
	and.b32  	%r24, %r5, 1;
	shl.b32 	%r25, %r24, 3;
	setp.eq.s32	%p5, %r24, 0;
	selp.f64	%fd57, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r26, %r25, 1;
	mul.wide.s32 	%rd34, %r26, 8;
	mov.u64 	%rd35, __cudart_sin_cos_coeffs;
	add.s64 	%rd36, %rd35, %rd34;
	ld.const.f64 	%fd58, [%rd36];
	mul.rn.f64 	%fd9, %fd99, %fd99;
	fma.rn.f64 	%fd59, %fd57, %fd9, %fd58;
	ld.const.f64 	%fd60, [%rd36+8];
	fma.rn.f64 	%fd61, %fd59, %fd9, %fd60;
	ld.const.f64 	%fd62, [%rd36+16];
	fma.rn.f64 	%fd63, %fd61, %fd9, %fd62;
	ld.const.f64 	%fd64, [%rd36+24];
	fma.rn.f64 	%fd65, %fd63, %fd9, %fd64;
	ld.const.f64 	%fd66, [%rd36+32];
	fma.rn.f64 	%fd67, %fd65, %fd9, %fd66;
	ld.const.f64 	%fd68, [%rd36+40];
	fma.rn.f64 	%fd10, %fd67, %fd9, %fd68;
	fma.rn.f64 	%fd100, %fd10, %fd99, %fd99;
	@%p5 bra 	BB93_8;

	mov.f64 	%fd69, 0d3FF0000000000000;
	fma.rn.f64 	%fd100, %fd10, %fd9, %fd69;

BB93_8:
	and.b32  	%r27, %r5, 2;
	setp.eq.s32	%p6, %r27, 0;
	@%p6 bra 	BB93_10;

	mov.f64 	%fd70, 0d0000000000000000;
	mov.f64 	%fd71, 0dBFF0000000000000;
	fma.rn.f64 	%fd100, %fd100, %fd71, %fd70;

BB93_10:
	cvta.to.global.u64 	%rd37, %rd11;
	mul.wide.s32 	%rd38, %r1, 4;
	add.s64 	%rd39, %rd37, %rd38;
	ld.global.u32 	%r38, [%rd39];
	cvta.to.global.u64 	%rd40, %rd13;
	mul.wide.s32 	%rd41, %r38, 8;
	add.s64 	%rd42, %rd40, %rd41;
	mul.f64 	%fd72, %fd3, %fd100;
	st.global.f64 	[%rd42], %fd72;
	ld.global.f64 	%fd16, [%rd32];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd102;
	}
	and.b32  	%r40, %r39, 2147483647;
	setp.ne.s32	%p7, %r40, 2146435072;
	@%p7 bra 	BB93_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd102;
	}
	setp.ne.s32	%p8, %r41, 0;
	@%p8 bra 	BB93_13;

	mov.f64 	%fd73, 0d0000000000000000;
	mul.rn.f64 	%fd102, %fd102, %fd73;

BB93_13:
	mul.f64 	%fd74, %fd102, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r50, %fd74;
	st.local.u32 	[%rd1], %r50;
	cvt.rn.f64.s32	%fd75, %r50;
	neg.f64 	%fd76, %fd75;
	fma.rn.f64 	%fd78, %fd76, %fd52, %fd102;
	fma.rn.f64 	%fd80, %fd76, %fd54, %fd78;
	fma.rn.f64 	%fd103, %fd76, %fd56, %fd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd102;
	}
	and.b32  	%r43, %r42, 2145386496;
	setp.lt.u32	%p9, %r43, 1105199104;
	@%p9 bra 	BB93_15;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd102;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd103, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r50, [%rd1];

BB93_15:
	and.b32  	%r44, %r50, 1;
	shl.b32 	%r45, %r44, 3;
	setp.eq.s32	%p10, %r44, 0;
	selp.f64	%fd82, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	add.s32 	%r46, %r45, 1;
	mul.wide.s32 	%rd47, %r46, 8;
	add.s64 	%rd49, %rd35, %rd47;
	ld.const.f64 	%fd83, [%rd49];
	mul.rn.f64 	%fd22, %fd103, %fd103;
	fma.rn.f64 	%fd84, %fd82, %fd22, %fd83;
	ld.const.f64 	%fd85, [%rd49+8];
	fma.rn.f64 	%fd86, %fd84, %fd22, %fd85;
	ld.const.f64 	%fd87, [%rd49+16];
	fma.rn.f64 	%fd88, %fd86, %fd22, %fd87;
	ld.const.f64 	%fd89, [%rd49+24];
	fma.rn.f64 	%fd90, %fd88, %fd22, %fd89;
	ld.const.f64 	%fd91, [%rd49+32];
	fma.rn.f64 	%fd92, %fd90, %fd22, %fd91;
	ld.const.f64 	%fd93, [%rd49+40];
	fma.rn.f64 	%fd23, %fd92, %fd22, %fd93;
	fma.rn.f64 	%fd104, %fd23, %fd103, %fd103;
	@%p10 bra 	BB93_17;

	mov.f64 	%fd94, 0d3FF0000000000000;
	fma.rn.f64 	%fd104, %fd23, %fd22, %fd94;

BB93_17:
	and.b32  	%r47, %r50, 2;
	setp.eq.s32	%p11, %r47, 0;
	@%p11 bra 	BB93_19;

	mov.f64 	%fd95, 0d0000000000000000;
	mov.f64 	%fd96, 0dBFF0000000000000;
	fma.rn.f64 	%fd104, %fd104, %fd96, %fd95;

BB93_19:
	cvta.to.global.u64 	%rd50, %rd12;
	add.s64 	%rd52, %rd50, %rd38;
	ld.global.u32 	%r48, [%rd52];
	mul.wide.s32 	%rd54, %r48, 8;
	add.s64 	%rd55, %rd40, %rd54;
	mul.f64 	%fd97, %fd16, %fd104;
	st.global.f64 	[%rd55], %fd97;

BB93_20:
	ret;
}

	// .globl	vec_computePSF_phaseNMany
.visible .entry vec_computePSF_phaseNMany(
	.param .u32 vec_computePSF_phaseNMany_param_0,
	.param .u32 vec_computePSF_phaseNMany_param_1,
	.param .u32 vec_computePSF_phaseNMany_param_2,
	.param .u64 vec_computePSF_phaseNMany_param_3,
	.param .u64 vec_computePSF_phaseNMany_param_4,
	.param .u64 vec_computePSF_phaseNMany_param_5,
	.param .u64 vec_computePSF_phaseNMany_param_6,
	.param .u64 vec_computePSF_phaseNMany_param_7,
	.param .u64 vec_computePSF_phaseNMany_param_8,
	.param .u64 vec_computePSF_phaseNMany_param_9,
	.param .u64 vec_computePSF_phaseNMany_param_10,
	.param .u64 vec_computePSF_phaseNMany_param_11,
	.param .u32 vec_computePSF_phaseNMany_param_12
)
{
	.local .align 4 .b8 	__local_depot94[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<48>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<56>;


	mov.u64 	%SPL, __local_depot94;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r16, [vec_computePSF_phaseNMany_param_0];
	ld.param.u32 	%r13, [vec_computePSF_phaseNMany_param_1];
	ld.param.u32 	%r14, [vec_computePSF_phaseNMany_param_2];
	ld.param.u64 	%rd1, [vec_computePSF_phaseNMany_param_3];
	ld.param.u64 	%rd2, [vec_computePSF_phaseNMany_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_phaseNMany_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_phaseNMany_param_6];
	ld.param.u64 	%rd5, [vec_computePSF_phaseNMany_param_7];
	ld.param.u64 	%rd6, [vec_computePSF_phaseNMany_param_8];
	ld.param.u64 	%rd7, [vec_computePSF_phaseNMany_param_9];
	ld.param.u64 	%rd8, [vec_computePSF_phaseNMany_param_10];
	ld.param.u64 	%rd9, [vec_computePSF_phaseNMany_param_11];
	ld.param.u32 	%r15, [vec_computePSF_phaseNMany_param_12];
	mov.u32 	%r17, %ntid.y;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %tid.y;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %nctaid.x;
	mov.u32 	%r22, %ctaid.x;
	mad.lo.s32 	%r23, %r20, %r21, %r22;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r1, %r23, %r24, %r25;
	setp.ge.s32	%p1, %r1, %r16;
	@%p1 bra 	BB94_20;

	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd4;
	rem.s32 	%r2, %r1, %r13;
	cvta.to.global.u64 	%rd12, %rd1;
	mul.wide.s32 	%rd13, %r2, 8;
	add.s64 	%rd14, %rd12, %rd13;
	div.s32 	%r3, %r1, %r13;
	mul.wide.s32 	%rd15, %r3, 8;
	add.s64 	%rd16, %rd10, %rd15;
	ld.global.f64 	%fd28, [%rd16];
	ld.global.f64 	%fd29, [%rd14];
	cvta.to.global.u64 	%rd17, %rd2;
	add.s64 	%rd18, %rd17, %rd13;
	shl.b32 	%r26, %r15, 3;
	cvt.s64.s32	%rd19, %r26;
	add.s64 	%rd20, %rd16, %rd19;
	ld.global.f64 	%fd30, [%rd20];
	ld.global.f64 	%fd31, [%rd18];
	mul.f64 	%fd32, %fd31, %fd30;
	fma.rn.f64 	%fd33, %fd29, %fd28, %fd32;
	cvta.to.global.u64 	%rd21, %rd3;
	add.s64 	%rd22, %rd21, %rd13;
	add.s64 	%rd23, %rd20, %rd19;
	ld.global.f64 	%fd34, [%rd23];
	ld.global.f64 	%fd35, [%rd22];
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd33;
	cvta.to.global.u64 	%rd24, %rd5;
	add.s64 	%rd25, %rd24, %rd13;
	ld.global.f64 	%fd37, [%rd25];
	add.f64 	%fd92, %fd37, %fd36;
	add.s64 	%rd26, %rd11, %rd13;
	ld.global.f64 	%fd2, [%rd26];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd92;
	}
	and.b32  	%r4, %r27, 2147483647;
	setp.ne.s32	%p2, %r4, 2146435072;
	mov.f64 	%fd88, %fd92;
	@%p2 bra 	BB94_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd92;
	}
	setp.ne.s32	%p3, %r28, 0;
	mov.f64 	%fd88, %fd92;
	@%p3 bra 	BB94_4;

	mov.f64 	%fd38, 0d0000000000000000;
	mul.rn.f64 	%fd88, %fd92, %fd38;

BB94_4:
	mul.f64 	%fd39, %fd88, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r46, %fd39;
	add.u64 	%rd27, %SP, 4;
	add.u64 	%rd28, %SPL, 4;
	st.local.u32 	[%rd28], %r46;
	cvt.rn.f64.s32	%fd40, %r46;
	neg.f64 	%fd41, %fd40;
	mov.f64 	%fd42, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd43, %fd41, %fd42, %fd88;
	mov.f64 	%fd44, 0d3C91A62633145C00;
	fma.rn.f64 	%fd45, %fd41, %fd44, %fd43;
	mov.f64 	%fd46, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd41, %fd46, %fd45;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd88;
	}
	and.b32  	%r30, %r29, 2145386496;
	setp.lt.u32	%p4, %r30, 1105199104;
	@%p4 bra 	BB94_6;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd88;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd27;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r46, [%rd28];

BB94_6:
	add.s32 	%r8, %r46, 1;
	and.b32  	%r31, %r8, 1;
	shl.b32 	%r32, %r31, 3;
	setp.eq.s32	%p5, %r31, 0;
	selp.f64	%fd47, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
	add.s32 	%r33, %r32, 1;
	mul.wide.s32 	%rd31, %r33, 8;
	mov.u64 	%rd32, __cudart_sin_cos_coeffs;
	add.s64 	%rd33, %rd32, %rd31;
	ld.const.f64 	%fd48, [%rd33];
	mul.rn.f64 	%fd8, %fd89, %fd89;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	ld.const.f64 	%fd50, [%rd33+8];
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	ld.const.f64 	%fd52, [%rd33+16];
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	ld.const.f64 	%fd54, [%rd33+24];
	fma.rn.f64 	%fd55, %fd53, %fd8, %fd54;
	ld.const.f64 	%fd56, [%rd33+32];
	fma.rn.f64 	%fd57, %fd55, %fd8, %fd56;
	ld.const.f64 	%fd58, [%rd33+40];
	fma.rn.f64 	%fd9, %fd57, %fd8, %fd58;
	fma.rn.f64 	%fd90, %fd9, %fd89, %fd89;
	@%p5 bra 	BB94_8;

	mov.f64 	%fd59, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd9, %fd8, %fd59;

BB94_8:
	and.b32  	%r34, %r8, 2;
	setp.eq.s32	%p6, %r34, 0;
	@%p6 bra 	BB94_10;

	mov.f64 	%fd60, 0d0000000000000000;
	mov.f64 	%fd61, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd61, %fd60;

BB94_10:
	cvta.to.global.u64 	%rd34, %rd7;
	mul.wide.s32 	%rd35, %r2, 4;
	add.s64 	%rd36, %rd34, %rd35;
	mul.lo.s32 	%r9, %r3, %r14;
	ld.global.u32 	%r35, [%rd36];
	add.s32 	%r36, %r35, %r9;
	cvta.to.global.u64 	%rd37, %rd9;
	mul.wide.s32 	%rd38, %r36, 8;
	add.s64 	%rd39, %rd37, %rd38;
	mul.f64 	%fd62, %fd2, %fd90;
	st.global.f64 	[%rd39], %fd62;
	ld.global.f64 	%fd15, [%rd26];
	@%p2 bra 	BB94_13;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd92;
	}
	setp.ne.s32	%p8, %r37, 0;
	@%p8 bra 	BB94_13;

	mov.f64 	%fd63, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd92, %fd63;

BB94_13:
	mul.f64 	%fd64, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r47, %fd64;
	add.u64 	%rd43, %SP, 0;
	add.u64 	%rd44, %SPL, 0;
	st.local.u32 	[%rd44], %r47;
	cvt.rn.f64.s32	%fd65, %r47;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd68, %fd66, %fd42, %fd92;
	fma.rn.f64 	%fd70, %fd66, %fd44, %fd68;
	fma.rn.f64 	%fd93, %fd66, %fd46, %fd70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd92;
	}
	and.b32  	%r39, %r38, 2145386496;
	setp.lt.u32	%p9, %r39, 1105199104;
	@%p9 bra 	BB94_15;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd93, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r47, [%rd44];

BB94_15:
	and.b32  	%r40, %r47, 1;
	shl.b32 	%r41, %r40, 3;
	setp.eq.s32	%p10, %r40, 0;
	selp.f64	%fd72, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	add.s32 	%r42, %r41, 1;
	mul.wide.s32 	%rd47, %r42, 8;
	add.s64 	%rd49, %rd32, %rd47;
	ld.const.f64 	%fd73, [%rd49];
	mul.rn.f64 	%fd21, %fd93, %fd93;
	fma.rn.f64 	%fd74, %fd72, %fd21, %fd73;
	ld.const.f64 	%fd75, [%rd49+8];
	fma.rn.f64 	%fd76, %fd74, %fd21, %fd75;
	ld.const.f64 	%fd77, [%rd49+16];
	fma.rn.f64 	%fd78, %fd76, %fd21, %fd77;
	ld.const.f64 	%fd79, [%rd49+24];
	fma.rn.f64 	%fd80, %fd78, %fd21, %fd79;
	ld.const.f64 	%fd81, [%rd49+32];
	fma.rn.f64 	%fd82, %fd80, %fd21, %fd81;
	ld.const.f64 	%fd83, [%rd49+40];
	fma.rn.f64 	%fd22, %fd82, %fd21, %fd83;
	fma.rn.f64 	%fd94, %fd22, %fd93, %fd93;
	@%p10 bra 	BB94_17;

	mov.f64 	%fd84, 0d3FF0000000000000;
	fma.rn.f64 	%fd94, %fd22, %fd21, %fd84;

BB94_17:
	and.b32  	%r43, %r47, 2;
	setp.eq.s32	%p11, %r43, 0;
	@%p11 bra 	BB94_19;

	mov.f64 	%fd85, 0d0000000000000000;
	mov.f64 	%fd86, 0dBFF0000000000000;
	fma.rn.f64 	%fd94, %fd94, %fd86, %fd85;

BB94_19:
	cvta.to.global.u64 	%rd50, %rd8;
	add.s64 	%rd52, %rd50, %rd35;
	ld.global.u32 	%r44, [%rd52];
	add.s32 	%r45, %r44, %r9;
	mul.wide.s32 	%rd54, %r45, 8;
	add.s64 	%rd55, %rd37, %rd54;
	mul.f64 	%fd87, %fd15, %fd94;
	st.global.f64 	[%rd55], %fd87;

BB94_20:
	ret;
}

	// .globl	vec_computeModelAndLikelihood
.visible .entry vec_computeModelAndLikelihood(
	.param .u32 vec_computeModelAndLikelihood_param_0,
	.param .u32 vec_computeModelAndLikelihood_param_1,
	.param .u32 vec_computeModelAndLikelihood_param_2,
	.param .u32 vec_computeModelAndLikelihood_param_3,
	.param .u32 vec_computeModelAndLikelihood_param_4,
	.param .u32 vec_computeModelAndLikelihood_param_5,
	.param .u64 vec_computeModelAndLikelihood_param_6,
	.param .u64 vec_computeModelAndLikelihood_param_7,
	.param .u64 vec_computeModelAndLikelihood_param_8,
	.param .u64 vec_computeModelAndLikelihood_param_9,
	.param .u64 vec_computeModelAndLikelihood_param_10,
	.param .u32 vec_computeModelAndLikelihood_param_11,
	.param .u32 vec_computeModelAndLikelihood_param_12,
	.param .u32 vec_computeModelAndLikelihood_param_13,
	.param .u64 vec_computeModelAndLikelihood_param_14
)
{
	.reg .pred 	%p<55>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<225>;
	.reg .f64 	%fd<105>;
	.reg .b64 	%rd<105>;


	ld.param.u32 	%r47, [vec_computeModelAndLikelihood_param_0];
	ld.param.u32 	%r48, [vec_computeModelAndLikelihood_param_2];
	ld.param.u32 	%r41, [vec_computeModelAndLikelihood_param_3];
	ld.param.u32 	%r42, [vec_computeModelAndLikelihood_param_4];
	ld.param.u32 	%r43, [vec_computeModelAndLikelihood_param_5];
	ld.param.u64 	%rd9, [vec_computeModelAndLikelihood_param_6];
	ld.param.u64 	%rd10, [vec_computeModelAndLikelihood_param_7];
	ld.param.u64 	%rd11, [vec_computeModelAndLikelihood_param_8];
	ld.param.u64 	%rd12, [vec_computeModelAndLikelihood_param_9];
	ld.param.u64 	%rd13, [vec_computeModelAndLikelihood_param_10];
	ld.param.u32 	%r44, [vec_computeModelAndLikelihood_param_11];
	ld.param.u32 	%r45, [vec_computeModelAndLikelihood_param_12];
	ld.param.u32 	%r46, [vec_computeModelAndLikelihood_param_13];
	ld.param.u64 	%rd14, [vec_computeModelAndLikelihood_param_14];
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r50, %ntid.y;
	mov.u32 	%r51, %ctaid.y;
	mov.u32 	%r52, %tid.y;
	mad.lo.s32 	%r53, %r50, %r51, %r52;
	mov.u32 	%r54, %nctaid.x;
	mov.u32 	%r55, %ctaid.x;
	mad.lo.s32 	%r56, %r53, %r54, %r55;
	mov.u32 	%r57, %ntid.x;
	mad.lo.s32 	%r1, %r56, %r57, %r49;
	div.s32 	%r2, %r1, %r48;
	rem.s32 	%r3, %r1, %r48;
	setp.ge.s32	%p1, %r1, %r47;
	@%p1 bra 	BB95_41;

	cvta.to.global.u64 	%rd15, %rd10;
	cvta.to.global.u64 	%rd16, %rd14;
	mul.wide.s32 	%rd17, %r1, 4;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.f32 	%f2, [%rd18];
	cvt.f64.f32	%fd98, %f2;
	mul.wide.s32 	%rd19, %r1, 8;
	add.s64 	%rd20, %rd15, %rd19;
	st.global.f64 	[%rd20], %fd98;
	setp.lt.s32	%p2, %r43, 1;
	@%p2 bra 	BB95_31;

	and.b32  	%r61, %r43, 3;
	mov.u32 	%r218, 0;
	setp.eq.s32	%p3, %r61, 0;
	@%p3 bra 	BB95_16;

	setp.eq.s32	%p4, %r61, 1;
	@%p4 bra 	BB95_12;

	setp.eq.s32	%p5, %r61, 2;
	@%p5 bra 	BB95_8;

	mul.lo.s32 	%r63, %r44, %r43;
	cvta.to.global.u64 	%rd21, %rd13;
	mul.wide.s32 	%rd22, %r63, 4;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.f32 	%f3, [%rd23];
	cvt.rzi.s32.f32	%r64, %f3;
	sub.s32 	%r4, %r2, %r64;
	setp.gt.s32	%p6, %r4, -1;
	setp.lt.s32	%p7, %r4, %r41;
	and.pred  	%p8, %p6, %p7;
	mov.u32 	%r218, 1;
	@!%p8 bra 	BB95_8;
	bra.uni 	BB95_6;

BB95_6:
	mul.lo.s32 	%r66, %r45, %r43;
	mul.wide.s32 	%rd25, %r66, 4;
	add.s64 	%rd26, %rd21, %rd25;
	ld.global.f32 	%f4, [%rd26];
	cvt.rzi.s32.f32	%r67, %f4;
	sub.s32 	%r5, %r3, %r67;
	setp.gt.s32	%p9, %r5, -1;
	setp.lt.s32	%p10, %r5, %r42;
	and.pred  	%p11, %p9, %p10;
	@!%p11 bra 	BB95_8;
	bra.uni 	BB95_7;

BB95_7:
	mad.lo.s32 	%r69, %r4, %r42, %r5;
	cvta.to.global.u64 	%rd27, %rd12;
	mul.wide.s32 	%rd28, %r69, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.f32 	%f5, [%rd29];
	cvt.f64.f32	%fd28, %f5;
	mul.lo.s32 	%r70, %r46, %r43;
	mul.wide.s32 	%rd31, %r70, 4;
	add.s64 	%rd32, %rd21, %rd31;
	ld.global.f32 	%f6, [%rd32];
	cvt.f64.f32	%fd29, %f6;
	fma.rn.f64 	%fd98, %fd28, %fd29, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_8:
	mad.lo.s32 	%r81, %r44, %r43, %r218;
	cvta.to.global.u64 	%rd36, %rd13;
	mul.wide.s32 	%rd37, %r81, 4;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.f32 	%f7, [%rd38];
	cvt.rzi.s32.f32	%r82, %f7;
	sub.s32 	%r7, %r2, %r82;
	setp.gt.s32	%p12, %r7, -1;
	setp.lt.s32	%p13, %r7, %r41;
	and.pred  	%p14, %p12, %p13;
	@!%p14 bra 	BB95_11;
	bra.uni 	BB95_9;

BB95_9:
	mad.lo.s32 	%r83, %r45, %r43, %r218;
	mul.wide.s32 	%rd40, %r83, 4;
	add.s64 	%rd41, %rd36, %rd40;
	ld.global.f32 	%f8, [%rd41];
	cvt.rzi.s32.f32	%r84, %f8;
	sub.s32 	%r8, %r3, %r84;
	setp.gt.s32	%p15, %r8, -1;
	setp.lt.s32	%p16, %r8, %r42;
	and.pred  	%p17, %p15, %p16;
	@!%p17 bra 	BB95_11;
	bra.uni 	BB95_10;

BB95_10:
	neg.s32 	%r85, %r218;
	and.b32  	%r86, %r85, %r41;
	add.s32 	%r87, %r7, %r86;
	mad.lo.s32 	%r88, %r87, %r42, %r8;
	cvta.to.global.u64 	%rd42, %rd12;
	mul.wide.s32 	%rd43, %r88, 4;
	add.s64 	%rd44, %rd42, %rd43;
	ld.global.f32 	%f9, [%rd44];
	cvt.f64.f32	%fd30, %f9;
	mad.lo.s32 	%r89, %r46, %r43, %r218;
	mul.wide.s32 	%rd46, %r89, 4;
	add.s64 	%rd47, %rd36, %rd46;
	ld.global.f32 	%f10, [%rd47];
	cvt.f64.f32	%fd31, %f10;
	fma.rn.f64 	%fd98, %fd30, %fd31, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_11:
	add.s32 	%r218, %r218, 1;

BB95_12:
	mad.lo.s32 	%r100, %r44, %r43, %r218;
	cvta.to.global.u64 	%rd51, %rd13;
	mul.wide.s32 	%rd52, %r100, 4;
	add.s64 	%rd53, %rd51, %rd52;
	ld.global.f32 	%f11, [%rd53];
	cvt.rzi.s32.f32	%r101, %f11;
	sub.s32 	%r11, %r2, %r101;
	setp.gt.s32	%p18, %r11, -1;
	setp.lt.s32	%p19, %r11, %r41;
	and.pred  	%p20, %p18, %p19;
	@!%p20 bra 	BB95_15;
	bra.uni 	BB95_13;

BB95_13:
	mad.lo.s32 	%r102, %r45, %r43, %r218;
	mul.wide.s32 	%rd55, %r102, 4;
	add.s64 	%rd56, %rd51, %rd55;
	ld.global.f32 	%f12, [%rd56];
	cvt.rzi.s32.f32	%r103, %f12;
	sub.s32 	%r12, %r3, %r103;
	setp.gt.s32	%p21, %r12, -1;
	setp.lt.s32	%p22, %r12, %r42;
	and.pred  	%p23, %p21, %p22;
	@!%p23 bra 	BB95_15;
	bra.uni 	BB95_14;

BB95_14:
	mad.lo.s32 	%r104, %r218, %r41, %r11;
	mad.lo.s32 	%r105, %r104, %r42, %r12;
	cvta.to.global.u64 	%rd57, %rd12;
	mul.wide.s32 	%rd58, %r105, 4;
	add.s64 	%rd59, %rd57, %rd58;
	ld.global.f32 	%f13, [%rd59];
	cvt.f64.f32	%fd32, %f13;
	mad.lo.s32 	%r106, %r46, %r43, %r218;
	mul.wide.s32 	%rd61, %r106, 4;
	add.s64 	%rd62, %rd51, %rd61;
	ld.global.f32 	%f14, [%rd62];
	cvt.f64.f32	%fd33, %f14;
	fma.rn.f64 	%fd98, %fd32, %fd33, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_15:
	add.s32 	%r218, %r218, 1;

BB95_16:
	setp.lt.u32	%p24, %r43, 4;
	@%p24 bra 	BB95_31;

	mad.lo.s32 	%r117, %r46, %r43, %r218;
	mul.wide.s32 	%rd1, %r117, 4;
	add.s32 	%r118, %r218, 3;
	mul.lo.s32 	%r15, %r41, %r118;
	mad.lo.s32 	%r119, %r45, %r43, %r218;
	mul.wide.s32 	%rd2, %r119, 4;
	mad.lo.s32 	%r120, %r44, %r43, %r218;
	mul.wide.s32 	%rd3, %r120, 4;
	add.s32 	%r121, %r218, 2;
	mul.lo.s32 	%r16, %r41, %r121;
	mul.lo.s32 	%r17, %r218, %r41;
	add.s32 	%r122, %r218, 1;
	mul.lo.s32 	%r18, %r41, %r122;
	cvta.to.global.u64 	%rd104, %rd13;
	mov.u32 	%r219, %r2;

BB95_18:
	add.s64 	%rd6, %rd104, %rd3;
	ld.global.f32 	%f15, [%rd6];
	cvt.rzi.s32.f32	%r21, %f15;
	sub.s32 	%r123, %r2, %r21;
	setp.gt.s32	%p25, %r123, -1;
	setp.lt.s32	%p26, %r123, %r41;
	and.pred  	%p27, %p25, %p26;
	@!%p27 bra 	BB95_21;
	bra.uni 	BB95_19;

BB95_19:
	add.s64 	%rd66, %rd104, %rd2;
	ld.global.f32 	%f16, [%rd66];
	cvt.rzi.s32.f32	%r124, %f16;
	sub.s32 	%r22, %r3, %r124;
	setp.gt.s32	%p28, %r22, -1;
	setp.lt.s32	%p29, %r22, %r42;
	and.pred  	%p30, %p28, %p29;
	@!%p30 bra 	BB95_21;
	bra.uni 	BB95_20;

BB95_20:
	add.s32 	%r125, %r17, %r219;
	mad.lo.s32 	%r126, %r21, -1, %r125;
	mad.lo.s32 	%r127, %r126, %r42, %r22;
	cvta.to.global.u64 	%rd67, %rd12;
	mul.wide.s32 	%rd68, %r127, 4;
	add.s64 	%rd69, %rd67, %rd68;
	ld.global.f32 	%f17, [%rd69];
	cvt.f64.f32	%fd34, %f17;
	add.s64 	%rd70, %rd104, %rd1;
	ld.global.f32 	%f18, [%rd70];
	cvt.f64.f32	%fd35, %f18;
	fma.rn.f64 	%fd98, %fd34, %fd35, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_21:
	ld.global.f32 	%f19, [%rd6+4];
	cvt.rzi.s32.f32	%r23, %f19;
	sub.s32 	%r138, %r2, %r23;
	setp.gt.s32	%p31, %r138, -1;
	setp.lt.s32	%p32, %r138, %r41;
	and.pred  	%p33, %p31, %p32;
	@!%p33 bra 	BB95_24;
	bra.uni 	BB95_22;

BB95_22:
	add.s64 	%rd74, %rd104, %rd2;
	ld.global.f32 	%f20, [%rd74+4];
	cvt.rzi.s32.f32	%r139, %f20;
	sub.s32 	%r24, %r3, %r139;
	setp.gt.s32	%p34, %r24, -1;
	setp.lt.s32	%p35, %r24, %r42;
	and.pred  	%p36, %p34, %p35;
	@!%p36 bra 	BB95_24;
	bra.uni 	BB95_23;

BB95_23:
	add.s32 	%r140, %r18, %r219;
	mad.lo.s32 	%r141, %r23, -1, %r140;
	mad.lo.s32 	%r142, %r141, %r42, %r24;
	cvta.to.global.u64 	%rd75, %rd12;
	mul.wide.s32 	%rd76, %r142, 4;
	add.s64 	%rd77, %rd75, %rd76;
	ld.global.f32 	%f21, [%rd77];
	cvt.f64.f32	%fd36, %f21;
	add.s64 	%rd78, %rd104, %rd1;
	ld.global.f32 	%f22, [%rd78+4];
	cvt.f64.f32	%fd37, %f22;
	fma.rn.f64 	%fd98, %fd36, %fd37, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_24:
	ld.global.f32 	%f23, [%rd6+8];
	cvt.rzi.s32.f32	%r25, %f23;
	sub.s32 	%r153, %r2, %r25;
	setp.gt.s32	%p37, %r153, -1;
	setp.lt.s32	%p38, %r153, %r41;
	and.pred  	%p39, %p37, %p38;
	@!%p39 bra 	BB95_27;
	bra.uni 	BB95_25;

BB95_25:
	add.s64 	%rd82, %rd104, %rd2;
	ld.global.f32 	%f24, [%rd82+8];
	cvt.rzi.s32.f32	%r154, %f24;
	sub.s32 	%r26, %r3, %r154;
	setp.gt.s32	%p40, %r26, -1;
	setp.lt.s32	%p41, %r26, %r42;
	and.pred  	%p42, %p40, %p41;
	@!%p42 bra 	BB95_27;
	bra.uni 	BB95_26;

BB95_26:
	add.s32 	%r155, %r16, %r219;
	mad.lo.s32 	%r156, %r25, -1, %r155;
	mad.lo.s32 	%r157, %r156, %r42, %r26;
	cvta.to.global.u64 	%rd83, %rd12;
	mul.wide.s32 	%rd84, %r157, 4;
	add.s64 	%rd85, %rd83, %rd84;
	ld.global.f32 	%f25, [%rd85];
	cvt.f64.f32	%fd38, %f25;
	add.s64 	%rd86, %rd104, %rd1;
	ld.global.f32 	%f26, [%rd86+8];
	cvt.f64.f32	%fd39, %f26;
	fma.rn.f64 	%fd98, %fd38, %fd39, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_27:
	ld.global.f32 	%f27, [%rd6+12];
	cvt.rzi.s32.f32	%r27, %f27;
	sub.s32 	%r168, %r2, %r27;
	setp.gt.s32	%p43, %r168, -1;
	setp.lt.s32	%p44, %r168, %r41;
	and.pred  	%p45, %p43, %p44;
	@!%p45 bra 	BB95_30;
	bra.uni 	BB95_28;

BB95_28:
	add.s64 	%rd90, %rd104, %rd2;
	ld.global.f32 	%f28, [%rd90+12];
	cvt.rzi.s32.f32	%r169, %f28;
	sub.s32 	%r28, %r3, %r169;
	setp.gt.s32	%p46, %r28, -1;
	setp.lt.s32	%p47, %r28, %r42;
	and.pred  	%p48, %p46, %p47;
	@!%p48 bra 	BB95_30;
	bra.uni 	BB95_29;

BB95_29:
	add.s32 	%r170, %r15, %r219;
	mad.lo.s32 	%r171, %r27, -1, %r170;
	mad.lo.s32 	%r172, %r171, %r42, %r28;
	cvta.to.global.u64 	%rd91, %rd12;
	mul.wide.s32 	%rd92, %r172, 4;
	add.s64 	%rd93, %rd91, %rd92;
	ld.global.f32 	%f29, [%rd93];
	cvt.f64.f32	%fd40, %f29;
	add.s64 	%rd94, %rd104, %rd1;
	ld.global.f32 	%f30, [%rd94+12];
	cvt.f64.f32	%fd41, %f30;
	fma.rn.f64 	%fd98, %fd40, %fd41, %fd98;
	st.global.f64 	[%rd20], %fd98;

BB95_30:
	add.s64 	%rd104, %rd104, 16;
	mad.lo.s32 	%r219, %r41, 4, %r219;
	add.s32 	%r218, %r218, 4;
	setp.lt.s32	%p49, %r218, %r43;
	@%p49 bra 	BB95_18;

BB95_31:
	cvta.to.global.u64 	%rd98, %rd9;
	add.s64 	%rd8, %rd98, %rd19;
	setp.gt.f64	%p50, %fd98, 0d0000000000000000;
	@%p50 bra 	BB95_33;
	bra.uni 	BB95_32;

BB95_33:
	cvta.to.global.u64 	%rd101, %rd11;
	add.s64 	%rd103, %rd101, %rd17;
	ld.global.f32 	%f1, [%rd103];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r222, %temp}, %fd98;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r221}, %fd98;
	}
	mov.u32 	%r223, -1023;
	setp.gt.s32	%p51, %r221, 1048575;
	mov.f64 	%fd102, %fd98;
	@%p51 bra 	BB95_35;

	mul.f64 	%fd102, %fd98, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r221}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r222, %temp}, %fd102;
	}
	mov.u32 	%r223, -1077;

BB95_35:
	add.s32 	%r205, %r221, -1;
	setp.lt.u32	%p52, %r205, 2146435071;
	@%p52 bra 	BB95_37;
	bra.uni 	BB95_36;

BB95_37:
	shr.u32 	%r207, %r221, 20;
	add.s32 	%r224, %r223, %r207;
	and.b32  	%r208, %r221, -2146435073;
	or.b32  	%r209, %r208, 1072693248;
	mov.b64 	%fd103, {%r222, %r209};
	setp.lt.s32	%p54, %r209, 1073127583;
	@%p54 bra 	BB95_39;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r210, %temp}, %fd103;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r211}, %fd103;
	}
	add.s32 	%r212, %r211, -1048576;
	mov.b64 	%fd103, {%r210, %r212};
	add.s32 	%r224, %r224, 1;

BB95_39:
	add.f64 	%fd44, %fd103, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd45, %fd44;
	neg.f64 	%fd46, %fd44;
	mov.f64 	%fd47, 0d3FF0000000000000;
	fma.rn.f64 	%fd48, %fd46, %fd45, %fd47;
	fma.rn.f64 	%fd49, %fd48, %fd48, %fd48;
	fma.rn.f64 	%fd50, %fd49, %fd45, %fd45;
	add.f64 	%fd51, %fd103, 0dBFF0000000000000;
	mul.f64 	%fd52, %fd51, %fd50;
	fma.rn.f64 	%fd53, %fd51, %fd50, %fd52;
	mul.f64 	%fd54, %fd53, %fd53;
	mov.f64 	%fd55, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd56, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd57, %fd56, %fd54, %fd55;
	mov.f64 	%fd58, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd59, %fd57, %fd54, %fd58;
	mov.f64 	%fd60, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd61, %fd59, %fd54, %fd60;
	mov.f64 	%fd62, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd63, %fd61, %fd54, %fd62;
	mov.f64 	%fd64, 0d3F624924923BE72D;
	fma.rn.f64 	%fd65, %fd63, %fd54, %fd64;
	mov.f64 	%fd66, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd67, %fd65, %fd54, %fd66;
	mov.f64 	%fd68, 0d3FB5555555555554;
	fma.rn.f64 	%fd69, %fd67, %fd54, %fd68;
	sub.f64 	%fd70, %fd51, %fd53;
	add.f64 	%fd71, %fd70, %fd70;
	neg.f64 	%fd72, %fd53;
	fma.rn.f64 	%fd73, %fd72, %fd51, %fd71;
	mul.f64 	%fd74, %fd50, %fd73;
	mul.f64 	%fd75, %fd54, %fd69;
	fma.rn.f64 	%fd76, %fd75, %fd53, %fd74;
	xor.b32  	%r213, %r224, -2147483648;
	mov.u32 	%r214, -2147483648;
	mov.u32 	%r215, 1127219200;
	mov.b64 	%fd77, {%r213, %r215};
	mov.b64 	%fd78, {%r214, %r215};
	sub.f64 	%fd79, %fd77, %fd78;
	mov.f64 	%fd80, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd81, %fd79, %fd80, %fd53;
	neg.f64 	%fd82, %fd79;
	fma.rn.f64 	%fd83, %fd82, %fd80, %fd81;
	sub.f64 	%fd84, %fd83, %fd53;
	sub.f64 	%fd85, %fd76, %fd84;
	mov.f64 	%fd86, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd87, %fd79, %fd86, %fd85;
	add.f64 	%fd104, %fd81, %fd87;
	bra.uni 	BB95_40;

BB95_32:
	mov.u64 	%rd100, 0;
	st.global.u64 	[%rd8], %rd100;
	bra.uni 	BB95_41;

BB95_36:
	mov.f64 	%fd42, 0d7FF0000000000000;
	fma.rn.f64 	%fd43, %fd102, %fd42, %fd42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r206}, %fd102;
	}
	mov.b32 	 %f31, %r206;
	setp.eq.f32	%p53, %f31, 0f00000000;
	selp.f64	%fd104, 0dFFF0000000000000, %fd43, %p53;

BB95_40:
	cvt.f64.f32	%fd88, %f1;
	mul.f64 	%fd89, %fd88, %fd104;
	sub.f64 	%fd90, %fd98, %fd89;
	st.global.f64 	[%rd8], %fd90;

BB95_41:
	ret;
}

	// .globl	vec_computeLikelihoodAndModelwithPhotonNumberAndBackground
.visible .entry vec_computeLikelihoodAndModelwithPhotonNumberAndBackground(
	.param .u32 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_0,
	.param .u32 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_1,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_2,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_3,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_4,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_5,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_6,
	.param .u32 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_7,
	.param .u64 vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_8
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<24>;


	ld.param.u32 	%r12, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_0];
	ld.param.u32 	%r13, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_1];
	ld.param.u64 	%rd2, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_2];
	ld.param.u64 	%rd3, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_3];
	ld.param.u64 	%rd4, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_4];
	ld.param.u64 	%rd5, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_5];
	ld.param.u64 	%rd6, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_6];
	ld.param.u32 	%r14, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_7];
	ld.param.u64 	%rd7, [vec_computeLikelihoodAndModelwithPhotonNumberAndBackground_param_8];
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r23;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB96_11;

	cvta.to.global.u64 	%rd8, %rd4;
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f2, [%rd11];
	cvt.f64.f32	%fd10, %f2;
	div.s32 	%r24, %r12, %r13;
	div.s32 	%r25, %r1, %r24;
	mad.lo.s32 	%r26, %r14, %r13, %r25;
	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r26, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f3, [%rd14];
	cvt.f64.f32	%fd11, %f3;
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f64 	%fd12, [%rd17];
	fma.rn.f64 	%fd1, %fd10, %fd11, %fd12;
	add.s64 	%rd18, %rd8, %rd16;
	st.global.f64 	[%rd18], %fd1;
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvta.to.global.u64 	%rd19, %rd2;
	add.s64 	%rd1, %rd19, %rd16;
	@%p2 bra 	BB96_3;
	bra.uni 	BB96_2;

BB96_3:
	cvta.to.global.u64 	%rd21, %rd3;
	add.s64 	%rd23, %rd21, %rd10;
	ld.global.f32 	%f1, [%rd23];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd1;
	}
	mov.u32 	%r42, -1023;
	setp.gt.s32	%p3, %r40, 1048575;
	mov.f64 	%fd62, %fd1;
	@%p3 bra 	BB96_5;

	mul.f64 	%fd62, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd62;
	}
	mov.u32 	%r42, -1077;

BB96_5:
	add.s32 	%r29, %r40, -1;
	setp.lt.u32	%p4, %r29, 2146435071;
	@%p4 bra 	BB96_7;
	bra.uni 	BB96_6;

BB96_7:
	shr.u32 	%r31, %r40, 20;
	add.s32 	%r43, %r42, %r31;
	and.b32  	%r32, %r40, -2146435073;
	or.b32  	%r33, %r32, 1072693248;
	mov.b64 	%fd63, {%r41, %r33};
	setp.lt.s32	%p6, %r33, 1073127583;
	@%p6 bra 	BB96_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd63;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd63;
	}
	add.s32 	%r36, %r35, -1048576;
	mov.b64 	%fd63, {%r34, %r36};
	add.s32 	%r43, %r43, 1;

BB96_9:
	add.f64 	%fd15, %fd63, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd16, %fd15;
	neg.f64 	%fd17, %fd15;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd16, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd16, %fd16;
	add.f64 	%fd22, %fd63, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd27, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F624924923BE72D;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	mov.f64 	%fd39, 0d3FB5555555555554;
	fma.rn.f64 	%fd40, %fd38, %fd25, %fd39;
	sub.f64 	%fd41, %fd22, %fd24;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd24;
	fma.rn.f64 	%fd44, %fd43, %fd22, %fd42;
	mul.f64 	%fd45, %fd21, %fd44;
	mul.f64 	%fd46, %fd25, %fd40;
	fma.rn.f64 	%fd47, %fd46, %fd24, %fd45;
	xor.b32  	%r37, %r43, -2147483648;
	mov.u32 	%r38, -2147483648;
	mov.u32 	%r39, 1127219200;
	mov.b64 	%fd48, {%r37, %r39};
	mov.b64 	%fd49, {%r38, %r39};
	sub.f64 	%fd50, %fd48, %fd49;
	mov.f64 	%fd51, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd52, %fd50, %fd51, %fd24;
	neg.f64 	%fd53, %fd50;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	sub.f64 	%fd55, %fd54, %fd24;
	sub.f64 	%fd56, %fd47, %fd55;
	mov.f64 	%fd57, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd58, %fd50, %fd57, %fd56;
	add.f64 	%fd64, %fd52, %fd58;
	bra.uni 	BB96_10;

BB96_2:
	mov.u64 	%rd20, 0;
	st.global.u64 	[%rd1], %rd20;
	bra.uni 	BB96_11;

BB96_6:
	mov.f64 	%fd13, 0d7FF0000000000000;
	fma.rn.f64 	%fd14, %fd62, %fd13, %fd13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd62;
	}
	mov.b32 	 %f4, %r30;
	setp.eq.f32	%p5, %f4, 0f00000000;
	selp.f64	%fd64, 0dFFF0000000000000, %fd14, %p5;

BB96_10:
	cvt.f64.f32	%fd59, %f1;
	mul.f64 	%fd60, %fd59, %fd64;
	sub.f64 	%fd61, %fd1, %fd60;
	st.global.f64 	[%rd1], %fd61;

BB96_11:
	ret;
}

	// .globl	vec_computeModelwithPhotonNumber
.visible .entry vec_computeModelwithPhotonNumber(
	.param .u32 vec_computeModelwithPhotonNumber_param_0,
	.param .u32 vec_computeModelwithPhotonNumber_param_1,
	.param .u64 vec_computeModelwithPhotonNumber_param_2,
	.param .u64 vec_computeModelwithPhotonNumber_param_3,
	.param .u64 vec_computeModelwithPhotonNumber_param_4,
	.param .u32 vec_computeModelwithPhotonNumber_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r2, [vec_computeModelwithPhotonNumber_param_0];
	ld.param.u32 	%r3, [vec_computeModelwithPhotonNumber_param_1];
	ld.param.u64 	%rd1, [vec_computeModelwithPhotonNumber_param_2];
	ld.param.u64 	%rd2, [vec_computeModelwithPhotonNumber_param_3];
	ld.param.u64 	%rd3, [vec_computeModelwithPhotonNumber_param_4];
	ld.param.u32 	%r4, [vec_computeModelwithPhotonNumber_param_5];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB97_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	cvt.f64.f32	%fd1, %f1;
	div.s32 	%r14, %r2, %r3;
	div.s32 	%r15, %r1, %r14;
	mad.lo.s32 	%r16, %r4, %r3, %r15;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r16, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f2, [%rd9];
	cvt.f64.f32	%fd2, %f2;
	mul.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r1, 8;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd3;

BB97_2:
	ret;
}

	// .globl	vec_subtractModelwithPhotonNumber
.visible .entry vec_subtractModelwithPhotonNumber(
	.param .u32 vec_subtractModelwithPhotonNumber_param_0,
	.param .u32 vec_subtractModelwithPhotonNumber_param_1,
	.param .u64 vec_subtractModelwithPhotonNumber_param_2,
	.param .u64 vec_subtractModelwithPhotonNumber_param_3,
	.param .u64 vec_subtractModelwithPhotonNumber_param_4,
	.param .u64 vec_subtractModelwithPhotonNumber_param_5,
	.param .u32 vec_subtractModelwithPhotonNumber_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r2, [vec_subtractModelwithPhotonNumber_param_0];
	ld.param.u32 	%r3, [vec_subtractModelwithPhotonNumber_param_1];
	ld.param.u64 	%rd1, [vec_subtractModelwithPhotonNumber_param_2];
	ld.param.u64 	%rd2, [vec_subtractModelwithPhotonNumber_param_3];
	ld.param.u64 	%rd3, [vec_subtractModelwithPhotonNumber_param_4];
	ld.param.u64 	%rd4, [vec_subtractModelwithPhotonNumber_param_5];
	ld.param.u32 	%r4, [vec_subtractModelwithPhotonNumber_param_6];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB98_2;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f32 	%f1, [%rd10];
	cvt.f64.f32	%fd1, %f1;
	div.s32 	%r14, %r2, %r3;
	div.s32 	%r15, %r1, %r14;
	mad.lo.s32 	%r16, %r4, %r3, %r15;
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r16, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f2, [%rd13];
	cvt.f64.f32	%fd2, %f2;
	mul.f64 	%fd3, %fd1, %fd2;
	ld.global.f64 	%fd4, [%rd7];
	sub.f64 	%fd5, %fd4, %fd3;
	cvta.to.global.u64 	%rd14, %rd1;
	add.s64 	%rd15, %rd14, %rd6;
	st.global.f64 	[%rd15], %fd5;

BB98_2:
	ret;
}

	// .globl	vec_partialModel
.visible .entry vec_partialModel(
	.param .u32 vec_partialModel_param_0,
	.param .u64 vec_partialModel_param_1,
	.param .u64 vec_partialModel_param_2,
	.param .u64 vec_partialModel_param_3,
	.param .f32 vec_partialModel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_partialModel_param_0];
	ld.param.u64 	%rd1, [vec_partialModel_param_1];
	ld.param.u64 	%rd2, [vec_partialModel_param_2];
	ld.param.u64 	%rd3, [vec_partialModel_param_3];
	ld.param.f32 	%f1, [vec_partialModel_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB99_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	add.f32 	%f2, %f1, %f1;
	cvt.f64.f32	%fd4, %f2;
	div.rn.f64 	%fd5, %fd3, %fd4;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd5;

BB99_2:
	ret;
}

	// .globl	vec_chi2
.visible .entry vec_chi2(
	.param .u64 vec_chi2_param_0,
	.param .u32 vec_chi2_param_1,
	.param .u64 vec_chi2_param_2,
	.param .u64 vec_chi2_param_3,
	.param .u64 vec_chi2_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd5, [vec_chi2_param_0];
	ld.param.u32 	%r2, [vec_chi2_param_1];
	ld.param.u64 	%rd2, [vec_chi2_param_2];
	ld.param.u64 	%rd3, [vec_chi2_param_3];
	ld.param.u64 	%rd4, [vec_chi2_param_4];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	cvt.s64.s32	%rd6, %r1;
	setp.ge.s64	%p1, %rd6, %rd5;
	@%p1 bra 	BB100_4;

	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r1, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvt.rn.f64.s32	%fd2, %r2;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd1, %rd10, %rd8;
	@%p2 bra 	BB100_3;
	bra.uni 	BB100_2;

BB100_3:
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	cvt.f64.f32	%fd4, %f1;
	sub.f64 	%fd5, %fd1, %fd4;
	mul.f64 	%fd6, %fd5, %fd5;
	mul.f64 	%fd7, %fd2, %fd1;
	div.rn.f64 	%fd8, %fd6, %fd7;
	st.global.f64 	[%rd1], %fd8;
	bra.uni 	BB100_4;

BB100_2:
	rcp.rn.f64 	%fd3, %fd2;
	st.global.f64 	[%rd1], %fd3;

BB100_4:
	ret;
}

	// .globl	vec_computeFisherMatrix
.visible .entry vec_computeFisherMatrix(
	.param .u32 vec_computeFisherMatrix_param_0,
	.param .u32 vec_computeFisherMatrix_param_1,
	.param .u32 vec_computeFisherMatrix_param_2,
	.param .u64 vec_computeFisherMatrix_param_3,
	.param .u64 vec_computeFisherMatrix_param_4,
	.param .u64 vec_computeFisherMatrix_param_5,
	.param .u64 vec_computeFisherMatrix_param_6,
	.param .u64 vec_computeFisherMatrix_param_7,
	.param .u64 vec_computeFisherMatrix_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<22>;


	ld.param.u32 	%r7, [vec_computeFisherMatrix_param_0];
	ld.param.u32 	%r6, [vec_computeFisherMatrix_param_1];
	ld.param.u64 	%rd6, [vec_computeFisherMatrix_param_3];
	ld.param.u64 	%rd7, [vec_computeFisherMatrix_param_4];
	ld.param.u64 	%rd8, [vec_computeFisherMatrix_param_5];
	ld.param.u64 	%rd9, [vec_computeFisherMatrix_param_6];
	ld.param.u64 	%rd10, [vec_computeFisherMatrix_param_7];
	ld.param.u64 	%rd11, [vec_computeFisherMatrix_param_8];
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r16;
	setp.ge.s32	%p1, %r1, %r7;
	@%p1 bra 	BB101_20;

	cvta.to.global.u64 	%rd12, %rd7;
	shl.b32 	%r17, %r6, 4;
	div.s32 	%r18, %r1, %r17;
	rem.s32 	%r19, %r1, %r17;
	div.s32 	%r2, %r19, %r6;
	rem.s32 	%r20, %r19, %r6;
	shr.s32 	%r21, %r2, 31;
	shr.u32 	%r22, %r21, 30;
	add.s32 	%r23, %r2, %r22;
	and.b32  	%r24, %r23, -4;
	sub.s32 	%r3, %r2, %r24;
	mad.lo.s32 	%r4, %r18, %r6, %r20;
	mul.wide.s32 	%rd13, %r4, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f64 	%fd1, [%rd14];
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvta.to.global.u64 	%rd15, %rd6;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd1, %rd15, %rd16;
	@%p2 bra 	BB101_3;
	bra.uni 	BB101_2;

BB101_3:
	cvta.to.global.u64 	%rd17, %rd8;
	cvta.to.global.u64 	%rd18, %rd9;
	cvta.to.global.u64 	%rd19, %rd10;
	cvta.to.global.u64 	%rd20, %rd11;
	rcp.rn.f64 	%fd2, %fd1;
	cvt.rn.f32.f64	%f19, %fd2;
	st.global.f32 	[%rd1], %f19;
	add.s32 	%r26, %r2, 3;
	setp.lt.u32	%p3, %r26, 7;
	add.s64 	%rd2, %rd20, %rd13;
	add.s64 	%rd3, %rd19, %rd13;
	add.s64 	%rd4, %rd18, %rd13;
	add.s64 	%rd5, %rd17, %rd13;
	@%p3 bra 	BB101_10;
	bra.uni 	BB101_4;

BB101_10:
	ld.global.f64 	%fd6, [%rd5];
	cvt.rn.f32.f64	%f10, %fd6;
	mul.f32 	%f19, %f19, %f10;
	bra.uni 	BB101_11;

BB101_2:
	mov.u32 	%r25, 0;
	st.global.u32 	[%rd1], %r25;
	bra.uni 	BB101_20;

BB101_4:
	and.b32  	%r5, %r2, -4;
	setp.eq.s32	%p4, %r5, 4;
	@%p4 bra 	BB101_9;
	bra.uni 	BB101_5;

BB101_9:
	ld.global.f64 	%fd5, [%rd4];
	cvt.rn.f32.f64	%f9, %fd5;
	mul.f32 	%f19, %f19, %f9;
	bra.uni 	BB101_11;

BB101_5:
	setp.eq.s32	%p5, %r5, 8;
	@%p5 bra 	BB101_8;
	bra.uni 	BB101_6;

BB101_8:
	ld.global.f64 	%fd4, [%rd3];
	cvt.rn.f32.f64	%f8, %fd4;
	mul.f32 	%f19, %f19, %f8;
	bra.uni 	BB101_11;

BB101_6:
	setp.ne.s32	%p6, %r5, 12;
	@%p6 bra 	BB101_12;

	ld.global.f64 	%fd3, [%rd2];
	cvt.rn.f32.f64	%f7, %fd3;
	mul.f32 	%f19, %f19, %f7;

BB101_11:
	st.global.f32 	[%rd1], %f19;

BB101_12:
	setp.eq.s32	%p7, %r3, 0;
	@%p7 bra 	BB101_19;

	setp.eq.s32	%p8, %r3, 1;
	@%p8 bra 	BB101_18;
	bra.uni 	BB101_14;

BB101_18:
	ld.global.f64 	%fd9, [%rd4];
	cvt.rn.f32.f64	%f15, %fd9;
	mul.f32 	%f16, %f19, %f15;
	st.global.f32 	[%rd1], %f16;
	bra.uni 	BB101_20;

BB101_19:
	ld.global.f64 	%fd10, [%rd5];
	cvt.rn.f32.f64	%f17, %fd10;
	mul.f32 	%f18, %f19, %f17;
	st.global.f32 	[%rd1], %f18;
	bra.uni 	BB101_20;

BB101_14:
	setp.eq.s32	%p9, %r3, 2;
	@%p9 bra 	BB101_17;
	bra.uni 	BB101_15;

BB101_17:
	ld.global.f64 	%fd8, [%rd3];
	cvt.rn.f32.f64	%f13, %fd8;
	mul.f32 	%f14, %f19, %f13;
	st.global.f32 	[%rd1], %f14;
	bra.uni 	BB101_20;

BB101_15:
	setp.ne.s32	%p10, %r3, 3;
	@%p10 bra 	BB101_20;

	ld.global.f64 	%fd7, [%rd2];
	cvt.rn.f32.f64	%f11, %fd7;
	mul.f32 	%f12, %f19, %f11;
	st.global.f32 	[%rd1], %f12;

BB101_20:
	ret;
}

	// .globl	vec_cropFromImage
.visible .entry vec_cropFromImage(
	.param .u32 vec_cropFromImage_param_0,
	.param .u32 vec_cropFromImage_param_1,
	.param .u32 vec_cropFromImage_param_2,
	.param .u32 vec_cropFromImage_param_3,
	.param .u64 vec_cropFromImage_param_4,
	.param .u32 vec_cropFromImage_param_5,
	.param .u32 vec_cropFromImage_param_6,
	.param .u64 vec_cropFromImage_param_7,
	.param .u64 vec_cropFromImage_param_8,
	.param .u32 vec_cropFromImage_param_9,
	.param .u32 vec_cropFromImage_param_10
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<33>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r11, [vec_cropFromImage_param_0];
	ld.param.u32 	%r4, [vec_cropFromImage_param_1];
	ld.param.u32 	%r5, [vec_cropFromImage_param_2];
	ld.param.u32 	%r6, [vec_cropFromImage_param_3];
	ld.param.u64 	%rd2, [vec_cropFromImage_param_4];
	ld.param.u32 	%r7, [vec_cropFromImage_param_5];
	ld.param.u32 	%r8, [vec_cropFromImage_param_6];
	ld.param.u64 	%rd3, [vec_cropFromImage_param_7];
	ld.param.u64 	%rd4, [vec_cropFromImage_param_8];
	ld.param.u32 	%r9, [vec_cropFromImage_param_9];
	ld.param.u32 	%r10, [vec_cropFromImage_param_10];
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %tid.y;
	mad.lo.s32 	%r15, %r12, %r13, %r14;
	mov.u32 	%r16, %nctaid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %tid.x;
	mad.lo.s32 	%r1, %r18, %r19, %r20;
	setp.ge.s32	%p1, %r1, %r11;
	@%p1 bra 	BB102_4;

	cvta.to.global.u64 	%rd5, %rd4;
	rem.s32 	%r21, %r1, %r6;
	div.s32 	%r22, %r1, %r6;
	cvta.to.global.u64 	%rd6, %rd2;
	mad.lo.s32 	%r23, %r9, %r6, %r21;
	mul.wide.s32 	%rd7, %r23, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.f32 	%f1, [%rd8];
	cvt.rzi.s32.f32	%r24, %f1;
	mad.lo.s32 	%r25, %r10, %r6, %r21;
	mul.wide.s32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	cvt.rzi.s32.f32	%r26, %f2;
	div.s32 	%r27, %r22, %r5;
	add.s32 	%r2, %r24, %r27;
	rem.s32 	%r28, %r22, %r5;
	add.s32 	%r3, %r26, %r28;
	or.b32  	%r29, %r3, %r2;
	setp.gt.s32	%p2, %r29, -1;
	setp.lt.s32	%p3, %r2, %r7;
	and.pred  	%p4, %p2, %p3;
	setp.lt.s32	%p5, %r3, %r8;
	and.pred  	%p6, %p4, %p5;
	mul.lo.s32 	%r30, %r5, %r4;
	mad.lo.s32 	%r31, %r30, %r21, %r22;
	mul.wide.s32 	%rd11, %r31, 8;
	add.s64 	%rd1, %rd6, %rd11;
	@%p6 bra 	BB102_3;
	bra.uni 	BB102_2;

BB102_3:
	cvta.to.global.u64 	%rd13, %rd3;
	mad.lo.s32 	%r32, %r2, %r8, %r3;
	mul.wide.s32 	%rd14, %r32, 8;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f64 	%fd1, [%rd15];
	st.global.f64 	[%rd1], %fd1;
	bra.uni 	BB102_4;

BB102_2:
	mov.u64 	%rd12, 0;
	st.global.u64 	[%rd1], %rd12;

BB102_4:
	ret;
}

	// .globl	vec_cropFromImageFloat
.visible .entry vec_cropFromImageFloat(
	.param .u32 vec_cropFromImageFloat_param_0,
	.param .u32 vec_cropFromImageFloat_param_1,
	.param .u32 vec_cropFromImageFloat_param_2,
	.param .u32 vec_cropFromImageFloat_param_3,
	.param .u64 vec_cropFromImageFloat_param_4,
	.param .u32 vec_cropFromImageFloat_param_5,
	.param .u32 vec_cropFromImageFloat_param_6,
	.param .u64 vec_cropFromImageFloat_param_7,
	.param .u64 vec_cropFromImageFloat_param_8,
	.param .u32 vec_cropFromImageFloat_param_9,
	.param .u32 vec_cropFromImageFloat_param_10
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<34>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r11, [vec_cropFromImageFloat_param_0];
	ld.param.u32 	%r4, [vec_cropFromImageFloat_param_1];
	ld.param.u32 	%r5, [vec_cropFromImageFloat_param_2];
	ld.param.u32 	%r6, [vec_cropFromImageFloat_param_3];
	ld.param.u64 	%rd2, [vec_cropFromImageFloat_param_4];
	ld.param.u32 	%r7, [vec_cropFromImageFloat_param_5];
	ld.param.u32 	%r8, [vec_cropFromImageFloat_param_6];
	ld.param.u64 	%rd3, [vec_cropFromImageFloat_param_7];
	ld.param.u64 	%rd4, [vec_cropFromImageFloat_param_8];
	ld.param.u32 	%r9, [vec_cropFromImageFloat_param_9];
	ld.param.u32 	%r10, [vec_cropFromImageFloat_param_10];
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %tid.y;
	mad.lo.s32 	%r15, %r12, %r13, %r14;
	mov.u32 	%r16, %nctaid.x;
	mov.u32 	%r17, %ctaid.x;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %tid.x;
	mad.lo.s32 	%r1, %r18, %r19, %r20;
	setp.ge.s32	%p1, %r1, %r11;
	@%p1 bra 	BB103_4;

	cvta.to.global.u64 	%rd5, %rd4;
	rem.s32 	%r21, %r1, %r6;
	div.s32 	%r22, %r1, %r6;
	cvta.to.global.u64 	%rd6, %rd2;
	mad.lo.s32 	%r23, %r9, %r6, %r21;
	mul.wide.s32 	%rd7, %r23, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.f32 	%f1, [%rd8];
	cvt.rzi.s32.f32	%r24, %f1;
	mad.lo.s32 	%r25, %r10, %r6, %r21;
	mul.wide.s32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	cvt.rzi.s32.f32	%r26, %f2;
	div.s32 	%r27, %r22, %r5;
	add.s32 	%r2, %r24, %r27;
	rem.s32 	%r28, %r22, %r5;
	add.s32 	%r3, %r26, %r28;
	or.b32  	%r29, %r3, %r2;
	setp.gt.s32	%p2, %r29, -1;
	setp.lt.s32	%p3, %r2, %r7;
	and.pred  	%p4, %p2, %p3;
	setp.lt.s32	%p5, %r3, %r8;
	and.pred  	%p6, %p4, %p5;
	mul.lo.s32 	%r30, %r5, %r4;
	mad.lo.s32 	%r31, %r30, %r21, %r22;
	mul.wide.s32 	%rd11, %r31, 4;
	add.s64 	%rd1, %rd6, %rd11;
	@%p6 bra 	BB103_3;
	bra.uni 	BB103_2;

BB103_3:
	cvta.to.global.u64 	%rd12, %rd3;
	mad.lo.s32 	%r33, %r2, %r8, %r3;
	mul.wide.s32 	%rd13, %r33, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f3, [%rd14];
	st.global.f32 	[%rd1], %f3;
	bra.uni 	BB103_4;

BB103_2:
	mov.u32 	%r32, 0;
	st.global.u32 	[%rd1], %r32;

BB103_4:
	ret;
}

	// .globl	vec_shiftParameter
.visible .entry vec_shiftParameter(
	.param .u32 vec_shiftParameter_param_0,
	.param .u32 vec_shiftParameter_param_1,
	.param .f32 vec_shiftParameter_param_2,
	.param .u64 vec_shiftParameter_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_shiftParameter_param_0];
	ld.param.u32 	%r3, [vec_shiftParameter_param_1];
	ld.param.f32 	%f1, [vec_shiftParameter_param_2];
	ld.param.u64 	%rd1, [vec_shiftParameter_param_3];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB104_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mad.lo.s32 	%r13, %r3, %r2, %r1;
	mul.wide.s32 	%rd3, %r13, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

BB104_2:
	ret;
}

	// .globl	vec_updateParameter
.visible .entry vec_updateParameter(
	.param .u32 vec_updateParameter_param_0,
	.param .u32 vec_updateParameter_param_1,
	.param .f32 vec_updateParameter_param_2,
	.param .u64 vec_updateParameter_param_3,
	.param .u64 vec_updateParameter_param_4,
	.param .u64 vec_updateParameter_param_5,
	.param .u64 vec_updateParameter_param_6,
	.param .u64 vec_updateParameter_param_7,
	.param .u64 vec_updateParameter_param_8,
	.param .f32 vec_updateParameter_param_9,
	.param .f32 vec_updateParameter_param_10
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<23>;


	ld.param.u32 	%r3, [vec_updateParameter_param_0];
	ld.param.u32 	%r4, [vec_updateParameter_param_1];
	ld.param.f32 	%f1, [vec_updateParameter_param_2];
	ld.param.u64 	%rd2, [vec_updateParameter_param_3];
	ld.param.u64 	%rd3, [vec_updateParameter_param_4];
	ld.param.u64 	%rd4, [vec_updateParameter_param_5];
	ld.param.u64 	%rd5, [vec_updateParameter_param_6];
	ld.param.u64 	%rd6, [vec_updateParameter_param_7];
	ld.param.u64 	%rd7, [vec_updateParameter_param_8];
	ld.param.f32 	%f2, [vec_updateParameter_param_9];
	ld.param.f32 	%f3, [vec_updateParameter_param_10];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB105_9;

	cvta.to.global.u64 	%rd8, %rd7;
	mad.lo.s32 	%r2, %r4, %r3, %r1;
	mul.wide.s32 	%rd9, %r2, 4;
	add.s64 	%rd1, %rd8, %rd9;
	ld.global.f32 	%f4, [%rd1];
	setp.leu.f32	%p2, %f4, 0f00000000;
	@%p2 bra 	BB105_9;

	cvta.to.global.u64 	%rd10, %rd3;
	cvta.to.global.u64 	%rd11, %rd2;
	cvta.to.global.u64 	%rd12, %rd4;
	cvt.f64.f32	%fd12, %f1;
	mul.wide.s32 	%rd13, %r1, 8;
	add.s64 	%rd14, %rd12, %rd13;
	add.s64 	%rd15, %rd11, %rd13;
	ld.global.f64 	%fd13, [%rd15];
	ld.global.f64 	%fd14, [%rd14];
	add.f64 	%fd15, %fd14, %fd13;
	add.s64 	%rd16, %rd10, %rd13;
	ld.global.f64 	%fd16, [%rd16];
	fma.rn.f64 	%fd17, %fd16, 0dC000000000000000, %fd15;
	mul.f64 	%fd18, %fd12, %fd12;
	div.rn.f64 	%fd19, %fd17, %fd18;
	abs.f64 	%fd1, %fd19;
	setp.eq.f64	%p3, %fd1, 0d0000000000000000;
	sub.f64 	%fd2, %fd14, %fd13;
	add.f64 	%fd3, %fd12, %fd12;
	@%p3 bra 	BB105_4;
	bra.uni 	BB105_3;

BB105_4:
	mul.f64 	%fd27, %fd3, %fd2;
	bra.uni 	BB105_5;

BB105_3:
	div.rn.f64 	%fd20, %fd2, %fd3;
	div.rn.f64 	%fd27, %fd20, %fd1;

BB105_5:
	cvt.f64.f32	%fd21, %f3;
	abs.f64 	%fd7, %fd21;
	cvt.f64.f32	%fd22, %f2;
	abs.f64 	%fd8, %fd22;
	setp.gt.f64	%p4, %fd27, 0d0000000000000000;
	@%p4 bra 	BB105_7;
	bra.uni 	BB105_6;

BB105_7:
	min.f64 	%fd26, %fd7, %fd27;
	max.f64 	%fd28, %fd8, %fd26;
	bra.uni 	BB105_8;

BB105_6:
	neg.f64 	%fd23, %fd7;
	max.f64 	%fd24, %fd23, %fd27;
	neg.f64 	%fd25, %fd8;
	min.f64 	%fd28, %fd25, %fd24;

BB105_8:
	cvta.to.global.u64 	%rd17, %rd6;
	cvta.to.global.u64 	%rd18, %rd5;
	add.s64 	%rd20, %rd18, %rd9;
	ld.global.f32 	%f5, [%rd20];
	mul.wide.s32 	%rd21, %r1, 4;
	add.s64 	%rd22, %rd17, %rd21;
	st.global.f32 	[%rd22], %f5;
	ld.global.f32 	%f6, [%rd1];
	cvt.rn.f32.f64	%f7, %fd28;
	mul.f32 	%f8, %f7, %f6;
	ld.global.f32 	%f9, [%rd20];
	sub.f32 	%f10, %f9, %f8;
	st.global.f32 	[%rd20], %f10;

BB105_9:
	ret;
}

	// .globl	vec_checkLikelihood
.visible .entry vec_checkLikelihood(
	.param .u32 vec_checkLikelihood_param_0,
	.param .u32 vec_checkLikelihood_param_1,
	.param .u64 vec_checkLikelihood_param_2,
	.param .u64 vec_checkLikelihood_param_3,
	.param .u64 vec_checkLikelihood_param_4,
	.param .u64 vec_checkLikelihood_param_5,
	.param .u64 vec_checkLikelihood_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r2, [vec_checkLikelihood_param_0];
	ld.param.u32 	%r3, [vec_checkLikelihood_param_1];
	ld.param.u64 	%rd5, [vec_checkLikelihood_param_2];
	ld.param.u64 	%rd6, [vec_checkLikelihood_param_3];
	ld.param.u64 	%rd7, [vec_checkLikelihood_param_4];
	ld.param.u64 	%rd8, [vec_checkLikelihood_param_5];
	ld.param.u64 	%rd9, [vec_checkLikelihood_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB106_4;

	cvta.to.global.u64 	%rd10, %rd6;
	mul.wide.s32 	%rd11, %r1, 8;
	add.s64 	%rd12, %rd10, %rd11;
	cvta.to.global.u64 	%rd13, %rd5;
	add.s64 	%rd1, %rd13, %rd11;
	ld.global.f64 	%fd2, [%rd1];
	ld.global.f64 	%fd1, [%rd12];
	setp.gt.f64	%p2, %fd1, %fd2;
	cvta.to.global.u64 	%rd14, %rd8;
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd2, %rd14, %rd15;
	mad.lo.s32 	%r13, %r3, %r2, %r1;
	cvta.to.global.u64 	%rd16, %rd7;
	mul.wide.s32 	%rd17, %r13, 4;
	add.s64 	%rd3, %rd16, %rd17;
	cvta.to.global.u64 	%rd18, %rd9;
	add.s64 	%rd4, %rd18, %rd17;
	@%p2 bra 	BB106_3;
	bra.uni 	BB106_2;

BB106_3:
	ld.global.f32 	%f2, [%rd2];
	st.global.f32 	[%rd3], %f2;
	ld.global.f32 	%f3, [%rd4];
	div.rn.f32 	%f4, %f3, 0f41200000;
	st.global.f32 	[%rd4], %f4;
	bra.uni 	BB106_4;

BB106_2:
	st.global.f64 	[%rd1], %fd1;
	mov.u32 	%r14, 0;
	st.global.u32 	[%rd4], %r14;
	ld.global.f32 	%f1, [%rd3];
	st.global.f32 	[%rd2], %f1;

BB106_4:
	ret;
}

	// .globl	vec_computePSF_phaseNMany_f
.visible .entry vec_computePSF_phaseNMany_f(
	.param .u32 vec_computePSF_phaseNMany_f_param_0,
	.param .u32 vec_computePSF_phaseNMany_f_param_1,
	.param .u32 vec_computePSF_phaseNMany_f_param_2,
	.param .u64 vec_computePSF_phaseNMany_f_param_3,
	.param .u64 vec_computePSF_phaseNMany_f_param_4,
	.param .u64 vec_computePSF_phaseNMany_f_param_5,
	.param .u64 vec_computePSF_phaseNMany_f_param_6,
	.param .u64 vec_computePSF_phaseNMany_f_param_7,
	.param .u64 vec_computePSF_phaseNMany_f_param_8,
	.param .u64 vec_computePSF_phaseNMany_f_param_9,
	.param .u64 vec_computePSF_phaseNMany_f_param_10,
	.param .u64 vec_computePSF_phaseNMany_f_param_11,
	.param .u32 vec_computePSF_phaseNMany_f_param_12
)
{
	.local .align 4 .b8 	__local_depot107[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .f32 	%f<109>;
	.reg .b32 	%r<205>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot107;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r78, [vec_computePSF_phaseNMany_f_param_0];
	ld.param.u32 	%r75, [vec_computePSF_phaseNMany_f_param_1];
	ld.param.u32 	%r76, [vec_computePSF_phaseNMany_f_param_2];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNMany_f_param_3];
	ld.param.u64 	%rd14, [vec_computePSF_phaseNMany_f_param_4];
	ld.param.u64 	%rd15, [vec_computePSF_phaseNMany_f_param_5];
	ld.param.u64 	%rd16, [vec_computePSF_phaseNMany_f_param_6];
	ld.param.u64 	%rd17, [vec_computePSF_phaseNMany_f_param_7];
	ld.param.u64 	%rd18, [vec_computePSF_phaseNMany_f_param_8];
	ld.param.u64 	%rd19, [vec_computePSF_phaseNMany_f_param_9];
	ld.param.u64 	%rd20, [vec_computePSF_phaseNMany_f_param_10];
	ld.param.u64 	%rd21, [vec_computePSF_phaseNMany_f_param_11];
	ld.param.u32 	%r77, [vec_computePSF_phaseNMany_f_param_12];
	mov.u32 	%r79, %ntid.y;
	mov.u32 	%r80, %ctaid.y;
	mov.u32 	%r81, %tid.y;
	mad.lo.s32 	%r82, %r79, %r80, %r81;
	mov.u32 	%r83, %nctaid.x;
	mov.u32 	%r84, %ctaid.x;
	mad.lo.s32 	%r85, %r82, %r83, %r84;
	mov.u32 	%r86, %ntid.x;
	mov.u32 	%r87, %tid.x;
	mad.lo.s32 	%r1, %r85, %r86, %r87;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB107_48;

	cvta.to.global.u64 	%rd22, %rd18;
	cvta.to.global.u64 	%rd23, %rd16;
	rem.s32 	%r2, %r1, %r75;
	cvta.to.global.u64 	%rd24, %rd13;
	mul.wide.s32 	%rd25, %r2, 4;
	add.s64 	%rd26, %rd24, %rd25;
	div.s32 	%r3, %r1, %r75;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd22, %rd27;
	ld.global.f32 	%f39, [%rd28];
	ld.global.f32 	%f40, [%rd26];
	cvta.to.global.u64 	%rd29, %rd14;
	add.s64 	%rd30, %rd29, %rd25;
	shl.b32 	%r88, %r77, 2;
	cvt.s64.s32	%rd31, %r88;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.f32 	%f41, [%rd32];
	ld.global.f32 	%f42, [%rd30];
	mul.f32 	%f43, %f42, %f41;
	fma.rn.f32 	%f44, %f40, %f39, %f43;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd33, %rd25;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.f32 	%f45, [%rd35];
	ld.global.f32 	%f46, [%rd34];
	fma.rn.f32 	%f47, %f46, %f45, %f44;
	cvta.to.global.u64 	%rd36, %rd17;
	add.s64 	%rd37, %rd36, %rd25;
	ld.global.f32 	%f48, [%rd37];
	add.f32 	%f103, %f48, %f47;
	add.s64 	%rd38, %rd23, %rd25;
	ld.global.f32 	%f2, [%rd38];
	abs.f32 	%f3, %f103;
	setp.neu.f32	%p2, %f3, 0f7F800000;
	mov.f32 	%f97, %f103;
	@%p2 bra 	BB107_3;

	mov.f32 	%f49, 0f00000000;
	mul.rn.f32 	%f97, %f103, %f49;

BB107_3:
	mul.f32 	%f50, %f97, 0f3F22F983;
	cvt.rni.s32.f32	%r194, %f50;
	cvt.rn.f32.s32	%f51, %r194;
	neg.f32 	%f52, %f51;
	mov.f32 	%f53, 0f3FC90FDA;
	fma.rn.f32 	%f54, %f52, %f53, %f97;
	mov.f32 	%f55, 0f33A22168;
	fma.rn.f32 	%f56, %f52, %f55, %f54;
	mov.f32 	%f57, 0f27C234C5;
	fma.rn.f32 	%f98, %f52, %f57, %f56;
	abs.f32 	%f58, %f97;
	setp.leu.f32	%p3, %f58, 0f47CE4780;
	@%p3 bra 	BB107_14;

	mov.b32 	 %r5, %f97;
	shl.b32 	%r91, %r5, 8;
	or.b32  	%r6, %r91, -2147483648;
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd64, %SPL, 0;
	mov.u32 	%r186, 0;
	mov.u64 	%rd63, __cudart_i2opi_f;
	mov.u32 	%r185, -6;

BB107_5:
	.pragma "nounroll";
	ld.const.u32 	%r94, [%rd63];
	// inline asm
	{
	mad.lo.cc.u32   %r92, %r94, %r6, %r186;
	madc.hi.u32     %r186, %r94, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd64], %r92;
	add.s64 	%rd64, %rd64, 4;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r185, %r185, 1;
	setp.ne.s32	%p4, %r185, 0;
	@%p4 bra 	BB107_5;

	bfe.u32 	%r97, %r5, 23, 8;
	add.s32 	%r98, %r97, -128;
	shr.u32 	%r99, %r98, 5;
	and.b32  	%r11, %r5, -2147483648;
	cvta.to.local.u64 	%rd42, %rd40;
	st.local.u32 	[%rd42+24], %r186;
	bfe.u32 	%r12, %r5, 23, 5;
	mov.u32 	%r100, 6;
	sub.s32 	%r101, %r100, %r99;
	mul.wide.s32 	%rd43, %r101, 4;
	add.s64 	%rd6, %rd42, %rd43;
	ld.local.u32 	%r187, [%rd6];
	ld.local.u32 	%r188, [%rd6+-4];
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB107_8;

	mov.u32 	%r102, 32;
	sub.s32 	%r103, %r102, %r12;
	shr.u32 	%r104, %r188, %r103;
	shl.b32 	%r105, %r187, %r12;
	add.s32 	%r187, %r104, %r105;
	ld.local.u32 	%r106, [%rd6+-8];
	shr.u32 	%r107, %r106, %r103;
	shl.b32 	%r108, %r188, %r12;
	add.s32 	%r188, %r107, %r108;

BB107_8:
	shr.u32 	%r109, %r188, 30;
	shl.b32 	%r110, %r187, 2;
	add.s32 	%r189, %r109, %r110;
	shl.b32 	%r20, %r188, 2;
	shr.u32 	%r111, %r189, 31;
	shr.u32 	%r112, %r187, 30;
	add.s32 	%r21, %r111, %r112;
	setp.eq.s32	%p6, %r111, 0;
	@%p6 bra 	BB107_9;

	not.b32 	%r113, %r189;
	neg.s32 	%r191, %r20;
	setp.eq.s32	%p7, %r20, 0;
	selp.u32	%r114, 1, 0, %p7;
	add.s32 	%r189, %r114, %r113;
	xor.b32  	%r190, %r11, -2147483648;
	bra.uni 	BB107_11;

BB107_9:
	mov.u32 	%r190, %r11;
	mov.u32 	%r191, %r20;

BB107_11:
	clz.b32 	%r193, %r189;
	setp.eq.s32	%p8, %r193, 0;
	shl.b32 	%r115, %r189, %r193;
	mov.u32 	%r116, 32;
	sub.s32 	%r117, %r116, %r193;
	shr.u32 	%r118, %r191, %r117;
	add.s32 	%r119, %r118, %r115;
	selp.b32	%r29, %r189, %r119, %p8;
	mov.u32 	%r120, -921707870;
	mul.hi.u32 	%r192, %r29, %r120;
	setp.eq.s32	%p9, %r11, 0;
	neg.s32 	%r121, %r21;
	selp.b32	%r194, %r21, %r121, %p9;
	setp.lt.s32	%p10, %r192, 1;
	@%p10 bra 	BB107_13;

	mul.lo.s32 	%r122, %r29, -921707870;
	shr.u32 	%r123, %r122, 31;
	shl.b32 	%r124, %r192, 1;
	add.s32 	%r192, %r123, %r124;
	add.s32 	%r193, %r193, 1;

BB107_13:
	mov.u32 	%r125, 126;
	sub.s32 	%r126, %r125, %r193;
	shl.b32 	%r127, %r126, 23;
	add.s32 	%r128, %r192, 1;
	shr.u32 	%r129, %r128, 7;
	add.s32 	%r130, %r129, 1;
	shr.u32 	%r131, %r130, 1;
	add.s32 	%r132, %r131, %r127;
	or.b32  	%r133, %r132, %r190;
	mov.b32 	 %f98, %r133;

BB107_14:
	mul.rn.f32 	%f9, %f98, %f98;
	add.s32 	%r37, %r194, 1;
	and.b32  	%r38, %r37, 1;
	setp.eq.s32	%p11, %r38, 0;
	@%p11 bra 	BB107_16;

	mov.f32 	%f59, 0fBAB6061A;
	mov.f32 	%f60, 0f37CCF5CE;
	fma.rn.f32 	%f99, %f60, %f9, %f59;
	bra.uni 	BB107_17;

BB107_16:
	mov.f32 	%f61, 0f3C08839E;
	mov.f32 	%f62, 0fB94CA1F9;
	fma.rn.f32 	%f99, %f62, %f9, %f61;

BB107_17:
	@%p11 bra 	BB107_19;

	mov.f32 	%f63, 0f3D2AAAA5;
	fma.rn.f32 	%f64, %f99, %f9, %f63;
	mov.f32 	%f65, 0fBF000000;
	fma.rn.f32 	%f100, %f64, %f9, %f65;
	bra.uni 	BB107_20;

BB107_19:
	mov.f32 	%f66, 0fBE2AAAA3;
	fma.rn.f32 	%f67, %f99, %f9, %f66;
	mov.f32 	%f68, 0f00000000;
	fma.rn.f32 	%f100, %f67, %f9, %f68;

BB107_20:
	fma.rn.f32 	%f101, %f100, %f98, %f98;
	@%p11 bra 	BB107_22;

	mov.f32 	%f69, 0f3F800000;
	fma.rn.f32 	%f101, %f100, %f9, %f69;

BB107_22:
	and.b32  	%r134, %r37, 2;
	setp.eq.s32	%p14, %r134, 0;
	@%p14 bra 	BB107_24;

	mov.f32 	%f70, 0f00000000;
	mov.f32 	%f71, 0fBF800000;
	fma.rn.f32 	%f101, %f101, %f71, %f70;

BB107_24:
	cvta.to.global.u64 	%rd44, %rd19;
	add.s64 	%rd46, %rd44, %rd25;
	mul.lo.s32 	%r39, %r3, %r76;
	ld.global.u32 	%r135, [%rd46];
	add.s32 	%r136, %r135, %r39;
	cvta.to.global.u64 	%rd47, %rd21;
	mul.wide.s32 	%rd48, %r136, 4;
	add.s64 	%rd49, %rd47, %rd48;
	mul.f32 	%f72, %f2, %f101;
	st.global.f32 	[%rd49], %f72;
	ld.global.f32 	%f21, [%rd38];
	@%p2 bra 	BB107_26;

	mov.f32 	%f73, 0f00000000;
	mul.rn.f32 	%f103, %f103, %f73;

BB107_26:
	mul.f32 	%f74, %f103, 0f3F22F983;
	cvt.rni.s32.f32	%r204, %f74;
	cvt.rn.f32.s32	%f75, %r204;
	neg.f32 	%f76, %f75;
	fma.rn.f32 	%f78, %f76, %f53, %f103;
	fma.rn.f32 	%f80, %f76, %f55, %f78;
	fma.rn.f32 	%f104, %f76, %f57, %f80;
	abs.f32 	%f82, %f103;
	setp.leu.f32	%p16, %f82, 0f47CE4780;
	@%p16 bra 	BB107_37;

	mov.b32 	 %r41, %f103;
	shr.u32 	%r42, %r41, 23;
	shl.b32 	%r139, %r41, 8;
	or.b32  	%r43, %r139, -2147483648;
	add.u64 	%rd53, %SP, 0;
	add.u64 	%rd66, %SPL, 0;
	mov.u32 	%r196, 0;
	mov.u64 	%rd65, __cudart_i2opi_f;
	mov.u32 	%r195, -6;

BB107_28:
	.pragma "nounroll";
	ld.const.u32 	%r142, [%rd65];
	// inline asm
	{
	mad.lo.cc.u32   %r140, %r142, %r43, %r196;
	madc.hi.u32     %r196, %r142, %r43,  0;
	}
	// inline asm
	st.local.u32 	[%rd66], %r140;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r195, %r195, 1;
	setp.ne.s32	%p17, %r195, 0;
	@%p17 bra 	BB107_28;

	and.b32  	%r145, %r42, 255;
	add.s32 	%r146, %r145, -128;
	shr.u32 	%r147, %r146, 5;
	and.b32  	%r48, %r41, -2147483648;
	cvta.to.local.u64 	%rd55, %rd53;
	st.local.u32 	[%rd55+24], %r196;
	mov.u32 	%r148, 6;
	sub.s32 	%r149, %r148, %r147;
	mul.wide.s32 	%rd56, %r149, 4;
	add.s64 	%rd12, %rd55, %rd56;
	ld.local.u32 	%r197, [%rd12];
	ld.local.u32 	%r198, [%rd12+-4];
	and.b32  	%r51, %r42, 31;
	setp.eq.s32	%p18, %r51, 0;
	@%p18 bra 	BB107_31;

	mov.u32 	%r150, 32;
	sub.s32 	%r151, %r150, %r51;
	shr.u32 	%r152, %r198, %r151;
	shl.b32 	%r153, %r197, %r51;
	add.s32 	%r197, %r152, %r153;
	ld.local.u32 	%r154, [%rd12+-8];
	shr.u32 	%r155, %r154, %r151;
	shl.b32 	%r156, %r198, %r51;
	add.s32 	%r198, %r155, %r156;

BB107_31:
	shr.u32 	%r157, %r198, 30;
	shl.b32 	%r158, %r197, 2;
	add.s32 	%r199, %r157, %r158;
	shl.b32 	%r57, %r198, 2;
	shr.u32 	%r159, %r199, 31;
	shr.u32 	%r160, %r197, 30;
	add.s32 	%r58, %r159, %r160;
	setp.eq.s32	%p19, %r159, 0;
	@%p19 bra 	BB107_32;

	not.b32 	%r161, %r199;
	neg.s32 	%r201, %r57;
	setp.eq.s32	%p20, %r57, 0;
	selp.u32	%r162, 1, 0, %p20;
	add.s32 	%r199, %r162, %r161;
	xor.b32  	%r200, %r48, -2147483648;
	bra.uni 	BB107_34;

BB107_32:
	mov.u32 	%r200, %r48;
	mov.u32 	%r201, %r57;

BB107_34:
	clz.b32 	%r203, %r199;
	setp.eq.s32	%p21, %r203, 0;
	shl.b32 	%r163, %r199, %r203;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r203;
	shr.u32 	%r166, %r201, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r66, %r199, %r167, %p21;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r202, %r66, %r168;
	setp.eq.s32	%p22, %r48, 0;
	neg.s32 	%r169, %r58;
	selp.b32	%r204, %r58, %r169, %p22;
	setp.lt.s32	%p23, %r202, 1;
	@%p23 bra 	BB107_36;

	mul.lo.s32 	%r170, %r66, -921707870;
	shr.u32 	%r171, %r170, 31;
	shl.b32 	%r172, %r202, 1;
	add.s32 	%r202, %r171, %r172;
	add.s32 	%r203, %r203, 1;

BB107_36:
	mov.u32 	%r173, 126;
	sub.s32 	%r174, %r173, %r203;
	shl.b32 	%r175, %r174, 23;
	add.s32 	%r176, %r202, 1;
	shr.u32 	%r177, %r176, 7;
	add.s32 	%r178, %r177, 1;
	shr.u32 	%r179, %r178, 1;
	add.s32 	%r180, %r179, %r175;
	or.b32  	%r181, %r180, %r200;
	mov.b32 	 %f104, %r181;

BB107_37:
	mul.rn.f32 	%f27, %f104, %f104;
	and.b32  	%r74, %r204, 1;
	setp.eq.s32	%p24, %r74, 0;
	@%p24 bra 	BB107_39;

	mov.f32 	%f83, 0fBAB6061A;
	mov.f32 	%f84, 0f37CCF5CE;
	fma.rn.f32 	%f105, %f84, %f27, %f83;
	bra.uni 	BB107_40;

BB107_39:
	mov.f32 	%f85, 0f3C08839E;
	mov.f32 	%f86, 0fB94CA1F9;
	fma.rn.f32 	%f105, %f86, %f27, %f85;

BB107_40:
	@%p24 bra 	BB107_42;

	mov.f32 	%f87, 0f3D2AAAA5;
	fma.rn.f32 	%f88, %f105, %f27, %f87;
	mov.f32 	%f89, 0fBF000000;
	fma.rn.f32 	%f106, %f88, %f27, %f89;
	bra.uni 	BB107_43;

BB107_42:
	mov.f32 	%f90, 0fBE2AAAA3;
	fma.rn.f32 	%f91, %f105, %f27, %f90;
	mov.f32 	%f92, 0f00000000;
	fma.rn.f32 	%f106, %f91, %f27, %f92;

BB107_43:
	fma.rn.f32 	%f107, %f106, %f104, %f104;
	@%p24 bra 	BB107_45;

	mov.f32 	%f93, 0f3F800000;
	fma.rn.f32 	%f107, %f106, %f27, %f93;

BB107_45:
	and.b32  	%r182, %r204, 2;
	setp.eq.s32	%p27, %r182, 0;
	@%p27 bra 	BB107_47;

	mov.f32 	%f94, 0f00000000;
	mov.f32 	%f95, 0fBF800000;
	fma.rn.f32 	%f107, %f107, %f95, %f94;

BB107_47:
	cvta.to.global.u64 	%rd57, %rd20;
	add.s64 	%rd59, %rd57, %rd25;
	ld.global.u32 	%r183, [%rd59];
	add.s32 	%r184, %r183, %r39;
	mul.wide.s32 	%rd61, %r184, 4;
	add.s64 	%rd62, %rd47, %rd61;
	mul.f32 	%f96, %f21, %f107;
	st.global.f32 	[%rd62], %f96;

BB107_48:
	ret;
}

	// .globl	vec_computePSF_phaseNManywithOil_f
.visible .entry vec_computePSF_phaseNManywithOil_f(
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_0,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_1,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_2,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_3,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_4,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_5,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_6,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_7,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_8,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_9,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_10,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_11,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_12,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_13,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_14,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_15
)
{
	.local .align 4 .b8 	__local_depot108[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .f32 	%f<120>;
	.reg .b32 	%r<205>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot108;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r78, [vec_computePSF_phaseNManywithOil_f_param_0];
	ld.param.u32 	%r75, [vec_computePSF_phaseNManywithOil_f_param_1];
	ld.param.u32 	%r76, [vec_computePSF_phaseNManywithOil_f_param_2];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNManywithOil_f_param_3];
	ld.param.u64 	%rd14, [vec_computePSF_phaseNManywithOil_f_param_4];
	ld.param.u64 	%rd15, [vec_computePSF_phaseNManywithOil_f_param_5];
	ld.param.u64 	%rd16, [vec_computePSF_phaseNManywithOil_f_param_6];
	ld.param.u64 	%rd17, [vec_computePSF_phaseNManywithOil_f_param_7];
	ld.param.u64 	%rd18, [vec_computePSF_phaseNManywithOil_f_param_8];
	ld.param.u64 	%rd19, [vec_computePSF_phaseNManywithOil_f_param_9];
	ld.param.u64 	%rd20, [vec_computePSF_phaseNManywithOil_f_param_10];
	ld.param.u64 	%rd21, [vec_computePSF_phaseNManywithOil_f_param_11];
	ld.param.u64 	%rd22, [vec_computePSF_phaseNManywithOil_f_param_12];
	ld.param.u64 	%rd23, [vec_computePSF_phaseNManywithOil_f_param_13];
	ld.param.u64 	%rd24, [vec_computePSF_phaseNManywithOil_f_param_14];
	ld.param.u32 	%r77, [vec_computePSF_phaseNManywithOil_f_param_15];
	mov.u32 	%r79, %ntid.y;
	mov.u32 	%r80, %ctaid.y;
	mov.u32 	%r81, %tid.y;
	mad.lo.s32 	%r82, %r79, %r80, %r81;
	mov.u32 	%r83, %nctaid.x;
	mov.u32 	%r84, %ctaid.x;
	mad.lo.s32 	%r85, %r82, %r83, %r84;
	mov.u32 	%r86, %ntid.x;
	mov.u32 	%r87, %tid.x;
	mad.lo.s32 	%r1, %r85, %r86, %r87;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB108_48;

	cvta.to.global.u64 	%rd25, %rd21;
	cvta.to.global.u64 	%rd26, %rd15;
	rem.s32 	%r2, %r1, %r75;
	cvta.to.global.u64 	%rd27, %rd13;
	mul.wide.s32 	%rd28, %r2, 4;
	add.s64 	%rd29, %rd27, %rd28;
	div.s32 	%r3, %r1, %r75;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd25, %rd30;
	ld.global.f32 	%f39, [%rd31];
	ld.global.f32 	%f40, [%rd29];
	cvta.to.global.u64 	%rd32, %rd14;
	add.s64 	%rd33, %rd32, %rd28;
	shl.b32 	%r88, %r77, 2;
	cvt.s64.s32	%rd34, %r88;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.f32 	%f41, [%rd35];
	ld.global.f32 	%f42, [%rd33];
	mul.f32 	%f43, %f42, %f41;
	fma.rn.f32 	%f44, %f40, %f39, %f43;
	cvta.to.global.u64 	%rd36, %rd20;
	add.s64 	%rd37, %rd36, %rd28;
	ld.global.f32 	%f45, [%rd37];
	add.f32 	%f46, %f45, %f44;
	add.s64 	%rd38, %rd26, %rd28;
	add.s64 	%rd39, %rd35, %rd34;
	ld.global.f32 	%f47, [%rd39];
	ld.global.f32 	%f48, [%rd38];
	mul.f32 	%f49, %f48, %f47;
	add.f32 	%f50, %f46, %f49;
	cvta.to.global.u64 	%rd40, %rd17;
	add.s64 	%rd41, %rd40, %rd28;
	add.s64 	%rd42, %rd39, %rd34;
	ld.global.f32 	%f51, [%rd42];
	ld.global.f32 	%f52, [%rd41];
	mul.f32 	%f53, %f52, %f51;
	sub.f32 	%f108, %f50, %f53;
	cvta.to.global.u64 	%rd43, %rd16;
	add.s64 	%rd44, %rd43, %rd28;
	ld.global.f32 	%f54, [%rd44];
	fma.rn.f32 	%f55, %f49, %f54, %f46;
	cvta.to.global.u64 	%rd45, %rd18;
	add.s64 	%rd46, %rd45, %rd28;
	ld.global.f32 	%f56, [%rd46];
	mul.f32 	%f57, %f53, %f56;
	sub.f32 	%f114, %f55, %f57;
	cvta.to.global.u64 	%rd47, %rd19;
	add.s64 	%rd48, %rd47, %rd28;
	ld.global.f32 	%f3, [%rd48];
	abs.f32 	%f58, %f108;
	setp.neu.f32	%p2, %f58, 0f7F800000;
	@%p2 bra 	BB108_3;

	mov.f32 	%f59, 0f00000000;
	mul.rn.f32 	%f108, %f108, %f59;

BB108_3:
	mul.f32 	%f60, %f108, 0f3F22F983;
	cvt.rni.s32.f32	%r194, %f60;
	cvt.rn.f32.s32	%f61, %r194;
	neg.f32 	%f62, %f61;
	mov.f32 	%f63, 0f3FC90FDA;
	fma.rn.f32 	%f64, %f62, %f63, %f108;
	mov.f32 	%f65, 0f33A22168;
	fma.rn.f32 	%f66, %f62, %f65, %f64;
	mov.f32 	%f67, 0f27C234C5;
	fma.rn.f32 	%f109, %f62, %f67, %f66;
	abs.f32 	%f68, %f108;
	setp.leu.f32	%p3, %f68, 0f47CE4780;
	@%p3 bra 	BB108_14;

	mov.b32 	 %r5, %f108;
	shl.b32 	%r91, %r5, 8;
	or.b32  	%r6, %r91, -2147483648;
	add.u64 	%rd50, %SP, 0;
	add.u64 	%rd74, %SPL, 0;
	mov.u32 	%r186, 0;
	mov.u64 	%rd73, __cudart_i2opi_f;
	mov.u32 	%r185, -6;

BB108_5:
	.pragma "nounroll";
	ld.const.u32 	%r94, [%rd73];
	// inline asm
	{
	mad.lo.cc.u32   %r92, %r94, %r6, %r186;
	madc.hi.u32     %r186, %r94, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd74], %r92;
	add.s64 	%rd74, %rd74, 4;
	add.s64 	%rd73, %rd73, 4;
	add.s32 	%r185, %r185, 1;
	setp.ne.s32	%p4, %r185, 0;
	@%p4 bra 	BB108_5;

	bfe.u32 	%r97, %r5, 23, 8;
	add.s32 	%r98, %r97, -128;
	shr.u32 	%r99, %r98, 5;
	and.b32  	%r11, %r5, -2147483648;
	cvta.to.local.u64 	%rd52, %rd50;
	st.local.u32 	[%rd52+24], %r186;
	bfe.u32 	%r12, %r5, 23, 5;
	mov.u32 	%r100, 6;
	sub.s32 	%r101, %r100, %r99;
	mul.wide.s32 	%rd53, %r101, 4;
	add.s64 	%rd6, %rd52, %rd53;
	ld.local.u32 	%r187, [%rd6];
	ld.local.u32 	%r188, [%rd6+-4];
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB108_8;

	mov.u32 	%r102, 32;
	sub.s32 	%r103, %r102, %r12;
	shr.u32 	%r104, %r188, %r103;
	shl.b32 	%r105, %r187, %r12;
	add.s32 	%r187, %r104, %r105;
	ld.local.u32 	%r106, [%rd6+-8];
	shr.u32 	%r107, %r106, %r103;
	shl.b32 	%r108, %r188, %r12;
	add.s32 	%r188, %r107, %r108;

BB108_8:
	shr.u32 	%r109, %r188, 30;
	shl.b32 	%r110, %r187, 2;
	add.s32 	%r189, %r109, %r110;
	shl.b32 	%r20, %r188, 2;
	shr.u32 	%r111, %r189, 31;
	shr.u32 	%r112, %r187, 30;
	add.s32 	%r21, %r111, %r112;
	setp.eq.s32	%p6, %r111, 0;
	@%p6 bra 	BB108_9;

	not.b32 	%r113, %r189;
	neg.s32 	%r191, %r20;
	setp.eq.s32	%p7, %r20, 0;
	selp.u32	%r114, 1, 0, %p7;
	add.s32 	%r189, %r114, %r113;
	xor.b32  	%r190, %r11, -2147483648;
	bra.uni 	BB108_11;

BB108_9:
	mov.u32 	%r190, %r11;
	mov.u32 	%r191, %r20;

BB108_11:
	clz.b32 	%r193, %r189;
	setp.eq.s32	%p8, %r193, 0;
	shl.b32 	%r115, %r189, %r193;
	mov.u32 	%r116, 32;
	sub.s32 	%r117, %r116, %r193;
	shr.u32 	%r118, %r191, %r117;
	add.s32 	%r119, %r118, %r115;
	selp.b32	%r29, %r189, %r119, %p8;
	mov.u32 	%r120, -921707870;
	mul.hi.u32 	%r192, %r29, %r120;
	setp.eq.s32	%p9, %r11, 0;
	neg.s32 	%r121, %r21;
	selp.b32	%r194, %r21, %r121, %p9;
	setp.lt.s32	%p10, %r192, 1;
	@%p10 bra 	BB108_13;

	mul.lo.s32 	%r122, %r29, -921707870;
	shr.u32 	%r123, %r122, 31;
	shl.b32 	%r124, %r192, 1;
	add.s32 	%r192, %r123, %r124;
	add.s32 	%r193, %r193, 1;

BB108_13:
	mov.u32 	%r125, 126;
	sub.s32 	%r126, %r125, %r193;
	shl.b32 	%r127, %r126, 23;
	add.s32 	%r128, %r192, 1;
	shr.u32 	%r129, %r128, 7;
	add.s32 	%r130, %r129, 1;
	shr.u32 	%r131, %r130, 1;
	add.s32 	%r132, %r131, %r127;
	or.b32  	%r133, %r132, %r190;
	mov.b32 	 %f109, %r133;

BB108_14:
	mul.rn.f32 	%f9, %f109, %f109;
	add.s32 	%r37, %r194, 1;
	and.b32  	%r38, %r37, 1;
	setp.eq.s32	%p11, %r38, 0;
	@%p11 bra 	BB108_16;

	mov.f32 	%f69, 0fBAB6061A;
	mov.f32 	%f70, 0f37CCF5CE;
	fma.rn.f32 	%f110, %f70, %f9, %f69;
	bra.uni 	BB108_17;

BB108_16:
	mov.f32 	%f71, 0f3C08839E;
	mov.f32 	%f72, 0fB94CA1F9;
	fma.rn.f32 	%f110, %f72, %f9, %f71;

BB108_17:
	@%p11 bra 	BB108_19;

	mov.f32 	%f73, 0f3D2AAAA5;
	fma.rn.f32 	%f74, %f110, %f9, %f73;
	mov.f32 	%f75, 0fBF000000;
	fma.rn.f32 	%f111, %f74, %f9, %f75;
	bra.uni 	BB108_20;

BB108_19:
	mov.f32 	%f76, 0fBE2AAAA3;
	fma.rn.f32 	%f77, %f110, %f9, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f111, %f77, %f9, %f78;

BB108_20:
	fma.rn.f32 	%f112, %f111, %f109, %f109;
	@%p11 bra 	BB108_22;

	mov.f32 	%f79, 0f3F800000;
	fma.rn.f32 	%f112, %f111, %f9, %f79;

BB108_22:
	and.b32  	%r134, %r37, 2;
	setp.eq.s32	%p14, %r134, 0;
	@%p14 bra 	BB108_24;

	mov.f32 	%f80, 0f00000000;
	mov.f32 	%f81, 0fBF800000;
	fma.rn.f32 	%f112, %f112, %f81, %f80;

BB108_24:
	cvta.to.global.u64 	%rd54, %rd22;
	add.s64 	%rd56, %rd54, %rd28;
	mul.lo.s32 	%r39, %r3, %r76;
	ld.global.u32 	%r135, [%rd56];
	add.s32 	%r136, %r135, %r39;
	cvta.to.global.u64 	%rd57, %rd24;
	mul.wide.s32 	%rd58, %r136, 4;
	add.s64 	%rd59, %rd57, %rd58;
	mul.f32 	%f82, %f3, %f112;
	st.global.f32 	[%rd59], %f82;
	ld.global.f32 	%f21, [%rd48];
	abs.f32 	%f83, %f114;
	setp.neu.f32	%p15, %f83, 0f7F800000;
	@%p15 bra 	BB108_26;

	mov.f32 	%f84, 0f00000000;
	mul.rn.f32 	%f114, %f114, %f84;

BB108_26:
	mul.f32 	%f85, %f114, 0f3F22F983;
	cvt.rni.s32.f32	%r204, %f85;
	cvt.rn.f32.s32	%f86, %r204;
	neg.f32 	%f87, %f86;
	fma.rn.f32 	%f89, %f87, %f63, %f114;
	fma.rn.f32 	%f91, %f87, %f65, %f89;
	fma.rn.f32 	%f115, %f87, %f67, %f91;
	abs.f32 	%f93, %f114;
	setp.leu.f32	%p16, %f93, 0f47CE4780;
	@%p16 bra 	BB108_37;

	mov.b32 	 %r41, %f114;
	shr.u32 	%r42, %r41, 23;
	shl.b32 	%r139, %r41, 8;
	or.b32  	%r43, %r139, -2147483648;
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd76, %SPL, 0;
	mov.u32 	%r196, 0;
	mov.u64 	%rd75, __cudart_i2opi_f;
	mov.u32 	%r195, -6;

BB108_28:
	.pragma "nounroll";
	ld.const.u32 	%r142, [%rd75];
	// inline asm
	{
	mad.lo.cc.u32   %r140, %r142, %r43, %r196;
	madc.hi.u32     %r196, %r142, %r43,  0;
	}
	// inline asm
	st.local.u32 	[%rd76], %r140;
	add.s64 	%rd76, %rd76, 4;
	add.s64 	%rd75, %rd75, 4;
	add.s32 	%r195, %r195, 1;
	setp.ne.s32	%p17, %r195, 0;
	@%p17 bra 	BB108_28;

	and.b32  	%r145, %r42, 255;
	add.s32 	%r146, %r145, -128;
	shr.u32 	%r147, %r146, 5;
	and.b32  	%r48, %r41, -2147483648;
	cvta.to.local.u64 	%rd65, %rd63;
	st.local.u32 	[%rd65+24], %r196;
	mov.u32 	%r148, 6;
	sub.s32 	%r149, %r148, %r147;
	mul.wide.s32 	%rd66, %r149, 4;
	add.s64 	%rd12, %rd65, %rd66;
	ld.local.u32 	%r197, [%rd12];
	ld.local.u32 	%r198, [%rd12+-4];
	and.b32  	%r51, %r42, 31;
	setp.eq.s32	%p18, %r51, 0;
	@%p18 bra 	BB108_31;

	mov.u32 	%r150, 32;
	sub.s32 	%r151, %r150, %r51;
	shr.u32 	%r152, %r198, %r151;
	shl.b32 	%r153, %r197, %r51;
	add.s32 	%r197, %r152, %r153;
	ld.local.u32 	%r154, [%rd12+-8];
	shr.u32 	%r155, %r154, %r151;
	shl.b32 	%r156, %r198, %r51;
	add.s32 	%r198, %r155, %r156;

BB108_31:
	shr.u32 	%r157, %r198, 30;
	shl.b32 	%r158, %r197, 2;
	add.s32 	%r199, %r157, %r158;
	shl.b32 	%r57, %r198, 2;
	shr.u32 	%r159, %r199, 31;
	shr.u32 	%r160, %r197, 30;
	add.s32 	%r58, %r159, %r160;
	setp.eq.s32	%p19, %r159, 0;
	@%p19 bra 	BB108_32;

	not.b32 	%r161, %r199;
	neg.s32 	%r201, %r57;
	setp.eq.s32	%p20, %r57, 0;
	selp.u32	%r162, 1, 0, %p20;
	add.s32 	%r199, %r162, %r161;
	xor.b32  	%r200, %r48, -2147483648;
	bra.uni 	BB108_34;

BB108_32:
	mov.u32 	%r200, %r48;
	mov.u32 	%r201, %r57;

BB108_34:
	clz.b32 	%r203, %r199;
	setp.eq.s32	%p21, %r203, 0;
	shl.b32 	%r163, %r199, %r203;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r203;
	shr.u32 	%r166, %r201, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r66, %r199, %r167, %p21;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r202, %r66, %r168;
	setp.eq.s32	%p22, %r48, 0;
	neg.s32 	%r169, %r58;
	selp.b32	%r204, %r58, %r169, %p22;
	setp.lt.s32	%p23, %r202, 1;
	@%p23 bra 	BB108_36;

	mul.lo.s32 	%r170, %r66, -921707870;
	shr.u32 	%r171, %r170, 31;
	shl.b32 	%r172, %r202, 1;
	add.s32 	%r202, %r171, %r172;
	add.s32 	%r203, %r203, 1;

BB108_36:
	mov.u32 	%r173, 126;
	sub.s32 	%r174, %r173, %r203;
	shl.b32 	%r175, %r174, 23;
	add.s32 	%r176, %r202, 1;
	shr.u32 	%r177, %r176, 7;
	add.s32 	%r178, %r177, 1;
	shr.u32 	%r179, %r178, 1;
	add.s32 	%r180, %r179, %r175;
	or.b32  	%r181, %r180, %r200;
	mov.b32 	 %f115, %r181;

BB108_37:
	mul.rn.f32 	%f27, %f115, %f115;
	and.b32  	%r74, %r204, 1;
	setp.eq.s32	%p24, %r74, 0;
	@%p24 bra 	BB108_39;

	mov.f32 	%f94, 0fBAB6061A;
	mov.f32 	%f95, 0f37CCF5CE;
	fma.rn.f32 	%f116, %f95, %f27, %f94;
	bra.uni 	BB108_40;

BB108_39:
	mov.f32 	%f96, 0f3C08839E;
	mov.f32 	%f97, 0fB94CA1F9;
	fma.rn.f32 	%f116, %f97, %f27, %f96;

BB108_40:
	@%p24 bra 	BB108_42;

	mov.f32 	%f98, 0f3D2AAAA5;
	fma.rn.f32 	%f99, %f116, %f27, %f98;
	mov.f32 	%f100, 0fBF000000;
	fma.rn.f32 	%f117, %f99, %f27, %f100;
	bra.uni 	BB108_43;

BB108_42:
	mov.f32 	%f101, 0fBE2AAAA3;
	fma.rn.f32 	%f102, %f116, %f27, %f101;
	mov.f32 	%f103, 0f00000000;
	fma.rn.f32 	%f117, %f102, %f27, %f103;

BB108_43:
	fma.rn.f32 	%f118, %f117, %f115, %f115;
	@%p24 bra 	BB108_45;

	mov.f32 	%f104, 0f3F800000;
	fma.rn.f32 	%f118, %f117, %f27, %f104;

BB108_45:
	and.b32  	%r182, %r204, 2;
	setp.eq.s32	%p27, %r182, 0;
	@%p27 bra 	BB108_47;

	mov.f32 	%f105, 0f00000000;
	mov.f32 	%f106, 0fBF800000;
	fma.rn.f32 	%f118, %f118, %f106, %f105;

BB108_47:
	cvta.to.global.u64 	%rd67, %rd23;
	add.s64 	%rd69, %rd67, %rd28;
	ld.global.u32 	%r183, [%rd69];
	add.s32 	%r184, %r183, %r39;
	mul.wide.s32 	%rd71, %r184, 4;
	add.s64 	%rd72, %rd57, %rd71;
	mul.f32 	%f107, %f21, %f118;
	st.global.f32 	[%rd72], %f107;

BB108_48:
	ret;
}

	// .globl	vec_addFloat
.visible .entry vec_addFloat(
	.param .u32 vec_addFloat_param_0,
	.param .u64 vec_addFloat_param_1,
	.param .u64 vec_addFloat_param_2,
	.param .u64 vec_addFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_addFloat_param_0];
	ld.param.u64 	%rd1, [vec_addFloat_param_1];
	ld.param.u64 	%rd2, [vec_addFloat_param_2];
	ld.param.u64 	%rd3, [vec_addFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB109_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	add.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

BB109_2:
	ret;
}

	// .globl	vec_addanddivide
.visible .entry vec_addanddivide(
	.param .u32 vec_addanddivide_param_0,
	.param .u64 vec_addanddivide_param_1,
	.param .u64 vec_addanddivide_param_2,
	.param .u64 vec_addanddivide_param_3,
	.param .u64 vec_addanddivide_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r2, [vec_addanddivide_param_0];
	ld.param.u64 	%rd1, [vec_addanddivide_param_1];
	ld.param.u64 	%rd2, [vec_addanddivide_param_2];
	ld.param.u64 	%rd3, [vec_addanddivide_param_3];
	ld.param.u64 	%rd4, [vec_addanddivide_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB110_2;

	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd4;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd8];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd9], %f3;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd7;
	ld.global.f32 	%f4, [%rd11];
	div.rn.f32 	%f5, %f4, %f3;
	cvta.to.global.u64 	%rd12, %rd1;
	add.s64 	%rd13, %rd12, %rd7;
	st.global.f32 	[%rd13], %f5;

BB110_2:
	ret;
}

	// .globl	vec_computeLikelihoodDeconvolution
.visible .entry vec_computeLikelihoodDeconvolution(
	.param .u32 vec_computeLikelihoodDeconvolution_param_0,
	.param .u64 vec_computeLikelihoodDeconvolution_param_1,
	.param .u64 vec_computeLikelihoodDeconvolution_param_2,
	.param .u64 vec_computeLikelihoodDeconvolution_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r2, [vec_computeLikelihoodDeconvolution_param_0];
	ld.param.u64 	%rd1, [vec_computeLikelihoodDeconvolution_param_1];
	ld.param.u64 	%rd2, [vec_computeLikelihoodDeconvolution_param_2];
	ld.param.u64 	%rd3, [vec_computeLikelihoodDeconvolution_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB111_4;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	setp.lt.f32	%p2, %f2, 0f00800000;
	mul.f32 	%f7, %f2, 0f4B000000;
	selp.f32	%f3, %f7, %f2, %p2;
	selp.f32	%f8, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	 %r12, %f3;
	add.s32 	%r13, %r12, -1059760811;
	and.b32  	%r14, %r13, -8388608;
	sub.s32 	%r15, %r12, %r14;
	mov.b32 	 %f9, %r15;
	cvt.rn.f32.s32	%f10, %r14;
	mov.f32 	%f11, 0f34000000;
	fma.rn.f32 	%f12, %f10, %f11, %f8;
	add.f32 	%f13, %f9, 0fBF800000;
	mov.f32 	%f14, 0f3E1039F6;
	mov.f32 	%f15, 0fBE055027;
	fma.rn.f32 	%f16, %f15, %f13, %f14;
	mov.f32 	%f17, 0fBDF8CDCC;
	fma.rn.f32 	%f18, %f16, %f13, %f17;
	mov.f32 	%f19, 0f3E0F2955;
	fma.rn.f32 	%f20, %f18, %f13, %f19;
	mov.f32 	%f21, 0fBE2AD8B9;
	fma.rn.f32 	%f22, %f20, %f13, %f21;
	mov.f32 	%f23, 0f3E4CED0B;
	fma.rn.f32 	%f24, %f22, %f13, %f23;
	mov.f32 	%f25, 0fBE7FFF22;
	fma.rn.f32 	%f26, %f24, %f13, %f25;
	mov.f32 	%f27, 0f3EAAAA78;
	fma.rn.f32 	%f28, %f26, %f13, %f27;
	mov.f32 	%f29, 0fBF000000;
	fma.rn.f32 	%f30, %f28, %f13, %f29;
	mul.f32 	%f31, %f13, %f30;
	fma.rn.f32 	%f32, %f31, %f13, %f13;
	mov.f32 	%f33, 0f3F317218;
	fma.rn.f32 	%f38, %f12, %f33, %f32;
	setp.lt.u32	%p3, %r12, 2139095040;
	@%p3 bra 	BB111_3;

	mov.f32 	%f34, 0f7F800000;
	fma.rn.f32 	%f38, %f3, %f34, %f34;

BB111_3:
	cvta.to.global.u64 	%rd9, %rd1;
	setp.eq.f32	%p4, %f3, 0f00000000;
	selp.f32	%f35, 0fFF800000, %f38, %p4;
	mul.f32 	%f36, %f1, %f35;
	sub.f32 	%f37, %f2, %f36;
	add.s64 	%rd11, %rd9, %rd5;
	st.global.f32 	[%rd11], %f37;

BB111_4:
	ret;
}

	// .globl	vec_skellam_order1
.visible .entry vec_skellam_order1(
	.param .u64 vec_skellam_order1_param_0,
	.param .u64 vec_skellam_order1_param_1,
	.param .u64 vec_skellam_order1_param_2,
	.param .u64 vec_skellam_order1_param_3,
	.param .u64 vec_skellam_order1_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<63>;
	.reg .f64 	%fd<86>;
	.reg .b64 	%rd<94>;


	ld.param.u64 	%rd25, [vec_skellam_order1_param_0];
	ld.param.u64 	%rd26, [vec_skellam_order1_param_1];
	ld.param.u64 	%rd27, [vec_skellam_order1_param_2];
	ld.param.u64 	%rd28, [vec_skellam_order1_param_3];
	ld.param.u64 	%rd29, [vec_skellam_order1_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	cvt.u64.u32	%rd30, %r6;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	cvt.u64.u32	%rd31, %r10;
	mov.u32 	%r11, %nctaid.x;
	mul.wide.u32 	%rd32, %r11, %r3;
	mul.lo.s64 	%rd33, %rd32, %rd31;
	add.s64 	%rd2, %rd33, %rd30;
	or.b64  	%rd34, %rd2, %rd26;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64	%p1, %rd35, 0;
	@%p1 bra 	BB112_2;

	div.s64 	%rd89, %rd2, %rd26;
	rem.s64 	%rd90, %rd2, %rd26;
	bra.uni 	BB112_3;

BB112_2:
	cvt.u32.u64	%r12, %rd26;
	cvt.u32.u64	%r13, %rd2;
	div.u32 	%r14, %r13, %r12;
	rem.u32 	%r15, %r13, %r12;
	cvt.u64.u32	%rd89, %r14;
	cvt.u64.u32	%rd90, %r15;

BB112_3:
	setp.ge.s64	%p2, %rd2, %rd25;
	@%p2 bra 	BB112_24;

	setp.ge.s64	%p3, %rd2, %rd26;
	@%p3 bra 	BB112_6;

	shl.b64 	%rd36, %rd25, 1;
	add.s64 	%rd37, %rd2, %rd36;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd39, %rd1, %rd38;
	mov.u32 	%r16, 0;
	st.global.u32 	[%rd39], %r16;

BB112_6:
	sub.s64 	%rd9, %rd25, %rd26;
	setp.lt.s64	%p4, %rd2, %rd9;
	add.s64 	%rd40, %rd2, %rd25;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd10, %rd1, %rd41;
	@%p4 bra 	BB112_8;

	mov.u32 	%r17, 0;
	st.global.u32 	[%rd10], %r17;

BB112_8:
	cvt.s64.s32 	%rd42, %rd89;
	mul.lo.s64 	%rd43, %rd42, %rd26;
	add.s64 	%rd11, %rd43, %rd90;
	add.s64 	%rd44, %rd89, 1;
	cvt.s64.s32 	%rd45, %rd44;
	mul.lo.s64 	%rd46, %rd45, %rd26;
	add.s64 	%rd12, %rd46, %rd90;
	setp.ge.s64	%p5, %rd2, %rd9;
	@%p5 bra 	BB112_24;

	cvta.to.global.u64 	%rd47, %rd29;
	cvta.to.global.u64 	%rd48, %rd28;
	cvt.s64.s32 	%rd13, %rd12;
	shl.b64 	%rd49, %rd13, 2;
	add.s64 	%rd50, %rd48, %rd49;
	cvt.s64.s32 	%rd14, %rd11;
	shl.b64 	%rd51, %rd14, 2;
	add.s64 	%rd52, %rd48, %rd51;
	ld.global.f32 	%f6, [%rd52];
	ld.global.f32 	%f7, [%rd50];
	sub.f32 	%f1, %f7, %f6;
	add.s64 	%rd53, %rd47, %rd51;
	add.s64 	%rd54, %rd47, %rd49;
	ld.global.f32 	%f2, [%rd54];
	ld.global.f32 	%f8, [%rd53];
	mul.f32 	%f9, %f8, %f2;
	sqrt.rn.f32 	%f3, %f9;
	add.f32 	%f4, %f3, %f3;
	abs.f32 	%f5, %f1;
	cvt.f64.f32	%fd1, %f5;
	cvt.f64.f32	%fd6, %f4;
	mov.f64 	%fd7, 0d4000000000000000;
	div.rn.f64 	%fd2, %fd7, %fd6;
	mov.f64 	%fd85, 0d0000000000000000;
	mov.u32 	%r62, 50;

BB112_10:
	cvt.rn.f64.s32	%fd8, %r62;
	add.f64 	%fd9, %fd1, %fd8;
	fma.rn.f64 	%fd10, %fd2, %fd9, %fd85;
	rcp.rn.f64 	%fd11, %fd10;
	add.s32 	%r19, %r62, -1;
	cvt.rn.f64.s32	%fd12, %r19;
	add.f64 	%fd13, %fd1, %fd12;
	fma.rn.f64 	%fd14, %fd2, %fd13, %fd11;
	rcp.rn.f64 	%fd15, %fd14;
	add.s32 	%r20, %r62, -2;
	cvt.rn.f64.s32	%fd16, %r20;
	add.f64 	%fd17, %fd1, %fd16;
	fma.rn.f64 	%fd18, %fd2, %fd17, %fd15;
	rcp.rn.f64 	%fd19, %fd18;
	add.s32 	%r21, %r62, -3;
	cvt.rn.f64.s32	%fd20, %r21;
	add.f64 	%fd21, %fd1, %fd20;
	fma.rn.f64 	%fd22, %fd2, %fd21, %fd19;
	rcp.rn.f64 	%fd23, %fd22;
	add.s32 	%r22, %r62, -4;
	cvt.rn.f64.s32	%fd24, %r22;
	add.f64 	%fd25, %fd1, %fd24;
	fma.rn.f64 	%fd26, %fd2, %fd25, %fd23;
	rcp.rn.f64 	%fd27, %fd26;
	add.s32 	%r23, %r62, -5;
	cvt.rn.f64.s32	%fd28, %r23;
	add.f64 	%fd29, %fd1, %fd28;
	fma.rn.f64 	%fd30, %fd2, %fd29, %fd27;
	rcp.rn.f64 	%fd31, %fd30;
	add.s32 	%r24, %r62, -6;
	cvt.rn.f64.s32	%fd32, %r24;
	add.f64 	%fd33, %fd1, %fd32;
	fma.rn.f64 	%fd34, %fd2, %fd33, %fd31;
	rcp.rn.f64 	%fd35, %fd34;
	add.s32 	%r25, %r62, -7;
	cvt.rn.f64.s32	%fd36, %r25;
	add.f64 	%fd37, %fd1, %fd36;
	fma.rn.f64 	%fd38, %fd2, %fd37, %fd35;
	rcp.rn.f64 	%fd39, %fd38;
	add.s32 	%r26, %r62, -8;
	cvt.rn.f64.s32	%fd40, %r26;
	add.f64 	%fd41, %fd1, %fd40;
	fma.rn.f64 	%fd42, %fd2, %fd41, %fd39;
	rcp.rn.f64 	%fd43, %fd42;
	add.s32 	%r27, %r62, -9;
	cvt.rn.f64.s32	%fd44, %r27;
	add.f64 	%fd45, %fd1, %fd44;
	fma.rn.f64 	%fd46, %fd2, %fd45, %fd43;
	rcp.rn.f64 	%fd47, %fd46;
	add.s32 	%r28, %r62, -10;
	cvt.rn.f64.s32	%fd48, %r28;
	add.f64 	%fd49, %fd1, %fd48;
	fma.rn.f64 	%fd50, %fd2, %fd49, %fd47;
	rcp.rn.f64 	%fd51, %fd50;
	add.s32 	%r29, %r62, -11;
	cvt.rn.f64.s32	%fd52, %r29;
	add.f64 	%fd53, %fd1, %fd52;
	fma.rn.f64 	%fd54, %fd2, %fd53, %fd51;
	rcp.rn.f64 	%fd55, %fd54;
	add.s32 	%r30, %r62, -12;
	cvt.rn.f64.s32	%fd56, %r30;
	add.f64 	%fd57, %fd1, %fd56;
	fma.rn.f64 	%fd58, %fd2, %fd57, %fd55;
	rcp.rn.f64 	%fd59, %fd58;
	add.s32 	%r31, %r62, -13;
	cvt.rn.f64.s32	%fd60, %r31;
	add.f64 	%fd61, %fd1, %fd60;
	fma.rn.f64 	%fd62, %fd2, %fd61, %fd59;
	rcp.rn.f64 	%fd63, %fd62;
	add.s32 	%r32, %r62, -14;
	cvt.rn.f64.s32	%fd64, %r32;
	add.f64 	%fd65, %fd1, %fd64;
	fma.rn.f64 	%fd66, %fd2, %fd65, %fd63;
	rcp.rn.f64 	%fd67, %fd66;
	add.s32 	%r33, %r62, -15;
	cvt.rn.f64.s32	%fd68, %r33;
	add.f64 	%fd69, %fd1, %fd68;
	fma.rn.f64 	%fd70, %fd2, %fd69, %fd67;
	rcp.rn.f64 	%fd71, %fd70;
	add.s32 	%r34, %r62, -16;
	cvt.rn.f64.s32	%fd72, %r34;
	add.f64 	%fd73, %fd1, %fd72;
	fma.rn.f64 	%fd74, %fd2, %fd73, %fd71;
	rcp.rn.f64 	%fd85, %fd74;
	add.s32 	%r62, %r62, -17;
	setp.gt.s32	%p6, %r62, -1;
	@%p6 bra 	BB112_10;

	rcp.rn.f64 	%fd75, %fd85;
	cvt.rn.f32.f64	%f10, %fd75;
	cvt.f64.f32	%fd76, %f10;
	div.rn.f32 	%f11, %f5, %f4;
	cvt.f64.f32	%fd77, %f11;
	sub.f64 	%fd78, %fd76, %fd77;
	cvt.f64.f32	%fd79, %f2;
	mul.f64 	%fd80, %fd79, %fd78;
	cvt.rn.f32.f64	%f12, %fd80;
	div.rn.f32 	%f13, %f12, %f3;
	st.global.f32 	[%rd10], %f13;
	ld.global.f32 	%f14, [%rd53];
	cvt.f64.f32	%fd81, %f14;
	mul.f64 	%fd82, %fd78, %fd81;
	cvt.f64.f32	%fd83, %f3;
	div.rn.f64 	%fd84, %fd82, %fd83;
	cvt.rn.f32.f64	%f15, %fd84;
	shl.b64 	%rd67, %rd25, 1;
	add.s64 	%rd68, %rd67, %rd26;
	add.s64 	%rd69, %rd68, %rd2;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd15, %rd1, %rd70;
	st.global.f32 	[%rd15], %f15;
	ld.global.f32 	%f16, [%rd53];
	mul.f32 	%f17, %f16, 0fC0000000;
	div.rn.f32 	%f18, %f1, %f17;
	ld.global.f32 	%f19, [%rd10];
	add.f32 	%f20, %f19, %f18;
	st.global.f32 	[%rd10], %f20;
	ld.global.f32 	%f21, [%rd54];
	add.f32 	%f22, %f21, %f21;
	div.rn.f32 	%f23, %f1, %f22;
	ld.global.f32 	%f24, [%rd15];
	add.f32 	%f25, %f23, %f24;
	st.global.f32 	[%rd15], %f25;
	or.b64  	%rd73, %rd14, %rd26;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64	%p7, %rd74, 0;
	@%p7 bra 	BB112_13;

	div.s64 	%rd91, %rd14, %rd26;
	bra.uni 	BB112_14;

BB112_13:
	cvt.u32.u64	%r44, %rd26;
	cvt.u32.u64	%r45, %rd14;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32	%rd91, %r46;

BB112_14:
	setp.eq.s64	%p8, %rd91, 0;
	@%p8 bra 	BB112_16;

	ld.global.f32 	%f26, [%rd10];
	mul.f32 	%f27, %f26, 0f3F000000;
	st.global.f32 	[%rd10], %f27;

BB112_16:
	or.b64  	%rd84, %rd13, %rd26;
	and.b64  	%rd85, %rd84, -4294967296;
	setp.eq.s64	%p9, %rd85, 0;
	@%p9 bra 	BB112_18;

	div.s64 	%rd92, %rd13, %rd26;
	bra.uni 	BB112_19;

BB112_18:
	cvt.u32.u64	%r56, %rd26;
	cvt.u32.u64	%r57, %rd13;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32	%rd92, %r58;

BB112_19:
	or.b64  	%rd86, %rd25, %rd26;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.eq.s64	%p10, %rd87, 0;
	@%p10 bra 	BB112_21;

	div.s64 	%rd93, %rd25, %rd26;
	bra.uni 	BB112_22;

BB112_21:
	cvt.u32.u64	%r59, %rd26;
	cvt.u32.u64	%r60, %rd25;
	div.u32 	%r61, %r60, %r59;
	cvt.u64.u32	%rd93, %r61;

BB112_22:
	add.s64 	%rd88, %rd93, -1;
	setp.eq.s64	%p11, %rd92, %rd88;
	@%p11 bra 	BB112_24;

	ld.global.f32 	%f28, [%rd15];
	mul.f32 	%f29, %f28, 0f3F000000;
	st.global.f32 	[%rd15], %f29;

BB112_24:
	ret;
}

	// .globl	vec_thetest
.visible .entry vec_thetest(
	.param .u32 vec_thetest_param_0,
	.param .u64 vec_thetest_param_1
)
{
	.local .align 8 .b8 	__local_depot113[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	mov.u64 	%SPL, __local_depot113;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [vec_thetest_param_0];
	ld.param.u64 	%rd1, [vec_thetest_param_1];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB113_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.u64 	%rd5, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	st.local.u32 	[%rd6], %r1;
	st.local.f64 	[%rd6+8], %fd1;
	mov.u64 	%rd7, $str2;
	cvta.global.u64 	%rd8, %rd7;
	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r12, [retval0+0];
	
	//{
	}// Callseq End 28

BB113_2:
	ret;
}

	// .globl	vec_computePSF_signal
.visible .entry vec_computePSF_signal(
	.param .u32 vec_computePSF_signal_param_0,
	.param .u64 vec_computePSF_signal_param_1,
	.param .u64 vec_computePSF_signal_param_2,
	.param .u64 vec_computePSF_signal_param_3,
	.param .f64 vec_computePSF_signal_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_computePSF_signal_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signal_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signal_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_signal_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signal_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB114_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd4, [%rd8];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd7;

BB114_2:
	ret;
}

	// .globl	vec_computePSF_signalN
.visible .entry vec_computePSF_signalN(
	.param .u32 vec_computePSF_signalN_param_0,
	.param .u64 vec_computePSF_signalN_param_1,
	.param .f64 vec_computePSF_signalN_param_2,
	.param .u64 vec_computePSF_signalN_param_3,
	.param .u64 vec_computePSF_signalN_param_4,
	.param .u64 vec_computePSF_signalN_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computePSF_signalN_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalN_param_1];
	ld.param.f64 	%fd1, [vec_computePSF_signalN_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalN_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalN_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalN_param_5];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB115_2;

	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r12, [%rd8];
	mul.wide.s32 	%rd9, %r12, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r13, [%rd12];
	mul.wide.s32 	%rd13, %r13, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd7;

BB115_2:
	ret;
}

	// .globl	vec_computePSF_signalN2
.visible .entry vec_computePSF_signalN2(
	.param .u32 vec_computePSF_signalN2_param_0,
	.param .f64 vec_computePSF_signalN2_param_1,
	.param .u64 vec_computePSF_signalN2_param_2,
	.param .u64 vec_computePSF_signalN2_param_3,
	.param .u64 vec_computePSF_signalN2_param_4,
	.param .u64 vec_computePSF_signalN2_param_5,
	.param .u64 vec_computePSF_signalN2_param_6,
	.param .u64 vec_computePSF_signalN2_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r2, [vec_computePSF_signalN2_param_0];
	ld.param.f64 	%fd1, [vec_computePSF_signalN2_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2_param_5];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2_param_6];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2_param_7];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB116_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%r12, [%rd11];
	mul.wide.s32 	%rd12, %r12, 8;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f64 	%fd2, [%rd13];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r13, [%rd15];
	mul.wide.s32 	%rd16, %r13, 8;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f64 	%fd4, [%rd17];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	ld.global.u32 	%r14, [%rd19];
	mul.wide.s32 	%rd20, %r14, 8;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f64 	[%rd21], %fd7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r15, [%rd23];
	mul.wide.s32 	%rd24, %r15, 8;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u64 	%rd26, 0;
	st.global.u64 	[%rd25], %rd26;

BB116_2:
	ret;
}

	// .globl	vec_computePSF_signalN2Many
.visible .entry vec_computePSF_signalN2Many(
	.param .u32 vec_computePSF_signalN2Many_param_0,
	.param .u32 vec_computePSF_signalN2Many_param_1,
	.param .u32 vec_computePSF_signalN2Many_param_2,
	.param .f64 vec_computePSF_signalN2Many_param_3,
	.param .u64 vec_computePSF_signalN2Many_param_4,
	.param .u64 vec_computePSF_signalN2Many_param_5,
	.param .u64 vec_computePSF_signalN2Many_param_6,
	.param .u64 vec_computePSF_signalN2Many_param_7,
	.param .u64 vec_computePSF_signalN2Many_param_8,
	.param .u64 vec_computePSF_signalN2Many_param_9
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r4, [vec_computePSF_signalN2Many_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalN2Many_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalN2Many_param_2];
	ld.param.f64 	%fd1, [vec_computePSF_signalN2Many_param_3];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2Many_param_4];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2Many_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2Many_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2Many_param_7];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2Many_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2Many_param_9];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB117_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	rem.s32 	%r14, %r1, %r2;
	mul.wide.s32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	div.s32 	%r15, %r1, %r2;
	mul.lo.s32 	%r16, %r15, %r3;
	ld.global.u32 	%r17, [%rd11];
	add.s32 	%r18, %r17, %r16;
	mul.wide.s32 	%rd12, %r18, 8;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f64 	%fd2, [%rd13];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r19, [%rd15];
	add.s32 	%r20, %r19, %r16;
	mul.wide.s32 	%rd16, %r20, 8;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f64 	%fd4, [%rd17];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	mul.lo.s32 	%r21, %r2, %r15;
	shl.b32 	%r22, %r21, 1;
	ld.global.u32 	%r23, [%rd19];
	add.s32 	%r24, %r23, %r22;
	mul.wide.s32 	%rd20, %r24, 8;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f64 	[%rd21], %fd7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r25, [%rd23];
	add.s32 	%r26, %r25, %r22;
	mul.wide.s32 	%rd24, %r26, 8;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u64 	%rd26, 0;
	st.global.u64 	[%rd25], %rd26;

BB117_2:
	ret;
}

	// .globl	vec_computePSF_signalN2Many_f
.visible .entry vec_computePSF_signalN2Many_f(
	.param .u32 vec_computePSF_signalN2Many_f_param_0,
	.param .u32 vec_computePSF_signalN2Many_f_param_1,
	.param .u32 vec_computePSF_signalN2Many_f_param_2,
	.param .f32 vec_computePSF_signalN2Many_f_param_3,
	.param .u64 vec_computePSF_signalN2Many_f_param_4,
	.param .u64 vec_computePSF_signalN2Many_f_param_5,
	.param .u64 vec_computePSF_signalN2Many_f_param_6,
	.param .u64 vec_computePSF_signalN2Many_f_param_7,
	.param .u64 vec_computePSF_signalN2Many_f_param_8,
	.param .u64 vec_computePSF_signalN2Many_f_param_9
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<26>;


	ld.param.u32 	%r4, [vec_computePSF_signalN2Many_f_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalN2Many_f_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalN2Many_f_param_2];
	ld.param.f32 	%f1, [vec_computePSF_signalN2Many_f_param_3];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2Many_f_param_4];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2Many_f_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2Many_f_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2Many_f_param_7];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2Many_f_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2Many_f_param_9];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB118_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	rem.s32 	%r14, %r1, %r2;
	mul.wide.s32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	div.s32 	%r15, %r1, %r2;
	mul.lo.s32 	%r16, %r15, %r3;
	ld.global.u32 	%r17, [%rd11];
	add.s32 	%r18, %r17, %r16;
	mul.wide.s32 	%rd12, %r18, 4;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f32 	%f2, [%rd13];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r19, [%rd15];
	add.s32 	%r20, %r19, %r16;
	mul.wide.s32 	%rd16, %r20, 4;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f32 	%f4, [%rd17];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	mul.lo.s32 	%r21, %r2, %r15;
	shl.b32 	%r22, %r21, 1;
	ld.global.u32 	%r23, [%rd19];
	add.s32 	%r24, %r23, %r22;
	mul.wide.s32 	%rd20, %r24, 4;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f32 	[%rd21], %f7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r25, [%rd23];
	add.s32 	%r26, %r25, %r22;
	mul.wide.s32 	%rd24, %r26, 4;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u32 	%r27, 0;
	st.global.u32 	[%rd25], %r27;

BB118_2:
	ret;
}

	// .globl	vec_computePSF_signalsqrt
.visible .entry vec_computePSF_signalsqrt(
	.param .u32 vec_computePSF_signalsqrt_param_0,
	.param .u64 vec_computePSF_signalsqrt_param_1,
	.param .u64 vec_computePSF_signalsqrt_param_2,
	.param .u64 vec_computePSF_signalsqrt_param_3,
	.param .f64 vec_computePSF_signalsqrt_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_computePSF_signalsqrt_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalsqrt_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signalsqrt_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_signalsqrt_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signalsqrt_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB119_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd4, [%rd8];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd8;

BB119_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrt
.visible .entry vec_computePSF_signalNsqrt(
	.param .u32 vec_computePSF_signalNsqrt_param_0,
	.param .u64 vec_computePSF_signalNsqrt_param_1,
	.param .u64 vec_computePSF_signalNsqrt_param_2,
	.param .f64 vec_computePSF_signalNsqrt_param_3,
	.param .u64 vec_computePSF_signalNsqrt_param_4,
	.param .u64 vec_computePSF_signalNsqrt_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computePSF_signalNsqrt_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrt_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrt_param_2];
	ld.param.f64 	%fd1, [vec_computePSF_signalNsqrt_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrt_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrt_param_5];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB120_2;

	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r12, [%rd8];
	mul.wide.s32 	%rd9, %r12, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r13, [%rd12];
	mul.wide.s32 	%rd13, %r13, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd8;

BB120_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany
.visible .entry vec_computePSF_signalNsqrtMany(
	.param .u32 vec_computePSF_signalNsqrtMany_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_param_1,
	.param .u64 vec_computePSF_signalNsqrtMany_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_param_3,
	.param .f64 vec_computePSF_signalNsqrtMany_param_4,
	.param .u64 vec_computePSF_signalNsqrtMany_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signalNsqrtMany_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_param_6];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB121_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	sub.s32 	%r14, %r1, %r13;
	shl.b32 	%r15, %r14, 1;
	ld.global.u32 	%r16, [%rd8];
	add.s32 	%r17, %r15, %r16;
	mul.wide.s32 	%rd9, %r17, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r18, [%rd12];
	add.s32 	%r19, %r18, %r15;
	mul.wide.s32 	%rd13, %r19, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd8;

BB121_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany_f
.visible .entry vec_computePSF_signalNsqrtMany_f(
	.param .u32 vec_computePSF_signalNsqrtMany_f_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_f_param_1,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_3,
	.param .f32 vec_computePSF_signalNsqrtMany_f_param_4,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_f_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_f_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_f_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_f_param_3];
	ld.param.f32 	%f1, [vec_computePSF_signalNsqrtMany_f_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_f_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_f_param_6];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB122_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	sub.s32 	%r14, %r1, %r13;
	shl.b32 	%r15, %r14, 1;
	ld.global.u32 	%r16, [%rd8];
	add.s32 	%r17, %r15, %r16;
	mul.wide.s32 	%rd9, %r17, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r18, [%rd12];
	add.s32 	%r19, %r18, %r15;
	mul.wide.s32 	%rd13, %r19, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f32 	%f4, [%rd14];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	sqrt.rn.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f8;

BB122_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany_fcrop
.visible .entry vec_computePSF_signalNsqrtMany_fcrop(
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_1,
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_3,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_4,
	.param .f32 vec_computePSF_signalNsqrtMany_fcrop_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_6,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r4, [vec_computePSF_signalNsqrtMany_fcrop_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_fcrop_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_fcrop_param_2];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_fcrop_param_3];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_fcrop_param_4];
	ld.param.f32 	%f1, [vec_computePSF_signalNsqrtMany_fcrop_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_fcrop_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_fcrop_param_7];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB123_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	div.s32 	%r15, %r1, %r2;
	mul.lo.s32 	%r16, %r3, %r15;
	shl.b32 	%r17, %r16, 1;
	ld.global.u32 	%r18, [%rd8];
	add.s32 	%r19, %r18, %r17;
	mul.wide.s32 	%rd9, %r19, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r20, [%rd12];
	add.s32 	%r21, %r20, %r17;
	mul.wide.s32 	%rd13, %r21, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f32 	%f4, [%rd14];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	sqrt.rn.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f8;

BB123_2:
	ret;
}

	// .globl	vec_mulMany
.visible .entry vec_mulMany(
	.param .u32 vec_mulMany_param_0,
	.param .u32 vec_mulMany_param_1,
	.param .u64 vec_mulMany_param_2,
	.param .u64 vec_mulMany_param_3,
	.param .u64 vec_mulMany_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_mulMany_param_0];
	ld.param.u32 	%r2, [vec_mulMany_param_1];
	ld.param.u64 	%rd1, [vec_mulMany_param_2];
	ld.param.u64 	%rd2, [vec_mulMany_param_3];
	ld.param.u64 	%rd3, [vec_mulMany_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB124_2;

	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd6];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f64 	[%rd11], %fd3;

BB124_2:
	ret;
}

	// .globl	vec_divScalarMany
.visible .entry vec_divScalarMany(
	.param .u32 vec_divScalarMany_param_0,
	.param .u32 vec_divScalarMany_param_1,
	.param .u64 vec_divScalarMany_param_2,
	.param .u64 vec_divScalarMany_param_3,
	.param .u64 vec_divScalarMany_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_divScalarMany_param_0];
	ld.param.u32 	%r2, [vec_divScalarMany_param_1];
	ld.param.u64 	%rd1, [vec_divScalarMany_param_2];
	ld.param.u64 	%rd2, [vec_divScalarMany_param_3];
	ld.param.u64 	%rd3, [vec_divScalarMany_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB125_3;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r13, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	setp.eq.f64	%p2, %fd1, 0d0000000000000000;
	@%p2 bra 	BB125_3;

	cvta.to.global.u64 	%rd7, %rd1;
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	add.s64 	%rd11, %rd7, %rd9;
	st.global.f64 	[%rd11], %fd3;

BB125_3:
	ret;
}

	// .globl	vec_mulMany_f
.visible .entry vec_mulMany_f(
	.param .u32 vec_mulMany_f_param_0,
	.param .u32 vec_mulMany_f_param_1,
	.param .u64 vec_mulMany_f_param_2,
	.param .u64 vec_mulMany_f_param_3,
	.param .u64 vec_mulMany_f_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_mulMany_f_param_0];
	ld.param.u32 	%r2, [vec_mulMany_f_param_1];
	ld.param.u64 	%rd1, [vec_mulMany_f_param_2];
	ld.param.u64 	%rd2, [vec_mulMany_f_param_3];
	ld.param.u64 	%rd3, [vec_mulMany_f_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB126_2;

	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f3;

BB126_2:
	ret;
}

	// .globl	vec_computeModelMany1
.visible .entry vec_computeModelMany1(
	.param .u32 vec_computeModelMany1_param_0,
	.param .u32 vec_computeModelMany1_param_1,
	.param .u64 vec_computeModelMany1_param_2,
	.param .u64 vec_computeModelMany1_param_3,
	.param .u64 vec_computeModelMany1_param_4,
	.param .f64 vec_computeModelMany1_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_computeModelMany1_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany1_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany1_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany1_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany1_param_4];
	ld.param.f64 	%fd1, [vec_computeModelMany1_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB127_2;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd2, [%rd9];
	ld.global.f64 	%fd3, [%rd6];
	fma.rn.f64 	%fd4, %fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f64 	[%rd11], %fd4;

BB127_2:
	ret;
}

	// .globl	vec_computeModelMany2
.visible .entry vec_computeModelMany2(
	.param .u32 vec_computeModelMany2_param_0,
	.param .u32 vec_computeModelMany2_param_1,
	.param .u64 vec_computeModelMany2_param_2,
	.param .u64 vec_computeModelMany2_param_3,
	.param .u64 vec_computeModelMany2_param_4,
	.param .u64 vec_computeModelMany2_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [vec_computeModelMany2_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany2_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany2_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany2_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany2_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany2_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB128_2;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd7];
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.f64 	%fd3, [%rd12];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd6;
	st.global.f64 	[%rd14], %fd4;

BB128_2:
	ret;
}

	// .globl	vec_computeModelMany3
.visible .entry vec_computeModelMany3(
	.param .u32 vec_computeModelMany3_param_0,
	.param .u32 vec_computeModelMany3_param_1,
	.param .u64 vec_computeModelMany3_param_2,
	.param .u64 vec_computeModelMany3_param_3,
	.param .u64 vec_computeModelMany3_param_4,
	.param .u64 vec_computeModelMany3_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r3, [vec_computeModelMany3_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany3_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany3_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany3_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany3_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany3_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB129_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd7];
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r14, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd3, [%rd13];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd14, %rd1;
	add.s64 	%rd15, %rd14, %rd6;
	st.global.f64 	[%rd15], %fd4;

BB129_2:
	ret;
}

	// .globl	vec_computeModelMany1_scmos
.visible .entry vec_computeModelMany1_scmos(
	.param .u32 vec_computeModelMany1_scmos_param_0,
	.param .u32 vec_computeModelMany1_scmos_param_1,
	.param .u64 vec_computeModelMany1_scmos_param_2,
	.param .u64 vec_computeModelMany1_scmos_param_3,
	.param .u64 vec_computeModelMany1_scmos_param_4,
	.param .f64 vec_computeModelMany1_scmos_param_5,
	.param .u64 vec_computeModelMany1_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r3, [vec_computeModelMany1_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany1_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany1_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany1_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany1_scmos_param_4];
	ld.param.f64 	%fd1, [vec_computeModelMany1_scmos_param_5];
	ld.param.u64 	%rd4, [vec_computeModelMany1_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB130_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	ld.global.f64 	%fd3, [%rd7];
	fma.rn.f64 	%fd4, %fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r14, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd5, [%rd13];
	add.f64 	%fd6, %fd5, %fd4;
	cvta.to.global.u64 	%rd14, %rd1;
	add.s64 	%rd15, %rd14, %rd6;
	st.global.f64 	[%rd15], %fd6;

BB130_2:
	ret;
}

	// .globl	vec_computeModelMany2_scmos
.visible .entry vec_computeModelMany2_scmos(
	.param .u32 vec_computeModelMany2_scmos_param_0,
	.param .u32 vec_computeModelMany2_scmos_param_1,
	.param .u64 vec_computeModelMany2_scmos_param_2,
	.param .u64 vec_computeModelMany2_scmos_param_3,
	.param .u64 vec_computeModelMany2_scmos_param_4,
	.param .u64 vec_computeModelMany2_scmos_param_5,
	.param .u64 vec_computeModelMany2_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r3, [vec_computeModelMany2_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany2_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany2_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany2_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany2_scmos_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany2_scmos_param_5];
	ld.param.u64 	%rd5, [vec_computeModelMany2_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB131_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r13, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	ld.global.f64 	%fd2, [%rd8];
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f64 	%fd3, [%rd13];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd14, %rd5;
	mul.wide.s32 	%rd15, %r14, 8;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f64 	%fd5, [%rd16];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd17, %rd1;
	add.s64 	%rd18, %rd17, %rd7;
	st.global.f64 	[%rd18], %fd6;

BB131_2:
	ret;
}

	// .globl	vec_computeModelMany3_scmos
.visible .entry vec_computeModelMany3_scmos(
	.param .u32 vec_computeModelMany3_scmos_param_0,
	.param .u32 vec_computeModelMany3_scmos_param_1,
	.param .u64 vec_computeModelMany3_scmos_param_2,
	.param .u64 vec_computeModelMany3_scmos_param_3,
	.param .u64 vec_computeModelMany3_scmos_param_4,
	.param .u64 vec_computeModelMany3_scmos_param_5,
	.param .u64 vec_computeModelMany3_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r3, [vec_computeModelMany3_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany3_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany3_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany3_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany3_scmos_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany3_scmos_param_5];
	ld.param.u64 	%rd5, [vec_computeModelMany3_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB132_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r13, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	ld.global.f64 	%fd2, [%rd8];
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r14, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f64 	%fd3, [%rd14];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd16, %rd15, %rd13;
	ld.global.f64 	%fd5, [%rd16];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd17, %rd1;
	add.s64 	%rd18, %rd17, %rd7;
	st.global.f64 	[%rd18], %fd6;

BB132_2:
	ret;
}

	// .globl	vec_divScalarMany_f
.visible .entry vec_divScalarMany_f(
	.param .u32 vec_divScalarMany_f_param_0,
	.param .u32 vec_divScalarMany_f_param_1,
	.param .u64 vec_divScalarMany_f_param_2,
	.param .u64 vec_divScalarMany_f_param_3,
	.param .u64 vec_divScalarMany_f_param_4,
	.param .u64 vec_divScalarMany_f_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r3, [vec_divScalarMany_f_param_0];
	ld.param.u32 	%r2, [vec_divScalarMany_f_param_1];
	ld.param.u64 	%rd1, [vec_divScalarMany_f_param_2];
	ld.param.u64 	%rd2, [vec_divScalarMany_f_param_3];
	ld.param.u64 	%rd3, [vec_divScalarMany_f_param_4];
	ld.param.u64 	%rd4, [vec_divScalarMany_f_param_5];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB133_3;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r13, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	setp.eq.f32	%p2, %f1, 0f00000000;
	@%p2 bra 	BB133_3;

	cvta.to.global.u64 	%rd8, %rd2;
	cvta.to.global.u64 	%rd9, %rd1;
	cvta.to.global.u64 	%rd10, %rd3;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f2, [%rd12];
	div.rn.f32 	%f3, %f2, %f1;
	add.s64 	%rd13, %rd9, %rd11;
	st.global.f32 	[%rd13], %f3;
	mul.wide.s32 	%rd14, %r1, 8;
	add.s64 	%rd15, %rd8, %rd14;
	mov.u64 	%rd16, 0;
	st.global.u64 	[%rd15], %rd16;
	ld.global.f32 	%f4, [%rd13];
	cvt.f64.f32	%fd1, %f4;
	st.global.f64 	[%rd15], %fd1;

BB133_3:
	ret;
}

	// .globl	vec_computePoissonLikelihood
.visible .entry vec_computePoissonLikelihood(
	.param .u32 vec_computePoissonLikelihood_param_0,
	.param .u64 vec_computePoissonLikelihood_param_1,
	.param .u64 vec_computePoissonLikelihood_param_2,
	.param .u64 vec_computePoissonLikelihood_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r12, [vec_computePoissonLikelihood_param_0];
	ld.param.u64 	%rd2, [vec_computePoissonLikelihood_param_1];
	ld.param.u64 	%rd3, [vec_computePoissonLikelihood_param_2];
	ld.param.u64 	%rd4, [vec_computePoissonLikelihood_param_3];
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %nctaid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r1, %r19, %r20, %r21;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB134_11;

	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd1, %rd8, %rd6;
	@%p2 bra 	BB134_3;
	bra.uni 	BB134_2;

BB134_3:
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd12, %rd10, %rd6;
	ld.global.f64 	%fd2, [%rd12];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p3, %r35, 1048575;
	mov.f64 	%fd59, %fd1;
	@%p3 bra 	BB134_5;

	mul.f64 	%fd59, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1077;

BB134_5:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p4, %r24, 2146435071;
	@%p4 bra 	BB134_7;
	bra.uni 	BB134_6;

BB134_7:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p6, %r28, 1073127583;
	@%p6 bra 	BB134_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB134_9:
	add.f64 	%fd13, %fd60, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd14, %fd13;
	neg.f64 	%fd15, %fd13;
	mov.f64 	%fd16, 0d3FF0000000000000;
	fma.rn.f64 	%fd17, %fd15, %fd14, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd17, %fd17;
	fma.rn.f64 	%fd19, %fd18, %fd14, %fd14;
	add.f64 	%fd20, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd21, %fd20, %fd19;
	fma.rn.f64 	%fd22, %fd20, %fd19, %fd21;
	mul.f64 	%fd23, %fd22, %fd22;
	mov.f64 	%fd24, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd25, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd26, %fd25, %fd23, %fd24;
	mov.f64 	%fd27, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	mov.f64 	%fd29, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd30, %fd28, %fd23, %fd29;
	mov.f64 	%fd31, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd32, %fd30, %fd23, %fd31;
	mov.f64 	%fd33, 0d3F624924923BE72D;
	fma.rn.f64 	%fd34, %fd32, %fd23, %fd33;
	mov.f64 	%fd35, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd36, %fd34, %fd23, %fd35;
	mov.f64 	%fd37, 0d3FB5555555555554;
	fma.rn.f64 	%fd38, %fd36, %fd23, %fd37;
	sub.f64 	%fd39, %fd20, %fd22;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd22;
	fma.rn.f64 	%fd42, %fd41, %fd20, %fd40;
	mul.f64 	%fd43, %fd19, %fd42;
	mul.f64 	%fd44, %fd23, %fd38;
	fma.rn.f64 	%fd45, %fd44, %fd22, %fd43;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd46, {%r32, %r34};
	mov.b64 	%fd47, {%r33, %r34};
	sub.f64 	%fd48, %fd46, %fd47;
	mov.f64 	%fd49, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd50, %fd48, %fd49, %fd22;
	neg.f64 	%fd51, %fd48;
	fma.rn.f64 	%fd52, %fd51, %fd49, %fd50;
	sub.f64 	%fd53, %fd52, %fd22;
	sub.f64 	%fd54, %fd45, %fd53;
	mov.f64 	%fd55, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd56, %fd48, %fd55, %fd54;
	add.f64 	%fd61, %fd50, %fd56;
	bra.uni 	BB134_10;

BB134_2:
	mov.u64 	%rd9, 4711630319722168320;
	st.global.u64 	[%rd1], %rd9;
	bra.uni 	BB134_11;

BB134_6:
	mov.f64 	%fd11, 0d7FF0000000000000;
	fma.rn.f64 	%fd12, %fd59, %fd11, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd59;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p5, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd12, %p5;

BB134_10:
	mul.f64 	%fd57, %fd2, %fd61;
	sub.f64 	%fd58, %fd1, %fd57;
	st.global.f64 	[%rd1], %fd58;

BB134_11:
	ret;
}

	// .globl	vec_computeGaussianLikelihood
.visible .entry vec_computeGaussianLikelihood(
	.param .u32 vec_computeGaussianLikelihood_param_0,
	.param .u64 vec_computeGaussianLikelihood_param_1,
	.param .u64 vec_computeGaussianLikelihood_param_2,
	.param .u64 vec_computeGaussianLikelihood_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r2, [vec_computeGaussianLikelihood_param_0];
	ld.param.u64 	%rd2, [vec_computeGaussianLikelihood_param_1];
	ld.param.u64 	%rd3, [vec_computeGaussianLikelihood_param_2];
	ld.param.u64 	%rd4, [vec_computeGaussianLikelihood_param_3];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB135_4;

	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd1, %rd8, %rd6;
	@%p2 bra 	BB135_3;
	bra.uni 	BB135_2;

BB135_3:
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd12, %rd10, %rd6;
	ld.global.f64 	%fd2, [%rd12];
	sub.f64 	%fd3, %fd1, %fd2;
	mul.f64 	%fd4, %fd3, %fd3;
	st.global.f64 	[%rd1], %fd4;
	bra.uni 	BB135_4;

BB135_2:
	mov.u64 	%rd9, 4711630319722168320;
	st.global.u64 	[%rd1], %rd9;

BB135_4:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundManyReshuffle
.visible .entry vec_addPhotonsAndBackgroundManyReshuffle(
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_0,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_1,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_2,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_3,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_4,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundManyReshuffle_param_0];
	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundManyReshuffle_param_1];
	ld.param.u32 	%r4, [vec_addPhotonsAndBackgroundManyReshuffle_param_2];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundManyReshuffle_param_3];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundManyReshuffle_param_4];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundManyReshuffle_param_5];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB136_2;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	div.s32 	%r14, %r1, %r3;
	div.s32 	%r15, %r14, %r4;
	rem.s32 	%r16, %r14, %r4;
	mul.lo.s32 	%r17, %r4, %r3;
	div.s32 	%r18, %r2, %r17;
	mad.lo.s32 	%r19, %r18, %r16, %r15;
	rem.s32 	%r20, %r1, %r3;
	mad.lo.s32 	%r21, %r19, %r3, %r20;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	shl.b32 	%r22, %r14, 1;
	mul.wide.s32 	%rd8, %r22, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	ld.global.f64 	%fd3, [%rd9+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r21, 8;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd4;

BB136_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundManyReshuffle_scmos
.visible .entry vec_addPhotonsAndBackgroundManyReshuffle_scmos(
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_0,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_1,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_2,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_3,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_4,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_5,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_0];
	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_1];
	ld.param.u32 	%r4, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_2];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_3];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_4];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_5];
	ld.param.u64 	%rd4, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_6];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB137_2;

	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	div.s32 	%r14, %r1, %r3;
	div.s32 	%r15, %r14, %r4;
	rem.s32 	%r16, %r14, %r4;
	mul.lo.s32 	%r17, %r4, %r3;
	div.s32 	%r18, %r2, %r17;
	mad.lo.s32 	%r19, %r18, %r16, %r15;
	rem.s32 	%r20, %r1, %r3;
	mad.lo.s32 	%r21, %r19, %r3, %r20;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	shl.b32 	%r22, %r14, 1;
	mul.wide.s32 	%rd9, %r22, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd8];
	ld.global.f64 	%fd3, [%rd10+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f64 	%fd5, [%rd12];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd13, %rd1;
	mul.wide.s32 	%rd14, %r21, 8;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f64 	[%rd15], %fd6;

BB137_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany
.visible .entry vec_addPhotonsAndBackgroundMany(
	.param .u32 vec_addPhotonsAndBackgroundMany_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB138_2;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	div.s32 	%r13, %r1, %r2;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd8, %r14, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	ld.global.f64 	%fd3, [%rd9+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.f64 	[%rd11], %fd4;

BB138_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany_scmos
.visible .entry vec_addPhotonsAndBackgroundMany_scmos(
	.param .u32 vec_addPhotonsAndBackgroundMany_scmos_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_scmos_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_4,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_scmos_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_scmos_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_scmos_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_scmos_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_scmos_param_4];
	ld.param.u64 	%rd4, [vec_addPhotonsAndBackgroundMany_scmos_param_5];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB139_2;

	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	div.s32 	%r13, %r1, %r2;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd9, %r14, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd8];
	ld.global.f64 	%fd3, [%rd10+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f64 	%fd5, [%rd12];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd7;
	st.global.f64 	[%rd14], %fd6;

BB139_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany_f
.visible .entry vec_addPhotonsAndBackgroundMany_f(
	.param .u32 vec_addPhotonsAndBackgroundMany_f_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_f_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_f_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_f_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_f_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_f_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_f_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB140_2;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	div.s32 	%r13, %r1, %r2;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd8, %r14, 4;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	ld.global.f32 	%f3, [%rd9+4];
	fma.rn.f32 	%f4, %f2, %f1, %f3;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.f32 	[%rd11], %f4;

BB140_2:
	ret;
}

	// .globl	vec_shrink
.visible .entry vec_shrink(
	.param .u32 vec_shrink_param_0,
	.param .u64 vec_shrink_param_1,
	.param .u64 vec_shrink_param_2,
	.param .f32 vec_shrink_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_shrink_param_0];
	ld.param.u64 	%rd2, [vec_shrink_param_1];
	ld.param.u64 	%rd3, [vec_shrink_param_2];
	ld.param.f32 	%f2, [vec_shrink_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB141_4;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	setp.lt.f32	%p2, %f1, %f2;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd1, %rd7, %rd5;
	@%p2 bra 	BB141_3;
	bra.uni 	BB141_2;

BB141_3:
	mov.u32 	%r12, 0;
	st.global.u32 	[%rd1], %r12;
	bra.uni 	BB141_4;

BB141_2:
	st.global.f32 	[%rd1], %f1;

BB141_4:
	ret;
}

	// .globl	vec_double2float
.visible .entry vec_double2float(
	.param .u32 vec_double2float_param_0,
	.param .u64 vec_double2float_param_1,
	.param .u64 vec_double2float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_double2float_param_0];
	ld.param.u64 	%rd1, [vec_double2float_param_1];
	ld.param.u64 	%rd2, [vec_double2float_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB142_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rn.f32.f64	%f1, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB142_2:
	ret;
}

	// .globl	vec_float2double
.visible .entry vec_float2double(
	.param .u32 vec_float2double_param_0,
	.param .u64 vec_float2double_param_1,
	.param .u64 vec_float2double_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_float2double_param_0];
	ld.param.u64 	%rd1, [vec_float2double_param_1];
	ld.param.u64 	%rd2, [vec_float2double_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB143_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvt.f64.f32	%fd1, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB143_2:
	ret;
}

	// .globl	vec_complexeConjugateKernel
.visible .entry vec_complexeConjugateKernel(
	.param .u32 vec_complexeConjugateKernel_param_0,
	.param .u32 vec_complexeConjugateKernel_param_1,
	.param .u64 vec_complexeConjugateKernel_param_2,
	.param .u64 vec_complexeConjugateKernel_param_3,
	.param .u64 vec_complexeConjugateKernel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_complexeConjugateKernel_param_0];
	ld.param.u32 	%r2, [vec_complexeConjugateKernel_param_1];
	ld.param.u64 	%rd1, [vec_complexeConjugateKernel_param_2];
	ld.param.u64 	%rd2, [vec_complexeConjugateKernel_param_3];
	ld.param.u64 	%rd3, [vec_complexeConjugateKernel_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	shl.b32 	%r1, %r13, 1;
	shl.b32 	%r14, %r3, 1;
	setp.ge.s32	%p1, %r1, %r14;
	@%p1 bra 	BB144_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	shl.b32 	%r15, %r2, 1;
	rem.s32 	%r16, %r1, %r15;
	mul.wide.s32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvt.rn.f32.s32	%f1, %r2;
	sqrt.rn.f32 	%f2, %f1;
	ld.global.f32 	%f3, [%rd8];
	div.rn.f32 	%f4, %f3, %f2;
	ld.global.f32 	%f5, [%rd8+4];
	div.rn.f32 	%f6, %f5, %f2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f7, [%rd10+4];
	ld.global.f32 	%f8, [%rd10];
	mul.f32 	%f9, %f4, %f8;
	fma.rn.f32 	%f10, %f6, %f7, %f9;
	mul.f32 	%f11, %f6, %f8;
	mul.f32 	%f12, %f4, %f7;
	sub.f32 	%f13, %f11, %f12;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f32 	[%rd11+4], %f13;
	st.global.f32 	[%rd11], %f10;

BB144_2:
	ret;
}

	// .globl	vec_complexeConjugateKernelSubtract
.visible .entry vec_complexeConjugateKernelSubtract(
	.param .u32 vec_complexeConjugateKernelSubtract_param_0,
	.param .u32 vec_complexeConjugateKernelSubtract_param_1,
	.param .u64 vec_complexeConjugateKernelSubtract_param_2,
	.param .u64 vec_complexeConjugateKernelSubtract_param_3,
	.param .u64 vec_complexeConjugateKernelSubtract_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_complexeConjugateKernelSubtract_param_0];
	ld.param.u32 	%r2, [vec_complexeConjugateKernelSubtract_param_1];
	ld.param.u64 	%rd1, [vec_complexeConjugateKernelSubtract_param_2];
	ld.param.u64 	%rd2, [vec_complexeConjugateKernelSubtract_param_3];
	ld.param.u64 	%rd3, [vec_complexeConjugateKernelSubtract_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	shl.b32 	%r1, %r13, 1;
	shl.b32 	%r14, %r3, 1;
	setp.ge.s32	%p1, %r1, %r14;
	@%p1 bra 	BB145_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	shl.b32 	%r15, %r2, 1;
	rem.s32 	%r16, %r1, %r15;
	mul.wide.s32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvt.rn.f32.s32	%f1, %r2;
	sqrt.rn.f32 	%f2, %f1;
	ld.global.f32 	%f3, [%rd8];
	div.rn.f32 	%f4, %f3, %f2;
	ld.global.f32 	%f5, [%rd8+4];
	div.rn.f32 	%f6, %f5, %f2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f7, [%rd10+4];
	ld.global.f32 	%f8, [%rd10];
	mul.f32 	%f9, %f4, %f8;
	fma.rn.f32 	%f10, %f6, %f7, %f9;
	mul.f32 	%f11, %f6, %f8;
	mul.f32 	%f12, %f4, %f7;
	sub.f32 	%f13, %f11, %f12;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f32 	[%rd11+4], %f13;
	st.global.f32 	[%rd11], %f10;

BB145_2:
	ret;
}

	// .globl	vec_complexeMulKernel
.visible .entry vec_complexeMulKernel(
	.param .u32 vec_complexeMulKernel_param_0,
	.param .u32 vec_complexeMulKernel_param_1,
	.param .u64 vec_complexeMulKernel_param_2,
	.param .u64 vec_complexeMulKernel_param_3,
	.param .u64 vec_complexeMulKernel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r4, [vec_complexeMulKernel_param_0];
	ld.param.u32 	%r3, [vec_complexeMulKernel_param_1];
	ld.param.u64 	%rd1, [vec_complexeMulKernel_param_2];
	ld.param.u64 	%rd2, [vec_complexeMulKernel_param_3];
	ld.param.u64 	%rd3, [vec_complexeMulKernel_param_4];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	shl.b32 	%r1, %r14, 1;
	shl.b32 	%r2, %r4, 1;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB146_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	rem.s32 	%r15, %r1, %r2;
	mul.wide.s32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvt.rn.f32.s32	%f1, %r3;
	sqrt.rn.f32 	%f2, %f1;
	ld.global.f32 	%f3, [%rd8];
	div.rn.f32 	%f4, %f3, %f2;
	ld.global.f32 	%f5, [%rd8+4];
	div.rn.f32 	%f6, %f5, %f2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f7, [%rd10];
	mul.f32 	%f8, %f4, %f7;
	ld.global.f32 	%f9, [%rd10+4];
	mul.f32 	%f10, %f6, %f9;
	sub.f32 	%f11, %f8, %f10;
	mul.f32 	%f12, %f6, %f7;
	fma.rn.f32 	%f13, %f4, %f9, %f12;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f32 	[%rd11+4], %f13;
	st.global.f32 	[%rd11], %f11;

BB146_2:
	ret;
}

	// .globl	vec_complexeMulKernelMany
.visible .entry vec_complexeMulKernelMany(
	.param .u64 vec_complexeMulKernelMany_param_0,
	.param .u64 vec_complexeMulKernelMany_param_1,
	.param .u64 vec_complexeMulKernelMany_param_2,
	.param .u64 vec_complexeMulKernelMany_param_3,
	.param .u64 vec_complexeMulKernelMany_param_4,
	.param .u64 vec_complexeMulKernelMany_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<31>;


	ld.param.u64 	%rd6, [vec_complexeMulKernelMany_param_0];
	ld.param.u64 	%rd11, [vec_complexeMulKernelMany_param_1];
	ld.param.u64 	%rd7, [vec_complexeMulKernelMany_param_2];
	ld.param.u64 	%rd8, [vec_complexeMulKernelMany_param_3];
	ld.param.u64 	%rd9, [vec_complexeMulKernelMany_param_4];
	ld.param.u64 	%rd10, [vec_complexeMulKernelMany_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32	%rd12, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd13, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd14, %r9, %r1;
	mul.lo.s64 	%rd15, %rd14, %rd13;
	add.s64 	%rd16, %rd15, %rd12;
	shl.b64 	%rd1, %rd16, 1;
	mul.lo.s64 	%rd17, %rd11, %rd7;
	or.b64  	%rd18, %rd16, %rd17;
	shl.b64 	%rd2, %rd17, 1;
	shl.b64 	%rd19, %rd18, 1;
	and.b64  	%rd20, %rd19, -4294967296;
	setp.eq.s64	%p1, %rd20, 0;
	@%p1 bra 	BB147_2;

	rem.s64 	%rd30, %rd1, %rd2;
	bra.uni 	BB147_3;

BB147_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	rem.u32 	%r12, %r11, %r10;
	cvt.u64.u32	%rd30, %r12;

BB147_3:
	shl.b64 	%rd21, %rd6, 1;
	setp.ge.s64	%p2, %rd1, %rd21;
	@%p2 bra 	BB147_5;

	cvta.to.global.u64 	%rd22, %rd9;
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd22, %rd23;
	cvt.rn.f32.s64	%f1, %rd7;
	ld.global.f32 	%f2, [%rd24];
	div.rn.f32 	%f3, %f2, %f1;
	ld.global.f32 	%f4, [%rd24+4];
	div.rn.f32 	%f5, %f4, %f1;
	cvta.to.global.u64 	%rd25, %rd10;
	shl.b64 	%rd26, %rd30, 2;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.f32 	%f6, [%rd27];
	mul.f32 	%f7, %f3, %f6;
	ld.global.f32 	%f8, [%rd27+4];
	mul.f32 	%f9, %f5, %f8;
	sub.f32 	%f10, %f7, %f9;
	mul.f32 	%f11, %f5, %f6;
	fma.rn.f32 	%f12, %f3, %f8, %f11;
	cvta.to.global.u64 	%rd28, %rd8;
	add.s64 	%rd29, %rd28, %rd23;
	st.global.f32 	[%rd29+4], %f12;
	st.global.f32 	[%rd29], %f10;

BB147_5:
	ret;
}

	// .globl	vec_copyMany
.visible .entry vec_copyMany(
	.param .u64 vec_copyMany_param_0,
	.param .u64 vec_copyMany_param_1,
	.param .u64 vec_copyMany_param_2,
	.param .u64 vec_copyMany_param_3,
	.param .u64 vec_copyMany_param_4,
	.param .u64 vec_copyMany_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<31>;


	ld.param.u64 	%rd9, [vec_copyMany_param_0];
	ld.param.u64 	%rd10, [vec_copyMany_param_1];
	ld.param.u64 	%rd13, [vec_copyMany_param_2];
	ld.param.u64 	%rd11, [vec_copyMany_param_4];
	ld.param.u64 	%rd12, [vec_copyMany_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvt.u64.u32	%rd14, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd15, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd16, %r9, %r2;
	mul.lo.s64 	%rd17, %rd16, %rd15;
	add.s64 	%rd1, %rd17, %rd14;
	mul.lo.s64 	%rd2, %rd13, %rd10;
	or.b64  	%rd18, %rd1, %rd2;
	and.b64  	%rd19, %rd18, -4294967296;
	setp.eq.s64	%p1, %rd19, 0;
	@%p1 bra 	BB148_2;

	div.s64 	%rd29, %rd1, %rd2;
	rem.s64 	%rd30, %rd1, %rd2;
	bra.uni 	BB148_3;

BB148_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd29, %r12;
	cvt.u64.u32	%rd30, %r13;

BB148_3:
	setp.ge.s64	%p2, %rd1, %rd9;
	@%p2 bra 	BB148_5;

	cvta.to.global.u64 	%rd20, %rd12;
	mul.lo.s64 	%rd21, %rd29, %rd10;
	rem.s64 	%rd22, %rd30, %rd10;
	add.s64 	%rd23, %rd22, %rd21;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd20, %rd24;
	ld.global.f32 	%f1, [%rd25];
	cvta.to.global.u64 	%rd26, %rd11;
	shl.b64 	%rd27, %rd1, 2;
	add.s64 	%rd28, %rd26, %rd27;
	st.global.f32 	[%rd28], %f1;

BB148_5:
	ret;
}

	// .globl	vec_mycusparsemoduloSsctrMany
.visible .entry vec_mycusparsemoduloSsctrMany(
	.param .u64 vec_mycusparsemoduloSsctrMany_param_0,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_1,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_2,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_3,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_4,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_5,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_6,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_7,
	.param .u64 vec_mycusparsemoduloSsctrMany_param_8
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd14, [vec_mycusparsemoduloSsctrMany_param_0];
	ld.param.u64 	%rd15, [vec_mycusparsemoduloSsctrMany_param_2];
	ld.param.u64 	%rd16, [vec_mycusparsemoduloSsctrMany_param_3];
	ld.param.u64 	%rd20, [vec_mycusparsemoduloSsctrMany_param_4];
	ld.param.u64 	%rd21, [vec_mycusparsemoduloSsctrMany_param_5];
	ld.param.u64 	%rd17, [vec_mycusparsemoduloSsctrMany_param_6];
	ld.param.u64 	%rd18, [vec_mycusparsemoduloSsctrMany_param_7];
	ld.param.u64 	%rd19, [vec_mycusparsemoduloSsctrMany_param_8];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvt.u64.u32	%rd22, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd23, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd24, %r9, %r2;
	mul.lo.s64 	%rd25, %rd24, %rd23;
	add.s64 	%rd1, %rd25, %rd22;
	mul.lo.s64 	%rd2, %rd21, %rd20;
	mul.lo.s64 	%rd3, %rd2, %rd15;
	or.b64  	%rd26, %rd1, %rd3;
	and.b64  	%rd27, %rd26, -4294967296;
	setp.eq.s64	%p1, %rd27, 0;
	@%p1 bra 	BB149_2;

	div.s64 	%rd44, %rd1, %rd3;
	rem.s64 	%rd45, %rd1, %rd3;
	bra.uni 	BB149_3;

BB149_2:
	cvt.u32.u64	%r10, %rd3;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd44, %r12;
	cvt.u64.u32	%rd45, %r13;

BB149_3:
	rem.s64 	%rd10, %rd45, %rd2;
	or.b64  	%rd28, %rd45, %rd2;
	and.b64  	%rd29, %rd28, -4294967296;
	setp.eq.s64	%p2, %rd29, 0;
	@%p2 bra 	BB149_5;

	div.s64 	%rd46, %rd45, %rd2;
	bra.uni 	BB149_6;

BB149_5:
	cvt.u32.u64	%r14, %rd2;
	cvt.u32.u64	%r15, %rd45;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32	%rd46, %r16;

BB149_6:
	setp.ge.s64	%p3, %rd1, %rd14;
	@%p3 bra 	BB149_8;

	cvta.to.global.u64 	%rd30, %rd18;
	shl.b64 	%rd31, %rd1, 2;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.f32 	%f1, [%rd32];
	cvta.to.global.u64 	%rd33, %rd19;
	shl.b64 	%rd34, %rd10, 2;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.s32 	%rd36, [%rd35];
	mul.lo.s64 	%rd37, %rd44, %rd15;
	add.s64 	%rd38, %rd46, %rd37;
	mul.lo.s64 	%rd39, %rd38, %rd16;
	add.s64 	%rd40, %rd39, %rd36;
	cvta.to.global.u64 	%rd41, %rd17;
	shl.b64 	%rd42, %rd40, 2;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.f32 	[%rd43], %f1;

BB149_8:
	ret;
}

	// .globl	vec_mycusparsemoduloSsctr
.visible .entry vec_mycusparsemoduloSsctr(
	.param .u32 vec_mycusparsemoduloSsctr_param_0,
	.param .u32 vec_mycusparsemoduloSsctr_param_1,
	.param .u32 vec_mycusparsemoduloSsctr_param_2,
	.param .u64 vec_mycusparsemoduloSsctr_param_3,
	.param .u64 vec_mycusparsemoduloSsctr_param_4,
	.param .u64 vec_mycusparsemoduloSsctr_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r4, [vec_mycusparsemoduloSsctr_param_0];
	ld.param.u32 	%r2, [vec_mycusparsemoduloSsctr_param_1];
	ld.param.u32 	%r3, [vec_mycusparsemoduloSsctr_param_2];
	ld.param.u64 	%rd1, [vec_mycusparsemoduloSsctr_param_3];
	ld.param.u64 	%rd2, [vec_mycusparsemoduloSsctr_param_4];
	ld.param.u64 	%rd3, [vec_mycusparsemoduloSsctr_param_5];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB150_2;

	rem.s32 	%r14, %r1, %r3;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r14, 4;
	add.s64 	%rd9, %rd7, %rd8;
	div.s32 	%r15, %r1, %r3;
	ld.global.u32 	%r16, [%rd9];
	mad.lo.s32 	%r17, %r15, %r2, %r16;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r17, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f32 	[%rd12], %f1;

BB150_2:
	ret;
}

	// .globl	vec_makeResultCorrelation
.visible .entry vec_makeResultCorrelation(
	.param .u32 vec_makeResultCorrelation_param_0,
	.param .u32 vec_makeResultCorrelation_param_1,
	.param .u32 vec_makeResultCorrelation_param_2,
	.param .u64 vec_makeResultCorrelation_param_3,
	.param .u64 vec_makeResultCorrelation_param_4,
	.param .u64 vec_makeResultCorrelation_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r4, [vec_makeResultCorrelation_param_0];
	ld.param.u32 	%r2, [vec_makeResultCorrelation_param_1];
	ld.param.u32 	%r3, [vec_makeResultCorrelation_param_2];
	ld.param.u64 	%rd1, [vec_makeResultCorrelation_param_3];
	ld.param.u64 	%rd2, [vec_makeResultCorrelation_param_4];
	ld.param.u64 	%rd3, [vec_makeResultCorrelation_param_5];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB151_2;

	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd4, %rd5;
	div.s32 	%r15, %r1, %r2;
	ld.global.u32 	%r16, [%rd6];
	mad.lo.s32 	%r17, %r15, %r3, %r16;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r17, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	cvt.f64.f32	%fd1, %f1;
	cvt.rn.f32.s32	%f2, %r3;
	cvt.f64.f32	%fd2, %f2;
	mul.f64 	%fd3, %fd2, 0d3FE0000000000000;
	sqrt.rn.f64 	%fd4, %fd3;
	div.rn.f64 	%fd5, %fd1, %fd4;
	cvt.rn.f32.f64	%f3, %fd5;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f32 	[%rd12], %f3;

BB151_2:
	ret;
}

	// .globl	vec_normalizeCorrelation
.visible .entry vec_normalizeCorrelation(
	.param .u64 vec_normalizeCorrelation_param_0,
	.param .u64 vec_normalizeCorrelation_param_1,
	.param .u64 vec_normalizeCorrelation_param_2,
	.param .u64 vec_normalizeCorrelation_param_3,
	.param .u64 vec_normalizeCorrelation_param_4,
	.param .u64 vec_normalizeCorrelation_param_5,
	.param .u64 vec_normalizeCorrelation_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd6, [vec_normalizeCorrelation_param_0];
	ld.param.u64 	%rd10, [vec_normalizeCorrelation_param_2];
	ld.param.u64 	%rd11, [vec_normalizeCorrelation_param_3];
	ld.param.u64 	%rd7, [vec_normalizeCorrelation_param_4];
	ld.param.u64 	%rd8, [vec_normalizeCorrelation_param_5];
	ld.param.u64 	%rd9, [vec_normalizeCorrelation_param_6];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvt.u64.u32	%rd12, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd13, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd14, %r9, %r2;
	mul.lo.s64 	%rd15, %rd14, %rd13;
	add.s64 	%rd1, %rd15, %rd12;
	mul.lo.s64 	%rd2, %rd11, %rd10;
	or.b64  	%rd16, %rd1, %rd2;
	and.b64  	%rd17, %rd16, -4294967296;
	setp.eq.s64	%p1, %rd17, 0;
	@%p1 bra 	BB152_2;

	rem.s64 	%rd26, %rd1, %rd2;
	bra.uni 	BB152_3;

BB152_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	rem.u32 	%r12, %r11, %r10;
	cvt.u64.u32	%rd26, %r12;

BB152_3:
	setp.ge.s64	%p2, %rd1, %rd6;
	@%p2 bra 	BB152_5;

	cvta.to.global.u64 	%rd18, %rd8;
	shl.b64 	%rd19, %rd1, 2;
	add.s64 	%rd20, %rd18, %rd19;
	cvta.to.global.u64 	%rd21, %rd9;
	shl.b64 	%rd22, %rd26, 2;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.f32 	%f1, [%rd23];
	ld.global.f32 	%f2, [%rd20];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd24, %rd7;
	add.s64 	%rd25, %rd24, %rd19;
	st.global.f32 	[%rd25], %f3;

BB152_5:
	ret;
}

	// .globl	vec_makeResultCorrelationMany
.visible .entry vec_makeResultCorrelationMany(
	.param .u64 vec_makeResultCorrelationMany_param_0,
	.param .u64 vec_makeResultCorrelationMany_param_1,
	.param .u64 vec_makeResultCorrelationMany_param_2,
	.param .u64 vec_makeResultCorrelationMany_param_3,
	.param .u64 vec_makeResultCorrelationMany_param_4,
	.param .u64 vec_makeResultCorrelationMany_param_5,
	.param .u64 vec_makeResultCorrelationMany_param_6,
	.param .u64 vec_makeResultCorrelationMany_param_7
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd13, [vec_makeResultCorrelationMany_param_0];
	ld.param.u64 	%rd14, [vec_makeResultCorrelationMany_param_2];
	ld.param.u64 	%rd15, [vec_makeResultCorrelationMany_param_3];
	ld.param.u64 	%rd16, [vec_makeResultCorrelationMany_param_4];
	ld.param.u64 	%rd17, [vec_makeResultCorrelationMany_param_5];
	ld.param.u64 	%rd18, [vec_makeResultCorrelationMany_param_6];
	ld.param.u64 	%rd19, [vec_makeResultCorrelationMany_param_7];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvt.u64.u32	%rd20, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd21, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd22, %r9, %r2;
	mul.lo.s64 	%rd23, %rd22, %rd21;
	add.s64 	%rd1, %rd23, %rd20;
	mul.lo.s64 	%rd2, %rd15, %rd14;
	or.b64  	%rd24, %rd1, %rd2;
	and.b64  	%rd25, %rd24, -4294967296;
	setp.eq.s64	%p1, %rd25, 0;
	@%p1 bra 	BB153_2;

	div.s64 	%rd42, %rd1, %rd2;
	rem.s64 	%rd43, %rd1, %rd2;
	bra.uni 	BB153_3;

BB153_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd42, %r12;
	cvt.u64.u32	%rd43, %r13;

BB153_3:
	rem.s64 	%rd9, %rd43, %rd15;
	or.b64  	%rd26, %rd43, %rd15;
	and.b64  	%rd27, %rd26, -4294967296;
	setp.eq.s64	%p2, %rd27, 0;
	@%p2 bra 	BB153_5;

	div.s64 	%rd44, %rd43, %rd15;
	bra.uni 	BB153_6;

BB153_5:
	cvt.u32.u64	%r14, %rd15;
	cvt.u32.u64	%r15, %rd43;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32	%rd44, %r16;

BB153_6:
	setp.ge.s64	%p3, %rd1, %rd13;
	@%p3 bra 	BB153_8;

	cvta.to.global.u64 	%rd28, %rd19;
	shl.b64 	%rd29, %rd9, 2;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.s32 	%rd31, [%rd30];
	mul.lo.s64 	%rd32, %rd42, %rd14;
	add.s64 	%rd33, %rd44, %rd32;
	mul.lo.s64 	%rd34, %rd33, %rd16;
	add.s64 	%rd35, %rd34, %rd31;
	cvta.to.global.u64 	%rd36, %rd18;
	shl.b64 	%rd37, %rd35, 2;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.f32 	%f1, [%rd38];
	cvta.to.global.u64 	%rd39, %rd17;
	shl.b64 	%rd40, %rd1, 2;
	add.s64 	%rd41, %rd39, %rd40;
	st.global.f32 	[%rd41], %f1;

BB153_8:
	ret;
}

	// .globl	vec_turnMatrixMany
.visible .entry vec_turnMatrixMany(
	.param .u64 vec_turnMatrixMany_param_0,
	.param .u64 vec_turnMatrixMany_param_1,
	.param .u64 vec_turnMatrixMany_param_2,
	.param .u64 vec_turnMatrixMany_param_3,
	.param .u64 vec_turnMatrixMany_param_4,
	.param .u64 vec_turnMatrixMany_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd9, [vec_turnMatrixMany_param_0];
	ld.param.u64 	%rd10, [vec_turnMatrixMany_param_1];
	ld.param.u64 	%rd14, [vec_turnMatrixMany_param_2];
	ld.param.u64 	%rd11, [vec_turnMatrixMany_param_3];
	ld.param.u64 	%rd12, [vec_turnMatrixMany_param_4];
	ld.param.u64 	%rd13, [vec_turnMatrixMany_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvt.u64.u32	%rd15, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd16, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd17, %r9, %r2;
	mul.lo.s64 	%rd18, %rd17, %rd16;
	add.s64 	%rd1, %rd18, %rd15;
	mul.lo.s64 	%rd2, %rd11, %rd14;
	or.b64  	%rd19, %rd1, %rd2;
	and.b64  	%rd20, %rd19, -4294967296;
	setp.eq.s64	%p1, %rd20, 0;
	@%p1 bra 	BB154_2;

	div.s64 	%rd33, %rd1, %rd2;
	rem.s64 	%rd34, %rd1, %rd2;
	bra.uni 	BB154_3;

BB154_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd33, %r12;
	cvt.u64.u32	%rd34, %r13;

BB154_3:
	setp.ge.s64	%p2, %rd1, %rd9;
	@%p2 bra 	BB154_5;

	rem.s64 	%rd21, %rd34, %rd11;
	cvta.to.global.u64 	%rd22, %rd13;
	mul.lo.s64 	%rd23, %rd33, %rd11;
	add.s64 	%rd24, %rd21, %rd23;
	sub.s64 	%rd25, %rd34, %rd21;
	mul.lo.s64 	%rd26, %rd25, %rd10;
	add.s64 	%rd27, %rd24, %rd26;
	shl.b64 	%rd28, %rd1, 2;
	add.s64 	%rd29, %rd22, %rd28;
	ld.global.f32 	%f1, [%rd29];
	cvta.to.global.u64 	%rd30, %rd12;
	shl.b64 	%rd31, %rd27, 2;
	add.s64 	%rd32, %rd30, %rd31;
	st.global.f32 	[%rd32], %f1;

BB154_5:
	ret;
}

	// .globl	vec_makeResultCorrelationNormalized
.visible .entry vec_makeResultCorrelationNormalized(
	.param .u32 vec_makeResultCorrelationNormalized_param_0,
	.param .u32 vec_makeResultCorrelationNormalized_param_1,
	.param .u32 vec_makeResultCorrelationNormalized_param_2,
	.param .u64 vec_makeResultCorrelationNormalized_param_3,
	.param .u64 vec_makeResultCorrelationNormalized_param_4,
	.param .u64 vec_makeResultCorrelationNormalized_param_5,
	.param .f32 vec_makeResultCorrelationNormalized_param_6,
	.param .u64 vec_makeResultCorrelationNormalized_param_7,
	.param .f32 vec_makeResultCorrelationNormalized_param_8
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_makeResultCorrelationNormalized_param_0];
	ld.param.u64 	%rd1, [vec_makeResultCorrelationNormalized_param_3];
	ld.param.f32 	%f1, [vec_makeResultCorrelationNormalized_param_8];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB155_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

BB155_2:
	ret;
}

	// .globl	vec_initializeDeconvolution
.visible .entry vec_initializeDeconvolution(
	.param .u32 vec_initializeDeconvolution_param_0,
	.param .u32 vec_initializeDeconvolution_param_1,
	.param .u64 vec_initializeDeconvolution_param_2,
	.param .u64 vec_initializeDeconvolution_param_3,
	.param .u64 vec_initializeDeconvolution_param_4,
	.param .f32 vec_initializeDeconvolution_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r2, [vec_initializeDeconvolution_param_0];
	ld.param.u32 	%r3, [vec_initializeDeconvolution_param_1];
	ld.param.u64 	%rd1, [vec_initializeDeconvolution_param_2];
	ld.param.u64 	%rd2, [vec_initializeDeconvolution_param_3];
	ld.param.u64 	%rd3, [vec_initializeDeconvolution_param_4];
	ld.param.f32 	%f1, [vec_initializeDeconvolution_param_5];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB156_3;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	st.global.f32 	[%rd6], %f1;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	st.global.f32 	[%rd8], %f1;
	div.s32 	%r13, %r2, %r3;
	setp.ge.s32	%p2, %r1, %r13;
	@%p2 bra 	BB156_3;

	cvta.to.global.u64 	%rd9, %rd3;
	cvt.rn.f32.s32	%f2, %r3;
	mul.f32 	%f3, %f2, %f1;
	add.s64 	%rd11, %rd9, %rd5;
	st.global.f32 	[%rd11], %f3;

BB156_3:
	ret;
}

	// .globl	vec_initializeVectorToValue
.visible .entry vec_initializeVectorToValue(
	.param .u32 vec_initializeVectorToValue_param_0,
	.param .u64 vec_initializeVectorToValue_param_1,
	.param .f32 vec_initializeVectorToValue_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_initializeVectorToValue_param_0];
	ld.param.u64 	%rd1, [vec_initializeVectorToValue_param_1];
	ld.param.f32 	%f1, [vec_initializeVectorToValue_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB157_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

BB157_2:
	ret;
}

	// .globl	vec_chiScore
.visible .entry vec_chiScore(
	.param .u32 vec_chiScore_param_0,
	.param .u64 vec_chiScore_param_1,
	.param .u64 vec_chiScore_param_2,
	.param .u64 vec_chiScore_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_chiScore_param_0];
	ld.param.u64 	%rd1, [vec_chiScore_param_1];
	ld.param.u64 	%rd2, [vec_chiScore_param_2];
	ld.param.u64 	%rd3, [vec_chiScore_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB158_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f2, %f1;
	mul.f32 	%f4, %f3, %f3;
	div.rn.f32 	%f5, %f4, %f1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f5;

BB158_2:
	ret;
}

	// .globl	vec_max
.visible .entry vec_max(
	.param .u32 vec_max_param_0,
	.param .u64 vec_max_param_1,
	.param .f32 vec_max_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_max_param_0];
	ld.param.u64 	%rd1, [vec_max_param_1];
	ld.param.f32 	%f1, [vec_max_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB159_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	max.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

BB159_2:
	ret;
}

	// .globl	vec_subtractMeanWithSumAsInputWithPositiveConstraint
.visible .entry vec_subtractMeanWithSumAsInputWithPositiveConstraint(
	.param .u32 vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_0,
	.param .u64 vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_1,
	.param .u64 vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_2,
	.param .u64 vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_3,
	.param .f32 vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_0];
	ld.param.u64 	%rd2, [vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_1];
	ld.param.u64 	%rd3, [vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_2];
	ld.param.u64 	%rd4, [vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_3];
	ld.param.f32 	%f2, [vec_subtractMeanWithSumAsInputWithPositiveConstraint_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB160_4;

	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvt.rn.f32.s32	%f3, %r2;
	cvta.to.global.u64 	%rd9, %rd4;
	ld.global.f32 	%f4, [%rd9];
	div.rn.f32 	%f5, %f4, %f3;
	ld.global.f32 	%f6, [%rd8];
	sub.f32 	%f1, %f6, %f5;
	setp.gt.f32	%p2, %f1, 0f00000000;
	add.s64 	%rd1, %rd5, %rd7;
	@%p2 bra 	BB160_3;
	bra.uni 	BB160_2;

BB160_3:
	st.global.f32 	[%rd1], %f1;
	bra.uni 	BB160_4;

BB160_2:
	st.global.f32 	[%rd1], %f2;

BB160_4:
	ret;
}

	// .globl	vec_divScalarFloat
.visible .entry vec_divScalarFloat(
	.param .u32 vec_divScalarFloat_param_0,
	.param .u64 vec_divScalarFloat_param_1,
	.param .u64 vec_divScalarFloat_param_2,
	.param .f32 vec_divScalarFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_divScalarFloat_param_0];
	ld.param.u64 	%rd1, [vec_divScalarFloat_param_1];
	ld.param.u64 	%rd2, [vec_divScalarFloat_param_2];
	ld.param.f32 	%f1, [vec_divScalarFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB161_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

BB161_2:
	ret;
}

	// .globl	vec_updateMandOP
.visible .entry vec_updateMandOP(
	.param .u32 vec_updateMandOP_param_0,
	.param .u64 vec_updateMandOP_param_1,
	.param .u64 vec_updateMandOP_param_2,
	.param .u64 vec_updateMandOP_param_3,
	.param .f32 vec_updateMandOP_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_updateMandOP_param_0];
	ld.param.u64 	%rd1, [vec_updateMandOP_param_1];
	ld.param.u64 	%rd2, [vec_updateMandOP_param_2];
	ld.param.u64 	%rd3, [vec_updateMandOP_param_3];
	ld.param.f32 	%f1, [vec_updateMandOP_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB162_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f2, [%rd8];
	ld.global.f32 	%f3, [%rd6];
	sub.f32 	%f4, %f3, %f2;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	ld.global.f32 	%f5, [%rd10];
	div.rn.f32 	%f6, %f5, %f1;
	st.global.f32 	[%rd8], %f6;
	add.f32 	%f7, %f4, %f6;
	st.global.f32 	[%rd6], %f7;

BB162_2:
	ret;
}

	// .globl	vec_computeCRLB
.visible .entry vec_computeCRLB(
	.param .u32 vec_computeCRLB_param_0,
	.param .u32 vec_computeCRLB_param_1,
	.param .u64 vec_computeCRLB_param_2,
	.param .u64 vec_computeCRLB_param_3,
	.param .f64 vec_computeCRLB_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computeCRLB_param_0];
	ld.param.u32 	%r3, [vec_computeCRLB_param_1];
	ld.param.u64 	%rd2, [vec_computeCRLB_param_2];
	ld.param.u64 	%rd3, [vec_computeCRLB_param_3];
	ld.param.f64 	%fd4, [vec_computeCRLB_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB163_4;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.lo.s32 	%r13, %r3, %r3;
	div.s32 	%r14, %r2, %r13;
	cvta.to.global.u64 	%rd5, %rd2;
	div.s32 	%r15, %r1, %r14;
	div.s32 	%r16, %r15, %r3;
	rem.s32 	%r17, %r15, %r3;
	shl.b32 	%r18, %r16, 1;
	add.s32 	%r19, %r18, 2;
	rem.s32 	%r20, %r1, %r14;
	mad.lo.s32 	%r21, %r19, %r14, %r20;
	mul.wide.s32 	%rd6, %r21, 8;
	add.s64 	%rd7, %rd4, %rd6;
	add.s32 	%r22, %r18, 1;
	mad.lo.s32 	%r23, %r22, %r14, %r20;
	mul.wide.s32 	%rd8, %r23, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd5, [%rd9];
	ld.global.f64 	%fd6, [%rd7];
	sub.f64 	%fd7, %fd6, %fd5;
	add.f64 	%fd8, %fd4, %fd4;
	div.rn.f64 	%fd1, %fd7, %fd8;
	shl.b32 	%r24, %r17, 1;
	add.s32 	%r25, %r24, 2;
	mad.lo.s32 	%r26, %r25, %r14, %r20;
	mul.wide.s32 	%rd10, %r26, 8;
	add.s64 	%rd11, %rd4, %rd10;
	add.s32 	%r27, %r24, 1;
	mad.lo.s32 	%r28, %r27, %r14, %r20;
	mul.wide.s32 	%rd12, %r28, 8;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.f64 	%fd9, [%rd13];
	ld.global.f64 	%fd10, [%rd11];
	sub.f64 	%fd11, %fd10, %fd9;
	div.rn.f64 	%fd2, %fd11, %fd8;
	mul.wide.s32 	%rd14, %r20, 8;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.f64 	%fd3, [%rd15];
	setp.gt.f64	%p2, %fd3, 0d0000000000000000;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd1, %rd5, %rd16;
	@%p2 bra 	BB163_3;
	bra.uni 	BB163_2;

BB163_3:
	mul.f64 	%fd12, %fd1, %fd2;
	div.rn.f64 	%fd13, %fd12, %fd3;
	st.global.f64 	[%rd1], %fd13;
	bra.uni 	BB163_4;

BB163_2:
	mov.u64 	%rd17, 4726483295884279808;
	st.global.u64 	[%rd1], %rd17;

BB163_4:
	ret;
}

	// .globl	vec_divCorrelation
.visible .entry vec_divCorrelation(
	.param .u32 vec_divCorrelation_param_0,
	.param .u64 vec_divCorrelation_param_1,
	.param .u32 vec_divCorrelation_param_2,
	.param .u64 vec_divCorrelation_param_3,
	.param .u64 vec_divCorrelation_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r3, [vec_divCorrelation_param_0];
	ld.param.u64 	%rd2, [vec_divCorrelation_param_1];
	ld.param.u32 	%r2, [vec_divCorrelation_param_2];
	ld.param.u64 	%rd3, [vec_divCorrelation_param_3];
	ld.param.u64 	%rd4, [vec_divCorrelation_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB164_4;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.lo.s32 	%r13, %r2, %r2;
	rem.s32 	%r14, %r1, %r13;
	div.s32 	%r15, %r1, %r13;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	mul.wide.s32 	%rd10, %r15, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f2, [%rd11];
	ld.global.f32 	%f3, [%rd8];
	mul.f32 	%f1, %f3, %f2;
	setp.gt.f32	%p2, %f1, 0f00000000;
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd1, %rd5, %rd12;
	@%p2 bra 	BB164_3;
	bra.uni 	BB164_2;

BB164_3:
	ld.global.f32 	%f4, [%rd1];
	sqrt.rn.f32 	%f5, %f1;
	div.rn.f32 	%f6, %f4, %f5;
	st.global.f32 	[%rd1], %f6;
	bra.uni 	BB164_4;

BB164_2:
	mov.u32 	%r16, -1082130432;
	st.global.u32 	[%rd1], %r16;

BB164_4:
	ret;
}

	// .globl	vec_manualFilteringTest
.visible .entry vec_manualFilteringTest(
	.param .u64 vec_manualFilteringTest_param_0,
	.param .u32 vec_manualFilteringTest_param_1,
	.param .u32 vec_manualFilteringTest_param_2,
	.param .u32 vec_manualFilteringTest_param_3,
	.param .u32 vec_manualFilteringTest_param_4,
	.param .u64 vec_manualFilteringTest_param_5,
	.param .u64 vec_manualFilteringTest_param_6,
	.param .u64 vec_manualFilteringTest_param_7,
	.param .u64 vec_manualFilteringTest_param_8,
	.param .u64 vec_manualFilteringTest_param_9
)
{
	.local .align 8 .b8 	__local_depot165[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<42>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<159>;


	mov.u64 	%SPL, __local_depot165;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd35, [vec_manualFilteringTest_param_0];
	ld.param.u32 	%r1, [vec_manualFilteringTest_param_1];
	ld.param.u32 	%r2, [vec_manualFilteringTest_param_2];
	ld.param.u32 	%r3, [vec_manualFilteringTest_param_3];
	ld.param.u64 	%rd36, [vec_manualFilteringTest_param_5];
	ld.param.u64 	%rd37, [vec_manualFilteringTest_param_6];
	ld.param.u64 	%rd38, [vec_manualFilteringTest_param_7];
	ld.param.u64 	%rd39, [vec_manualFilteringTest_param_9];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	cvt.u64.u32	%rd40, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	cvt.u64.u32	%rd41, %r11;
	mov.u32 	%r12, %nctaid.x;
	mul.wide.u32 	%rd42, %r12, %r4;
	mul.lo.s64 	%rd43, %rd42, %rd41;
	add.s64 	%rd1, %rd43, %rd40;
	mul.lo.s32 	%r13, %r3, %r3;
	cvt.u64.u32	%rd2, %r13;
	and.b64  	%rd44, %rd1, -4294967296;
	setp.eq.s64	%p1, %rd44, 0;
	@%p1 bra 	BB165_2;

	div.u64 	%rd150, %rd1, %rd2;
	rem.u64 	%rd151, %rd1, %rd2;
	bra.uni 	BB165_3;

BB165_2:
	cvt.u32.u64	%r14, %rd2;
	cvt.u32.u64	%r15, %rd1;
	div.u32 	%r16, %r15, %r14;
	rem.u32 	%r17, %r15, %r14;
	cvt.u64.u32	%rd150, %r16;
	cvt.u64.u32	%rd151, %r17;

BB165_3:
	cvt.s64.s32	%rd45, %r2;
	or.b64  	%rd46, %rd150, %rd45;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64	%p2, %rd47, 0;
	@%p2 bra 	BB165_5;

	div.s64 	%rd152, %rd150, %rd45;
	rem.s64 	%rd153, %rd150, %rd45;
	bra.uni 	BB165_6;

BB165_5:
	cvt.u32.u64	%r18, %rd150;
	div.u32 	%r19, %r18, %r2;
	rem.u32 	%r20, %r18, %r2;
	cvt.u64.u32	%rd152, %r19;
	cvt.u64.u32	%rd153, %r20;

BB165_6:
	setp.ge.s64	%p3, %rd1, %rd35;
	@%p3 bra 	BB165_51;

	cvta.to.global.u64 	%rd49, %rd36;
	shl.b64 	%rd50, %rd150, 2;
	add.s64 	%rd15, %rd49, %rd50;
	setp.ne.s64	%p4, %rd151, 0;
	@%p4 bra 	BB165_9;

	mov.u32 	%r21, 0;
	st.global.u32 	[%rd15], %r21;

BB165_9:
	cvta.to.global.u64 	%rd51, %rd37;
	add.s64 	%rd53, %rd51, %rd50;
	cvta.to.global.u64 	%rd54, %rd38;
	shl.b64 	%rd55, %rd151, 2;
	add.s64 	%rd56, %rd54, %rd55;
	ld.global.f32 	%f1, [%rd56];
	ld.global.f32 	%f2, [%rd53];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd57, %rd39;
	shl.b64 	%rd58, %rd1, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f3;
	bar.sync 	0;
	mul.lo.s64 	%rd61, %rd152, %rd45;
	add.s64 	%rd62, %rd61, %rd153;
	setp.eq.s64	%p5, %rd150, %rd62;
	@%p5 bra 	BB165_11;

	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd64, %SPL, 0;
	st.local.u64 	[%rd64], %rd150;
	st.local.u64 	[%rd64+8], %rd152;
	st.local.u64 	[%rd64+16], %rd153;
	st.local.u32 	[%rd64+24], %r2;
	mov.u64 	%rd65, $str3;
	cvta.global.u64 	%rd66, %rd65;
	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd63;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r22, [retval0+0];
	
	//{
	}// Callseq End 29

BB165_11:
	@%p4 bra 	BB165_50;

	setp.lt.s32	%p7, %r3, 1;
	@%p7 bra 	BB165_50;

	mul.wide.s32 	%rd68, %r3, %r3;
	mul.lo.s64 	%rd16, %rd68, %rd150;
	mov.u64 	%rd67, 0;
	mov.u64 	%rd154, %rd67;

BB165_14:
	mul.lo.s32 	%r24, %r13, %r2;
	cvt.s64.s32	%rd73, %r24;
	cvt.s64.s32	%rd74, %r3;
	add.s64 	%rd75, %rd73, %rd74;
	mul.lo.s64 	%rd18, %rd154, %rd75;
	cvt.u64.u32	%rd76, %r3;
	and.b64  	%rd72, %rd76, 3;
	setp.eq.s64	%p8, %rd72, 0;
	mov.u64 	%rd158, %rd67;
	@%p8 bra 	BB165_31;

	setp.eq.s64	%p9, %rd72, 1;
	mov.u64 	%rd156, %rd67;
	@%p9 bra 	BB165_26;

	setp.eq.s64	%p10, %rd72, 2;
	mov.u64 	%rd155, %rd67;
	@%p10 bra 	BB165_21;

	add.s64 	%rd77, %rd154, %rd152;
	cvt.s64.s32	%rd78, %r1;
	setp.lt.s64	%p11, %rd77, %rd78;
	add.s64 	%rd19, %rd16, %rd18;
	setp.gt.s32	%p12, %r2, -1;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB165_19;
	bra.uni 	BB165_18;

BB165_19:
	mov.u64 	%rd155, 1;
	setp.ge.s64	%p14, %rd19, %rd35;
	@%p14 bra 	BB165_21;

	shl.b64 	%rd83, %rd19, 2;
	add.s64 	%rd84, %rd57, %rd83;
	ld.global.f32 	%f4, [%rd15];
	ld.global.f32 	%f5, [%rd84];
	add.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd15], %f6;
	bra.uni 	BB165_21;

BB165_18:
	mov.u32 	%r25, -1110651699;
	st.global.u32 	[%rd15], %r25;
	mov.u64 	%rd155, 1;

BB165_21:
	cvt.s64.s32	%rd85, %r1;
	add.s64 	%rd86, %rd154, %rd152;
	setp.lt.s64	%p15, %rd86, %rd85;
	mad.lo.s32 	%r26, %r3, %r3, 1;
	cvt.s64.s32	%rd87, %r26;
	neg.s64 	%rd88, %rd155;
	and.b64  	%rd89, %rd87, %rd88;
	add.s64 	%rd90, %rd89, %rd16;
	add.s64 	%rd21, %rd90, %rd18;
	add.s64 	%rd92, %rd155, %rd153;
	setp.lt.s64	%p16, %rd92, %rd45;
	and.pred  	%p17, %p15, %p16;
	@%p17 bra 	BB165_23;
	bra.uni 	BB165_22;

BB165_23:
	setp.ge.s64	%p18, %rd21, %rd35;
	@%p18 bra 	BB165_25;

	shl.b64 	%rd94, %rd21, 2;
	add.s64 	%rd95, %rd57, %rd94;
	ld.global.f32 	%f7, [%rd15];
	ld.global.f32 	%f8, [%rd95];
	add.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd15], %f9;
	bra.uni 	BB165_25;

BB165_22:
	mov.u32 	%r27, -1110651699;
	st.global.u32 	[%rd15], %r27;

BB165_25:
	add.s64 	%rd156, %rd155, 1;

BB165_26:
	cvt.s64.s32	%rd96, %r1;
	add.s64 	%rd97, %rd154, %rd152;
	setp.lt.s64	%p19, %rd97, %rd96;
	mad.lo.s32 	%r28, %r3, %r3, 1;
	cvt.s64.s32	%rd98, %r28;
	mul.lo.s64 	%rd99, %rd156, %rd98;
	add.s64 	%rd100, %rd99, %rd16;
	add.s64 	%rd24, %rd100, %rd18;
	add.s64 	%rd102, %rd156, %rd153;
	setp.lt.s64	%p20, %rd102, %rd45;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	BB165_28;
	bra.uni 	BB165_27;

BB165_28:
	setp.ge.s64	%p22, %rd24, %rd35;
	@%p22 bra 	BB165_30;

	shl.b64 	%rd104, %rd24, 2;
	add.s64 	%rd105, %rd57, %rd104;
	ld.global.f32 	%f10, [%rd15];
	ld.global.f32 	%f11, [%rd105];
	add.f32 	%f12, %f11, %f10;
	st.global.f32 	[%rd15], %f12;
	bra.uni 	BB165_30;

BB165_27:
	mov.u32 	%r29, -1110651699;
	st.global.u32 	[%rd15], %r29;

BB165_30:
	add.s64 	%rd158, %rd156, 1;

BB165_31:
	setp.lt.u32	%p23, %r3, 4;
	@%p23 bra 	BB165_49;

BB165_32:
	cvt.s64.s32	%rd106, %r1;
	add.s64 	%rd107, %rd154, %rd152;
	setp.lt.s64	%p24, %rd107, %rd106;
	mad.lo.s32 	%r30, %r3, %r3, 1;
	cvt.s64.s32	%rd108, %r30;
	mul.lo.s64 	%rd109, %rd158, %rd108;
	add.s64 	%rd110, %rd109, %rd16;
	add.s64 	%rd28, %rd110, %rd18;
	add.s64 	%rd29, %rd153, %rd158;
	setp.lt.s64	%p25, %rd29, %rd45;
	and.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB165_34;
	bra.uni 	BB165_33;

BB165_34:
	setp.ge.s64	%p27, %rd28, %rd35;
	@%p27 bra 	BB165_36;

	shl.b64 	%rd113, %rd28, 2;
	add.s64 	%rd114, %rd57, %rd113;
	ld.global.f32 	%f13, [%rd15];
	ld.global.f32 	%f14, [%rd114];
	add.f32 	%f15, %f14, %f13;
	st.global.f32 	[%rd15], %f15;
	bra.uni 	BB165_36;

BB165_33:
	mov.u32 	%r31, -1110651699;
	st.global.u32 	[%rd15], %r31;

BB165_36:
	add.s64 	%rd118, %rd158, 1;
	mul.lo.s64 	%rd119, %rd118, %rd108;
	add.s64 	%rd120, %rd119, %rd16;
	add.s64 	%rd30, %rd120, %rd18;
	add.s64 	%rd122, %rd29, 1;
	setp.lt.s64	%p29, %rd122, %rd45;
	and.pred  	%p30, %p24, %p29;
	@%p30 bra 	BB165_38;
	bra.uni 	BB165_37;

BB165_38:
	setp.ge.s64	%p31, %rd30, %rd35;
	@%p31 bra 	BB165_40;

	shl.b64 	%rd124, %rd30, 2;
	add.s64 	%rd125, %rd57, %rd124;
	ld.global.f32 	%f16, [%rd15];
	ld.global.f32 	%f17, [%rd125];
	add.f32 	%f18, %f17, %f16;
	st.global.f32 	[%rd15], %f18;
	bra.uni 	BB165_40;

BB165_37:
	mov.u32 	%r33, -1110651699;
	st.global.u32 	[%rd15], %r33;

BB165_40:
	add.s64 	%rd129, %rd158, 2;
	mul.lo.s64 	%rd130, %rd129, %rd108;
	add.s64 	%rd131, %rd130, %rd16;
	add.s64 	%rd31, %rd131, %rd18;
	add.s64 	%rd133, %rd29, 2;
	setp.lt.s64	%p33, %rd133, %rd45;
	and.pred  	%p34, %p24, %p33;
	@%p34 bra 	BB165_42;
	bra.uni 	BB165_41;

BB165_42:
	setp.ge.s64	%p35, %rd31, %rd35;
	@%p35 bra 	BB165_44;

	shl.b64 	%rd135, %rd31, 2;
	add.s64 	%rd136, %rd57, %rd135;
	ld.global.f32 	%f19, [%rd15];
	ld.global.f32 	%f20, [%rd136];
	add.f32 	%f21, %f20, %f19;
	st.global.f32 	[%rd15], %f21;
	bra.uni 	BB165_44;

BB165_41:
	mov.u32 	%r35, -1110651699;
	st.global.u32 	[%rd15], %r35;

BB165_44:
	add.s64 	%rd140, %rd158, 3;
	mul.lo.s64 	%rd141, %rd140, %rd108;
	add.s64 	%rd142, %rd141, %rd16;
	add.s64 	%rd32, %rd142, %rd18;
	add.s64 	%rd144, %rd29, 3;
	setp.lt.s64	%p37, %rd144, %rd45;
	and.pred  	%p38, %p24, %p37;
	@%p38 bra 	BB165_46;
	bra.uni 	BB165_45;

BB165_46:
	setp.ge.s64	%p39, %rd32, %rd35;
	@%p39 bra 	BB165_48;

	shl.b64 	%rd146, %rd32, 2;
	add.s64 	%rd147, %rd57, %rd146;
	ld.global.f32 	%f22, [%rd15];
	ld.global.f32 	%f23, [%rd147];
	add.f32 	%f24, %f23, %f22;
	st.global.f32 	[%rd15], %f24;
	bra.uni 	BB165_48;

BB165_45:
	mov.u32 	%r37, -1110651699;
	st.global.u32 	[%rd15], %r37;

BB165_48:
	add.s64 	%rd158, %rd158, 4;
	setp.lt.s64	%p40, %rd158, %rd74;
	@%p40 bra 	BB165_32;

BB165_49:
	add.s64 	%rd154, %rd154, 1;
	setp.lt.s64	%p41, %rd154, %rd74;
	@%p41 bra 	BB165_14;

BB165_50:
	bar.sync 	0;

BB165_51:
	ret;
}

	// .globl	vec_localMinimum
.visible .entry vec_localMinimum(
	.param .u32 vec_localMinimum_param_0,
	.param .u32 vec_localMinimum_param_1,
	.param .u32 vec_localMinimum_param_2,
	.param .u32 vec_localMinimum_param_3,
	.param .u64 vec_localMinimum_param_4,
	.param .u64 vec_localMinimum_param_5
)
{
	.reg .pred 	%p<65>;
	.reg .f32 	%f<54>;
	.reg .b32 	%r<177>;
	.reg .b64 	%rd<44>;


	ld.param.u32 	%r36, [vec_localMinimum_param_0];
	ld.param.u32 	%r33, [vec_localMinimum_param_1];
	ld.param.u32 	%r34, [vec_localMinimum_param_2];
	ld.param.u32 	%r35, [vec_localMinimum_param_3];
	ld.param.u64 	%rd4, [vec_localMinimum_param_4];
	ld.param.u64 	%rd5, [vec_localMinimum_param_5];
	mov.u32 	%r37, %ntid.y;
	mov.u32 	%r38, %ctaid.y;
	mov.u32 	%r39, %tid.y;
	mad.lo.s32 	%r40, %r37, %r38, %r39;
	mov.u32 	%r41, %nctaid.x;
	mov.u32 	%r42, %ctaid.x;
	mad.lo.s32 	%r43, %r40, %r41, %r42;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r43, %r44, %r45;
	setp.ge.s32	%p1, %r1, %r36;
	@%p1 bra 	BB166_36;

	cvta.to.global.u64 	%rd6, %rd5;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.f32 	%f49, [%rd9];
	add.s64 	%rd10, %rd7, %rd8;
	st.global.f32 	[%rd10], %f49;
	shr.u32 	%r46, %r35, 31;
	add.s32 	%r47, %r35, %r46;
	shr.s32 	%r2, %r47, 1;
	setp.lt.s32	%p2, %r35, 1;
	@%p2 bra 	BB166_36;

	rem.s32 	%r49, %r1, %r34;
	sub.s32 	%r3, %r49, %r2;
	div.s32 	%r50, %r1, %r34;
	sub.s32 	%r167, %r50, %r2;
	mov.u32 	%r48, 0;
	mov.u32 	%r166, %r48;

BB166_3:
	mul.lo.s32 	%r7, %r167, %r34;
	and.b32  	%r54, %r35, 3;
	setp.eq.s32	%p3, %r54, 0;
	@%p3 bra 	BB166_4;

	setp.eq.s32	%p4, %r54, 1;
	@%p4 bra 	BB166_6;
	bra.uni 	BB166_7;

BB166_6:
	mov.u32 	%r170, %r48;
	mov.u32 	%r171, %r3;
	bra.uni 	BB166_16;

BB166_4:
	mov.u32 	%r172, %r48;
	mov.u32 	%r173, %r3;
	bra.uni 	BB166_20;

BB166_7:
	setp.eq.s32	%p5, %r54, 2;
	@%p5 bra 	BB166_8;
	bra.uni 	BB166_9;

BB166_8:
	mov.u32 	%r168, %r48;
	mov.u32 	%r169, %r3;
	bra.uni 	BB166_12;

BB166_9:
	setp.gt.s32	%p6, %r167, -1;
	setp.lt.s32	%p7, %r3, %r34;
	setp.lt.s32	%p8, %r167, %r33;
	and.pred  	%p9, %p8, %p7;
	and.pred  	%p10, %p9, %p6;
	setp.gt.s32	%p11, %r3, -1;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r169, %r3, 1;
	mov.u32 	%r168, 1;
	@!%p12 bra 	BB166_12;
	bra.uni 	BB166_10;

BB166_10:
	sub.s32 	%r57, %r2, %r166;
	mul.lo.s32 	%r58, %r57, %r57;
	mad.lo.s32 	%r59, %r2, %r2, %r58;
	cvt.rn.f32.s32	%f21, %r59;
	sqrt.rn.f32 	%f22, %f21;
	cvt.rzi.s32.f32	%r60, %f22;
	setp.gt.s32	%p13, %r60, %r2;
	@%p13 bra 	BB166_12;

	add.s32 	%r62, %r3, %r7;
	mul.wide.s32 	%rd12, %r62, 4;
	add.s64 	%rd13, %rd6, %rd12;
	ld.global.f32 	%f23, [%rd13];
	min.f32 	%f49, %f23, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_12:
	setp.lt.s32	%p14, %r169, %r34;
	setp.lt.s32	%p15, %r167, %r33;
	and.pred  	%p16, %p15, %p14;
	setp.gt.s32	%p17, %r167, -1;
	and.pred  	%p18, %p16, %p17;
	setp.gt.s32	%p19, %r169, -1;
	and.pred  	%p20, %p18, %p19;
	@!%p20 bra 	BB166_15;
	bra.uni 	BB166_13;

BB166_13:
	sub.s32 	%r73, %r2, %r168;
	sub.s32 	%r74, %r2, %r166;
	mul.lo.s32 	%r75, %r74, %r74;
	mad.lo.s32 	%r76, %r73, %r73, %r75;
	cvt.rn.f32.s32	%f24, %r76;
	sqrt.rn.f32 	%f25, %f24;
	cvt.rzi.s32.f32	%r77, %f25;
	setp.gt.s32	%p21, %r77, %r2;
	@%p21 bra 	BB166_15;

	add.s32 	%r78, %r169, %r7;
	mul.wide.s32 	%rd18, %r78, 4;
	add.s64 	%rd19, %rd6, %rd18;
	ld.global.f32 	%f26, [%rd19];
	min.f32 	%f49, %f26, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_15:
	add.s32 	%r170, %r168, 1;
	add.s32 	%r171, %r169, 1;

BB166_16:
	setp.lt.s32	%p22, %r171, %r34;
	setp.lt.s32	%p23, %r167, %r33;
	and.pred  	%p24, %p23, %p22;
	setp.gt.s32	%p25, %r167, -1;
	and.pred  	%p26, %p24, %p25;
	setp.gt.s32	%p27, %r171, -1;
	and.pred  	%p28, %p26, %p27;
	@!%p28 bra 	BB166_19;
	bra.uni 	BB166_17;

BB166_17:
	sub.s32 	%r89, %r2, %r170;
	sub.s32 	%r90, %r2, %r166;
	mul.lo.s32 	%r91, %r90, %r90;
	mad.lo.s32 	%r92, %r89, %r89, %r91;
	cvt.rn.f32.s32	%f27, %r92;
	sqrt.rn.f32 	%f28, %f27;
	cvt.rzi.s32.f32	%r93, %f28;
	setp.gt.s32	%p29, %r93, %r2;
	@%p29 bra 	BB166_19;

	add.s32 	%r94, %r171, %r7;
	mul.wide.s32 	%rd24, %r94, 4;
	add.s64 	%rd25, %rd6, %rd24;
	ld.global.f32 	%f29, [%rd25];
	min.f32 	%f49, %f29, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_19:
	add.s32 	%r172, %r170, 1;
	add.s32 	%r173, %r171, 1;

BB166_20:
	setp.lt.u32	%p30, %r35, 4;
	@%p30 bra 	BB166_35;

	sub.s32 	%r174, %r2, %r172;
	mad.lo.s32 	%r105, %r34, %r167, %r173;
	mul.wide.s32 	%rd30, %r105, 4;
	add.s64 	%rd43, %rd6, %rd30;

BB166_22:
	setp.lt.s32	%p31, %r173, %r34;
	setp.lt.s32	%p32, %r167, %r33;
	and.pred  	%p33, %p32, %p31;
	setp.gt.s32	%p34, %r167, -1;
	and.pred  	%p35, %p33, %p34;
	setp.gt.s32	%p36, %r173, -1;
	and.pred  	%p37, %p35, %p36;
	@!%p37 bra 	BB166_25;
	bra.uni 	BB166_23;

BB166_23:
	sub.s32 	%r106, %r2, %r166;
	mul.lo.s32 	%r107, %r106, %r106;
	mad.lo.s32 	%r108, %r174, %r174, %r107;
	cvt.rn.f32.s32	%f30, %r108;
	sqrt.rn.f32 	%f31, %f30;
	cvt.rzi.s32.f32	%r109, %f31;
	setp.gt.s32	%p38, %r109, %r2;
	@%p38 bra 	BB166_25;

	ld.global.f32 	%f32, [%rd43];
	min.f32 	%f49, %f32, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_25:
	add.s32 	%r120, %r173, 1;
	setp.lt.s32	%p39, %r120, %r34;
	and.pred  	%p41, %p32, %p39;
	and.pred  	%p43, %p41, %p34;
	setp.gt.s32	%p44, %r173, -2;
	and.pred  	%p45, %p43, %p44;
	@!%p45 bra 	BB166_28;
	bra.uni 	BB166_26;

BB166_26:
	add.s32 	%r121, %r174, -1;
	sub.s32 	%r122, %r2, %r166;
	mul.lo.s32 	%r123, %r122, %r122;
	mad.lo.s32 	%r124, %r121, %r121, %r123;
	cvt.rn.f32.s32	%f33, %r124;
	sqrt.rn.f32 	%f34, %f33;
	cvt.rzi.s32.f32	%r125, %f34;
	setp.gt.s32	%p46, %r125, %r2;
	@%p46 bra 	BB166_28;

	ld.global.f32 	%f35, [%rd43+4];
	min.f32 	%f49, %f35, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_28:
	add.s32 	%r26, %r173, 2;
	setp.lt.s32	%p47, %r26, %r34;
	and.pred  	%p49, %p32, %p47;
	and.pred  	%p51, %p49, %p34;
	setp.gt.s32	%p52, %r26, -1;
	and.pred  	%p53, %p51, %p52;
	@!%p53 bra 	BB166_31;
	bra.uni 	BB166_29;

BB166_29:
	add.s32 	%r136, %r174, -2;
	sub.s32 	%r137, %r2, %r166;
	mul.lo.s32 	%r138, %r137, %r137;
	mad.lo.s32 	%r139, %r136, %r136, %r138;
	cvt.rn.f32.s32	%f36, %r139;
	sqrt.rn.f32 	%f37, %f36;
	cvt.rzi.s32.f32	%r140, %f37;
	setp.gt.s32	%p54, %r140, %r2;
	@%p54 bra 	BB166_31;

	ld.global.f32 	%f38, [%rd43+8];
	min.f32 	%f49, %f38, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_31:
	add.s32 	%r27, %r26, 1;
	setp.lt.s32	%p55, %r27, %r34;
	and.pred  	%p57, %p32, %p55;
	and.pred  	%p59, %p57, %p34;
	setp.gt.s32	%p60, %r27, -1;
	and.pred  	%p61, %p59, %p60;
	@!%p61 bra 	BB166_34;
	bra.uni 	BB166_32;

BB166_32:
	add.s32 	%r151, %r174, -3;
	sub.s32 	%r152, %r2, %r166;
	mul.lo.s32 	%r153, %r152, %r152;
	mad.lo.s32 	%r154, %r151, %r151, %r153;
	cvt.rn.f32.s32	%f39, %r154;
	sqrt.rn.f32 	%f40, %f39;
	cvt.rzi.s32.f32	%r155, %f40;
	setp.gt.s32	%p62, %r155, %r2;
	@%p62 bra 	BB166_34;

	ld.global.f32 	%f41, [%rd43+12];
	min.f32 	%f49, %f41, %f49;
	st.global.f32 	[%rd10], %f49;

BB166_34:
	add.s32 	%r174, %r174, -4;
	add.s64 	%rd43, %rd43, 16;
	add.s32 	%r172, %r172, 4;
	setp.lt.s32	%p63, %r172, %r35;
	add.s32 	%r173, %r27, 1;
	@%p63 bra 	BB166_22;

BB166_35:
	add.s32 	%r167, %r167, 1;
	add.s32 	%r166, %r166, 1;
	setp.lt.s32	%p64, %r166, %r35;
	@%p64 bra 	BB166_3;

BB166_36:
	ret;
}

	// .globl	vec_localMaximum
.visible .entry vec_localMaximum(
	.param .u32 vec_localMaximum_param_0,
	.param .u32 vec_localMaximum_param_1,
	.param .u32 vec_localMaximum_param_2,
	.param .u32 vec_localMaximum_param_3,
	.param .u64 vec_localMaximum_param_4,
	.param .u64 vec_localMaximum_param_5
)
{
	.reg .pred 	%p<65>;
	.reg .f32 	%f<54>;
	.reg .b32 	%r<177>;
	.reg .b64 	%rd<44>;


	ld.param.u32 	%r36, [vec_localMaximum_param_0];
	ld.param.u32 	%r33, [vec_localMaximum_param_1];
	ld.param.u32 	%r34, [vec_localMaximum_param_2];
	ld.param.u32 	%r35, [vec_localMaximum_param_3];
	ld.param.u64 	%rd4, [vec_localMaximum_param_4];
	ld.param.u64 	%rd5, [vec_localMaximum_param_5];
	mov.u32 	%r37, %ntid.y;
	mov.u32 	%r38, %ctaid.y;
	mov.u32 	%r39, %tid.y;
	mad.lo.s32 	%r40, %r37, %r38, %r39;
	mov.u32 	%r41, %nctaid.x;
	mov.u32 	%r42, %ctaid.x;
	mad.lo.s32 	%r43, %r40, %r41, %r42;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r43, %r44, %r45;
	setp.ge.s32	%p1, %r1, %r36;
	@%p1 bra 	BB167_36;

	cvta.to.global.u64 	%rd6, %rd5;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.f32 	%f49, [%rd9];
	add.s64 	%rd10, %rd7, %rd8;
	st.global.f32 	[%rd10], %f49;
	shr.u32 	%r46, %r35, 31;
	add.s32 	%r47, %r35, %r46;
	shr.s32 	%r2, %r47, 1;
	setp.lt.s32	%p2, %r35, 1;
	@%p2 bra 	BB167_36;

	rem.s32 	%r49, %r1, %r34;
	sub.s32 	%r3, %r49, %r2;
	div.s32 	%r50, %r1, %r34;
	sub.s32 	%r167, %r50, %r2;
	mov.u32 	%r48, 0;
	mov.u32 	%r166, %r48;

BB167_3:
	mul.lo.s32 	%r7, %r167, %r34;
	and.b32  	%r54, %r35, 3;
	setp.eq.s32	%p3, %r54, 0;
	@%p3 bra 	BB167_4;

	setp.eq.s32	%p4, %r54, 1;
	@%p4 bra 	BB167_6;
	bra.uni 	BB167_7;

BB167_6:
	mov.u32 	%r170, %r48;
	mov.u32 	%r171, %r3;
	bra.uni 	BB167_16;

BB167_4:
	mov.u32 	%r172, %r48;
	mov.u32 	%r173, %r3;
	bra.uni 	BB167_20;

BB167_7:
	setp.eq.s32	%p5, %r54, 2;
	@%p5 bra 	BB167_8;
	bra.uni 	BB167_9;

BB167_8:
	mov.u32 	%r168, %r48;
	mov.u32 	%r169, %r3;
	bra.uni 	BB167_12;

BB167_9:
	setp.gt.s32	%p6, %r167, -1;
	setp.lt.s32	%p7, %r3, %r34;
	setp.lt.s32	%p8, %r167, %r33;
	and.pred  	%p9, %p8, %p7;
	and.pred  	%p10, %p9, %p6;
	setp.gt.s32	%p11, %r3, -1;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r169, %r3, 1;
	mov.u32 	%r168, 1;
	@!%p12 bra 	BB167_12;
	bra.uni 	BB167_10;

BB167_10:
	sub.s32 	%r57, %r2, %r166;
	mul.lo.s32 	%r58, %r57, %r57;
	mad.lo.s32 	%r59, %r2, %r2, %r58;
	cvt.rn.f32.s32	%f21, %r59;
	sqrt.rn.f32 	%f22, %f21;
	cvt.rzi.s32.f32	%r60, %f22;
	setp.ge.s32	%p13, %r60, %r2;
	@%p13 bra 	BB167_12;

	add.s32 	%r62, %r3, %r7;
	mul.wide.s32 	%rd12, %r62, 4;
	add.s64 	%rd13, %rd6, %rd12;
	ld.global.f32 	%f23, [%rd13];
	max.f32 	%f49, %f23, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_12:
	setp.lt.s32	%p14, %r169, %r34;
	setp.lt.s32	%p15, %r167, %r33;
	and.pred  	%p16, %p15, %p14;
	setp.gt.s32	%p17, %r167, -1;
	and.pred  	%p18, %p16, %p17;
	setp.gt.s32	%p19, %r169, -1;
	and.pred  	%p20, %p18, %p19;
	@!%p20 bra 	BB167_15;
	bra.uni 	BB167_13;

BB167_13:
	sub.s32 	%r73, %r2, %r168;
	sub.s32 	%r74, %r2, %r166;
	mul.lo.s32 	%r75, %r74, %r74;
	mad.lo.s32 	%r76, %r73, %r73, %r75;
	cvt.rn.f32.s32	%f24, %r76;
	sqrt.rn.f32 	%f25, %f24;
	cvt.rzi.s32.f32	%r77, %f25;
	setp.ge.s32	%p21, %r77, %r2;
	@%p21 bra 	BB167_15;

	add.s32 	%r78, %r169, %r7;
	mul.wide.s32 	%rd18, %r78, 4;
	add.s64 	%rd19, %rd6, %rd18;
	ld.global.f32 	%f26, [%rd19];
	max.f32 	%f49, %f26, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_15:
	add.s32 	%r170, %r168, 1;
	add.s32 	%r171, %r169, 1;

BB167_16:
	setp.lt.s32	%p22, %r171, %r34;
	setp.lt.s32	%p23, %r167, %r33;
	and.pred  	%p24, %p23, %p22;
	setp.gt.s32	%p25, %r167, -1;
	and.pred  	%p26, %p24, %p25;
	setp.gt.s32	%p27, %r171, -1;
	and.pred  	%p28, %p26, %p27;
	@!%p28 bra 	BB167_19;
	bra.uni 	BB167_17;

BB167_17:
	sub.s32 	%r89, %r2, %r170;
	sub.s32 	%r90, %r2, %r166;
	mul.lo.s32 	%r91, %r90, %r90;
	mad.lo.s32 	%r92, %r89, %r89, %r91;
	cvt.rn.f32.s32	%f27, %r92;
	sqrt.rn.f32 	%f28, %f27;
	cvt.rzi.s32.f32	%r93, %f28;
	setp.ge.s32	%p29, %r93, %r2;
	@%p29 bra 	BB167_19;

	add.s32 	%r94, %r171, %r7;
	mul.wide.s32 	%rd24, %r94, 4;
	add.s64 	%rd25, %rd6, %rd24;
	ld.global.f32 	%f29, [%rd25];
	max.f32 	%f49, %f29, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_19:
	add.s32 	%r172, %r170, 1;
	add.s32 	%r173, %r171, 1;

BB167_20:
	setp.lt.u32	%p30, %r35, 4;
	@%p30 bra 	BB167_35;

	sub.s32 	%r174, %r2, %r172;
	mad.lo.s32 	%r105, %r34, %r167, %r173;
	mul.wide.s32 	%rd30, %r105, 4;
	add.s64 	%rd43, %rd6, %rd30;

BB167_22:
	setp.lt.s32	%p31, %r173, %r34;
	setp.lt.s32	%p32, %r167, %r33;
	and.pred  	%p33, %p32, %p31;
	setp.gt.s32	%p34, %r167, -1;
	and.pred  	%p35, %p33, %p34;
	setp.gt.s32	%p36, %r173, -1;
	and.pred  	%p37, %p35, %p36;
	@!%p37 bra 	BB167_25;
	bra.uni 	BB167_23;

BB167_23:
	sub.s32 	%r106, %r2, %r166;
	mul.lo.s32 	%r107, %r106, %r106;
	mad.lo.s32 	%r108, %r174, %r174, %r107;
	cvt.rn.f32.s32	%f30, %r108;
	sqrt.rn.f32 	%f31, %f30;
	cvt.rzi.s32.f32	%r109, %f31;
	setp.ge.s32	%p38, %r109, %r2;
	@%p38 bra 	BB167_25;

	ld.global.f32 	%f32, [%rd43];
	max.f32 	%f49, %f32, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_25:
	add.s32 	%r120, %r173, 1;
	setp.lt.s32	%p39, %r120, %r34;
	and.pred  	%p41, %p32, %p39;
	and.pred  	%p43, %p41, %p34;
	setp.gt.s32	%p44, %r173, -2;
	and.pred  	%p45, %p43, %p44;
	@!%p45 bra 	BB167_28;
	bra.uni 	BB167_26;

BB167_26:
	add.s32 	%r121, %r174, -1;
	sub.s32 	%r122, %r2, %r166;
	mul.lo.s32 	%r123, %r122, %r122;
	mad.lo.s32 	%r124, %r121, %r121, %r123;
	cvt.rn.f32.s32	%f33, %r124;
	sqrt.rn.f32 	%f34, %f33;
	cvt.rzi.s32.f32	%r125, %f34;
	setp.ge.s32	%p46, %r125, %r2;
	@%p46 bra 	BB167_28;

	ld.global.f32 	%f35, [%rd43+4];
	max.f32 	%f49, %f35, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_28:
	add.s32 	%r26, %r173, 2;
	setp.lt.s32	%p47, %r26, %r34;
	and.pred  	%p49, %p32, %p47;
	and.pred  	%p51, %p49, %p34;
	setp.gt.s32	%p52, %r26, -1;
	and.pred  	%p53, %p51, %p52;
	@!%p53 bra 	BB167_31;
	bra.uni 	BB167_29;

BB167_29:
	add.s32 	%r136, %r174, -2;
	sub.s32 	%r137, %r2, %r166;
	mul.lo.s32 	%r138, %r137, %r137;
	mad.lo.s32 	%r139, %r136, %r136, %r138;
	cvt.rn.f32.s32	%f36, %r139;
	sqrt.rn.f32 	%f37, %f36;
	cvt.rzi.s32.f32	%r140, %f37;
	setp.ge.s32	%p54, %r140, %r2;
	@%p54 bra 	BB167_31;

	ld.global.f32 	%f38, [%rd43+8];
	max.f32 	%f49, %f38, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_31:
	add.s32 	%r27, %r26, 1;
	setp.lt.s32	%p55, %r27, %r34;
	and.pred  	%p57, %p32, %p55;
	and.pred  	%p59, %p57, %p34;
	setp.gt.s32	%p60, %r27, -1;
	and.pred  	%p61, %p59, %p60;
	@!%p61 bra 	BB167_34;
	bra.uni 	BB167_32;

BB167_32:
	add.s32 	%r151, %r174, -3;
	sub.s32 	%r152, %r2, %r166;
	mul.lo.s32 	%r153, %r152, %r152;
	mad.lo.s32 	%r154, %r151, %r151, %r153;
	cvt.rn.f32.s32	%f39, %r154;
	sqrt.rn.f32 	%f40, %f39;
	cvt.rzi.s32.f32	%r155, %f40;
	setp.ge.s32	%p62, %r155, %r2;
	@%p62 bra 	BB167_34;

	ld.global.f32 	%f41, [%rd43+12];
	max.f32 	%f49, %f41, %f49;
	st.global.f32 	[%rd10], %f49;

BB167_34:
	add.s32 	%r174, %r174, -4;
	add.s64 	%rd43, %rd43, 16;
	add.s32 	%r172, %r172, 4;
	setp.lt.s32	%p63, %r172, %r35;
	add.s32 	%r173, %r27, 1;
	@%p63 bra 	BB167_22;

BB167_35:
	add.s32 	%r167, %r167, 1;
	add.s32 	%r166, %r166, 1;
	setp.lt.s32	%p64, %r166, %r35;
	@%p64 bra 	BB167_3;

BB167_36:
	ret;
}

	// .globl	vec_localMean
.visible .entry vec_localMean(
	.param .u32 vec_localMean_param_0,
	.param .u32 vec_localMean_param_1,
	.param .u32 vec_localMean_param_2,
	.param .u32 vec_localMean_param_3,
	.param .u64 vec_localMean_param_4,
	.param .u64 vec_localMean_param_5
)
{
	.reg .pred 	%p<65>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<117>;
	.reg .b64 	%rd<21>;


	ld.param.u32 	%r36, [vec_localMean_param_0];
	ld.param.u32 	%r33, [vec_localMean_param_1];
	ld.param.u32 	%r34, [vec_localMean_param_2];
	ld.param.u32 	%r35, [vec_localMean_param_3];
	ld.param.u64 	%rd4, [vec_localMean_param_4];
	ld.param.u64 	%rd5, [vec_localMean_param_5];
	mov.u32 	%r37, %tid.x;
	mov.u32 	%r38, %ntid.y;
	mov.u32 	%r39, %ctaid.y;
	mov.u32 	%r40, %tid.y;
	mad.lo.s32 	%r41, %r38, %r39, %r40;
	mov.u32 	%r42, %nctaid.x;
	mov.u32 	%r43, %ctaid.x;
	mad.lo.s32 	%r44, %r41, %r42, %r43;
	mov.u32 	%r45, %ntid.x;
	mad.lo.s32 	%r1, %r44, %r45, %r37;
	setp.ge.s32	%p1, %r1, %r36;
	@%p1 bra 	BB168_38;

	shr.u32 	%r46, %r35, 31;
	add.s32 	%r47, %r35, %r46;
	shr.s32 	%r2, %r47, 1;
	mov.f32 	%f88, 0f00000000;
	setp.lt.s32	%p2, %r35, 1;
	mov.f32 	%f87, %f88;
	@%p2 bra 	BB168_37;

	rem.s32 	%r49, %r1, %r34;
	sub.s32 	%r3, %r49, %r2;
	div.s32 	%r50, %r1, %r34;
	sub.s32 	%r107, %r50, %r2;
	mov.u32 	%r48, 0;
	mov.f32 	%f46, 0f00000000;
	mov.f32 	%f88, %f46;
	mov.f32 	%f87, %f46;
	mov.u32 	%r106, %r48;

BB168_3:
	mul.lo.s32 	%r7, %r107, %r34;
	and.b32  	%r54, %r35, 3;
	setp.eq.s32	%p3, %r54, 0;
	@%p3 bra 	BB168_4;

	setp.eq.s32	%p4, %r54, 1;
	@%p4 bra 	BB168_6;
	bra.uni 	BB168_7;

BB168_6:
	mov.f32 	%f73, %f88;
	mov.f32 	%f74, %f87;
	mov.u32 	%r110, %r48;
	mov.u32 	%r111, %r3;
	bra.uni 	BB168_17;

BB168_4:
	mov.f32 	%f73, %f88;
	mov.f32 	%f74, %f87;
	mov.u32 	%r115, %r48;
	mov.u32 	%r116, %r3;
	mov.f32 	%f88, %f46;
	mov.f32 	%f87, %f46;
	bra.uni 	BB168_21;

BB168_7:
	setp.eq.s32	%p5, %r54, 2;
	@%p5 bra 	BB168_8;
	bra.uni 	BB168_9;

BB168_8:
	mov.f32 	%f73, %f88;
	mov.f32 	%f74, %f87;
	mov.u32 	%r108, %r48;
	mov.u32 	%r109, %r3;
	bra.uni 	BB168_13;

BB168_9:
	setp.gt.s32	%p6, %r107, -1;
	setp.lt.s32	%p7, %r3, %r34;
	setp.lt.s32	%p8, %r107, %r33;
	and.pred  	%p9, %p8, %p7;
	and.pred  	%p10, %p9, %p6;
	setp.gt.s32	%p11, %r3, -1;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r109, %r3, 1;
	mov.u32 	%r108, 1;
	mov.f32 	%f73, %f88;
	mov.f32 	%f74, %f87;
	@!%p12 bra 	BB168_13;
	bra.uni 	BB168_10;

BB168_10:
	sub.s32 	%r57, %r2, %r106;
	mul.lo.s32 	%r58, %r57, %r57;
	mad.lo.s32 	%r59, %r2, %r2, %r58;
	cvt.rn.f32.s32	%f49, %r59;
	sqrt.rn.f32 	%f50, %f49;
	cvt.rzi.s32.f32	%r60, %f50;
	setp.gt.s32	%p13, %r60, %r2;
	@%p13 bra 	BB168_11;
	bra.uni 	BB168_12;

BB168_11:
	mov.f32 	%f73, %f88;
	mov.f32 	%f74, %f87;
	bra.uni 	BB168_13;

BB168_12:
	add.s32 	%r62, %r3, %r7;
	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r62, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f51, [%rd8];
	add.f32 	%f74, %f87, %f51;
	add.f32 	%f73, %f88, 0f3F800000;

BB168_13:
	setp.lt.s32	%p14, %r109, %r34;
	setp.lt.s32	%p15, %r107, %r33;
	and.pred  	%p16, %p15, %p14;
	setp.gt.s32	%p17, %r107, -1;
	and.pred  	%p18, %p16, %p17;
	setp.gt.s32	%p19, %r109, -1;
	and.pred  	%p20, %p18, %p19;
	@!%p20 bra 	BB168_16;
	bra.uni 	BB168_14;

BB168_14:
	sub.s32 	%r63, %r2, %r108;
	sub.s32 	%r64, %r2, %r106;
	mul.lo.s32 	%r65, %r64, %r64;
	mad.lo.s32 	%r66, %r63, %r63, %r65;
	cvt.rn.f32.s32	%f52, %r66;
	sqrt.rn.f32 	%f53, %f52;
	cvt.rzi.s32.f32	%r67, %f53;
	setp.gt.s32	%p21, %r67, %r2;
	@%p21 bra 	BB168_16;

	add.s32 	%r68, %r109, %r7;
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.s32 	%rd10, %r68, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f54, [%rd11];
	add.f32 	%f74, %f74, %f54;
	add.f32 	%f73, %f73, 0f3F800000;

BB168_16:
	add.s32 	%r110, %r108, 1;
	add.s32 	%r111, %r109, 1;

BB168_17:
	setp.lt.s32	%p22, %r111, %r34;
	setp.lt.s32	%p23, %r107, %r33;
	and.pred  	%p24, %p23, %p22;
	setp.gt.s32	%p25, %r107, -1;
	and.pred  	%p26, %p24, %p25;
	setp.gt.s32	%p27, %r111, -1;
	and.pred  	%p28, %p26, %p27;
	@!%p28 bra 	BB168_20;
	bra.uni 	BB168_18;

BB168_18:
	sub.s32 	%r69, %r2, %r110;
	sub.s32 	%r70, %r2, %r106;
	mul.lo.s32 	%r71, %r70, %r70;
	mad.lo.s32 	%r72, %r69, %r69, %r71;
	cvt.rn.f32.s32	%f55, %r72;
	sqrt.rn.f32 	%f56, %f55;
	cvt.rzi.s32.f32	%r73, %f56;
	setp.gt.s32	%p29, %r73, %r2;
	@%p29 bra 	BB168_20;

	add.s32 	%r74, %r111, %r7;
	cvta.to.global.u64 	%rd12, %rd5;
	mul.wide.s32 	%rd13, %r74, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f57, [%rd14];
	add.f32 	%f74, %f74, %f57;
	add.f32 	%f73, %f73, 0f3F800000;

BB168_20:
	add.s32 	%r115, %r110, 1;
	add.s32 	%r116, %r111, 1;
	mov.f32 	%f88, %f73;
	mov.f32 	%f87, %f74;

BB168_21:
	setp.lt.u32	%p30, %r35, 4;
	@%p30 bra 	BB168_36;

	sub.s32 	%r114, %r2, %r115;
	mad.lo.s32 	%r75, %r34, %r107, %r116;
	cvta.to.global.u64 	%rd15, %rd5;
	mul.wide.s32 	%rd16, %r75, 4;
	add.s64 	%rd20, %rd15, %rd16;
	mov.f32 	%f88, %f73;
	mov.f32 	%f87, %f74;

BB168_23:
	setp.lt.s32	%p31, %r116, %r34;
	setp.lt.s32	%p32, %r107, %r33;
	and.pred  	%p33, %p32, %p31;
	setp.gt.s32	%p34, %r107, -1;
	and.pred  	%p35, %p33, %p34;
	setp.gt.s32	%p36, %r116, -1;
	and.pred  	%p37, %p35, %p36;
	@!%p37 bra 	BB168_26;
	bra.uni 	BB168_24;

BB168_24:
	sub.s32 	%r76, %r2, %r106;
	mul.lo.s32 	%r77, %r76, %r76;
	mad.lo.s32 	%r78, %r114, %r114, %r77;
	cvt.rn.f32.s32	%f58, %r78;
	sqrt.rn.f32 	%f59, %f58;
	cvt.rzi.s32.f32	%r79, %f59;
	setp.gt.s32	%p38, %r79, %r2;
	@%p38 bra 	BB168_26;

	ld.global.f32 	%f60, [%rd20];
	add.f32 	%f87, %f87, %f60;
	add.f32 	%f88, %f88, 0f3F800000;

BB168_26:
	add.s32 	%r80, %r116, 1;
	setp.lt.s32	%p39, %r80, %r34;
	and.pred  	%p41, %p32, %p39;
	and.pred  	%p43, %p41, %p34;
	setp.gt.s32	%p44, %r116, -2;
	and.pred  	%p45, %p43, %p44;
	@!%p45 bra 	BB168_29;
	bra.uni 	BB168_27;

BB168_27:
	add.s32 	%r81, %r114, -1;
	sub.s32 	%r82, %r2, %r106;
	mul.lo.s32 	%r83, %r82, %r82;
	mad.lo.s32 	%r84, %r81, %r81, %r83;
	cvt.rn.f32.s32	%f61, %r84;
	sqrt.rn.f32 	%f62, %f61;
	cvt.rzi.s32.f32	%r85, %f62;
	setp.gt.s32	%p46, %r85, %r2;
	@%p46 bra 	BB168_29;

	ld.global.f32 	%f63, [%rd20+4];
	add.f32 	%f87, %f87, %f63;
	add.f32 	%f88, %f88, 0f3F800000;

BB168_29:
	add.s32 	%r26, %r116, 2;
	setp.lt.s32	%p47, %r26, %r34;
	and.pred  	%p49, %p32, %p47;
	and.pred  	%p51, %p49, %p34;
	setp.gt.s32	%p52, %r26, -1;
	and.pred  	%p53, %p51, %p52;
	@!%p53 bra 	BB168_32;
	bra.uni 	BB168_30;

BB168_30:
	add.s32 	%r86, %r114, -2;
	sub.s32 	%r87, %r2, %r106;
	mul.lo.s32 	%r88, %r87, %r87;
	mad.lo.s32 	%r89, %r86, %r86, %r88;
	cvt.rn.f32.s32	%f64, %r89;
	sqrt.rn.f32 	%f65, %f64;
	cvt.rzi.s32.f32	%r90, %f65;
	setp.gt.s32	%p54, %r90, %r2;
	@%p54 bra 	BB168_32;

	ld.global.f32 	%f66, [%rd20+8];
	add.f32 	%f87, %f87, %f66;
	add.f32 	%f88, %f88, 0f3F800000;

BB168_32:
	add.s32 	%r27, %r26, 1;
	setp.lt.s32	%p55, %r27, %r34;
	and.pred  	%p57, %p32, %p55;
	and.pred  	%p59, %p57, %p34;
	setp.gt.s32	%p60, %r27, -1;
	and.pred  	%p61, %p59, %p60;
	@!%p61 bra 	BB168_35;
	bra.uni 	BB168_33;

BB168_33:
	add.s32 	%r91, %r114, -3;
	sub.s32 	%r92, %r2, %r106;
	mul.lo.s32 	%r93, %r92, %r92;
	mad.lo.s32 	%r94, %r91, %r91, %r93;
	cvt.rn.f32.s32	%f67, %r94;
	sqrt.rn.f32 	%f68, %f67;
	cvt.rzi.s32.f32	%r95, %f68;
	setp.gt.s32	%p62, %r95, %r2;
	@%p62 bra 	BB168_35;

	ld.global.f32 	%f69, [%rd20+12];
	add.f32 	%f87, %f87, %f69;
	add.f32 	%f88, %f88, 0f3F800000;

BB168_35:
	add.s32 	%r114, %r114, -4;
	add.s64 	%rd20, %rd20, 16;
	add.s32 	%r115, %r115, 4;
	setp.lt.s32	%p63, %r115, %r35;
	add.s32 	%r116, %r27, 1;
	@%p63 bra 	BB168_23;

BB168_36:
	add.s32 	%r107, %r107, 1;
	add.s32 	%r106, %r106, 1;
	setp.lt.s32	%p64, %r106, %r35;
	@%p64 bra 	BB168_3;

BB168_37:
	cvta.to.global.u64 	%rd17, %rd4;
	mul.wide.s32 	%rd18, %r1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	div.rn.f32 	%f70, %f87, %f88;
	st.global.f32 	[%rd19], %f70;

BB168_38:
	ret;
}

	// .globl	vec_manualFiltering
.visible .entry vec_manualFiltering(
	.param .u32 vec_manualFiltering_param_0,
	.param .u32 vec_manualFiltering_param_1,
	.param .u32 vec_manualFiltering_param_2,
	.param .u32 vec_manualFiltering_param_3,
	.param .u64 vec_manualFiltering_param_4,
	.param .u64 vec_manualFiltering_param_5,
	.param .u64 vec_manualFiltering_param_6
)
{
	.reg .pred 	%p<58>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<190>;
	.reg .b64 	%rd<61>;


	ld.param.u32 	%r53, [vec_manualFiltering_param_0];
	ld.param.u32 	%r50, [vec_manualFiltering_param_1];
	ld.param.u32 	%r51, [vec_manualFiltering_param_2];
	ld.param.u32 	%r52, [vec_manualFiltering_param_3];
	ld.param.u64 	%rd7, [vec_manualFiltering_param_4];
	ld.param.u64 	%rd8, [vec_manualFiltering_param_5];
	ld.param.u64 	%rd9, [vec_manualFiltering_param_6];
	mov.u32 	%r54, %tid.x;
	mov.u32 	%r55, %ntid.y;
	mov.u32 	%r56, %ctaid.y;
	mov.u32 	%r57, %tid.y;
	mad.lo.s32 	%r58, %r55, %r56, %r57;
	mov.u32 	%r59, %nctaid.x;
	mov.u32 	%r60, %ctaid.x;
	mad.lo.s32 	%r61, %r58, %r59, %r60;
	mov.u32 	%r62, %ntid.x;
	mad.lo.s32 	%r1, %r61, %r62, %r54;
	setp.ge.s32	%p1, %r1, %r53;
	@%p1 bra 	BB169_31;

	cvta.to.global.u64 	%rd10, %rd7;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.u32 	%r184, 0;
	st.global.u32 	[%rd12], %r184;
	mov.f32 	%f46, 0f00000000;
	setp.lt.s32	%p2, %r52, 1;
	@%p2 bra 	BB169_30;

	shr.u32 	%r66, %r52, 31;
	add.s32 	%r67, %r52, %r66;
	shr.s32 	%r68, %r67, 1;
	rem.s32 	%r69, %r1, %r51;
	sub.s32 	%r2, %r69, %r68;
	div.s32 	%r70, %r1, %r51;
	sub.s32 	%r167, %r70, %r68;
	mov.u32 	%r65, 0;
	mov.f32 	%f46, 0f00000000;
	mov.u32 	%r166, %r65;
	mov.u32 	%r184, %r65;

BB169_3:
	mul.lo.s32 	%r7, %r167, %r51;
	mul.lo.s32 	%r8, %r166, %r52;
	and.b32  	%r75, %r52, 3;
	setp.eq.s32	%p3, %r75, 0;
	@%p3 bra 	BB169_4;

	setp.eq.s32	%p4, %r75, 1;
	@%p4 bra 	BB169_6;
	bra.uni 	BB169_7;

BB169_6:
	mov.u32 	%r173, %r65;
	mov.u32 	%r174, %r2;
	bra.uni 	BB169_14;

BB169_4:
	mov.u32 	%r177, %r65;
	mov.u32 	%r178, %r2;
	mov.u32 	%r180, %r65;
	bra.uni 	BB169_17;

BB169_7:
	setp.eq.s32	%p5, %r75, 2;
	@%p5 bra 	BB169_8;
	bra.uni 	BB169_9;

BB169_8:
	mov.u32 	%r169, %r65;
	mov.u32 	%r170, %r2;
	bra.uni 	BB169_11;

BB169_9:
	setp.gt.s32	%p6, %r167, -1;
	setp.lt.s32	%p7, %r2, %r51;
	setp.lt.s32	%p8, %r167, %r50;
	and.pred  	%p9, %p8, %p7;
	and.pred  	%p10, %p9, %p6;
	setp.gt.s32	%p11, %r2, -1;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r170, %r2, 1;
	mov.u32 	%r169, 1;
	@!%p12 bra 	BB169_11;
	bra.uni 	BB169_10;

BB169_10:
	add.s32 	%r78, %r2, %r7;
	cvta.to.global.u64 	%rd13, %rd8;
	mul.wide.s32 	%rd14, %r78, 4;
	add.s64 	%rd15, %rd13, %rd14;
	cvta.to.global.u64 	%rd16, %rd9;
	mul.wide.s32 	%rd17, %r8, 4;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.f32 	%f23, [%rd18];
	ld.global.f32 	%f24, [%rd15];
	fma.rn.f32 	%f46, %f24, %f23, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_11:
	setp.lt.s32	%p13, %r170, %r51;
	setp.lt.s32	%p14, %r167, %r50;
	and.pred  	%p15, %p14, %p13;
	setp.gt.s32	%p16, %r167, -1;
	and.pred  	%p17, %p15, %p16;
	setp.gt.s32	%p18, %r170, -1;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB169_13;
	bra.uni 	BB169_12;

BB169_12:
	add.s32 	%r89, %r170, %r7;
	cvta.to.global.u64 	%rd22, %rd8;
	mul.wide.s32 	%rd23, %r89, 4;
	add.s64 	%rd24, %rd22, %rd23;
	add.s32 	%r90, %r169, %r8;
	cvta.to.global.u64 	%rd25, %rd9;
	mul.wide.s32 	%rd26, %r90, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.f32 	%f25, [%rd27];
	ld.global.f32 	%f26, [%rd24];
	fma.rn.f32 	%f46, %f26, %f25, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_13:
	add.s32 	%r173, %r169, 1;
	add.s32 	%r174, %r170, 1;

BB169_14:
	setp.lt.s32	%p20, %r174, %r51;
	setp.lt.s32	%p21, %r167, %r50;
	and.pred  	%p22, %p21, %p20;
	setp.gt.s32	%p23, %r167, -1;
	and.pred  	%p24, %p22, %p23;
	setp.gt.s32	%p25, %r174, -1;
	and.pred  	%p26, %p24, %p25;
	@!%p26 bra 	BB169_16;
	bra.uni 	BB169_15;

BB169_15:
	add.s32 	%r101, %r174, %r7;
	cvta.to.global.u64 	%rd31, %rd8;
	mul.wide.s32 	%rd32, %r101, 4;
	add.s64 	%rd33, %rd31, %rd32;
	add.s32 	%r102, %r173, %r8;
	cvta.to.global.u64 	%rd34, %rd9;
	mul.wide.s32 	%rd35, %r102, 4;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.f32 	%f27, [%rd36];
	ld.global.f32 	%f28, [%rd33];
	fma.rn.f32 	%f46, %f28, %f27, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_16:
	add.s32 	%r177, %r173, 1;
	add.s32 	%r178, %r174, 1;
	mov.u32 	%r180, %r184;

BB169_17:
	setp.lt.u32	%p27, %r52, 4;
	@%p27 bra 	BB169_18;
	bra.uni 	BB169_19;

BB169_18:
	mov.u32 	%r184, %r180;
	bra.uni 	BB169_29;

BB169_19:
	mad.lo.s32 	%r113, %r52, %r166, %r177;
	cvta.to.global.u64 	%rd40, %rd9;
	mul.wide.s32 	%rd41, %r113, 4;
	add.s64 	%rd60, %rd40, %rd41;
	mad.lo.s32 	%r114, %r51, %r167, %r178;
	cvta.to.global.u64 	%rd42, %rd8;
	mul.wide.s32 	%rd43, %r114, 4;
	add.s64 	%rd59, %rd42, %rd43;

BB169_20:
	setp.lt.s32	%p28, %r178, %r51;
	setp.lt.s32	%p29, %r167, %r50;
	and.pred  	%p30, %p29, %p28;
	setp.gt.s32	%p31, %r167, -1;
	and.pred  	%p32, %p30, %p31;
	setp.gt.s32	%p33, %r178, -1;
	and.pred  	%p34, %p32, %p33;
	@!%p34 bra 	BB169_22;
	bra.uni 	BB169_21;

BB169_21:
	ld.global.f32 	%f29, [%rd59];
	ld.global.f32 	%f30, [%rd60];
	fma.rn.f32 	%f46, %f29, %f30, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_22:
	add.s32 	%r125, %r178, 1;
	setp.lt.s32	%p35, %r125, %r51;
	and.pred  	%p37, %p29, %p35;
	and.pred  	%p39, %p37, %p31;
	setp.gt.s32	%p40, %r178, -2;
	and.pred  	%p41, %p39, %p40;
	@!%p41 bra 	BB169_24;
	bra.uni 	BB169_23;

BB169_23:
	ld.global.f32 	%f31, [%rd60+4];
	ld.global.f32 	%f32, [%rd59+4];
	fma.rn.f32 	%f46, %f32, %f31, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_24:
	add.s32 	%r38, %r178, 2;
	setp.lt.s32	%p42, %r38, %r51;
	and.pred  	%p44, %p29, %p42;
	and.pred  	%p46, %p44, %p31;
	setp.gt.s32	%p47, %r38, -1;
	and.pred  	%p48, %p46, %p47;
	@!%p48 bra 	BB169_26;
	bra.uni 	BB169_25;

BB169_25:
	ld.global.f32 	%f33, [%rd60+8];
	ld.global.f32 	%f34, [%rd59+8];
	fma.rn.f32 	%f46, %f34, %f33, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_26:
	add.s32 	%r41, %r38, 1;
	setp.lt.s32	%p49, %r41, %r51;
	and.pred  	%p51, %p29, %p49;
	and.pred  	%p53, %p51, %p31;
	setp.gt.s32	%p54, %r41, -1;
	and.pred  	%p55, %p53, %p54;
	@!%p55 bra 	BB169_28;
	bra.uni 	BB169_27;

BB169_27:
	ld.global.f32 	%f35, [%rd60+12];
	ld.global.f32 	%f36, [%rd59+12];
	fma.rn.f32 	%f46, %f36, %f35, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r184, %r184, 1;

BB169_28:
	add.s64 	%rd60, %rd60, 16;
	add.s64 	%rd59, %rd59, 16;
	add.s32 	%r177, %r177, 4;
	setp.lt.s32	%p56, %r177, %r52;
	add.s32 	%r178, %r41, 1;
	@%p56 bra 	BB169_20;

BB169_29:
	add.s32 	%r167, %r167, 1;
	add.s32 	%r166, %r166, 1;
	setp.lt.s32	%p57, %r166, %r52;
	@%p57 bra 	BB169_3;

BB169_30:
	cvt.rn.f32.s32	%f37, %r184;
	div.rn.f32 	%f38, %f46, %f37;
	st.global.f32 	[%rd12], %f38;

BB169_31:
	ret;
}

	// .globl	vec_manualFilteringStacked
.visible .entry vec_manualFilteringStacked(
	.param .u32 vec_manualFilteringStacked_param_0,
	.param .u32 vec_manualFilteringStacked_param_1,
	.param .u32 vec_manualFilteringStacked_param_2,
	.param .u32 vec_manualFilteringStacked_param_3,
	.param .u32 vec_manualFilteringStacked_param_4,
	.param .u64 vec_manualFilteringStacked_param_5,
	.param .u64 vec_manualFilteringStacked_param_6,
	.param .u64 vec_manualFilteringStacked_param_7
)
{
	.reg .pred 	%p<58>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<197>;
	.reg .b64 	%rd<61>;


	ld.param.u32 	%r57, [vec_manualFilteringStacked_param_0];
	ld.param.u32 	%r53, [vec_manualFilteringStacked_param_1];
	ld.param.u32 	%r54, [vec_manualFilteringStacked_param_2];
	ld.param.u32 	%r55, [vec_manualFilteringStacked_param_3];
	ld.param.u32 	%r56, [vec_manualFilteringStacked_param_4];
	ld.param.u64 	%rd7, [vec_manualFilteringStacked_param_5];
	ld.param.u64 	%rd8, [vec_manualFilteringStacked_param_6];
	ld.param.u64 	%rd9, [vec_manualFilteringStacked_param_7];
	mov.u32 	%r58, %tid.x;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r61, %tid.y;
	mad.lo.s32 	%r62, %r59, %r60, %r61;
	mov.u32 	%r63, %nctaid.x;
	mov.u32 	%r64, %ctaid.x;
	mad.lo.s32 	%r65, %r62, %r63, %r64;
	mov.u32 	%r66, %ntid.x;
	mad.lo.s32 	%r1, %r65, %r66, %r58;
	div.s32 	%r2, %r1, %r56;
	setp.ge.s32	%p1, %r2, %r57;
	@%p1 bra 	BB170_31;

	cvta.to.global.u64 	%rd10, %rd7;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.u32 	%r191, 0;
	st.global.u32 	[%rd12], %r191;
	mov.f32 	%f46, 0f00000000;
	setp.lt.s32	%p2, %r55, 1;
	@%p2 bra 	BB170_30;

	rem.s32 	%r70, %r1, %r56;
	shr.u32 	%r71, %r55, 31;
	add.s32 	%r72, %r55, %r71;
	shr.s32 	%r73, %r72, 1;
	rem.s32 	%r74, %r2, %r54;
	sub.s32 	%r3, %r74, %r73;
	mul.lo.s32 	%r5, %r70, %r53;
	div.s32 	%r75, %r2, %r54;
	sub.s32 	%r174, %r75, %r73;
	mov.u32 	%r69, 0;
	mov.f32 	%f46, 0f00000000;
	mov.u32 	%r173, %r69;
	mov.u32 	%r191, %r69;

BB170_3:
	add.s32 	%r81, %r174, %r5;
	mul.lo.s32 	%r10, %r81, %r54;
	mul.lo.s32 	%r11, %r173, %r55;
	and.b32  	%r80, %r55, 3;
	setp.eq.s32	%p3, %r80, 0;
	@%p3 bra 	BB170_4;

	setp.eq.s32	%p4, %r80, 1;
	@%p4 bra 	BB170_6;
	bra.uni 	BB170_7;

BB170_6:
	mov.u32 	%r180, %r69;
	mov.u32 	%r181, %r3;
	bra.uni 	BB170_14;

BB170_4:
	mov.u32 	%r184, %r69;
	mov.u32 	%r185, %r3;
	mov.u32 	%r187, %r69;
	bra.uni 	BB170_17;

BB170_7:
	setp.eq.s32	%p5, %r80, 2;
	@%p5 bra 	BB170_8;
	bra.uni 	BB170_9;

BB170_8:
	mov.u32 	%r176, %r69;
	mov.u32 	%r177, %r3;
	bra.uni 	BB170_11;

BB170_9:
	setp.gt.s32	%p6, %r174, -1;
	setp.lt.s32	%p7, %r3, %r54;
	setp.lt.s32	%p8, %r174, %r53;
	and.pred  	%p9, %p8, %p7;
	and.pred  	%p10, %p9, %p6;
	setp.gt.s32	%p11, %r3, -1;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r177, %r3, 1;
	mov.u32 	%r176, 1;
	@!%p12 bra 	BB170_11;
	bra.uni 	BB170_10;

BB170_10:
	add.s32 	%r84, %r10, %r3;
	cvta.to.global.u64 	%rd13, %rd8;
	mul.wide.s32 	%rd14, %r84, 4;
	add.s64 	%rd15, %rd13, %rd14;
	cvta.to.global.u64 	%rd16, %rd9;
	mul.wide.s32 	%rd17, %r11, 4;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.f32 	%f23, [%rd18];
	ld.global.f32 	%f24, [%rd15];
	fma.rn.f32 	%f46, %f24, %f23, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_11:
	setp.lt.s32	%p13, %r177, %r54;
	setp.lt.s32	%p14, %r174, %r53;
	and.pred  	%p15, %p14, %p13;
	setp.gt.s32	%p16, %r174, -1;
	and.pred  	%p17, %p15, %p16;
	setp.gt.s32	%p18, %r177, -1;
	and.pred  	%p19, %p17, %p18;
	@!%p19 bra 	BB170_13;
	bra.uni 	BB170_12;

BB170_12:
	add.s32 	%r95, %r10, %r177;
	cvta.to.global.u64 	%rd22, %rd8;
	mul.wide.s32 	%rd23, %r95, 4;
	add.s64 	%rd24, %rd22, %rd23;
	add.s32 	%r96, %r176, %r11;
	cvta.to.global.u64 	%rd25, %rd9;
	mul.wide.s32 	%rd26, %r96, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.f32 	%f25, [%rd27];
	ld.global.f32 	%f26, [%rd24];
	fma.rn.f32 	%f46, %f26, %f25, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_13:
	add.s32 	%r180, %r176, 1;
	add.s32 	%r181, %r177, 1;

BB170_14:
	setp.lt.s32	%p20, %r181, %r54;
	setp.lt.s32	%p21, %r174, %r53;
	and.pred  	%p22, %p21, %p20;
	setp.gt.s32	%p23, %r174, -1;
	and.pred  	%p24, %p22, %p23;
	setp.gt.s32	%p25, %r181, -1;
	and.pred  	%p26, %p24, %p25;
	@!%p26 bra 	BB170_16;
	bra.uni 	BB170_15;

BB170_15:
	add.s32 	%r107, %r10, %r181;
	cvta.to.global.u64 	%rd31, %rd8;
	mul.wide.s32 	%rd32, %r107, 4;
	add.s64 	%rd33, %rd31, %rd32;
	add.s32 	%r108, %r180, %r11;
	cvta.to.global.u64 	%rd34, %rd9;
	mul.wide.s32 	%rd35, %r108, 4;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.f32 	%f27, [%rd36];
	ld.global.f32 	%f28, [%rd33];
	fma.rn.f32 	%f46, %f28, %f27, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_16:
	add.s32 	%r184, %r180, 1;
	add.s32 	%r185, %r181, 1;
	mov.u32 	%r187, %r191;

BB170_17:
	setp.lt.u32	%p27, %r55, 4;
	@%p27 bra 	BB170_18;
	bra.uni 	BB170_19;

BB170_18:
	mov.u32 	%r191, %r187;
	bra.uni 	BB170_29;

BB170_19:
	mad.lo.s32 	%r119, %r55, %r173, %r184;
	cvta.to.global.u64 	%rd40, %rd9;
	mul.wide.s32 	%rd41, %r119, 4;
	add.s64 	%rd60, %rd40, %rd41;
	add.s32 	%r120, %r5, %r174;
	mad.lo.s32 	%r121, %r54, %r120, %r185;
	cvta.to.global.u64 	%rd42, %rd8;
	mul.wide.s32 	%rd43, %r121, 4;
	add.s64 	%rd59, %rd42, %rd43;

BB170_20:
	setp.lt.s32	%p28, %r185, %r54;
	setp.lt.s32	%p29, %r174, %r53;
	and.pred  	%p30, %p29, %p28;
	setp.gt.s32	%p31, %r174, -1;
	and.pred  	%p32, %p30, %p31;
	setp.gt.s32	%p33, %r185, -1;
	and.pred  	%p34, %p32, %p33;
	@!%p34 bra 	BB170_22;
	bra.uni 	BB170_21;

BB170_21:
	ld.global.f32 	%f29, [%rd59];
	ld.global.f32 	%f30, [%rd60];
	fma.rn.f32 	%f46, %f29, %f30, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_22:
	add.s32 	%r132, %r185, 1;
	setp.lt.s32	%p35, %r132, %r54;
	and.pred  	%p37, %p29, %p35;
	and.pred  	%p39, %p37, %p31;
	setp.gt.s32	%p40, %r185, -2;
	and.pred  	%p41, %p39, %p40;
	@!%p41 bra 	BB170_24;
	bra.uni 	BB170_23;

BB170_23:
	ld.global.f32 	%f31, [%rd60+4];
	ld.global.f32 	%f32, [%rd59+4];
	fma.rn.f32 	%f46, %f32, %f31, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_24:
	add.s32 	%r41, %r185, 2;
	setp.lt.s32	%p42, %r41, %r54;
	and.pred  	%p44, %p29, %p42;
	and.pred  	%p46, %p44, %p31;
	setp.gt.s32	%p47, %r41, -1;
	and.pred  	%p48, %p46, %p47;
	@!%p48 bra 	BB170_26;
	bra.uni 	BB170_25;

BB170_25:
	ld.global.f32 	%f33, [%rd60+8];
	ld.global.f32 	%f34, [%rd59+8];
	fma.rn.f32 	%f46, %f34, %f33, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_26:
	add.s32 	%r44, %r41, 1;
	setp.lt.s32	%p49, %r44, %r54;
	and.pred  	%p51, %p29, %p49;
	and.pred  	%p53, %p51, %p31;
	setp.gt.s32	%p54, %r44, -1;
	and.pred  	%p55, %p53, %p54;
	@!%p55 bra 	BB170_28;
	bra.uni 	BB170_27;

BB170_27:
	ld.global.f32 	%f35, [%rd60+12];
	ld.global.f32 	%f36, [%rd59+12];
	fma.rn.f32 	%f46, %f36, %f35, %f46;
	st.global.f32 	[%rd12], %f46;
	add.s32 	%r191, %r191, 1;

BB170_28:
	add.s64 	%rd60, %rd60, 16;
	add.s64 	%rd59, %rd59, 16;
	add.s32 	%r184, %r184, 4;
	setp.lt.s32	%p56, %r184, %r55;
	add.s32 	%r185, %r44, 1;
	@%p56 bra 	BB170_20;

BB170_29:
	add.s32 	%r174, %r174, 1;
	add.s32 	%r173, %r173, 1;
	setp.lt.s32	%p57, %r173, %r55;
	@%p57 bra 	BB170_3;

BB170_30:
	cvt.rn.f32.s32	%f37, %r191;
	div.rn.f32 	%f38, %f46, %f37;
	st.global.f32 	[%rd12], %f38;

BB170_31:
	ret;
}

	// .globl	vec_manualFilteringFast
.visible .entry vec_manualFilteringFast(
	.param .u64 vec_manualFilteringFast_param_0,
	.param .u64 vec_manualFilteringFast_param_1,
	.param .u64 vec_manualFilteringFast_param_2,
	.param .u64 vec_manualFilteringFast_param_3,
	.param .u64 vec_manualFilteringFast_param_4,
	.param .u64 vec_manualFilteringFast_param_5,
	.param .u64 vec_manualFilteringFast_param_6
)
{
	.local .align 16 .b8 	__local_depot171[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot171;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd20, [vec_manualFilteringFast_param_0];
	ld.param.u64 	%rd21, [vec_manualFilteringFast_param_1];
	ld.param.u64 	%rd22, [vec_manualFilteringFast_param_2];
	ld.param.u64 	%rd23, [vec_manualFilteringFast_param_3];
	ld.param.u64 	%rd24, [vec_manualFilteringFast_param_4];
	ld.param.u64 	%rd25, [vec_manualFilteringFast_param_5];
	ld.param.u64 	%rd26, [vec_manualFilteringFast_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32	%rd27, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd28, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd29, %r9, %r1;
	mul.lo.s64 	%rd30, %rd29, %rd28;
	add.s64 	%rd1, %rd30, %rd27;
	mul.lo.s64 	%rd2, %rd23, %rd23;
	or.b64  	%rd31, %rd1, %rd2;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64	%p1, %rd32, 0;
	@%p1 bra 	BB171_2;

	div.u64 	%rd80, %rd1, %rd2;
	rem.u64 	%rd81, %rd1, %rd2;
	bra.uni 	BB171_3;

BB171_2:
	cvt.u32.u64	%r10, %rd2;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd80, %r12;
	cvt.u64.u32	%rd81, %r13;

BB171_3:
	div.s64 	%rd9, %rd80, %rd22;
	or.b64  	%rd33, %rd80, %rd22;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64	%p2, %rd34, 0;
	@%p2 bra 	BB171_5;

	rem.s64 	%rd82, %rd80, %rd22;
	bra.uni 	BB171_6;

BB171_5:
	cvt.u32.u64	%r14, %rd22;
	cvt.u32.u64	%r15, %rd80;
	rem.u32 	%r16, %r15, %r14;
	cvt.u64.u32	%rd82, %r16;

BB171_6:
	cvt.s64.s32 	%rd35, %rd81;
	or.b64  	%rd36, %rd35, %rd23;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64	%p3, %rd37, 0;
	@%p3 bra 	BB171_8;

	div.s64 	%rd83, %rd35, %rd23;
	rem.s64 	%rd84, %rd35, %rd23;
	bra.uni 	BB171_9;

BB171_8:
	cvt.u32.u64	%r17, %rd23;
	cvt.u32.u64	%r18, %rd81;
	div.u32 	%r19, %r18, %r17;
	rem.u32 	%r20, %r18, %r17;
	cvt.u64.u32	%rd83, %r19;
	cvt.u64.u32	%rd84, %r20;

BB171_9:
	setp.ge.s64	%p4, %rd1, %rd20;
	@%p4 bra 	BB171_12;

	and.b64  	%rd44, %rd83, 4294967295;
	add.s64 	%rd45, %rd44, %rd9;
	shr.u64 	%rd46, %rd23, 63;
	add.s64 	%rd47, %rd23, %rd46;
	shr.s64 	%rd48, %rd47, 1;
	sub.s64 	%rd49, %rd45, %rd48;
	cvt.u32.u64	%r30, %rd49;
	and.b64  	%rd50, %rd84, 4294967295;
	add.s64 	%rd51, %rd50, %rd82;
	sub.s64 	%rd52, %rd51, %rd48;
	cvt.u32.u64	%r31, %rd52;
	cvt.s64.s32 	%rd53, %rd49;
	mul.lo.s64 	%rd54, %rd53, %rd22;
	cvt.s64.s32 	%rd55, %rd52;
	add.s64 	%rd19, %rd54, %rd55;
	setp.lt.s64	%p5, %rd53, %rd21;
	setp.lt.s64	%p6, %rd55, %rd22;
	and.pred  	%p7, %p5, %p6;
	setp.gt.s32	%p8, %r30, -1;
	and.pred  	%p9, %p7, %p8;
	setp.gt.s32	%p10, %r31, -1;
	and.pred  	%p11, %p9, %p10;
	@!%p11 bra 	BB171_12;
	bra.uni 	BB171_11;

BB171_11:
	cvta.to.global.u64 	%rd56, %rd25;
	shl.b64 	%rd57, %rd19, 2;
	add.s64 	%rd58, %rd56, %rd57;
	ld.global.f32 	%f1, [%rd58];
	cvta.to.global.u64 	%rd59, %rd26;
	shl.b64 	%rd60, %rd81, 2;
	add.s64 	%rd61, %rd59, %rd60;
	ld.global.f32 	%f2, [%rd61];
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd68, %SPL, 0;
	st.local.v2.u64 	[%rd68], {%rd1, %rd80};
	st.local.u64 	[%rd68+16], %rd81;
	cvt.u32.u64	%r41, %rd84;
	cvt.u32.u64	%r42, %rd83;
	st.local.v2.u64 	[%rd68+32], {%rd19, %rd20};
	cvt.f64.f32	%fd1, %f2;
	cvt.f64.f32	%fd2, %f1;
	st.local.v2.f64 	[%rd68+48], {%fd2, %fd1};
	st.local.v2.u32 	[%rd68+24], {%r42, %r41};
	mov.u64 	%rd69, $str4;
	cvta.global.u64 	%rd70, %rd69;
	// Callseq Start 30
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd67;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r43, [retval0+0];
	
	//{
	}// Callseq End 30
	add.s64 	%rd72, %rd2, -1;
	sub.s64 	%rd74, %rd72, %rd35;
	shl.b64 	%rd75, %rd74, 2;
	add.s64 	%rd76, %rd59, %rd75;
	ld.global.f32 	%f3, [%rd76];
	ld.global.f32 	%f4, [%rd58];
	mul.f32 	%f5, %f4, %f3;
	cvta.to.global.u64 	%rd77, %rd24;
	shl.b64 	%rd78, %rd1, 2;
	add.s64 	%rd79, %rd77, %rd78;
	st.global.f32 	[%rd79], %f5;

BB171_12:
	ret;
}

	// .globl	vec_manualFilteringStackedFast
.visible .entry vec_manualFilteringStackedFast(
	.param .u64 vec_manualFilteringStackedFast_param_0,
	.param .u64 vec_manualFilteringStackedFast_param_1,
	.param .u64 vec_manualFilteringStackedFast_param_2,
	.param .u64 vec_manualFilteringStackedFast_param_3,
	.param .u64 vec_manualFilteringStackedFast_param_4,
	.param .u64 vec_manualFilteringStackedFast_param_5,
	.param .u64 vec_manualFilteringStackedFast_param_6,
	.param .u64 vec_manualFilteringStackedFast_param_7
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd27, [vec_manualFilteringStackedFast_param_0];
	ld.param.u64 	%rd28, [vec_manualFilteringStackedFast_param_1];
	ld.param.u64 	%rd29, [vec_manualFilteringStackedFast_param_2];
	ld.param.u64 	%rd30, [vec_manualFilteringStackedFast_param_3];
	ld.param.u64 	%rd31, [vec_manualFilteringStackedFast_param_4];
	ld.param.u64 	%rd32, [vec_manualFilteringStackedFast_param_5];
	ld.param.u64 	%rd33, [vec_manualFilteringStackedFast_param_6];
	ld.param.u64 	%rd34, [vec_manualFilteringStackedFast_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32	%rd35, %r4;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	cvt.u64.u32	%rd36, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.wide.u32 	%rd37, %r9, %r1;
	mul.lo.s64 	%rd38, %rd37, %rd36;
	add.s64 	%rd1, %rd38, %rd35;
	or.b64  	%rd39, %rd1, %rd31;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64	%p1, %rd40, 0;
	@%p1 bra 	BB172_2;

	div.s64 	%rd72, %rd1, %rd31;
	rem.s64 	%rd73, %rd1, %rd31;
	bra.uni 	BB172_3;

BB172_2:
	cvt.u32.u64	%r10, %rd31;
	cvt.u32.u64	%r11, %rd1;
	div.u32 	%r12, %r11, %r10;
	rem.u32 	%r13, %r11, %r10;
	cvt.u64.u32	%rd72, %r12;
	cvt.u64.u32	%rd73, %r13;

BB172_3:
	mul.lo.s64 	%rd8, %rd30, %rd30;
	or.b64  	%rd41, %rd72, %rd8;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64	%p2, %rd42, 0;
	@%p2 bra 	BB172_5;

	div.s64 	%rd74, %rd72, %rd8;
	rem.s64 	%rd75, %rd72, %rd8;
	bra.uni 	BB172_6;

BB172_5:
	cvt.u32.u64	%r14, %rd8;
	cvt.u32.u64	%r15, %rd72;
	div.u32 	%r16, %r15, %r14;
	rem.u32 	%r17, %r15, %r14;
	cvt.u64.u32	%rd74, %r16;
	cvt.u64.u32	%rd75, %r17;

BB172_6:
	div.s64 	%rd15, %rd74, %rd29;
	or.b64  	%rd43, %rd74, %rd29;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64	%p3, %rd44, 0;
	@%p3 bra 	BB172_8;

	rem.s64 	%rd76, %rd74, %rd29;
	bra.uni 	BB172_9;

BB172_8:
	cvt.u32.u64	%r18, %rd29;
	cvt.u32.u64	%r19, %rd74;
	rem.u32 	%r20, %r19, %r18;
	cvt.u64.u32	%rd76, %r20;

BB172_9:
	cvt.s64.s32 	%rd19, %rd75;
	or.b64  	%rd45, %rd19, %rd30;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64	%p4, %rd46, 0;
	@%p4 bra 	BB172_11;

	div.s64 	%rd77, %rd19, %rd30;
	rem.s64 	%rd78, %rd19, %rd30;
	bra.uni 	BB172_12;

BB172_11:
	cvt.u32.u64	%r21, %rd30;
	cvt.u32.u64	%r22, %rd19;
	div.u32 	%r23, %r22, %r21;
	rem.u32 	%r24, %r22, %r21;
	cvt.u64.u32	%rd77, %r23;
	cvt.u64.u32	%rd78, %r24;

BB172_12:
	setp.ge.s64	%p5, %rd72, %rd27;
	@%p5 bra 	BB172_15;

	and.b64  	%rd47, %rd77, 4294967295;
	add.s64 	%rd48, %rd47, %rd15;
	shr.u64 	%rd49, %rd30, 63;
	add.s64 	%rd50, %rd30, %rd49;
	shr.s64 	%rd51, %rd50, 1;
	sub.s64 	%rd52, %rd48, %rd51;
	cvt.u32.u64	%r25, %rd52;
	and.b64  	%rd53, %rd78, 4294967295;
	add.s64 	%rd54, %rd53, %rd76;
	sub.s64 	%rd55, %rd54, %rd51;
	cvt.u32.u64	%r26, %rd55;
	cvt.s64.s32 	%rd56, %rd52;
	mul.lo.s64 	%rd57, %rd73, %rd28;
	add.s64 	%rd58, %rd56, %rd57;
	mul.lo.s64 	%rd59, %rd58, %rd29;
	cvt.s64.s32 	%rd60, %rd55;
	add.s64 	%rd26, %rd59, %rd60;
	setp.lt.s64	%p6, %rd56, %rd28;
	setp.lt.s64	%p7, %rd60, %rd29;
	and.pred  	%p8, %p6, %p7;
	setp.gt.s32	%p9, %r25, -1;
	and.pred  	%p10, %p8, %p9;
	setp.gt.s32	%p11, %r26, -1;
	and.pred  	%p12, %p10, %p11;
	@!%p12 bra 	BB172_15;
	bra.uni 	BB172_14;

BB172_14:
	cvta.to.global.u64 	%rd61, %rd33;
	shl.b64 	%rd62, %rd26, 2;
	add.s64 	%rd63, %rd61, %rd62;
	add.s64 	%rd64, %rd8, -1;
	sub.s64 	%rd65, %rd64, %rd19;
	cvta.to.global.u64 	%rd66, %rd34;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	ld.global.f32 	%f1, [%rd68];
	ld.global.f32 	%f2, [%rd63];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd69, %rd32;
	shl.b64 	%rd70, %rd1, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f3;

BB172_15:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot173[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%SPL, __local_depot173;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB173_13;

	add.s32 	%r15, %r4, -1024;
	shr.u32 	%r16, %r15, 6;
	mov.u32 	%r17, 15;
	sub.s32 	%r5, %r17, %r16;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r16;
	mov.u32 	%r20, 18;
	min.s32 	%r6, %r20, %r19;
	mov.u64 	%rd94, 0;
	setp.ge.s32	%p2, %r5, %r6;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB173_4;

	bfe.u32 	%r21, %r1, 20, 11;
	add.s32 	%r22, %r21, -1024;
	shr.u32 	%r23, %r22, 6;
	sub.s32 	%r25, %r17, %r23;
	mul.wide.s32 	%rd41, %r25, 8;
	mov.u64 	%rd42, __cudart_i2opi_d;
	add.s64 	%rd89, %rd42, %rd41;
	mov.b64 	 %rd43, %fd4;
	shl.b64 	%rd44, %rd43, 11;
	or.b64  	%rd5, %rd44, -9223372036854775808;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r5;

BB173_3:
	.pragma "nounroll";
	ld.const.u64 	%rd47, [%rd89];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd5;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd91], %rd45;
	add.s32 	%r39, %r39, 1;
	sub.s32 	%r26, %r39, %r5;
	mul.wide.s32 	%rd50, %r26, 8;
	add.s64 	%rd91, %rd1, %rd50;
	add.s64 	%rd93, %rd93, 8;
	add.s64 	%rd89, %rd89, 8;
	setp.lt.s32	%p3, %r39, %r6;
	@%p3 bra 	BB173_3;

BB173_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB173_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r9;
	shl.b64 	%rd51, %rd96, %r9;
	shr.u64 	%rd52, %rd95, %r28;
	or.b64  	%rd96, %rd51, %rd52;
	shl.b64 	%rd53, %rd95, %r9;
	ld.local.u64 	%rd54, [%rd1+8];
	shr.u64 	%rd55, %rd54, %r28;
	or.b64  	%rd95, %rd55, %rd53;

BB173_6:
	shr.u64 	%rd56, %rd96, 62;
	cvt.u32.u64	%r29, %rd56;
	shr.u64 	%rd57, %rd95, 62;
	shl.b64 	%rd58, %rd96, 2;
	or.b64  	%rd98, %rd58, %rd57;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd59, %rd96, 61;
	cvt.u32.u64	%r30, %rd59;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.eq.s32	%p5, %r40, 0;
	selp.b32	%r34, %r32, %r33, %p5;
	cvta.to.local.u64 	%rd60, %rd37;
	st.local.u32 	[%rd60], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB173_8;

	mov.u64 	%rd64, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd64;
	mov.b64         {a2,a3}, %rd64;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB173_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB173_10;

	shl.b64 	%rd67, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd68, %rd97, %r36;
	or.b64  	%rd98, %rd68, %rd67;

BB173_10:
	mov.u64 	%rd72, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd72;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd69, {r0,r1};     
	mov.b64         %rd100, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd100, 1;
	@%p8 bra 	BB173_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd69;
	mov.b64         {a2,a3}, %rd100;
	mov.b64         {b0,b1}, %rd69;
	mov.b64         {b2,b3}, %rd100;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd73, {r0,r1};
	mov.b64         %rd100, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB173_12:
	cvt.u64.u32	%rd79, %r40;
	shl.b64 	%rd80, %rd79, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd81, %r38;
	shl.b64 	%rd82, %rd81, 52;
	add.s64 	%rd83, %rd100, 1;
	shr.u64 	%rd84, %rd83, 10;
	add.s64 	%rd85, %rd84, 1;
	shr.u64 	%rd86, %rd85, 1;
	add.s64 	%rd87, %rd86, %rd82;
	or.b64  	%rd88, %rd87, %rd80;
	mov.b64 	 %fd4, %rd88;

BB173_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32	%p1, %r51, 0;
	@%p1 bra 	BB174_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

BB174_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32	%p2, %r18, 1073127583;
	@%p2 bra 	BB174_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

BB174_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd16, %fd15;
	neg.f64 	%fd17, %fd15;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd16, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd16, %fd16;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, -2147483648;
	mov.u32 	%r27, 1127219200;
	mov.b64 	%fd79, {%r25, %r27};
	mov.b64 	%fd80, {%r26, %r27};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	add.s32 	%r29, %r28, %r28;
	setp.gt.u32	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd18;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	 %f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB174_7;

	setp.lt.f64	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB174_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r39, %r15;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

BB174_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.ne.s32	%p7, %r46, 2146435072;
	@%p7 bra 	BB174_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32	%p8, %r47, 0;
	@%p8 bra 	BB174_10;

BB174_9:
	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

BB174_10:
	st.param.f64	[func_retval0+0], %fd136;
	ret;
}

.func  (.param .b64 func_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<295>;


	ld.param.f64 	%fd26, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd26;
	}
	setp.gt.s32	%p1, %r54, 1074266111;
	@%p1 bra 	BB175_12;
	bra.uni 	BB175_1;

BB175_12:
	setp.gt.s32	%p8, %r54, 1075838975;
	@%p8 bra 	BB175_14;
	bra.uni 	BB175_13;

BB175_14:
	rcp.approx.ftz.f64 	%fd216, %fd26;
	neg.f64 	%fd14, %fd26;
	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd218, %fd14, %fd216, %fd217;
	fma.rn.f64 	%fd219, %fd218, %fd218, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd216, %fd216;
	mul.f64 	%fd221, %fd220, %fd220;
	mov.f64 	%fd222, 0d3F4B68B992738FBF;
	mov.f64 	%fd223, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd224, %fd223, %fd221, %fd222;
	mov.f64 	%fd225, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd226, %fd224, %fd221, %fd225;
	mov.f64 	%fd227, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd228, %fd226, %fd221, %fd227;
	mov.f64 	%fd229, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd230, %fd228, %fd221, %fd229;
	mov.f64 	%fd231, 0d3FB5555555555545;
	fma.rn.f64 	%fd232, %fd230, %fd221, %fd231;
	mov.f64 	%fd233, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd15, %fd232, %fd220, %fd233;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd26;
	}
	mov.u32 	%r56, -1023;
	setp.gt.s32	%p9, %r54, 1048575;
	mov.f64 	%fd291, %fd26;
	@%p9 bra 	BB175_16;

	mul.f64 	%fd291, %fd26, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd291;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd291;
	}
	mov.u32 	%r56, -1077;

BB175_16:
	add.s32 	%r36, %r54, -1;
	setp.lt.u32	%p10, %r36, 2146435071;
	@%p10 bra 	BB175_18;
	bra.uni 	BB175_17;

BB175_18:
	shr.u32 	%r38, %r54, 20;
	add.s32 	%r57, %r56, %r38;
	and.b32  	%r39, %r54, -2146435073;
	or.b32  	%r40, %r39, 1072693248;
	mov.b64 	%fd292, {%r55, %r40};
	setp.lt.s32	%p12, %r40, 1073127583;
	@%p12 bra 	BB175_20;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd292;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd292;
	}
	add.s32 	%r43, %r42, -1048576;
	mov.b64 	%fd292, {%r41, %r43};
	add.s32 	%r57, %r57, 1;

BB175_20:
	add.f64 	%fd236, %fd292, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd237, %fd236;
	neg.f64 	%fd238, %fd236;
	fma.rn.f64 	%fd240, %fd238, %fd237, %fd217;
	fma.rn.f64 	%fd241, %fd240, %fd240, %fd240;
	fma.rn.f64 	%fd242, %fd241, %fd237, %fd237;
	add.f64 	%fd243, %fd292, 0dBFF0000000000000;
	mul.f64 	%fd244, %fd243, %fd242;
	fma.rn.f64 	%fd245, %fd243, %fd242, %fd244;
	mul.f64 	%fd246, %fd245, %fd245;
	mov.f64 	%fd247, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd248, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd249, %fd248, %fd246, %fd247;
	mov.f64 	%fd250, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd251, %fd249, %fd246, %fd250;
	mov.f64 	%fd252, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd253, %fd251, %fd246, %fd252;
	mov.f64 	%fd254, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd255, %fd253, %fd246, %fd254;
	mov.f64 	%fd256, 0d3F624924923BE72D;
	fma.rn.f64 	%fd257, %fd255, %fd246, %fd256;
	mov.f64 	%fd258, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd259, %fd257, %fd246, %fd258;
	mov.f64 	%fd260, 0d3FB5555555555554;
	fma.rn.f64 	%fd261, %fd259, %fd246, %fd260;
	sub.f64 	%fd262, %fd243, %fd245;
	add.f64 	%fd263, %fd262, %fd262;
	neg.f64 	%fd264, %fd245;
	fma.rn.f64 	%fd265, %fd264, %fd243, %fd263;
	mul.f64 	%fd266, %fd242, %fd265;
	mul.f64 	%fd267, %fd246, %fd261;
	fma.rn.f64 	%fd268, %fd267, %fd245, %fd266;
	xor.b32  	%r44, %r57, -2147483648;
	mov.u32 	%r45, -2147483648;
	mov.u32 	%r46, 1127219200;
	mov.b64 	%fd269, {%r44, %r46};
	mov.b64 	%fd270, {%r45, %r46};
	sub.f64 	%fd271, %fd269, %fd270;
	mov.f64 	%fd272, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd273, %fd271, %fd272, %fd245;
	neg.f64 	%fd274, %fd271;
	fma.rn.f64 	%fd275, %fd274, %fd272, %fd273;
	sub.f64 	%fd276, %fd275, %fd245;
	sub.f64 	%fd277, %fd268, %fd276;
	mov.f64 	%fd278, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd279, %fd271, %fd278, %fd277;
	add.f64 	%fd293, %fd273, %fd279;
	bra.uni 	BB175_21;

BB175_1:
	setp.gt.s32	%p2, %r54, 1073217535;
	@%p2 bra 	BB175_11;
	bra.uni 	BB175_2;

BB175_11:
	add.f64 	%fd146, %fd26, 0dC000000000000000;
	mov.f64 	%fd147, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd148, 0d3E452636124338B3;
	fma.rn.f64 	%fd149, %fd148, %fd146, %fd147;
	mov.f64 	%fd150, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd151, %fd149, %fd146, %fd150;
	mov.f64 	%fd152, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd153, %fd151, %fd146, %fd152;
	mov.f64 	%fd154, 0d3EB06850B2970292;
	fma.rn.f64 	%fd155, %fd153, %fd146, %fd154;
	mov.f64 	%fd156, 0dBEC108474875033D;
	fma.rn.f64 	%fd157, %fd155, %fd146, %fd156;
	mov.f64 	%fd158, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd159, %fd157, %fd146, %fd158;
	mov.f64 	%fd160, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd161, %fd159, %fd146, %fd160;
	mov.f64 	%fd162, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd163, %fd161, %fd146, %fd162;
	mov.f64 	%fd164, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd165, %fd163, %fd146, %fd164;
	mov.f64 	%fd166, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd167, %fd165, %fd146, %fd166;
	mov.f64 	%fd168, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd169, %fd167, %fd146, %fd168;
	mov.f64 	%fd170, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd171, %fd169, %fd146, %fd170;
	mov.f64 	%fd172, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd173, %fd171, %fd146, %fd172;
	mov.f64 	%fd174, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd175, %fd173, %fd146, %fd174;
	mov.f64 	%fd176, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd177, %fd175, %fd146, %fd176;
	mov.f64 	%fd178, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd179, %fd177, %fd146, %fd178;
	mov.f64 	%fd180, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd181, %fd179, %fd146, %fd180;
	mov.f64 	%fd182, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd183, %fd181, %fd146, %fd182;
	mov.f64 	%fd184, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd185, %fd183, %fd146, %fd184;
	mul.f64 	%fd294, %fd146, %fd185;
	bra.uni 	BB175_22;

BB175_13:
	add.f64 	%fd186, %fd26, 0dC008000000000000;
	mov.f64 	%fd187, 0dC1122B7730207EF3;
	mov.f64 	%fd188, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd189, %fd188, %fd186, %fd187;
	mov.f64 	%fd190, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd191, %fd189, %fd186, %fd190;
	mov.f64 	%fd192, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd193, %fd191, %fd186, %fd192;
	mov.f64 	%fd194, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd195, %fd193, %fd186, %fd194;
	mov.f64 	%fd196, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd197, %fd195, %fd186, %fd196;
	mov.f64 	%fd198, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd199, %fd197, %fd186, %fd198;
	mov.f64 	%fd200, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd201, %fd199, %fd186, %fd200;
	add.f64 	%fd202, %fd186, 0dC08FF62E0BE189FE;
	mov.f64 	%fd203, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd204, %fd202, %fd186, %fd203;
	mov.f64 	%fd205, 0dC151B662F8D75791;
	fma.rn.f64 	%fd206, %fd204, %fd186, %fd205;
	mov.f64 	%fd207, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd208, %fd206, %fd186, %fd207;
	mov.f64 	%fd209, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd210, %fd208, %fd186, %fd209;
	mov.f64 	%fd211, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd212, %fd210, %fd186, %fd211;
	mov.f64 	%fd213, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd214, %fd212, %fd186, %fd213;
	div.rn.f64 	%fd215, %fd201, %fd214;
	add.f64 	%fd294, %fd186, %fd215;
	bra.uni 	BB175_22;

BB175_2:
	setp.gt.s32	%p3, %r54, 1072064101;
	@%p3 bra 	BB175_10;
	bra.uni 	BB175_3;

BB175_10:
	mov.f64 	%fd101, 0d3FF0000000000000;
	sub.f64 	%fd102, %fd101, %fd26;
	mov.f64 	%fd103, 0d3FA3EB504359EB88;
	mov.f64 	%fd104, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd105, %fd104, %fd102, %fd103;
	mov.f64 	%fd106, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd107, %fd105, %fd102, %fd106;
	mov.f64 	%fd108, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd109, %fd107, %fd102, %fd108;
	mov.f64 	%fd110, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd111, %fd109, %fd102, %fd110;
	mov.f64 	%fd112, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd113, %fd111, %fd102, %fd112;
	mov.f64 	%fd114, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd115, %fd113, %fd102, %fd114;
	mov.f64 	%fd116, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd117, %fd115, %fd102, %fd116;
	mov.f64 	%fd118, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd119, %fd117, %fd102, %fd118;
	mov.f64 	%fd120, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd121, %fd119, %fd102, %fd120;
	mov.f64 	%fd122, 0d3FB556723452ED57;
	fma.rn.f64 	%fd123, %fd121, %fd102, %fd122;
	mov.f64 	%fd124, 0d3FB748C00891544F;
	fma.rn.f64 	%fd125, %fd123, %fd102, %fd124;
	mov.f64 	%fd126, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd127, %fd125, %fd102, %fd126;
	mov.f64 	%fd128, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd129, %fd127, %fd102, %fd128;
	mov.f64 	%fd130, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd131, %fd129, %fd102, %fd130;
	mov.f64 	%fd132, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd133, %fd131, %fd102, %fd132;
	mov.f64 	%fd134, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd135, %fd133, %fd102, %fd134;
	mov.f64 	%fd136, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd137, %fd135, %fd102, %fd136;
	mov.f64 	%fd138, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd139, %fd137, %fd102, %fd138;
	mov.f64 	%fd140, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd141, %fd139, %fd102, %fd140;
	mov.f64 	%fd142, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd143, %fd141, %fd102, %fd142;
	mov.f64 	%fd144, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd145, %fd143, %fd102, %fd144;
	mul.f64 	%fd294, %fd102, %fd145;
	bra.uni 	BB175_22;

BB175_17:
	mov.f64 	%fd234, 0d7FF0000000000000;
	fma.rn.f64 	%fd235, %fd291, %fd234, %fd234;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd291;
	}
	mov.b32 	 %f2, %r37;
	setp.eq.f32	%p11, %f2, 0f00000000;
	selp.f64	%fd293, 0dFFF0000000000000, %fd235, %p11;

BB175_21:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd293;
	}
	add.s32 	%r48, %r47, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd293;
	}
	mov.b64 	%fd280, {%r49, %r48};
	add.f64 	%fd281, %fd26, 0dBFE0000000000000;
	fma.rn.f64 	%fd282, %fd280, %fd281, %fd15;
	fma.rn.f64 	%fd283, %fd280, %fd281, %fd14;
	add.f64 	%fd284, %fd282, %fd283;
	setp.eq.f64	%p13, %fd26, 0d7FF0000000000000;
	selp.f64	%fd294, %fd26, %fd284, %p13;
	bra.uni 	BB175_22;

BB175_3:
	mov.f64 	%fd27, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd28, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd29, %fd28, %fd26, %fd27;
	mov.f64 	%fd30, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd31, %fd29, %fd26, %fd30;
	mov.f64 	%fd32, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd33, %fd31, %fd26, %fd32;
	mov.f64 	%fd34, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd35, %fd33, %fd26, %fd34;
	mov.f64 	%fd36, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd37, %fd35, %fd26, %fd36;
	mov.f64 	%fd38, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd39, %fd37, %fd26, %fd38;
	mov.f64 	%fd40, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd41, %fd39, %fd26, %fd40;
	mov.f64 	%fd42, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd43, %fd41, %fd26, %fd42;
	mov.f64 	%fd44, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd45, %fd43, %fd26, %fd44;
	mov.f64 	%fd46, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd47, %fd45, %fd26, %fd46;
	mov.f64 	%fd48, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd49, %fd47, %fd26, %fd48;
	mov.f64 	%fd50, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd51, %fd49, %fd26, %fd50;
	mov.f64 	%fd52, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd53, %fd51, %fd26, %fd52;
	mul.f64 	%fd54, %fd53, %fd26;
	fma.rn.f64 	%fd289, %fd54, %fd26, %fd26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd289;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd289;
	}
	mov.u32 	%r52, -1023;
	setp.gt.s32	%p4, %r50, 1048575;
	@%p4 bra 	BB175_5;

	mul.f64 	%fd289, %fd289, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd289;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd289;
	}
	mov.u32 	%r52, -1077;

BB175_5:
	add.s32 	%r23, %r50, -1;
	setp.lt.u32	%p5, %r23, 2146435071;
	@%p5 bra 	BB175_7;
	bra.uni 	BB175_6;

BB175_7:
	shr.u32 	%r25, %r50, 20;
	add.s32 	%r53, %r52, %r25;
	and.b32  	%r26, %r50, -2146435073;
	or.b32  	%r27, %r26, 1072693248;
	mov.b64 	%fd290, {%r51, %r27};
	setp.lt.s32	%p7, %r27, 1073127583;
	@%p7 bra 	BB175_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd290;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd290;
	}
	add.s32 	%r30, %r29, -1048576;
	mov.b64 	%fd290, {%r28, %r30};
	add.s32 	%r53, %r53, 1;

BB175_9:
	add.f64 	%fd57, %fd290, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd58, %fd57;
	neg.f64 	%fd59, %fd57;
	mov.f64 	%fd60, 0d3FF0000000000000;
	fma.rn.f64 	%fd61, %fd59, %fd58, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd61, %fd61;
	fma.rn.f64 	%fd63, %fd62, %fd58, %fd58;
	add.f64 	%fd64, %fd290, 0dBFF0000000000000;
	mul.f64 	%fd65, %fd64, %fd63;
	fma.rn.f64 	%fd66, %fd64, %fd63, %fd65;
	mul.f64 	%fd67, %fd66, %fd66;
	mov.f64 	%fd68, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd69, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd70, %fd69, %fd67, %fd68;
	mov.f64 	%fd71, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd72, %fd70, %fd67, %fd71;
	mov.f64 	%fd73, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd74, %fd72, %fd67, %fd73;
	mov.f64 	%fd75, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd76, %fd74, %fd67, %fd75;
	mov.f64 	%fd77, 0d3F624924923BE72D;
	fma.rn.f64 	%fd78, %fd76, %fd67, %fd77;
	mov.f64 	%fd79, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd80, %fd78, %fd67, %fd79;
	mov.f64 	%fd81, 0d3FB5555555555554;
	fma.rn.f64 	%fd82, %fd80, %fd67, %fd81;
	sub.f64 	%fd83, %fd64, %fd66;
	add.f64 	%fd84, %fd83, %fd83;
	neg.f64 	%fd85, %fd66;
	fma.rn.f64 	%fd86, %fd85, %fd64, %fd84;
	mul.f64 	%fd87, %fd63, %fd86;
	mul.f64 	%fd88, %fd67, %fd82;
	fma.rn.f64 	%fd89, %fd88, %fd66, %fd87;
	xor.b32  	%r31, %r53, -2147483648;
	mov.u32 	%r32, -2147483648;
	mov.u32 	%r33, 1127219200;
	mov.b64 	%fd90, {%r31, %r33};
	mov.b64 	%fd91, {%r32, %r33};
	sub.f64 	%fd92, %fd90, %fd91;
	mov.f64 	%fd93, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd94, %fd92, %fd93, %fd66;
	neg.f64 	%fd95, %fd92;
	fma.rn.f64 	%fd96, %fd95, %fd93, %fd94;
	sub.f64 	%fd97, %fd96, %fd66;
	sub.f64 	%fd98, %fd89, %fd97;
	mov.f64 	%fd99, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd100, %fd92, %fd99, %fd98;
	add.f64 	%fd8, %fd94, %fd100;
	neg.f64 	%fd294, %fd8;
	bra.uni 	BB175_22;

BB175_6:
	mov.f64 	%fd55, 0d7FF0000000000000;
	fma.rn.f64 	%fd56, %fd289, %fd55, %fd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd289;
	}
	mov.b32 	 %f1, %r24;
	setp.eq.f32	%p6, %f1, 0f00000000;
	selp.f64	%fd4, 0dFFF0000000000000, %fd56, %p6;
	neg.f64 	%fd294, %fd4;

BB175_22:
	st.param.f64	[func_retval0+0], %fd294;
	ret;
}


