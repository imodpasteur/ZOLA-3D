//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21554848
// Cuda compilation tools, release 8.0, V8.0.61
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	vec_test1
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.extern .func free
(
	.param .b64 free_param_0
)
;
.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.func  (.param .b64 func_retval0) __internal_lgamma_pos
(
	.param .b64 __internal_lgamma_pos_param_0
)
;
.global .align 4 .u32 truc;
.global .align 4 .u32 barrier;
.global .align 4 .u32 barrier2;
.global .align 1 .b8 _ZN6thrust6system6detail10sequential3seqE[1];
.shared .align 8 .b8 _ZN6thrust6system4cuda6detail5bulk_6detail71_GLOBAL__N__47_tmpxft_00011d35_00000000_7_ZOLA_kernels_cpp1_ii_f6adc78b19s_on_chip_allocatorE[24];
.global .align 1 .b8 _ZN6thrust6system4cuda6detail5bulk_4rootE[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN6thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN6thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN6thrust3seqE[1];
.global .align 1 .b8 $str[29] = {109, 97, 120, 32, 115, 105, 100, 101, 32, 114, 101, 115, 117, 108, 116, 32, 61, 32, 37, 102, 32, 32, 32, 32, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str1[16] = {107, 122, 32, 45, 62, 32, 37, 100, 32, 32, 37, 102, 32, 32, 10, 0};
.global .align 1 .b8 $str2[56] = {116, 101, 109, 112, 111, 114, 97, 114, 121, 95, 98, 117, 102, 102, 101, 114, 58, 58, 97, 108, 108, 111, 99, 97, 116, 101, 58, 32, 103, 101, 116, 95, 116, 101, 109, 112, 111, 114, 97, 114, 121, 95, 98, 117, 102, 102, 101, 114, 32, 102, 97, 105, 108, 101, 100, 0};
.global .align 1 .b8 $str3[4] = {37, 115, 10, 0};
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry vec_test1(
	.param .u32 vec_test1_param_0,
	.param .u64 vec_test1_param_1,
	.param .u32 vec_test1_param_2
)
{
	.local .align 8 .b8 	__local_depot0[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<59>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<312>;


	mov.u64 	%rd311, __local_depot0;
	cvta.local.u64 	%SP, %rd311;
	ld.param.u64 	%rd131, [vec_test1_param_1];
	ld.param.u32 	%r2, [vec_test1_param_2];
	cvta.to.global.u64 	%rd198, %rd131;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r12, %r11, %r3;
	mul.lo.s32 	%r13, %r12, %r10;
	neg.s32 	%r14, %r6;
	setp.ne.s32	%p3, %r13, %r14;
	@%p3 bra 	BB0_51;

	cvt.s64.s32	%rd2, %r2;
	setp.eq.s32	%p4, %r2, 0;
	mov.u64 	%rd133, 0;
	mov.u64 	%rd306, %rd133;
	mov.u64 	%rd310, %rd133;
	@%p4 bra 	BB0_4;

	shl.b64 	%rd134, %rd2, 2;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd134;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd3, [retval0+0];
	
	//{
	}// Callseq End 0
	setp.eq.s64	%p5, %rd3, 0;
	selp.b64	%rd135, 0, %rd2, %p5;
	mov.u64 	%rd306, %rd3;
	setp.ge.u64	%p6, %rd135, %rd2;
	mov.u64 	%rd310, %rd2;
	@%p6 bra 	BB0_4;

	add.u64 	%rd136, %SP, 0;
	cvta.to.local.u64 	%rd137, %rd136;
	mov.u64 	%rd306, %rd3;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd3;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 1
	mov.u64 	%rd138, $str2;
	cvta.global.u64 	%rd139, %rd138;
	st.local.u64 	[%rd137], %rd139;
	mov.u64 	%rd140, $str3;
	cvta.global.u64 	%rd141, %rd140;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd141;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd136;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r15, [retval0+0];
	
	//{
	}// Callseq End 2
	// inline asm
	trap;
	// inline asm
	mov.u64 	%rd307, %rd2;
	mov.u64 	%rd310, %rd307;

BB0_4:
	mov.u64 	%rd308, %rd310;
	mov.u64 	%rd7, %rd308;
	mov.u64 	%rd6, %rd306;
	shl.b64 	%rd142, %rd2, 2;
	add.s64 	%rd8, %rd198, %rd142;
	setp.ge.u64	%p7, %rd198, %rd8;
	@%p7 bra 	BB0_16;

BB0_5:
	mov.u64 	%rd9, %rd198;
	add.s64 	%rd198, %rd9, 128;
	setp.lt.u64	%p8, %rd198, %rd8;
	selp.b64	%rd10, %rd198, %rd8, %p8;
	setp.eq.s64	%p9, %rd9, %rd10;
	@%p9 bra 	BB0_15;

	add.s64 	%rd205, %rd9, 4;
	setp.eq.s64	%p10, %rd205, %rd10;
	mov.u64 	%rd199, 0;
	@%p10 bra 	BB0_15;

BB0_7:
	mov.u64 	%rd200, %rd205;
	mov.u64 	%rd13, %rd200;
	ld.global.f32 	%f9, [%rd9];
	ld.global.f32 	%f1, [%rd13];
	setp.lt.f32	%p11, %f1, %f9;
	@%p11 bra 	BB0_11;
	bra.uni 	BB0_8;

BB0_11:
	add.s64 	%rd207, %rd199, -4;
	setp.eq.s64	%p14, %rd13, %rd9;
	add.s64 	%rd206, %rd13, -4;
	@%p14 bra 	BB0_13;

BB0_12:
	ld.global.f32 	%f10, [%rd206];
	st.global.f32 	[%rd206+4], %f10;
	add.s64 	%rd207, %rd207, 4;
	add.s64 	%rd206, %rd206, -4;
	setp.ne.s64	%p15, %rd207, 0;
	@%p15 bra 	BB0_12;

BB0_13:
	st.global.f32 	[%rd9], %f1;
	bra.uni 	BB0_14;

BB0_8:
	ld.global.f32 	%f17, [%rd13+-4];
	setp.geu.f32	%p12, %f1, %f17;
	mov.u64 	%rd203, %rd13;
	mov.u64 	%rd204, %rd13;
	@%p12 bra 	BB0_10;

BB0_9:
	mov.u64 	%rd14, %rd204;
	st.global.f32 	[%rd14], %f17;
	ld.global.f32 	%f17, [%rd14+-8];
	setp.lt.f32	%p13, %f1, %f17;
	add.s64 	%rd15, %rd14, -4;
	mov.u64 	%rd203, %rd15;
	mov.u64 	%rd204, %rd15;
	@%p13 bra 	BB0_9;

BB0_10:
	st.global.f32 	[%rd203], %f1;

BB0_14:
	add.s64 	%rd205, %rd13, 4;
	add.s64 	%rd199, %rd199, -4;
	setp.ne.s64	%p16, %rd205, %rd10;
	@%p16 bra 	BB0_7;

BB0_15:
	@%p8 bra 	BB0_5;

BB0_16:
	setp.lt.s32	%p18, %r2, 33;
	@%p18 bra 	BB0_48;

	mov.u64 	%rd208, 32;
	mov.pred 	%p58, 0;

BB0_18:
	@%p58 bra 	BB0_32;
	bra.uni 	BB0_19;

BB0_32:
	setp.lt.s64	%p38, %rd7, 1;
	@%p38 bra 	BB0_45;

	cvta.to.global.u64 	%rd269, %rd131;
	shl.b64 	%rd171, %rd7, 2;
	add.s64 	%rd89, %rd6, %rd171;
	mov.u64 	%rd301, %rd6;

BB0_34:
	mov.u64 	%rd290, %rd301;
	mov.u64 	%rd91, %rd290;
	mov.u64 	%rd262, %rd269;
	mov.u64 	%rd90, %rd262;
	shl.b64 	%rd172, %rd208, 2;
	add.s64 	%rd173, %rd91, %rd172;
	sub.s64 	%rd174, %rd173, %rd89;
	setp.lt.s64	%p39, %rd174, 0;
	selp.b64	%rd92, %rd173, %rd89, %p39;
	add.s64 	%rd175, %rd92, %rd172;
	sub.s64 	%rd176, %rd175, %rd89;
	setp.lt.s64	%p40, %rd176, 0;
	selp.b64	%rd93, %rd175, %rd89, %p40;
	setp.ne.s64	%p41, %rd92, %rd93;
	setp.ne.s64	%p42, %rd91, %rd92;
	and.pred  	%p43, %p41, %p42;
	mov.u64 	%rd267, %rd90;
	mov.u64 	%rd268, %rd90;
	mov.u64 	%rd278, %rd92;
	mov.u64 	%rd279, %rd92;
	mov.u64 	%rd298, %rd91;
	mov.u64 	%rd299, %rd91;
	@!%p43 bra 	BB0_39;
	bra.uni 	BB0_35;

BB0_35:
	mov.u64 	%rd293, %rd299;
	mov.u64 	%rd300, %rd293;
	mov.u64 	%rd274, %rd279;
	mov.u64 	%rd280, %rd274;
	mov.u64 	%rd94, %rd268;
	ld.f32 	%f7, [%rd300];
	ld.f32 	%f8, [%rd280];
	setp.lt.f32	%p44, %f8, %f7;
	@%p44 bra 	BB0_37;
	bra.uni 	BB0_36;

BB0_37:
	st.global.f32 	[%rd94], %f8;
	add.s64 	%rd280, %rd280, 4;
	bra.uni 	BB0_38;

BB0_36:
	st.global.f32 	[%rd94], %f7;
	add.s64 	%rd300, %rd300, 4;

BB0_38:
	mov.u64 	%rd100, %rd300;
	mov.u64 	%rd99, %rd280;
	add.s64 	%rd101, %rd94, 4;
	setp.ne.s64	%p45, %rd100, %rd92;
	setp.ne.s64	%p46, %rd99, %rd93;
	and.pred  	%p47, %p46, %p45;
	mov.u64 	%rd267, %rd101;
	mov.u64 	%rd268, %rd101;
	mov.u64 	%rd278, %rd99;
	mov.u64 	%rd279, %rd99;
	mov.u64 	%rd298, %rd100;
	mov.u64 	%rd299, %rd100;
	@%p47 bra 	BB0_35;

BB0_39:
	mov.u64 	%rd297, %rd298;
	mov.u64 	%rd277, %rd278;
	mov.u64 	%rd102, %rd267;
	sub.s64 	%rd177, %rd92, %rd297;
	shr.s64 	%rd105, %rd177, 2;
	setp.lt.s64	%p48, %rd177, 1;
	mov.u64 	%rd266, %rd102;
	mov.u64 	%rd270, %rd105;
	@%p48 bra 	BB0_41;

BB0_40:
	mov.u64 	%rd108, %rd270;
	mov.u64 	%rd106, %rd266;
	ld.f32 	%f13, [%rd297];
	st.global.f32 	[%rd106], %f13;
	add.s64 	%rd297, %rd297, 4;
	add.s64 	%rd110, %rd106, 4;
	add.s64 	%rd111, %rd108, -1;
	setp.gt.s64	%p49, %rd111, 0;
	mov.u64 	%rd266, %rd110;
	mov.u64 	%rd270, %rd111;
	@%p49 bra 	BB0_40;

BB0_41:
	sub.s64 	%rd112, %rd93, %rd277;
	setp.lt.s64	%p50, %rd112, 1;
	@%p50 bra 	BB0_44;

	shl.b64 	%rd178, %rd105, 2;
	add.s64 	%rd271, %rd102, %rd178;
	shr.s64 	%rd281, %rd112, 2;

BB0_43:
	ld.f32 	%f14, [%rd277];
	st.global.f32 	[%rd271], %f14;
	add.s64 	%rd277, %rd277, 4;
	add.s64 	%rd271, %rd271, 4;
	add.s64 	%rd281, %rd281, -1;
	setp.gt.s64	%p51, %rd281, 0;
	@%p51 bra 	BB0_43;

BB0_44:
	shl.b64 	%rd197, %rd208, 1;
	shl.b64 	%rd179, %rd197, 2;
	add.s64 	%rd269, %rd90, %rd179;
	add.s64 	%rd301, %rd91, %rd179;
	sub.s64 	%rd180, %rd301, %rd89;
	setp.lt.s64	%p52, %rd180, 0;
	@%p52 bra 	BB0_34;
	bra.uni 	BB0_45;

BB0_19:
	cvta.to.global.u64 	%rd239, %rd131;
	mul.wide.s32 	%rd147, %r2, 4;
	add.s64 	%rd148, %rd239, %rd147;
	setp.ge.u64	%p20, %rd239, %rd148;
	@%p20 bra 	BB0_45;

	mov.u64 	%rd218, %rd131;
	mov.u64 	%rd305, %rd6;

BB0_21:
	mov.u64 	%rd286, %rd305;
	mov.u64 	%rd30, %rd286;
	mov.u64 	%rd219, %rd239;
	mov.u64 	%rd28, %rd219;
	mov.u64 	%rd210, %rd218;
	mov.u64 	%rd29, %rd210;
	mul.wide.s32 	%rd194, %r2, 4;
	mul.wide.s32 	%rd193, %r2, 4;
	cvta.to.global.u64 	%rd192, %rd131;
	add.s64 	%rd191, %rd192, %rd193;
	shl.b64 	%rd152, %rd208, 2;
	add.s64 	%rd153, %rd28, %rd152;
	setp.lt.u64	%p21, %rd153, %rd191;
	selp.b64	%rd31, %rd153, %rd191, %p21;
	add.s64 	%rd154, %rd31, %rd152;
	add.s64 	%rd155, %rd131, %rd193;
	add.s64 	%rd156, %rd29, %rd152;
	selp.b64	%rd209, %rd156, %rd155, %p21;
	setp.lt.u64	%p22, %rd154, %rd191;
	selp.b64	%rd33, %rd154, %rd191, %p22;
	setp.ne.s64	%p23, %rd31, %rd33;
	setp.ne.s64	%p24, %rd28, %rd31;
	and.pred  	%p25, %p23, %p24;
	mov.u64 	%rd215, %rd29;
	mov.u64 	%rd216, %rd29;
	mov.u64 	%rd235, %rd28;
	mov.u64 	%rd233, %rd28;
	mov.u64 	%rd232, %rd28;
	mov.u64 	%rd237, %rd28;
	mov.u64 	%rd257, %rd31;
	mov.u64 	%rd255, %rd31;
	mov.u64 	%rd254, %rd31;
	mov.u64 	%rd259, %rd31;
	mov.u64 	%rd303, %rd30;
	mov.u64 	%rd304, %rd30;
	@!%p25 bra 	BB0_26;
	bra.uni 	BB0_22;

BB0_22:
	mov.u64 	%rd42, %rd304;
	mov.u64 	%rd250, %rd259;
	mov.u64 	%rd248, %rd257;
	mov.u64 	%rd246, %rd255;
	mov.u64 	%rd260, %rd250;
	mov.u64 	%rd258, %rd248;
	mov.u64 	%rd256, %rd246;
	mov.u64 	%rd228, %rd237;
	mov.u64 	%rd226, %rd235;
	mov.u64 	%rd224, %rd233;
	mov.u64 	%rd238, %rd228;
	mov.u64 	%rd236, %rd226;
	mov.u64 	%rd234, %rd224;
	mov.u64 	%rd212, %rd216;
	mov.u64 	%rd217, %rd212;
	ld.global.f32 	%f5, [%rd234];
	ld.global.f32 	%f6, [%rd256];
	setp.lt.f32	%p26, %f6, %f5;
	@%p26 bra 	BB0_24;
	bra.uni 	BB0_23;

BB0_24:
	st.f32 	[%rd42], %f6;
	add.s64 	%rd258, %rd258, 4;
	add.s64 	%rd256, %rd256, 4;
	add.s64 	%rd260, %rd260, 4;
	add.s64 	%rd209, %rd209, 4;
	bra.uni 	BB0_25;

BB0_23:
	st.f32 	[%rd42], %f5;
	add.s64 	%rd236, %rd236, 4;
	add.s64 	%rd234, %rd234, 4;
	add.s64 	%rd238, %rd238, 4;
	add.s64 	%rd217, %rd217, 4;

BB0_25:
	mov.u64 	%rd57, %rd260;
	mov.u64 	%rd257, %rd258;
	mov.u64 	%rd255, %rd256;
	mov.u64 	%rd53, %rd238;
	mov.u64 	%rd235, %rd236;
	mov.u64 	%rd233, %rd234;
	mov.u64 	%rd54, %rd217;
	add.s64 	%rd59, %rd42, 4;
	setp.ne.s64	%p27, %rd235, %rd31;
	setp.ne.s64	%p28, %rd257, %rd33;
	and.pred  	%p29, %p28, %p27;
	mov.u64 	%rd215, %rd54;
	mov.u64 	%rd216, %rd54;
	mov.u64 	%rd232, %rd53;
	mov.u64 	%rd237, %rd53;
	mov.u64 	%rd254, %rd57;
	mov.u64 	%rd259, %rd57;
	mov.u64 	%rd303, %rd59;
	mov.u64 	%rd304, %rd59;
	@%p29 bra 	BB0_22;

BB0_26:
	mov.u64 	%rd64, %rd303;
	mov.u64 	%rd253, %rd254;
	mov.u64 	%rd231, %rd232;
	mul.wide.s32 	%rd189, %r2, 4;
	cvta.to.global.u64 	%rd157, %rd131;
	add.s64 	%rd65, %rd157, %rd189;
	setp.lt.u64	%p30, %rd153, %rd65;
	selp.b64	%rd67, %rd156, %rd155, %p30;
	sub.s64 	%rd162, %rd67, %rd215;
	shr.s64 	%rd240, %rd162, 2;
	setp.lt.s64	%p31, %rd162, 1;
	mov.u64 	%rd302, %rd64;
	@%p31 bra 	BB0_28;

BB0_27:
	mov.u64 	%rd69, %rd302;
	ld.global.f32 	%f11, [%rd231];
	st.f32 	[%rd69], %f11;
	add.s64 	%rd231, %rd231, 4;
	add.s64 	%rd73, %rd69, 4;
	add.s64 	%rd240, %rd240, -1;
	setp.gt.s64	%p32, %rd240, 0;
	mov.u64 	%rd302, %rd73;
	@%p32 bra 	BB0_27;

BB0_28:
	shl.b64 	%rd190, %rd208, 2;
	selp.b64	%rd165, %rd153, %rd65, %p30;
	add.s64 	%rd166, %rd165, %rd190;
	setp.lt.u64	%p34, %rd166, %rd65;
	add.s64 	%rd167, %rd67, %rd190;
	selp.b64	%rd168, %rd167, %rd155, %p34;
	sub.s64 	%rd75, %rd168, %rd209;
	setp.lt.s64	%p35, %rd75, 1;
	@%p35 bra 	BB0_31;

	sub.s64 	%rd196, %rd67, %rd215;
	shr.s64 	%rd195, %rd196, 2;
	shl.b64 	%rd169, %rd195, 2;
	add.s64 	%rd241, %rd64, %rd169;
	shr.s64 	%rd261, %rd75, 2;

BB0_30:
	ld.global.f32 	%f12, [%rd253];
	st.f32 	[%rd241], %f12;
	add.s64 	%rd253, %rd253, 4;
	add.s64 	%rd241, %rd241, 4;
	add.s64 	%rd261, %rd261, -1;
	setp.gt.s64	%p36, %rd261, 0;
	@%p36 bra 	BB0_30;

BB0_31:
	shl.b64 	%rd170, %rd208, 3;
	add.s64 	%rd239, %rd28, %rd170;
	add.s64 	%rd85, %rd29, %rd170;
	add.s64 	%rd305, %rd30, %rd170;
	setp.lt.u64	%p37, %rd239, %rd65;
	mov.u64 	%rd218, %rd85;
	@%p37 bra 	BB0_21;

BB0_45:
	shl.b64 	%rd208, %rd208, 1;
	not.pred 	%p58, %p58;
	cvt.s64.s32	%rd181, %r2;
	setp.lt.s64	%p53, %rd208, %rd181;
	@%p53 bra 	BB0_18;

	setp.gt.s64	%p54, %rd7, 0;
	and.pred  	%p55, %p54, %p58;
	cvta.to.global.u64 	%rd282, %rd131;
	mov.u64 	%rd296, %rd6;
	mov.u64 	%rd309, %rd7;
	@!%p55 bra 	BB0_48;
	bra.uni 	BB0_47;

BB0_47:
	mov.u64 	%rd127, %rd309;
	mov.u64 	%rd126, %rd296;
	ld.f32 	%f15, [%rd126];
	st.global.f32 	[%rd282], %f15;
	add.s64 	%rd128, %rd126, 4;
	add.s64 	%rd282, %rd282, 4;
	add.s64 	%rd130, %rd127, -1;
	setp.gt.s64	%p56, %rd130, 0;
	mov.u64 	%rd296, %rd128;
	mov.u64 	%rd309, %rd130;
	@%p56 bra 	BB0_47;

BB0_48:
	setp.eq.s64	%p57, %rd7, 0;
	@%p57 bra 	BB0_50;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd6;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 3

BB0_50:
	ld.param.u32 	%r17, [vec_test1_param_0];
	cvta.to.global.u64 	%rd182, %rd131;
	mul.wide.s32 	%rd183, %r2, 4;
	add.s64 	%rd184, %rd183, %rd182;
	ld.global.f32 	%f16, [%rd184+-4];
	cvt.f64.f32	%fd1, %f16;
	add.u64 	%rd185, %SP, 8;
	cvta.to.local.u64 	%rd186, %rd185;
	st.local.f64 	[%rd186], %fd1;
	st.local.u32 	[%rd186+8], %r17;
	mov.u64 	%rd187, $str;
	cvta.global.u64 	%rd188, %rd187;
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd188;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd185;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r16, [retval0+0];
	
	//{
	}// Callseq End 4

BB0_51:
	ret;
}

	// .globl	vec_initIndex
.visible .entry vec_initIndex(
	.param .u32 vec_initIndex_param_0,
	.param .u64 vec_initIndex_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_initIndex_param_0];
	ld.param.u64 	%rd1, [vec_initIndex_param_1];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u32 	[%rd4], %r1;

BB1_2:
	ret;
}

	// .globl	vec_computeLocalMaxima
.visible .entry vec_computeLocalMaxima(
	.param .u32 vec_computeLocalMaxima_param_0,
	.param .u64 vec_computeLocalMaxima_param_1,
	.param .u32 vec_computeLocalMaxima_param_2,
	.param .u64 vec_computeLocalMaxima_param_3,
	.param .u32 vec_computeLocalMaxima_param_4,
	.param .u32 vec_computeLocalMaxima_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r8, [vec_computeLocalMaxima_param_0];
	ld.param.u64 	%rd5, [vec_computeLocalMaxima_param_1];
	ld.param.u32 	%r5, [vec_computeLocalMaxima_param_2];
	ld.param.u64 	%rd6, [vec_computeLocalMaxima_param_3];
	ld.param.u32 	%r6, [vec_computeLocalMaxima_param_4];
	ld.param.u32 	%r7, [vec_computeLocalMaxima_param_5];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %nctaid.x;
	mov.u32 	%r14, %ctaid.x;
	mad.lo.s32 	%r15, %r12, %r13, %r14;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r15, %r16, %r17;
	setp.ge.s32	%p1, %r1, %r8;
	@%p1 bra 	BB2_8;

	cvta.to.global.u64 	%rd7, %rd5;
	cvt.s64.s32	%rd2, %r1;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd3, %rd7, %rd8;
	mov.u32 	%r18, 0;
	st.global.u32 	[%rd3], %r18;
	mul.lo.s32 	%r19, %r5, %r5;
	rem.s32 	%r20, %r1, %r19;
	div.s32 	%r21, %r20, %r5;
	rem.s32 	%r22, %r20, %r5;
	max.s32 	%r23, %r6, %r7;
	sub.s32 	%r24, %r21, %r23;
	sub.s32 	%r25, %r22, %r23;
	or.b32  	%r26, %r24, %r25;
	setp.gt.s32	%p2, %r26, -1;
	add.s32 	%r27, %r23, %r21;
	setp.lt.s32	%p3, %r27, %r5;
	and.pred  	%p4, %p2, %p3;
	add.s32 	%r28, %r23, %r22;
	setp.lt.s32	%p5, %r28, %r5;
	and.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB2_8;
	bra.uni 	BB2_2;

BB2_2:
	neg.s32 	%r2, %r6;
	setp.gt.s32	%p7, %r2, %r6;
	@%p7 bra 	BB2_7;

	shl.b64 	%rd9, %rd2, 2;
	add.s64 	%rd4, %rd1, %rd9;

BB2_4:
	ld.global.f32 	%f1, [%rd4];
	ld.global.f32 	%f2, [%rd1];
	setp.lt.f32	%p8, %f1, %f2;
	mov.u32 	%r30, %r2;
	@%p8 bra 	BB2_8;

BB2_5:
	mov.u32 	%r3, %r30;
	add.s32 	%r4, %r3, 1;
	setp.lt.s32	%p9, %r3, %r6;
	mov.u32 	%r30, %r4;
	@%p9 bra 	BB2_5;

	setp.lt.s32	%p10, %r4, %r6;
	@%p10 bra 	BB2_4;

BB2_7:
	mov.u32 	%r29, 1;
	st.global.u32 	[%rd3], %r29;

BB2_8:
	ret;
}

	// .globl	vec_eraseNonLocalMaxima
.visible .entry vec_eraseNonLocalMaxima(
	.param .u32 vec_eraseNonLocalMaxima_param_0,
	.param .u64 vec_eraseNonLocalMaxima_param_1,
	.param .u64 vec_eraseNonLocalMaxima_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [vec_eraseNonLocalMaxima_param_0];
	ld.param.u64 	%rd2, [vec_eraseNonLocalMaxima_param_1];
	ld.param.u64 	%rd3, [vec_eraseNonLocalMaxima_param_2];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB3_3;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r12, [%rd6];
	setp.ne.s32	%p2, %r12, 0;
	@%p2 bra 	BB3_3;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	mov.u32 	%r13, -1082130432;
	st.global.u32 	[%rd9], %r13;

BB3_3:
	ret;
}

	// .globl	vec_sortRows
.visible .entry vec_sortRows(
	.param .u32 vec_sortRows_param_0,
	.param .u64 vec_sortRows_param_1,
	.param .u64 vec_sortRows_param_2,
	.param .u32 vec_sortRows_param_3
)
{
	.local .align 8 .b8 	__local_depot4[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<66>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<505>;


	mov.u64 	%rd504, __local_depot4;
	cvta.local.u64 	%SP, %rd504;
	ld.param.u64 	%rd200, [vec_sortRows_param_1];
	ld.param.u64 	%rd201, [vec_sortRows_param_2];
	ld.param.u32 	%r2, [vec_sortRows_param_3];
	cvta.to.global.u64 	%rd327, %rd200;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r12, %r11, %r3;
	mul.lo.s32 	%r13, %r12, %r10;
	neg.s32 	%r14, %r6;
	setp.ne.s32	%p3, %r13, %r14;
	@%p3 bra 	BB4_65;

	cvt.s64.s32	%rd2, %r2;
	mul.wide.s32 	%rd4, %r2, 4;
	add.s64 	%rd3, %rd327, %rd4;
	setp.eq.s32	%p4, %r2, 0;
	mov.u64 	%rd203, 0;
	mov.u64 	%rd463, %rd203;
	mov.u64 	%rd503, %rd203;
	@%p4 bra 	BB4_4;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd5, [retval0+0];
	
	//{
	}// Callseq End 5
	setp.eq.s64	%p5, %rd5, 0;
	selp.b64	%rd204, 0, %rd2, %p5;
	mov.u64 	%rd463, %rd5;
	setp.ge.u64	%p6, %rd204, %rd2;
	mov.u64 	%rd503, %rd2;
	@%p6 bra 	BB4_4;

	add.u64 	%rd205, %SP, 8;
	cvta.to.local.u64 	%rd206, %rd205;
	mov.u64 	%rd463, %rd5;
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd5;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 6
	mov.u64 	%rd207, $str2;
	cvta.global.u64 	%rd208, %rd207;
	st.local.u64 	[%rd206], %rd208;
	mov.u64 	%rd209, $str3;
	cvta.global.u64 	%rd210, %rd209;
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd205;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r15, [retval0+0];
	
	//{
	}// Callseq End 7
	// inline asm
	trap;
	// inline asm
	mov.u64 	%rd495, %rd2;
	mov.u64 	%rd503, %rd495;

BB4_4:
	mov.u64 	%rd498, %rd503;
	mov.u64 	%rd9, %rd498;
	mov.u64 	%rd8, %rd463;
	mov.u64 	%rd493, %rd203;
	mov.u64 	%rd502, %rd203;
	@%p4 bra 	BB4_7;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64	%rd10, [retval0+0];
	
	//{
	}// Callseq End 8
	setp.eq.s64	%p8, %rd10, 0;
	selp.b64	%rd213, 0, %rd2, %p8;
	mov.u64 	%rd493, %rd10;
	setp.ge.u64	%p9, %rd213, %rd2;
	mov.u64 	%rd502, %rd2;
	@%p9 bra 	BB4_7;

	add.u64 	%rd214, %SP, 0;
	cvta.to.local.u64 	%rd215, %rd214;
	mov.u64 	%rd493, %rd10;
	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd10;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 9
	mov.u64 	%rd216, $str2;
	cvta.global.u64 	%rd217, %rd216;
	st.local.u64 	[%rd215], %rd217;
	mov.u64 	%rd218, $str3;
	cvta.global.u64 	%rd219, %rd218;
	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd219;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd214;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r16, [retval0+0];
	
	//{
	}// Callseq End 10
	// inline asm
	trap;
	// inline asm
	mov.u64 	%rd502, %rd2;

BB4_7:
	mov.u64 	%rd14, %rd502;
	mov.u64 	%rd13, %rd493;
	setp.ge.u64	%p10, %rd327, %rd3;
	@%p10 bra 	BB4_23;

	cvta.to.global.u64 	%rd331, %rd201;
	mov.u64 	%rd326, 0;

BB4_9:
	mov.u64 	%rd329, %rd331;
	mov.u64 	%rd18, %rd329;
	mov.u64 	%rd17, %rd327;
	add.s64 	%rd327, %rd17, 128;
	setp.lt.u64	%p11, %rd327, %rd3;
	selp.b64	%rd19, %rd327, %rd3, %p11;
	setp.eq.s64	%p12, %rd17, %rd19;
	@%p12 bra 	BB4_22;

	add.s64 	%rd336, %rd17, 4;
	setp.eq.s64	%p13, %rd336, %rd19;
	mov.u64 	%rd328, 0;
	mov.u64 	%rd330, %rd18;
	@%p13 bra 	BB4_22;

BB4_11:
	mov.u64 	%rd334, %rd336;
	mov.u64 	%rd22, %rd334;
	mov.u64 	%rd23, %rd330;
	mov.u64 	%rd21, %rd328;
	shl.b64 	%rd226, %rd21, 2;
	mov.u64 	%rd227, -4;
	sub.s64 	%rd24, %rd227, %rd226;
	ld.global.u32 	%r1, [%rd23+4];
	ld.global.f32 	%f9, [%rd17];
	ld.global.f32 	%f1, [%rd22];
	setp.gt.f32	%p14, %f1, %f9;
	@%p14 bra 	BB4_16;
	bra.uni 	BB4_12;

BB4_16:
	setp.eq.s64	%p17, %rd22, %rd17;
	add.s64 	%rd337, %rd22, -4;
	mov.u64 	%rd341, %rd24;
	@%p17 bra 	BB4_18;

BB4_17:
	mov.u64 	%rd36, %rd341;
	ld.global.f32 	%f10, [%rd337];
	st.global.f32 	[%rd337+4], %f10;
	add.s64 	%rd37, %rd36, 4;
	add.s64 	%rd337, %rd337, -4;
	setp.ne.s64	%p18, %rd37, 0;
	mov.u64 	%rd341, %rd37;
	@%p18 bra 	BB4_17;

BB4_18:
	shl.b64 	%rd325, %rd326, 5;
	cvta.to.global.u64 	%rd242, %rd201;
	add.s64 	%rd244, %rd325, %rd21;
	shl.b64 	%rd245, %rd244, 2;
	add.s64 	%rd338, %rd242, %rd245;
	add.s64 	%rd246, %rd338, 4;
	setp.eq.s64	%p19, %rd246, %rd18;
	mov.u64 	%rd340, %rd24;
	@%p19 bra 	BB4_20;

BB4_19:
	ld.global.u32 	%r18, [%rd338];
	st.global.u32 	[%rd338+4], %r18;
	add.s64 	%rd340, %rd340, 4;
	add.s64 	%rd338, %rd338, -4;
	setp.ne.s64	%p20, %rd340, 0;
	@%p20 bra 	BB4_19;

BB4_20:
	st.global.f32 	[%rd17], %f1;
	st.global.u32 	[%rd18], %r1;
	bra.uni 	BB4_21;

BB4_12:
	shl.b64 	%rd323, %rd326, 5;
	shl.b64 	%rd228, %rd326, 7;
	add.s64 	%rd332, %rd228, %rd226;
	ld.global.f32 	%f16, [%rd22+-4];
	add.s64 	%rd230, %rd323, %rd21;
	cvta.to.global.u64 	%rd231, %rd201;
	shl.b64 	%rd232, %rd230, 2;
	add.s64 	%rd233, %rd231, %rd232;
	add.s64 	%rd333, %rd233, 4;
	setp.leu.f32	%p15, %f1, %f16;
	mov.u64 	%rd335, %rd22;
	@%p15 bra 	BB4_15;

BB4_13:
	mov.u64 	%rd28, %rd332;
	cvta.to.global.u64 	%rd319, %rd201;
	cvta.to.global.u64 	%rd235, %rd200;
	add.s64 	%rd236, %rd235, %rd28;
	st.global.f32 	[%rd236+4], %f16;
	add.s64 	%rd237, %rd319, %rd28;
	ld.global.u32 	%r17, [%rd237];
	st.global.u32 	[%rd237+4], %r17;
	ld.global.f32 	%f16, [%rd236+-4];
	setp.gt.f32	%p16, %f1, %f16;
	add.s64 	%rd332, %rd28, -4;
	@%p16 bra 	BB4_13;

	add.s64 	%rd318, %rd28, -4;
	cvta.to.global.u64 	%rd317, %rd200;
	cvta.to.global.u64 	%rd316, %rd201;
	add.s64 	%rd239, %rd317, %rd318;
	add.s64 	%rd241, %rd316, %rd318;
	add.s64 	%rd333, %rd241, 4;
	add.s64 	%rd31, %rd239, 4;
	mov.u64 	%rd335, %rd31;

BB4_15:
	mov.u64 	%rd33, %rd335;
	st.global.f32 	[%rd33], %f1;
	st.global.u32 	[%rd333], %r1;

BB4_21:
	shl.b64 	%rd324, %rd326, 5;
	add.s64 	%rd336, %rd22, 4;
	add.s64 	%rd328, %rd21, 1;
	add.s64 	%rd248, %rd324, %rd21;
	cvta.to.global.u64 	%rd249, %rd201;
	shl.b64 	%rd250, %rd248, 2;
	add.s64 	%rd251, %rd249, %rd250;
	add.s64 	%rd46, %rd251, 4;
	setp.ne.s64	%p21, %rd336, %rd19;
	mov.u64 	%rd330, %rd46;
	@%p21 bra 	BB4_11;

BB4_22:
	cvta.to.global.u64 	%rd252, %rd200;
	add.s64 	%rd254, %rd252, %rd4;
	setp.lt.u64	%p22, %rd327, %rd254;
	add.s64 	%rd331, %rd18, 128;
	add.s64 	%rd326, %rd326, 1;
	@%p22 bra 	BB4_9;

BB4_23:
	ld.param.u32 	%r29, [vec_sortRows_param_3];
	setp.lt.s32	%p23, %r29, 33;
	@%p23 bra 	BB4_61;

	mov.u64 	%rd342, 32;
	mov.pred 	%p65, 0;

BB4_25:
	mov.pred 	%p1, %p65;
	@%p1 bra 	BB4_40;
	bra.uni 	BB4_26;

BB4_40:
	mov.u64 	%rd395, %rd8;
	cvta.to.global.u64 	%rd423, %rd200;
	cvta.to.global.u64 	%rd410, %rd201;
	shl.b64 	%rd301, %rd9, 2;
	add.s64 	%rd126, %rd8, %rd301;
	setp.lt.s64	%p43, %rd9, 1;
	mov.u64 	%rd456, %rd8;
	mov.u64 	%rd486, %rd13;
	@%p43 bra 	BB4_53;

BB4_41:
	mov.u64 	%rd475, %rd486;
	mov.u64 	%rd131, %rd475;
	mov.u64 	%rd445, %rd456;
	mov.u64 	%rd130, %rd445;
	mov.u64 	%rd411, %rd423;
	mov.u64 	%rd129, %rd411;
	mov.u64 	%rd398, %rd410;
	mov.u64 	%rd128, %rd398;
	mov.u64 	%rd127, %rd395;
	shl.b64 	%rd302, %rd342, 2;
	add.s64 	%rd303, %rd130, %rd302;
	sub.s64 	%rd304, %rd303, %rd126;
	setp.lt.s64	%p44, %rd304, 0;
	selp.b64	%rd132, %rd303, %rd126, %p44;
	add.s64 	%rd305, %rd132, %rd302;
	sub.s64 	%rd306, %rd305, %rd126;
	setp.lt.s64	%p45, %rd306, 0;
	selp.b64	%rd133, %rd305, %rd126, %p45;
	sub.s64 	%rd307, %rd132, %rd127;
	and.b64  	%rd308, %rd307, -4;
	add.s64 	%rd433, %rd131, %rd308;
	setp.ne.s64	%p46, %rd132, %rd133;
	setp.ne.s64	%p47, %rd130, %rd132;
	and.pred  	%p48, %p46, %p47;
	mov.u64 	%rd408, %rd128;
	mov.u64 	%rd409, %rd128;
	mov.u64 	%rd421, %rd129;
	mov.u64 	%rd422, %rd129;
	mov.u64 	%rd430, %rd132;
	mov.u64 	%rd431, %rd132;
	mov.u64 	%rd453, %rd130;
	mov.u64 	%rd454, %rd130;
	mov.u64 	%rd483, %rd131;
	mov.u64 	%rd484, %rd131;
	@!%p48 bra 	BB4_46;
	bra.uni 	BB4_42;

BB4_42:
	mov.u64 	%rd478, %rd484;
	mov.u64 	%rd485, %rd478;
	mov.u64 	%rd448, %rd454;
	mov.u64 	%rd455, %rd448;
	mov.u64 	%rd426, %rd431;
	mov.u64 	%rd432, %rd426;
	mov.u64 	%rd136, %rd422;
	mov.u64 	%rd135, %rd409;
	ld.f32 	%f7, [%rd455];
	ld.f32 	%f8, [%rd432];
	setp.leu.f32	%p49, %f8, %f7;
	@%p49 bra 	BB4_44;
	bra.uni 	BB4_43;

BB4_44:
	st.global.f32 	[%rd136], %f7;
	ld.u32 	%r24, [%rd485];
	st.global.u32 	[%rd135], %r24;
	add.s64 	%rd455, %rd455, 4;
	add.s64 	%rd485, %rd485, 4;
	bra.uni 	BB4_45;

BB4_43:
	st.global.f32 	[%rd136], %f8;
	ld.u32 	%r23, [%rd433];
	st.global.u32 	[%rd135], %r23;
	add.s64 	%rd432, %rd432, 4;
	add.s64 	%rd433, %rd433, 4;

BB4_45:
	mov.u64 	%rd146, %rd485;
	mov.u64 	%rd148, %rd455;
	mov.u64 	%rd147, %rd432;
	add.s64 	%rd149, %rd136, 4;
	add.s64 	%rd150, %rd135, 4;
	setp.ne.s64	%p50, %rd148, %rd132;
	setp.ne.s64	%p51, %rd147, %rd133;
	and.pred  	%p52, %p51, %p50;
	mov.u64 	%rd408, %rd150;
	mov.u64 	%rd409, %rd150;
	mov.u64 	%rd421, %rd149;
	mov.u64 	%rd422, %rd149;
	mov.u64 	%rd430, %rd147;
	mov.u64 	%rd431, %rd147;
	mov.u64 	%rd453, %rd148;
	mov.u64 	%rd454, %rd148;
	mov.u64 	%rd483, %rd146;
	mov.u64 	%rd484, %rd146;
	@%p52 bra 	BB4_42;

BB4_46:
	mov.u64 	%rd482, %rd483;
	mov.u64 	%rd452, %rd453;
	mov.u64 	%rd429, %rd430;
	mov.u64 	%rd419, %rd421;
	mov.u64 	%rd406, %rd408;
	setp.eq.s64	%p53, %rd452, %rd132;
	@%p53 bra 	BB4_49;

	sub.s64 	%rd396, %rd132, %rd452;
	mov.u64 	%rd407, %rd406;
	mov.u64 	%rd420, %rd419;

BB4_48:
	ld.f32 	%f13, [%rd452];
	st.global.f32 	[%rd420], %f13;
	ld.u32 	%r25, [%rd482];
	st.global.u32 	[%rd407], %r25;
	add.s64 	%rd452, %rd452, 4;
	add.s64 	%rd482, %rd482, 4;
	add.s64 	%rd420, %rd420, 4;
	add.s64 	%rd407, %rd407, 4;
	add.s64 	%rd396, %rd396, -4;
	setp.ne.s64	%p54, %rd396, 0;
	mov.u64 	%rd406, %rd407;
	mov.u64 	%rd419, %rd420;
	@%p54 bra 	BB4_48;

BB4_49:
	mov.u64 	%rd418, %rd419;
	mov.u64 	%rd405, %rd406;
	setp.eq.s64	%p55, %rd429, %rd133;
	@%p55 bra 	BB4_52;

	sub.s64 	%rd397, %rd133, %rd429;

BB4_51:
	ld.f32 	%f14, [%rd429];
	st.global.f32 	[%rd418], %f14;
	ld.u32 	%r26, [%rd433];
	st.global.u32 	[%rd405], %r26;
	add.s64 	%rd429, %rd429, 4;
	add.s64 	%rd433, %rd433, 4;
	add.s64 	%rd418, %rd418, 4;
	add.s64 	%rd405, %rd405, 4;
	add.s64 	%rd397, %rd397, -4;
	setp.ne.s64	%p56, %rd397, 0;
	@%p56 bra 	BB4_51;

BB4_52:
	shl.b64 	%rd322, %rd342, 1;
	shl.b64 	%rd309, %rd322, 2;
	add.s64 	%rd181, %rd131, %rd309;
	add.s64 	%rd423, %rd129, %rd309;
	add.s64 	%rd410, %rd128, %rd309;
	add.s64 	%rd395, %rd130, %rd309;
	mov.u64 	%rd184, %rd395;
	sub.s64 	%rd310, %rd395, %rd126;
	setp.lt.s64	%p57, %rd310, 0;
	mov.u64 	%rd456, %rd184;
	mov.u64 	%rd486, %rd181;
	@%p57 bra 	BB4_41;
	bra.uni 	BB4_53;

BB4_26:
	cvta.to.global.u64 	%rd375, %rd200;
	add.s64 	%rd258, %rd375, %rd4;
	setp.ge.u64	%p25, %rd375, %rd258;
	@%p25 bra 	BB4_53;

	cvta.to.global.u64 	%rd356, %rd201;
	mov.u64 	%rd343, 0;
	mov.u64 	%rd344, %rd200;
	mov.u64 	%rd462, %rd8;
	mov.u64 	%rd492, %rd13;

BB4_28:
	mov.u64 	%rd468, %rd492;
	mov.u64 	%rd59, %rd468;
	mov.u64 	%rd438, %rd462;
	mov.u64 	%rd58, %rd438;
	mov.u64 	%rd357, %rd375;
	mov.u64 	%rd57, %rd357;
	mov.u64 	%rd346, %rd356;
	mov.u64 	%rd55, %rd346;
	mov.u64 	%rd56, %rd344;
	cvta.to.global.u64 	%rd315, %rd200;
	add.s64 	%rd314, %rd315, %rd4;
	shl.b64 	%rd263, %rd342, 2;
	add.s64 	%rd264, %rd57, %rd263;
	setp.lt.u64	%p26, %rd264, %rd314;
	selp.b64	%rd60, %rd264, %rd314, %p26;
	add.s64 	%rd265, %rd200, %rd4;
	add.s64 	%rd266, %rd56, %rd263;
	selp.b64	%rd267, %rd266, %rd265, %p26;
	add.s64 	%rd268, %rd60, %rd263;
	setp.lt.u64	%p27, %rd268, %rd314;
	selp.b64	%rd61, %rd268, %rd314, %p27;
	sub.s64 	%rd269, %rd267, %rd56;
	and.b64  	%rd270, %rd269, -4;
	add.s64 	%rd377, %rd55, %rd270;
	setp.ne.s64	%p28, %rd60, %rd61;
	setp.ne.s64	%p29, %rd57, %rd60;
	and.pred  	%p30, %p28, %p29;
	mov.u64 	%rd353, %rd55;
	mov.u64 	%rd354, %rd55;
	mov.u64 	%rd371, %rd57;
	mov.u64 	%rd370, %rd57;
	mov.u64 	%rd369, %rd57;
	mov.u64 	%rd373, %rd57;
	mov.u64 	%rd391, %rd60;
	mov.u64 	%rd390, %rd60;
	mov.u64 	%rd389, %rd60;
	mov.u64 	%rd393, %rd60;
	mov.u64 	%rd460, %rd58;
	mov.u64 	%rd461, %rd58;
	mov.u64 	%rd490, %rd59;
	mov.u64 	%rd491, %rd59;
	@!%p30 bra 	BB4_33;
	bra.uni 	BB4_29;

BB4_29:
	mov.u64 	%rd70, %rd491;
	mov.u64 	%rd69, %rd461;
	mov.u64 	%rd385, %rd393;
	mov.u64 	%rd382, %rd391;
	mov.u64 	%rd394, %rd385;
	mov.u64 	%rd392, %rd382;
	mov.u64 	%rd365, %rd373;
	mov.u64 	%rd362, %rd371;
	mov.u64 	%rd374, %rd365;
	mov.u64 	%rd372, %rd362;
	mov.u64 	%rd349, %rd354;
	mov.u64 	%rd355, %rd349;
	ld.global.f32 	%f5, [%rd372];
	ld.global.f32 	%f6, [%rd392];
	setp.leu.f32	%p31, %f6, %f5;
	@%p31 bra 	BB4_31;
	bra.uni 	BB4_30;

BB4_31:
	st.f32 	[%rd69], %f5;
	ld.global.u32 	%r20, [%rd355];
	st.u32 	[%rd70], %r20;
	add.s64 	%rd374, %rd374, 4;
	add.s64 	%rd372, %rd372, 4;
	add.s64 	%rd355, %rd355, 4;
	bra.uni 	BB4_32;

BB4_30:
	st.f32 	[%rd69], %f6;
	ld.global.u32 	%r19, [%rd377];
	st.u32 	[%rd70], %r19;
	add.s64 	%rd394, %rd394, 4;
	add.s64 	%rd392, %rd392, 4;
	add.s64 	%rd377, %rd377, 4;

BB4_32:
	mov.u64 	%rd77, %rd394;
	mov.u64 	%rd391, %rd392;
	mov.u64 	%rd79, %rd374;
	mov.u64 	%rd371, %rd372;
	mov.u64 	%rd81, %rd355;
	add.s64 	%rd83, %rd69, 4;
	add.s64 	%rd84, %rd70, 4;
	setp.ne.s64	%p32, %rd79, %rd60;
	setp.ne.s64	%p33, %rd77, %rd61;
	and.pred  	%p34, %p33, %p32;
	mov.u64 	%rd353, %rd81;
	mov.u64 	%rd354, %rd81;
	mov.u64 	%rd369, %rd371;
	mov.u64 	%rd370, %rd79;
	mov.u64 	%rd373, %rd79;
	mov.u64 	%rd389, %rd391;
	mov.u64 	%rd390, %rd77;
	mov.u64 	%rd393, %rd77;
	mov.u64 	%rd460, %rd83;
	mov.u64 	%rd461, %rd83;
	mov.u64 	%rd490, %rd84;
	mov.u64 	%rd491, %rd84;
	@%p34 bra 	BB4_29;

BB4_33:
	mov.u64 	%rd488, %rd490;
	mov.u64 	%rd458, %rd460;
	mov.u64 	%rd388, %rd389;
	mov.u64 	%rd368, %rd369;
	mov.u64 	%rd352, %rd353;
	setp.eq.s64	%p35, %rd370, %rd60;
	@%p35 bra 	BB4_36;

	shl.b64 	%rd312, %rd342, 2;
	not.b64 	%rd271, %rd370;
	cvta.to.global.u64 	%rd272, %rd200;
	add.s64 	%rd274, %rd272, %rd312;
	not.b64 	%rd275, %rd274;
	mul.lo.s64 	%rd276, %rd342, %rd343;
	shl.b64 	%rd277, %rd276, 3;
	sub.s64 	%rd278, %rd275, %rd277;
	add.s64 	%rd280, %rd272, %rd4;
	not.b64 	%rd281, %rd280;
	setp.gt.u64	%p36, %rd278, %rd281;
	selp.b64	%rd282, %rd278, %rd281, %p36;
	sub.s64 	%rd345, %rd271, %rd282;
	mov.u64 	%rd459, %rd458;
	mov.u64 	%rd489, %rd488;

BB4_35:
	ld.global.f32 	%f11, [%rd368];
	st.f32 	[%rd459], %f11;
	ld.global.u32 	%r21, [%rd352];
	st.u32 	[%rd489], %r21;
	add.s64 	%rd368, %rd368, 4;
	add.s64 	%rd352, %rd352, 4;
	add.s64 	%rd459, %rd459, 4;
	add.s64 	%rd489, %rd489, 4;
	add.s64 	%rd345, %rd345, -4;
	setp.ne.s64	%p37, %rd345, 0;
	mov.u64 	%rd458, %rd459;
	mov.u64 	%rd488, %rd489;
	@%p37 bra 	BB4_35;

BB4_36:
	mov.u64 	%rd487, %rd488;
	mov.u64 	%rd457, %rd458;
	setp.eq.s64	%p38, %rd390, %rd61;
	@%p38 bra 	BB4_39;

	shl.b64 	%rd313, %rd342, 2;
	not.b64 	%rd283, %rd390;
	cvta.to.global.u64 	%rd284, %rd200;
	add.s64 	%rd286, %rd284, %rd313;
	not.b64 	%rd287, %rd286;
	mul.lo.s64 	%rd288, %rd342, %rd343;
	shl.b64 	%rd289, %rd288, 3;
	sub.s64 	%rd290, %rd287, %rd289;
	add.s64 	%rd292, %rd284, %rd4;
	not.b64 	%rd293, %rd292;
	setp.gt.u64	%p39, %rd290, %rd293;
	selp.b64	%rd294, %rd290, %rd293, %p39;
	sub.s64 	%rd295, %rd294, %rd313;
	setp.gt.u64	%p40, %rd295, %rd293;
	selp.b64	%rd296, %rd295, %rd293, %p40;
	sub.s64 	%rd376, %rd283, %rd296;

BB4_38:
	ld.global.f32 	%f12, [%rd388];
	st.f32 	[%rd457], %f12;
	ld.global.u32 	%r22, [%rd377];
	st.u32 	[%rd487], %r22;
	add.s64 	%rd388, %rd388, 4;
	add.s64 	%rd377, %rd377, 4;
	add.s64 	%rd457, %rd457, 4;
	add.s64 	%rd487, %rd487, 4;
	add.s64 	%rd376, %rd376, -4;
	setp.ne.s64	%p41, %rd376, 0;
	@%p41 bra 	BB4_38;

BB4_39:
	shl.b64 	%rd320, %rd342, 1;
	shl.b64 	%rd297, %rd320, 2;
	add.s64 	%rd375, %rd57, %rd297;
	add.s64 	%rd118, %rd56, %rd297;
	add.s64 	%rd356, %rd55, %rd297;
	add.s64 	%rd462, %rd58, %rd297;
	add.s64 	%rd492, %rd59, %rd297;
	cvta.to.global.u64 	%rd298, %rd200;
	add.s64 	%rd300, %rd298, %rd4;
	setp.lt.u64	%p42, %rd375, %rd300;
	add.s64 	%rd343, %rd343, 1;
	mov.u64 	%rd344, %rd118;
	@%p42 bra 	BB4_28;

BB4_53:
	shl.b64 	%rd342, %rd342, 1;
	ld.param.u32 	%r28, [vec_sortRows_param_3];
	not.pred 	%p65, %p1;
	cvt.s64.s32	%rd311, %r28;
	setp.lt.s64	%p58, %rd342, %rd311;
	@%p58 bra 	BB4_25;

	@%p1 bra 	BB4_61;

	cvta.to.global.u64 	%rd434, %rd200;
	setp.lt.s64	%p59, %rd9, 1;
	@%p59 bra 	BB4_58;

	mov.u64 	%rd451, %rd8;
	mov.u64 	%rd501, %rd9;

BB4_57:
	mov.u64 	%rd189, %rd501;
	ld.f32 	%f15, [%rd451];
	st.global.f32 	[%rd434], %f15;
	add.s64 	%rd451, %rd451, 4;
	add.s64 	%rd434, %rd434, 4;
	add.s64 	%rd192, %rd189, -1;
	setp.gt.s64	%p60, %rd192, 0;
	mov.u64 	%rd501, %rd192;
	@%p60 bra 	BB4_57;

BB4_58:
	cvta.to.global.u64 	%rd464, %rd201;
	setp.lt.s64	%p61, %rd14, 1;
	@%p61 bra 	BB4_61;

	mov.u64 	%rd481, %rd13;
	mov.u64 	%rd500, %rd14;

BB4_60:
	mov.u64 	%rd196, %rd500;
	ld.u32 	%r27, [%rd481];
	st.global.u32 	[%rd464], %r27;
	add.s64 	%rd481, %rd481, 4;
	add.s64 	%rd464, %rd464, 4;
	add.s64 	%rd199, %rd196, -1;
	setp.gt.s64	%p62, %rd199, 0;
	mov.u64 	%rd500, %rd199;
	@%p62 bra 	BB4_60;

BB4_61:
	setp.eq.s64	%p63, %rd14, 0;
	@%p63 bra 	BB4_63;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd13;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 11

BB4_63:
	setp.eq.s64	%p64, %rd9, 0;
	@%p64 bra 	BB4_65;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd8;
	call.uni 
	free, 
	(
	param0
	);
	
	//{
	}// Callseq End 12

BB4_65:
	ret;
}

	// .globl	vec_set
.visible .entry vec_set(
	.param .u32 vec_set_param_0,
	.param .u64 vec_set_param_1,
	.param .f64 vec_set_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [vec_set_param_0];
	ld.param.u64 	%rd1, [vec_set_param_1];
	ld.param.f64 	%fd1, [vec_set_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f64 	[%rd4], %fd1;

BB5_2:
	ret;
}

	// .globl	vec_add
.visible .entry vec_add(
	.param .u32 vec_add_param_0,
	.param .u64 vec_add_param_1,
	.param .u64 vec_add_param_2,
	.param .u64 vec_add_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_add_param_0];
	ld.param.u64 	%rd1, [vec_add_param_1];
	ld.param.u64 	%rd2, [vec_add_param_2];
	ld.param.u64 	%rd3, [vec_add_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB6_2:
	ret;
}

	// .globl	vec_sub
.visible .entry vec_sub(
	.param .u32 vec_sub_param_0,
	.param .u64 vec_sub_param_1,
	.param .u64 vec_sub_param_2,
	.param .u64 vec_sub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_sub_param_0];
	ld.param.u64 	%rd1, [vec_sub_param_1];
	ld.param.u64 	%rd2, [vec_sub_param_2];
	ld.param.u64 	%rd3, [vec_sub_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB7_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB7_2:
	ret;
}

	// .globl	vec_mul
.visible .entry vec_mul(
	.param .u32 vec_mul_param_0,
	.param .u64 vec_mul_param_1,
	.param .u64 vec_mul_param_2,
	.param .u64 vec_mul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_mul_param_0];
	ld.param.u64 	%rd1, [vec_mul_param_1];
	ld.param.u64 	%rd2, [vec_mul_param_2];
	ld.param.u64 	%rd3, [vec_mul_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB8_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB8_2:
	ret;
}

	// .globl	vec_mul_fl
.visible .entry vec_mul_fl(
	.param .u32 vec_mul_fl_param_0,
	.param .u64 vec_mul_fl_param_1,
	.param .u64 vec_mul_fl_param_2,
	.param .u64 vec_mul_fl_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_mul_fl_param_0];
	ld.param.u64 	%rd1, [vec_mul_fl_param_1];
	ld.param.u64 	%rd2, [vec_mul_fl_param_2];
	ld.param.u64 	%rd3, [vec_mul_fl_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB9_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

BB9_2:
	ret;
}

	// .globl	vec_div
.visible .entry vec_div(
	.param .u32 vec_div_param_0,
	.param .u64 vec_div_param_1,
	.param .u64 vec_div_param_2,
	.param .u64 vec_div_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_div_param_0];
	ld.param.u64 	%rd1, [vec_div_param_1];
	ld.param.u64 	%rd2, [vec_div_param_2];
	ld.param.u64 	%rd3, [vec_div_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB10_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB10_2:
	ret;
}

	// .globl	vec_negate
.visible .entry vec_negate(
	.param .u32 vec_negate_param_0,
	.param .u64 vec_negate_param_1,
	.param .u64 vec_negate_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_negate_param_0];
	ld.param.u64 	%rd1, [vec_negate_param_1];
	ld.param.u64 	%rd2, [vec_negate_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	neg.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB11_2:
	ret;
}

	// .globl	vec_addScalar
.visible .entry vec_addScalar(
	.param .u32 vec_addScalar_param_0,
	.param .u64 vec_addScalar_param_1,
	.param .u64 vec_addScalar_param_2,
	.param .f64 vec_addScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_addScalar_param_0];
	ld.param.u64 	%rd1, [vec_addScalar_param_1];
	ld.param.u64 	%rd2, [vec_addScalar_param_2];
	ld.param.f64 	%fd1, [vec_addScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB12_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB12_2:
	ret;
}

	// .globl	vec_subScalar
.visible .entry vec_subScalar(
	.param .u32 vec_subScalar_param_0,
	.param .u64 vec_subScalar_param_1,
	.param .u64 vec_subScalar_param_2,
	.param .f64 vec_subScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_subScalar_param_0];
	ld.param.u64 	%rd1, [vec_subScalar_param_1];
	ld.param.u64 	%rd2, [vec_subScalar_param_2];
	ld.param.f64 	%fd1, [vec_subScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB13_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB13_2:
	ret;
}

	// .globl	vec_mulScalar
.visible .entry vec_mulScalar(
	.param .u32 vec_mulScalar_param_0,
	.param .u64 vec_mulScalar_param_1,
	.param .u64 vec_mulScalar_param_2,
	.param .f64 vec_mulScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_mulScalar_param_0];
	ld.param.u64 	%rd1, [vec_mulScalar_param_1];
	ld.param.u64 	%rd2, [vec_mulScalar_param_2];
	ld.param.f64 	%fd1, [vec_mulScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB14_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB14_2:
	ret;
}

	// .globl	vec_divScalar
.visible .entry vec_divScalar(
	.param .u32 vec_divScalar_param_0,
	.param .u64 vec_divScalar_param_1,
	.param .u64 vec_divScalar_param_2,
	.param .f64 vec_divScalar_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_divScalar_param_0];
	ld.param.u64 	%rd1, [vec_divScalar_param_1];
	ld.param.u64 	%rd2, [vec_divScalar_param_2];
	ld.param.f64 	%fd1, [vec_divScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB15_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB15_2:
	ret;
}

	// .globl	vec_scalarAdd
.visible .entry vec_scalarAdd(
	.param .u32 vec_scalarAdd_param_0,
	.param .u64 vec_scalarAdd_param_1,
	.param .f64 vec_scalarAdd_param_2,
	.param .u64 vec_scalarAdd_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarAdd_param_0];
	ld.param.u64 	%rd1, [vec_scalarAdd_param_1];
	ld.param.f64 	%fd1, [vec_scalarAdd_param_2];
	ld.param.u64 	%rd2, [vec_scalarAdd_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB16_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	add.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB16_2:
	ret;
}

	// .globl	vec_scalarSub
.visible .entry vec_scalarSub(
	.param .u32 vec_scalarSub_param_0,
	.param .u64 vec_scalarSub_param_1,
	.param .f64 vec_scalarSub_param_2,
	.param .u64 vec_scalarSub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarSub_param_0];
	ld.param.u64 	%rd1, [vec_scalarSub_param_1];
	ld.param.f64 	%fd1, [vec_scalarSub_param_2];
	ld.param.u64 	%rd2, [vec_scalarSub_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB17_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	sub.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB17_2:
	ret;
}

	// .globl	vec_scalarMul
.visible .entry vec_scalarMul(
	.param .u32 vec_scalarMul_param_0,
	.param .u64 vec_scalarMul_param_1,
	.param .f64 vec_scalarMul_param_2,
	.param .u64 vec_scalarMul_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarMul_param_0];
	ld.param.u64 	%rd1, [vec_scalarMul_param_1];
	ld.param.f64 	%fd1, [vec_scalarMul_param_2];
	ld.param.u64 	%rd2, [vec_scalarMul_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB18_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB18_2:
	ret;
}

	// .globl	vec_scalarDiv
.visible .entry vec_scalarDiv(
	.param .u32 vec_scalarDiv_param_0,
	.param .u64 vec_scalarDiv_param_1,
	.param .f64 vec_scalarDiv_param_2,
	.param .u64 vec_scalarDiv_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_scalarDiv_param_0];
	ld.param.u64 	%rd1, [vec_scalarDiv_param_1];
	ld.param.f64 	%fd1, [vec_scalarDiv_param_2];
	ld.param.u64 	%rd2, [vec_scalarDiv_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB19_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	div.rn.f64 	%fd3, %fd1, %fd2;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd3;

BB19_2:
	ret;
}

	// .globl	vec_lt
.visible .entry vec_lt(
	.param .u32 vec_lt_param_0,
	.param .u64 vec_lt_param_1,
	.param .u64 vec_lt_param_2,
	.param .u64 vec_lt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_lt_param_0];
	ld.param.u64 	%rd1, [vec_lt_param_1];
	ld.param.u64 	%rd2, [vec_lt_param_2];
	ld.param.u64 	%rd3, [vec_lt_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB20_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB20_2:
	ret;
}

	// .globl	vec_lte
.visible .entry vec_lte(
	.param .u32 vec_lte_param_0,
	.param .u64 vec_lte_param_1,
	.param .u64 vec_lte_param_2,
	.param .u64 vec_lte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_lte_param_0];
	ld.param.u64 	%rd1, [vec_lte_param_1];
	ld.param.u64 	%rd2, [vec_lte_param_2];
	ld.param.u64 	%rd3, [vec_lte_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB21_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB21_2:
	ret;
}

	// .globl	vec_eq
.visible .entry vec_eq(
	.param .u32 vec_eq_param_0,
	.param .u64 vec_eq_param_1,
	.param .u64 vec_eq_param_2,
	.param .u64 vec_eq_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_eq_param_0];
	ld.param.u64 	%rd1, [vec_eq_param_1];
	ld.param.u64 	%rd2, [vec_eq_param_2];
	ld.param.u64 	%rd3, [vec_eq_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB22_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB22_2:
	ret;
}

	// .globl	vec_gte
.visible .entry vec_gte(
	.param .u32 vec_gte_param_0,
	.param .u64 vec_gte_param_1,
	.param .u64 vec_gte_param_2,
	.param .u64 vec_gte_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_gte_param_0];
	ld.param.u64 	%rd1, [vec_gte_param_1];
	ld.param.u64 	%rd2, [vec_gte_param_2];
	ld.param.u64 	%rd3, [vec_gte_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB23_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB23_2:
	ret;
}

	// .globl	vec_gt
.visible .entry vec_gt(
	.param .u32 vec_gt_param_0,
	.param .u64 vec_gt_param_1,
	.param .u64 vec_gt_param_2,
	.param .u64 vec_gt_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_gt_param_0];
	ld.param.u64 	%rd1, [vec_gt_param_1];
	ld.param.u64 	%rd2, [vec_gt_param_2];
	ld.param.u64 	%rd3, [vec_gt_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB24_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB24_2:
	ret;
}

	// .globl	vec_ne
.visible .entry vec_ne(
	.param .u32 vec_ne_param_0,
	.param .u64 vec_ne_param_1,
	.param .u64 vec_ne_param_2,
	.param .u64 vec_ne_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_ne_param_0];
	ld.param.u64 	%rd1, [vec_ne_param_1];
	ld.param.u64 	%rd2, [vec_ne_param_2];
	ld.param.u64 	%rd3, [vec_ne_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB25_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd10, %rd4, %rd6;
	st.global.f64 	[%rd10], %fd3;

BB25_2:
	ret;
}

	// .globl	vec_ltScalar
.visible .entry vec_ltScalar(
	.param .u32 vec_ltScalar_param_0,
	.param .u64 vec_ltScalar_param_1,
	.param .u64 vec_ltScalar_param_2,
	.param .f64 vec_ltScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_ltScalar_param_0];
	ld.param.u64 	%rd1, [vec_ltScalar_param_1];
	ld.param.u64 	%rd2, [vec_ltScalar_param_2];
	ld.param.f64 	%fd1, [vec_ltScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB26_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.lt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB26_2:
	ret;
}

	// .globl	vec_lteScalar
.visible .entry vec_lteScalar(
	.param .u32 vec_lteScalar_param_0,
	.param .u64 vec_lteScalar_param_1,
	.param .u64 vec_lteScalar_param_2,
	.param .f64 vec_lteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_lteScalar_param_0];
	ld.param.u64 	%rd1, [vec_lteScalar_param_1];
	ld.param.u64 	%rd2, [vec_lteScalar_param_2];
	ld.param.f64 	%fd1, [vec_lteScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB27_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB27_2:
	ret;
}

	// .globl	vec_eqScalar
.visible .entry vec_eqScalar(
	.param .u32 vec_eqScalar_param_0,
	.param .u64 vec_eqScalar_param_1,
	.param .u64 vec_eqScalar_param_2,
	.param .f64 vec_eqScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_eqScalar_param_0];
	ld.param.u64 	%rd1, [vec_eqScalar_param_1];
	ld.param.u64 	%rd2, [vec_eqScalar_param_2];
	ld.param.f64 	%fd1, [vec_eqScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB28_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.eq.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB28_2:
	ret;
}

	// .globl	vec_gteScalar
.visible .entry vec_gteScalar(
	.param .u32 vec_gteScalar_param_0,
	.param .u64 vec_gteScalar_param_1,
	.param .u64 vec_gteScalar_param_2,
	.param .f64 vec_gteScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_gteScalar_param_0];
	ld.param.u64 	%rd1, [vec_gteScalar_param_1];
	ld.param.u64 	%rd2, [vec_gteScalar_param_2];
	ld.param.f64 	%fd1, [vec_gteScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB29_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.ltu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d0000000000000000, 0d3FF0000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB29_2:
	ret;
}

	// .globl	vec_gtScalar
.visible .entry vec_gtScalar(
	.param .u32 vec_gtScalar_param_0,
	.param .u64 vec_gtScalar_param_1,
	.param .u64 vec_gtScalar_param_2,
	.param .f64 vec_gtScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_gtScalar_param_0];
	ld.param.u64 	%rd1, [vec_gtScalar_param_1];
	ld.param.u64 	%rd2, [vec_gtScalar_param_2];
	ld.param.f64 	%fd1, [vec_gtScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB30_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.gt.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB30_2:
	ret;
}

	// .globl	vec_neScalar
.visible .entry vec_neScalar(
	.param .u32 vec_neScalar_param_0,
	.param .u64 vec_neScalar_param_1,
	.param .u64 vec_neScalar_param_2,
	.param .f64 vec_neScalar_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_neScalar_param_0];
	ld.param.u64 	%rd1, [vec_neScalar_param_1];
	ld.param.u64 	%rd2, [vec_neScalar_param_2];
	ld.param.f64 	%fd1, [vec_neScalar_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB31_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	setp.neu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd3, 0d3FF0000000000000, 0d0000000000000000, %p2;
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd3;

BB31_2:
	ret;
}

	// .globl	vec_acos
.visible .entry vec_acos(
	.param .u32 vec_acos_param_0,
	.param .u64 vec_acos_param_1,
	.param .u64 vec_acos_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<95>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r4, [vec_acos_param_0];
	ld.param.u64 	%rd2, [vec_acos_param_1];
	ld.param.u64 	%rd3, [vec_acos_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB32_14;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd16, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd16;
	}
	abs.f64 	%fd1, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	setp.lt.s32	%p2, %r14, 1071801958;
	@%p2 bra 	BB32_9;
	bra.uni 	BB32_2;

BB32_9:
	mul.f64 	%fd62, %fd1, %fd1;
	mov.f64 	%fd63, 0dBFB3823B180754AF;
	mov.f64 	%fd64, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0d3FB333333320F91B;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mul.f64 	%fd88, %fd62, %fd87;
	fma.rn.f64 	%fd10, %fd88, %fd1, %fd1;
	setp.lt.s32	%p6, %r2, 0;
	@%p6 bra 	BB32_11;

	mov.f64 	%fd89, 0dBC91A62633145C07;
	add.rn.f64 	%fd90, %fd10, %fd89;
	neg.f64 	%fd93, %fd90;
	bra.uni 	BB32_12;

BB32_2:
	mov.f64 	%fd19, 0d3FF0000000000000;
	sub.f64 	%fd2, %fd19, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	add.s32 	%r16, %r3, -1048576;
	mov.b64 	%fd18, {%r15, %r16};
	// inline asm
	rsqrt.approx.ftz.f64 %fd17, %fd18;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd17;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd17;
	}
	add.s32 	%r19, %r18, -1048576;
	mov.b64 	%fd20, {%r17, %r19};
	mul.f64 	%fd21, %fd18, %fd17;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd18;
	fma.rn.f64 	%fd24, %fd23, %fd20, %fd21;
	neg.f64 	%fd25, %fd24;
	fma.rn.f64 	%fd26, %fd17, %fd25, %fd19;
	fma.rn.f64 	%fd27, %fd26, %fd20, %fd20;
	fma.rn.f64 	%fd28, %fd24, %fd25, %fd18;
	fma.rn.f64 	%fd3, %fd28, %fd27, %fd24;
	setp.lt.s32	%p3, %r3, 1;
	@%p3 bra 	BB32_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd3;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd3;
	}
	add.s32 	%r22, %r21, 1048576;
	mov.b64 	%fd29, {%r20, %r22};
	mov.f64 	%fd30, 0dBEBAC2FE66FAAC4B;
	mov.f64 	%fd31, 0d3EC715B371155F70;
	fma.rn.f64 	%fd32, %fd31, %fd2, %fd30;
	mov.f64 	%fd33, 0d3ED9A9B88EFCD9B8;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0d3EDD0F40A8A0C4C3;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3EF46D4CFA9E0E1F;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3F079C168D1E2422;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3F1C9A88C3BCA540;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0d3F31C4E64BD476DF;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3F46E8BA60009C8F;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F5F1C71C62B05A2;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3F76DB6DB6DC9F2C;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0d3F9333333333329C;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3FB5555555555555;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mul.f64 	%fd55, %fd2, %fd54;
	fma.rn.f64 	%fd94, %fd55, %fd29, %fd29;
	bra.uni 	BB32_5;

BB32_11:
	mov.f64 	%fd91, 0d3C91A62633145C07;
	add.rn.f64 	%fd93, %fd10, %fd91;

BB32_12:
	mov.f64 	%fd92, 0d3FF921FB54442D18;
	add.rn.f64 	%fd94, %fd92, %fd93;
	bra.uni 	BB32_13;

BB32_4:
	mov.f64 	%fd56, 0d0000000000000000;
	mul.rn.f64 	%fd94, %fd1, %fd56;

BB32_5:
	setp.gt.s32	%p4, %r3, -1;
	@%p4 bra 	BB32_7;

	mov.f64 	%fd57, 0d7FF0000000000000;
	mul.rn.f64 	%fd94, %fd94, %fd57;

BB32_7:
	setp.gt.s32	%p5, %r2, -1;
	@%p5 bra 	BB32_13;

	mov.f64 	%fd58, 0dBCA1A62633145C07;
	add.rn.f64 	%fd59, %fd94, %fd58;
	neg.f64 	%fd60, %fd59;
	mov.f64 	%fd61, 0d400921FB54442D18;
	add.rn.f64 	%fd94, %fd61, %fd60;

BB32_13:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd94;

BB32_14:
	ret;
}

	// .globl	vec_acosh
.visible .entry vec_acosh(
	.param .u32 vec_acosh_param_0,
	.param .u64 vec_acosh_param_1,
	.param .u64 vec_acosh_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<160>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r22, [vec_acosh_param_0];
	ld.param.u64 	%rd2, [vec_acosh_param_1];
	ld.param.u64 	%rd3, [vec_acosh_param_2];
	mov.u32 	%r23, %tid.x;
	mov.u32 	%r24, %ntid.y;
	mov.u32 	%r25, %ctaid.y;
	mov.u32 	%r26, %tid.y;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %nctaid.x;
	mov.u32 	%r29, %ctaid.x;
	mad.lo.s32 	%r30, %r27, %r28, %r29;
	mov.u32 	%r31, %ntid.x;
	mad.lo.s32 	%r1, %r30, %r31, %r23;
	setp.ge.s32	%p1, %r1, %r22;
	@%p1 bra 	BB33_21;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	add.f64 	%fd156, %fd1, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd156;
	}
	setp.gt.u32	%p2, %r66, 1127219199;
	@%p2 bra 	BB33_12;
	bra.uni 	BB33_2;

BB33_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd156;
	}
	mov.u32 	%r68, -1023;
	setp.gt.s32	%p11, %r66, 1048575;
	@%p11 bra 	BB33_14;

	mul.f64 	%fd156, %fd156, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd156;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd156;
	}
	mov.u32 	%r68, -1077;

BB33_14:
	add.s32 	%r51, %r66, -1;
	setp.lt.u32	%p12, %r51, 2146435071;
	@%p12 bra 	BB33_16;
	bra.uni 	BB33_15;

BB33_16:
	shr.u32 	%r53, %r66, 20;
	add.s32 	%r69, %r68, %r53;
	and.b32  	%r54, %r66, -2146435073;
	or.b32  	%r55, %r54, 1072693248;
	mov.b64 	%fd157, {%r67, %r55};
	setp.lt.s32	%p14, %r55, 1073127583;
	@%p14 bra 	BB33_18;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd157;
	}
	add.s32 	%r58, %r57, -1048576;
	mov.b64 	%fd157, {%r56, %r58};
	add.s32 	%r69, %r69, 1;

BB33_18:
	add.f64 	%fd110, %fd157, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd109,%fd110;
	// inline asm
	neg.f64 	%fd111, %fd110;
	mov.f64 	%fd112, 0d3FF0000000000000;
	fma.rn.f64 	%fd113, %fd111, %fd109, %fd112;
	fma.rn.f64 	%fd114, %fd113, %fd113, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd109, %fd109;
	add.f64 	%fd116, %fd157, 0dBFF0000000000000;
	mul.f64 	%fd117, %fd116, %fd115;
	fma.rn.f64 	%fd118, %fd116, %fd115, %fd117;
	mul.f64 	%fd119, %fd118, %fd118;
	mov.f64 	%fd120, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd121, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd122, %fd121, %fd119, %fd120;
	mov.f64 	%fd123, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd124, %fd122, %fd119, %fd123;
	mov.f64 	%fd125, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd126, %fd124, %fd119, %fd125;
	mov.f64 	%fd127, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd128, %fd126, %fd119, %fd127;
	mov.f64 	%fd129, 0d3F624924923BE72D;
	fma.rn.f64 	%fd130, %fd128, %fd119, %fd129;
	mov.f64 	%fd131, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd132, %fd130, %fd119, %fd131;
	mov.f64 	%fd133, 0d3FB5555555555554;
	fma.rn.f64 	%fd134, %fd132, %fd119, %fd133;
	sub.f64 	%fd135, %fd116, %fd118;
	add.f64 	%fd136, %fd135, %fd135;
	neg.f64 	%fd137, %fd118;
	fma.rn.f64 	%fd138, %fd137, %fd116, %fd136;
	mul.f64 	%fd139, %fd115, %fd138;
	mul.f64 	%fd140, %fd119, %fd134;
	fma.rn.f64 	%fd141, %fd140, %fd118, %fd139;
	xor.b32  	%r59, %r69, -2147483648;
	mov.u32 	%r60, 1127219200;
	mov.b64 	%fd142, {%r59, %r60};
	mov.u32 	%r61, -2147483648;
	mov.b64 	%fd143, {%r61, %r60};
	sub.f64 	%fd144, %fd142, %fd143;
	mov.f64 	%fd145, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd146, %fd144, %fd145, %fd118;
	neg.f64 	%fd147, %fd144;
	fma.rn.f64 	%fd148, %fd147, %fd145, %fd146;
	sub.f64 	%fd149, %fd148, %fd118;
	sub.f64 	%fd150, %fd141, %fd149;
	mov.f64 	%fd151, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd152, %fd144, %fd151, %fd150;
	add.f64 	%fd158, %fd146, %fd152;
	bra.uni 	BB33_19;

BB33_2:
	fma.rn.f64 	%fd26, %fd1, %fd156, %fd156;
	// inline asm
	rsqrt.approx.ftz.f64 %fd25, %fd26;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd25;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd25;
	}
	add.s32 	%r34, %r33, -1048576;
	mov.b64 	%fd27, {%r32, %r34};
	mul.f64 	%fd28, %fd26, %fd25;
	neg.f64 	%fd29, %fd28;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd26;
	fma.rn.f64 	%fd31, %fd30, %fd27, %fd28;
	neg.f64 	%fd32, %fd31;
	mov.f64 	%fd33, 0d3FF0000000000000;
	fma.rn.f64 	%fd34, %fd25, %fd32, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd27, %fd27;
	fma.rn.f64 	%fd36, %fd31, %fd32, %fd26;
	fma.rn.f64 	%fd37, %fd36, %fd35, %fd31;
	add.f64 	%fd3, %fd156, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd3;
	}
	setp.lt.u32	%p3, %r35, 1071994197;
	setp.lt.s32	%p4, %r35, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB33_10;
	bra.uni 	BB33_3;

BB33_10:
	add.f64 	%fd84, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd85, %fd3, %fd84;
	mul.f64 	%fd86, %fd3, %fd85;
	neg.f64 	%fd87, %fd86;
	sub.f64 	%fd88, %fd3, %fd86;
	mul.f64 	%fd89, %fd88, %fd88;
	mov.f64 	%fd90, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd91, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd92, %fd91, %fd89, %fd90;
	mov.f64 	%fd93, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd94, %fd92, %fd89, %fd93;
	mov.f64 	%fd95, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd96, %fd94, %fd89, %fd95;
	mov.f64 	%fd97, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd98, %fd96, %fd89, %fd97;
	mov.f64 	%fd99, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd100, %fd98, %fd89, %fd99;
	mov.f64 	%fd101, 0d3F899999999D70C4;
	fma.rn.f64 	%fd102, %fd100, %fd89, %fd101;
	mov.f64 	%fd103, 0d3FB5555555555462;
	fma.rn.f64 	%fd104, %fd102, %fd89, %fd103;
	mul.f64 	%fd105, %fd89, %fd104;
	fma.rn.f64 	%fd106, %fd105, %fd88, %fd87;
	add.f64 	%fd155, %fd3, %fd106;
	bra.uni 	BB33_11;

BB33_15:
	mov.f64 	%fd107, 0d7FF0000000000000;
	fma.rn.f64 	%fd108, %fd156, %fd107, %fd107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd156;
	}
	mov.b32 	 %f2, %r52;
	setp.eq.f32	%p13, %f2, 0f00000000;
	selp.f64	%fd158, 0dFFF0000000000000, %fd108, %p13;

BB33_19:
	add.f64 	%fd159, %fd158, 0d3FE62E42FEFA39EF;
	bra.uni 	BB33_20;

BB33_3:
	add.f64 	%fd153, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd153;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd153;
	}
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p6, %r62, 1048575;
	@%p6 bra 	BB33_5;

	mul.f64 	%fd153, %fd153, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd153;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd153;
	}
	mov.u32 	%r64, -1077;

BB33_5:
	add.s32 	%r38, %r62, -1;
	setp.lt.u32	%p7, %r38, 2146435071;
	@%p7 bra 	BB33_7;
	bra.uni 	BB33_6;

BB33_7:
	shr.u32 	%r40, %r62, 20;
	add.s32 	%r65, %r64, %r40;
	and.b32  	%r41, %r62, -2146435073;
	or.b32  	%r42, %r41, 1072693248;
	mov.b64 	%fd154, {%r63, %r42};
	setp.lt.s32	%p9, %r42, 1073127583;
	@%p9 bra 	BB33_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd154;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd154;
	}
	add.s32 	%r45, %r44, -1048576;
	mov.b64 	%fd154, {%r43, %r45};
	add.s32 	%r65, %r65, 1;

BB33_9:
	add.f64 	%fd41, %fd154, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd40,%fd41;
	// inline asm
	neg.f64 	%fd42, %fd41;
	fma.rn.f64 	%fd44, %fd42, %fd40, %fd33;
	fma.rn.f64 	%fd45, %fd44, %fd44, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd40, %fd40;
	add.f64 	%fd47, %fd154, 0dBFF0000000000000;
	mul.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd47, %fd46, %fd48;
	mul.f64 	%fd50, %fd49, %fd49;
	mov.f64 	%fd51, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd52, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	mov.f64 	%fd54, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd55, %fd53, %fd50, %fd54;
	mov.f64 	%fd56, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd57, %fd55, %fd50, %fd56;
	mov.f64 	%fd58, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd59, %fd57, %fd50, %fd58;
	mov.f64 	%fd60, 0d3F624924923BE72D;
	fma.rn.f64 	%fd61, %fd59, %fd50, %fd60;
	mov.f64 	%fd62, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd63, %fd61, %fd50, %fd62;
	mov.f64 	%fd64, 0d3FB5555555555554;
	fma.rn.f64 	%fd65, %fd63, %fd50, %fd64;
	sub.f64 	%fd66, %fd47, %fd49;
	add.f64 	%fd67, %fd66, %fd66;
	neg.f64 	%fd68, %fd49;
	fma.rn.f64 	%fd69, %fd68, %fd47, %fd67;
	mul.f64 	%fd70, %fd46, %fd69;
	mul.f64 	%fd71, %fd50, %fd65;
	fma.rn.f64 	%fd72, %fd71, %fd49, %fd70;
	xor.b32  	%r46, %r65, -2147483648;
	mov.u32 	%r47, 1127219200;
	mov.b64 	%fd73, {%r46, %r47};
	mov.u32 	%r48, -2147483648;
	mov.b64 	%fd74, {%r48, %r47};
	sub.f64 	%fd75, %fd73, %fd74;
	mov.f64 	%fd76, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd77, %fd75, %fd76, %fd49;
	neg.f64 	%fd78, %fd75;
	fma.rn.f64 	%fd79, %fd78, %fd76, %fd77;
	sub.f64 	%fd80, %fd79, %fd49;
	sub.f64 	%fd81, %fd72, %fd80;
	mov.f64 	%fd82, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd83, %fd75, %fd82, %fd81;
	add.f64 	%fd155, %fd77, %fd83;
	bra.uni 	BB33_11;

BB33_6:
	mov.f64 	%fd38, 0d7FF0000000000000;
	fma.rn.f64 	%fd39, %fd153, %fd38, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd153;
	}
	mov.b32 	 %f1, %r39;
	setp.eq.f32	%p8, %f1, 0f00000000;
	selp.f64	%fd155, 0dFFF0000000000000, %fd39, %p8;

BB33_11:
	setp.eq.s32	%p10, %r66, 0;
	selp.f64	%fd159, %fd156, %fd155, %p10;

BB33_20:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd159;

BB33_21:
	ret;
}

	// .globl	vec_asin
.visible .entry vec_asin(
	.param .u32 vec_asin_param_0,
	.param .u64 vec_asin_param_1,
	.param .u64 vec_asin_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<83>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r3, [vec_asin_param_0];
	ld.param.u64 	%rd2, [vec_asin_param_1];
	ld.param.u64 	%rd3, [vec_asin_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB34_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f1, %r2;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FE26666;
	@%p2 bra 	BB34_3;
	bra.uni 	BB34_2;

BB34_3:
	mul.f64 	%fd55, %fd1, %fd1;
	mov.f64 	%fd56, 0dBFB3823B180754AF;
	mov.f64 	%fd57, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0d3FB333333320F91B;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mul.f64 	%fd81, %fd55, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd1, %fd1;
	bra.uni 	BB34_4;

BB34_2:
	abs.f64 	%fd7, %fd1;
	mov.f64 	%fd8, 0d3FE0000000000000;
	mov.f64 	%fd9, 0dBFE0000000000000;
	fma.rn.f64 	%fd6, %fd9, %fd7, %fd8;
	// inline asm
	rsqrt.approx.ftz.f64 %fd5, %fd6;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd5;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd5;
	}
	add.s32 	%r15, %r14, -1048576;
	mov.b64 	%fd10, {%r13, %r15};
	mul.f64 	%fd11, %fd6, %fd5;
	neg.f64 	%fd12, %fd11;
	fma.rn.f64 	%fd13, %fd11, %fd12, %fd6;
	fma.rn.f64 	%fd14, %fd13, %fd10, %fd11;
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF0000000000000;
	fma.rn.f64 	%fd17, %fd5, %fd15, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd10, %fd10;
	fma.rn.f64 	%fd19, %fd14, %fd15, %fd6;
	fma.rn.f64 	%fd20, %fd19, %fd18, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd6;
	}
	setp.lt.s32	%p3, %r16, 0;
	selp.f64	%fd21, 0dFFF8000000000000, %fd20, %p3;
	setp.equ.f64	%p4, %fd6, 0d0000000000000000;
	selp.f64	%fd22, %fd6, %fd21, %p4;
	mov.f64 	%fd23, 0dBFB3823B180754AF;
	mov.f64 	%fd24, 0d3FB0066BDC1895E9;
	fma.rn.f64 	%fd25, %fd24, %fd6, %fd23;
	mov.f64 	%fd26, 0d3FB11E52CC2F79AE;
	fma.rn.f64 	%fd27, %fd25, %fd6, %fd26;
	mov.f64 	%fd28, 0dBF924EAF3526861B;
	fma.rn.f64 	%fd29, %fd27, %fd6, %fd28;
	mov.f64 	%fd30, 0d3F91DF02A31E6CB7;
	fma.rn.f64 	%fd31, %fd29, %fd6, %fd30;
	mov.f64 	%fd32, 0d3F847D18B0EEC6CC;
	fma.rn.f64 	%fd33, %fd31, %fd6, %fd32;
	mov.f64 	%fd34, 0d3F8D0AF961BA53B0;
	fma.rn.f64 	%fd35, %fd33, %fd6, %fd34;
	mov.f64 	%fd36, 0d3F91BF7734CF1C48;
	fma.rn.f64 	%fd37, %fd35, %fd6, %fd36;
	mov.f64 	%fd38, 0d3F96E91483144EF7;
	fma.rn.f64 	%fd39, %fd37, %fd6, %fd38;
	mov.f64 	%fd40, 0d3F9F1C6E0A4F9F81;
	fma.rn.f64 	%fd41, %fd39, %fd6, %fd40;
	mov.f64 	%fd42, 0d3FA6DB6DC27FA92B;
	fma.rn.f64 	%fd43, %fd41, %fd6, %fd42;
	mov.f64 	%fd44, 0d3FB333333320F91B;
	fma.rn.f64 	%fd45, %fd43, %fd6, %fd44;
	mov.f64 	%fd46, 0d3FC5555555555F4D;
	fma.rn.f64 	%fd47, %fd45, %fd6, %fd46;
	mul.f64 	%fd48, %fd6, %fd47;
	mul.f64 	%fd49, %fd22, 0dC000000000000000;
	mov.f64 	%fd50, 0d3C91A62633145C07;
	fma.rn.f64 	%fd51, %fd49, %fd48, %fd50;
	add.f64 	%fd52, %fd49, 0d3FE921FB54442D18;
	add.f64 	%fd53, %fd52, %fd51;
	add.f64 	%fd54, %fd53, 0d3FE921FB54442D18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd54;
	}
	and.b32  	%r19, %r2, -2147483648;
	or.b32  	%r20, %r18, %r19;
	mov.b64 	%fd82, {%r17, %r20};

BB34_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd82;

BB34_5:
	ret;
}

	// .globl	vec_asinh
.visible .entry vec_asinh(
	.param .u32 vec_asinh_param_0,
	.param .u64 vec_asinh_param_1,
	.param .u64 vec_asinh_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<77>;
	.reg .f64 	%fd<161>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r23, [vec_asinh_param_0];
	ld.param.u64 	%rd2, [vec_asinh_param_1];
	ld.param.u64 	%rd3, [vec_asinh_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r28, %r25, %r26, %r27;
	mov.u32 	%r29, %nctaid.x;
	mov.u32 	%r30, %ctaid.x;
	mad.lo.s32 	%r31, %r28, %r29, %r30;
	mov.u32 	%r32, %ntid.x;
	mad.lo.s32 	%r1, %r31, %r32, %r24;
	setp.ge.s32	%p1, %r1, %r23;
	@%p1 bra 	BB35_20;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd1;
	}
	and.b32  	%r34, %r2, 2147483647;
	mov.b64 	%fd157, {%r33, %r34};
	setp.gt.u32	%p2, %r34, 1138753535;
	@%p2 bra 	BB35_11;
	bra.uni 	BB35_2;

BB35_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %fd157;
	}
	mov.u32 	%r75, -1023;
	setp.gt.s32	%p10, %r73, 1048575;
	@%p10 bra 	BB35_13;

	mul.f64 	%fd157, %fd157, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r73}, %fd157;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %fd157;
	}
	mov.u32 	%r75, -1077;

BB35_13:
	add.s32 	%r54, %r73, -1;
	setp.lt.u32	%p11, %r54, 2146435071;
	@%p11 bra 	BB35_15;
	bra.uni 	BB35_14;

BB35_15:
	shr.u32 	%r56, %r73, 20;
	add.s32 	%r76, %r75, %r56;
	and.b32  	%r57, %r73, -2146435073;
	or.b32  	%r58, %r57, 1072693248;
	mov.b64 	%fd158, {%r74, %r58};
	setp.lt.s32	%p13, %r58, 1073127583;
	@%p13 bra 	BB35_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd158;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd158;
	}
	add.s32 	%r61, %r60, -1048576;
	mov.b64 	%fd158, {%r59, %r61};
	add.s32 	%r76, %r76, 1;

BB35_17:
	add.f64 	%fd111, %fd158, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd110,%fd111;
	// inline asm
	neg.f64 	%fd112, %fd111;
	mov.f64 	%fd113, 0d3FF0000000000000;
	fma.rn.f64 	%fd114, %fd112, %fd110, %fd113;
	fma.rn.f64 	%fd115, %fd114, %fd114, %fd114;
	fma.rn.f64 	%fd116, %fd115, %fd110, %fd110;
	add.f64 	%fd117, %fd158, 0dBFF0000000000000;
	mul.f64 	%fd118, %fd117, %fd116;
	fma.rn.f64 	%fd119, %fd117, %fd116, %fd118;
	mul.f64 	%fd120, %fd119, %fd119;
	mov.f64 	%fd121, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd122, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd123, %fd122, %fd120, %fd121;
	mov.f64 	%fd124, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd125, %fd123, %fd120, %fd124;
	mov.f64 	%fd126, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd127, %fd125, %fd120, %fd126;
	mov.f64 	%fd128, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd129, %fd127, %fd120, %fd128;
	mov.f64 	%fd130, 0d3F624924923BE72D;
	fma.rn.f64 	%fd131, %fd129, %fd120, %fd130;
	mov.f64 	%fd132, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd133, %fd131, %fd120, %fd132;
	mov.f64 	%fd134, 0d3FB5555555555554;
	fma.rn.f64 	%fd135, %fd133, %fd120, %fd134;
	sub.f64 	%fd136, %fd117, %fd119;
	add.f64 	%fd137, %fd136, %fd136;
	neg.f64 	%fd138, %fd119;
	fma.rn.f64 	%fd139, %fd138, %fd117, %fd137;
	mul.f64 	%fd140, %fd116, %fd139;
	mul.f64 	%fd141, %fd120, %fd135;
	fma.rn.f64 	%fd142, %fd141, %fd119, %fd140;
	xor.b32  	%r62, %r76, -2147483648;
	mov.u32 	%r63, 1127219200;
	mov.b64 	%fd143, {%r62, %r63};
	mov.u32 	%r64, -2147483648;
	mov.b64 	%fd144, {%r64, %r63};
	sub.f64 	%fd145, %fd143, %fd144;
	mov.f64 	%fd146, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd147, %fd145, %fd146, %fd119;
	neg.f64 	%fd148, %fd145;
	fma.rn.f64 	%fd149, %fd148, %fd146, %fd147;
	sub.f64 	%fd150, %fd149, %fd119;
	sub.f64 	%fd151, %fd142, %fd150;
	mov.f64 	%fd152, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd153, %fd145, %fd152, %fd151;
	add.f64 	%fd159, %fd147, %fd153;
	bra.uni 	BB35_18;

BB35_2:
	mul.rn.f64 	%fd25, %fd1, %fd1;
	mov.f64 	%fd26, 0d3FF0000000000000;
	fma.rn.f64 	%fd24, %fd1, %fd1, %fd26;
	// inline asm
	rsqrt.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd23;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd23;
	}
	add.s32 	%r37, %r36, -1048576;
	mov.b64 	%fd27, {%r35, %r37};
	mul.f64 	%fd28, %fd24, %fd23;
	neg.f64 	%fd29, %fd28;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd24;
	fma.rn.f64 	%fd31, %fd30, %fd27, %fd28;
	neg.f64 	%fd32, %fd31;
	fma.rn.f64 	%fd33, %fd23, %fd32, %fd26;
	fma.rn.f64 	%fd34, %fd33, %fd27, %fd27;
	fma.rn.f64 	%fd35, %fd31, %fd32, %fd24;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd31;
	add.f64 	%fd37, %fd36, 0d3FF0000000000000;
	div.rn.f64 	%fd38, %fd25, %fd37;
	add.f64 	%fd3, %fd157, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd3;
	}
	setp.lt.u32	%p3, %r38, 1071994197;
	setp.lt.s32	%p4, %r38, -1076258407;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB35_10;
	bra.uni 	BB35_3;

BB35_10:
	add.f64 	%fd85, %fd3, 0d4000000000000000;
	div.rn.f64 	%fd86, %fd3, %fd85;
	mul.f64 	%fd87, %fd3, %fd86;
	neg.f64 	%fd88, %fd87;
	sub.f64 	%fd89, %fd3, %fd87;
	mul.f64 	%fd90, %fd89, %fd89;
	mov.f64 	%fd91, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd92, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	mov.f64 	%fd94, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd95, %fd93, %fd90, %fd94;
	mov.f64 	%fd96, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd97, %fd95, %fd90, %fd96;
	mov.f64 	%fd98, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd99, %fd97, %fd90, %fd98;
	mov.f64 	%fd100, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd101, %fd99, %fd90, %fd100;
	mov.f64 	%fd102, 0d3F899999999D70C4;
	fma.rn.f64 	%fd103, %fd101, %fd90, %fd102;
	mov.f64 	%fd104, 0d3FB5555555555462;
	fma.rn.f64 	%fd105, %fd103, %fd90, %fd104;
	mul.f64 	%fd106, %fd90, %fd105;
	fma.rn.f64 	%fd107, %fd106, %fd89, %fd88;
	add.f64 	%fd160, %fd3, %fd107;
	bra.uni 	BB35_19;

BB35_14:
	mov.f64 	%fd108, 0d7FF0000000000000;
	fma.rn.f64 	%fd109, %fd157, %fd108, %fd108;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd157;
	}
	mov.b32 	 %f2, %r55;
	setp.eq.f32	%p12, %f2, 0f00000000;
	selp.f64	%fd159, 0dFFF0000000000000, %fd109, %p12;

BB35_18:
	add.f64 	%fd160, %fd159, 0d3FE62E42FEFA39EF;
	bra.uni 	BB35_19;

BB35_3:
	add.f64 	%fd155, %fd3, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd155;
	}
	mov.u32 	%r71, -1023;
	setp.gt.s32	%p6, %r69, 1048575;
	@%p6 bra 	BB35_5;

	mul.f64 	%fd155, %fd155, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd155;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd155;
	}
	mov.u32 	%r71, -1077;

BB35_5:
	add.s32 	%r41, %r69, -1;
	setp.lt.u32	%p7, %r41, 2146435071;
	@%p7 bra 	BB35_7;
	bra.uni 	BB35_6;

BB35_7:
	shr.u32 	%r43, %r69, 20;
	add.s32 	%r72, %r71, %r43;
	and.b32  	%r44, %r69, -2146435073;
	or.b32  	%r45, %r44, 1072693248;
	mov.b64 	%fd156, {%r70, %r45};
	setp.lt.s32	%p9, %r45, 1073127583;
	@%p9 bra 	BB35_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd156;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd156;
	}
	add.s32 	%r48, %r47, -1048576;
	mov.b64 	%fd156, {%r46, %r48};
	add.s32 	%r72, %r72, 1;

BB35_9:
	add.f64 	%fd42, %fd156, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd41,%fd42;
	// inline asm
	neg.f64 	%fd43, %fd42;
	fma.rn.f64 	%fd45, %fd43, %fd41, %fd26;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd41, %fd41;
	add.f64 	%fd48, %fd156, 0dBFF0000000000000;
	mul.f64 	%fd49, %fd48, %fd47;
	fma.rn.f64 	%fd50, %fd48, %fd47, %fd49;
	mul.f64 	%fd51, %fd50, %fd50;
	mov.f64 	%fd52, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd53, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0d3F624924923BE72D;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mov.f64 	%fd65, 0d3FB5555555555554;
	fma.rn.f64 	%fd66, %fd64, %fd51, %fd65;
	sub.f64 	%fd67, %fd48, %fd50;
	add.f64 	%fd68, %fd67, %fd67;
	neg.f64 	%fd69, %fd50;
	fma.rn.f64 	%fd70, %fd69, %fd48, %fd68;
	mul.f64 	%fd71, %fd47, %fd70;
	mul.f64 	%fd72, %fd51, %fd66;
	fma.rn.f64 	%fd73, %fd72, %fd50, %fd71;
	xor.b32  	%r49, %r72, -2147483648;
	mov.u32 	%r50, 1127219200;
	mov.b64 	%fd74, {%r49, %r50};
	mov.u32 	%r51, -2147483648;
	mov.b64 	%fd75, {%r51, %r50};
	sub.f64 	%fd76, %fd74, %fd75;
	mov.f64 	%fd77, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd78, %fd76, %fd77, %fd50;
	neg.f64 	%fd79, %fd76;
	fma.rn.f64 	%fd80, %fd79, %fd77, %fd78;
	sub.f64 	%fd81, %fd80, %fd50;
	sub.f64 	%fd82, %fd73, %fd81;
	mov.f64 	%fd83, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd84, %fd76, %fd83, %fd82;
	add.f64 	%fd160, %fd78, %fd84;
	bra.uni 	BB35_19;

BB35_6:
	mov.f64 	%fd39, 0d7FF0000000000000;
	fma.rn.f64 	%fd40, %fd155, %fd39, %fd39;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd155;
	}
	mov.b32 	 %f1, %r42;
	setp.eq.f32	%p8, %f1, 0f00000000;
	selp.f64	%fd160, 0dFFF0000000000000, %fd40, %p8;

BB35_19:
	cvta.to.global.u64 	%rd7, %rd2;
	and.b32  	%r65, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r66}, %fd160;
	}
	or.b32  	%r67, %r66, %r65;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd160;
	}
	mov.b64 	%fd154, {%r68, %r67};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd154;

BB35_20:
	ret;
}

	// .globl	vec_atan
.visible .entry vec_atan(
	.param .u32 vec_atan_param_0,
	.param .u64 vec_atan_param_1,
	.param .u64 vec_atan_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<57>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [vec_atan_param_0];
	ld.param.u64 	%rd2, [vec_atan_param_1];
	ld.param.u64 	%rd3, [vec_atan_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB36_4;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	abs.f64 	%fd2, %fd1;
	setp.leu.f64	%p2, %fd2, 0d3FF0000000000000;
	mov.f64 	%fd56, %fd2;
	@%p2 bra 	BB36_3;

	// inline asm
	rcp.approx.ftz.f64 %fd5,%fd2;
	// inline asm
	neg.f64 	%fd7, %fd2;
	mov.f64 	%fd8, 0d3FF0000000000000;
	fma.rn.f64 	%fd9, %fd7, %fd5, %fd8;
	fma.rn.f64 	%fd10, %fd9, %fd9, %fd9;
	fma.rn.f64 	%fd11, %fd10, %fd5, %fd5;
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	selp.f64	%fd3, 0d0000000000000000, %fd11, %p3;
	mov.f64 	%fd56, %fd3;

BB36_3:
	mov.f64 	%fd4, %fd56;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.f64 	%fd12, %fd4, %fd4;
	mov.f64 	%fd13, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd14, 0dBEF53E1D2A25FF7E;
	fma.rn.f64 	%fd15, %fd14, %fd12, %fd13;
	mov.f64 	%fd16, 0dBF5312788DDE082E;
	fma.rn.f64 	%fd17, %fd15, %fd12, %fd16;
	mov.f64 	%fd18, 0d3F6F9690C8249315;
	fma.rn.f64 	%fd19, %fd17, %fd12, %fd18;
	mov.f64 	%fd20, 0dBF82CF5AABC7CF0D;
	fma.rn.f64 	%fd21, %fd19, %fd12, %fd20;
	mov.f64 	%fd22, 0d3F9162B0B2A3BFDE;
	fma.rn.f64 	%fd23, %fd21, %fd12, %fd22;
	mov.f64 	%fd24, 0dBF9A7256FEB6FC6B;
	fma.rn.f64 	%fd25, %fd23, %fd12, %fd24;
	mov.f64 	%fd26, 0d3FA171560CE4A489;
	fma.rn.f64 	%fd27, %fd25, %fd12, %fd26;
	mov.f64 	%fd28, 0dBFA4F44D841450E4;
	fma.rn.f64 	%fd29, %fd27, %fd12, %fd28;
	mov.f64 	%fd30, 0d3FA7EE3D3F36BB95;
	fma.rn.f64 	%fd31, %fd29, %fd12, %fd30;
	mov.f64 	%fd32, 0dBFAAD32AE04A9FD1;
	fma.rn.f64 	%fd33, %fd31, %fd12, %fd32;
	mov.f64 	%fd34, 0d3FAE17813D66954F;
	fma.rn.f64 	%fd35, %fd33, %fd12, %fd34;
	mov.f64 	%fd36, 0dBFB11089CA9A5BCD;
	fma.rn.f64 	%fd37, %fd35, %fd12, %fd36;
	mov.f64 	%fd38, 0d3FB3B12B2DB51738;
	fma.rn.f64 	%fd39, %fd37, %fd12, %fd38;
	mov.f64 	%fd40, 0dBFB745D022F8DC5C;
	fma.rn.f64 	%fd41, %fd39, %fd12, %fd40;
	mov.f64 	%fd42, 0d3FBC71C709DFE927;
	fma.rn.f64 	%fd43, %fd41, %fd12, %fd42;
	mov.f64 	%fd44, 0dBFC2492491FA1744;
	fma.rn.f64 	%fd45, %fd43, %fd12, %fd44;
	mov.f64 	%fd46, 0d3FC99999999840D2;
	fma.rn.f64 	%fd47, %fd45, %fd12, %fd46;
	mov.f64 	%fd48, 0dBFD555555555544C;
	fma.rn.f64 	%fd49, %fd47, %fd12, %fd48;
	mul.f64 	%fd50, %fd12, %fd49;
	fma.rn.f64 	%fd51, %fd50, %fd4, %fd4;
	mov.f64 	%fd52, 0d3FF921FB54442D18;
	sub.f64 	%fd53, %fd52, %fd51;
	setp.gt.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd54, %fd53, %fd51, %p4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r13, %r15;
	mov.b64 	%fd55, {%r12, %r16};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd55;

BB36_4:
	ret;
}

	// .globl	vec_atanh
.visible .entry vec_atanh(
	.param .u32 vec_atanh_param_0,
	.param .u64 vec_atanh_param_1,
	.param .u64 vec_atanh_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<45>;
	.reg .f64 	%fd<91>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r12, [vec_atanh_param_0];
	ld.param.u64 	%rd2, [vec_atanh_param_1];
	ld.param.u64 	%rd3, [vec_atanh_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB37_11;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	abs.f64 	%fd13, %fd1;
	add.f64 	%fd14, %fd13, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	sub.f64 	%fd16, %fd15, %fd13;
	div.rn.f64 	%fd2, %fd14, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd2;
	}
	setp.lt.u32	%p2, %r22, 1071994197;
	setp.lt.s32	%p3, %r22, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB37_9;
	bra.uni 	BB37_2;

BB37_9:
	add.f64 	%fd63, %fd2, 0d4000000000000000;
	div.rn.f64 	%fd64, %fd2, %fd63;
	mul.f64 	%fd65, %fd2, %fd64;
	neg.f64 	%fd66, %fd65;
	sub.f64 	%fd67, %fd2, %fd65;
	mul.f64 	%fd68, %fd67, %fd67;
	mov.f64 	%fd69, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd70, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	mov.f64 	%fd72, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd73, %fd71, %fd68, %fd72;
	mov.f64 	%fd74, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd75, %fd73, %fd68, %fd74;
	mov.f64 	%fd76, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd77, %fd75, %fd68, %fd76;
	mov.f64 	%fd78, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd79, %fd77, %fd68, %fd78;
	mov.f64 	%fd80, 0d3F899999999D70C4;
	fma.rn.f64 	%fd81, %fd79, %fd68, %fd80;
	mov.f64 	%fd82, 0d3FB5555555555462;
	fma.rn.f64 	%fd83, %fd81, %fd68, %fd82;
	mul.f64 	%fd84, %fd68, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd67, %fd66;
	add.f64 	%fd90, %fd2, %fd85;
	bra.uni 	BB37_10;

BB37_2:
	add.f64 	%fd88, %fd2, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd88;
	}
	mov.u32 	%r43, -1023;
	setp.gt.s32	%p5, %r41, 1048575;
	@%p5 bra 	BB37_4;

	mul.f64 	%fd88, %fd88, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd88;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd88;
	}
	mov.u32 	%r43, -1077;

BB37_4:
	add.s32 	%r25, %r41, -1;
	setp.lt.u32	%p6, %r25, 2146435071;
	@%p6 bra 	BB37_6;
	bra.uni 	BB37_5;

BB37_6:
	shr.u32 	%r27, %r41, 20;
	add.s32 	%r44, %r43, %r27;
	and.b32  	%r28, %r41, -2146435073;
	or.b32  	%r29, %r28, 1072693248;
	mov.b64 	%fd89, {%r42, %r29};
	setp.lt.s32	%p8, %r29, 1073127583;
	@%p8 bra 	BB37_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd89;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd89;
	}
	add.s32 	%r32, %r31, -1048576;
	mov.b64 	%fd89, {%r30, %r32};
	add.s32 	%r44, %r44, 1;

BB37_8:
	add.f64 	%fd20, %fd89, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd19,%fd20;
	// inline asm
	neg.f64 	%fd21, %fd20;
	fma.rn.f64 	%fd23, %fd21, %fd19, %fd15;
	fma.rn.f64 	%fd24, %fd23, %fd23, %fd23;
	fma.rn.f64 	%fd25, %fd24, %fd19, %fd19;
	add.f64 	%fd26, %fd89, 0dBFF0000000000000;
	mul.f64 	%fd27, %fd26, %fd25;
	fma.rn.f64 	%fd28, %fd26, %fd25, %fd27;
	mul.f64 	%fd29, %fd28, %fd28;
	mov.f64 	%fd30, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd31, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd32, %fd31, %fd29, %fd30;
	mov.f64 	%fd33, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd34, %fd32, %fd29, %fd33;
	mov.f64 	%fd35, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd36, %fd34, %fd29, %fd35;
	mov.f64 	%fd37, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd38, %fd36, %fd29, %fd37;
	mov.f64 	%fd39, 0d3F624924923BE72D;
	fma.rn.f64 	%fd40, %fd38, %fd29, %fd39;
	mov.f64 	%fd41, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd42, %fd40, %fd29, %fd41;
	mov.f64 	%fd43, 0d3FB5555555555554;
	fma.rn.f64 	%fd44, %fd42, %fd29, %fd43;
	sub.f64 	%fd45, %fd26, %fd28;
	add.f64 	%fd46, %fd45, %fd45;
	neg.f64 	%fd47, %fd28;
	fma.rn.f64 	%fd48, %fd47, %fd26, %fd46;
	mul.f64 	%fd49, %fd25, %fd48;
	mul.f64 	%fd50, %fd29, %fd44;
	fma.rn.f64 	%fd51, %fd50, %fd28, %fd49;
	xor.b32  	%r33, %r44, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd52, {%r33, %r34};
	mov.u32 	%r35, -2147483648;
	mov.b64 	%fd53, {%r35, %r34};
	sub.f64 	%fd54, %fd52, %fd53;
	mov.f64 	%fd55, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd56, %fd54, %fd55, %fd28;
	neg.f64 	%fd57, %fd54;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	sub.f64 	%fd59, %fd58, %fd28;
	sub.f64 	%fd60, %fd51, %fd59;
	mov.f64 	%fd61, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd62, %fd54, %fd61, %fd60;
	add.f64 	%fd90, %fd56, %fd62;
	bra.uni 	BB37_10;

BB37_5:
	mov.f64 	%fd17, 0d7FF0000000000000;
	fma.rn.f64 	%fd18, %fd88, %fd17, %fd17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd88;
	}
	mov.b32 	 %f1, %r26;
	setp.eq.f32	%p7, %f1, 0f00000000;
	selp.f64	%fd90, 0dFFF0000000000000, %fd18, %p7;

BB37_10:
	cvta.to.global.u64 	%rd7, %rd2;
	mul.f64 	%fd86, %fd90, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd86;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd1;
	}
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r37, %r39;
	mov.b64 	%fd87, {%r36, %r40};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd87;

BB37_11:
	ret;
}

	// .globl	vec_cbrt
.visible .entry vec_cbrt(
	.param .u32 vec_cbrt_param_0,
	.param .u64 vec_cbrt_param_1,
	.param .u64 vec_cbrt_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r13, [vec_cbrt_param_0];
	ld.param.u64 	%rd2, [vec_cbrt_param_1];
	ld.param.u64 	%rd3, [vec_cbrt_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB38_7;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r41, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r41, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB38_3;
	bra.uni 	BB38_2;

BB38_3:
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 0;
	setp.ne.s32	%p5, %r42, 0;
	@%p5 bra 	BB38_5;

	mov.b64 	%fd5, {%r40, %r41};
	mul.f64 	%fd6, %fd5, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd6;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd6;
	}
	shr.u32 	%r42, %r41, 20;
	mov.u32 	%r43, 18;

BB38_5:
	add.s32 	%r25, %r42, -1022;
	cvt.rn.f32.s32	%f5, %r25;
	mul.f32 	%f6, %f5, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r26, %f6;
	mad.lo.s32 	%r27, %r26, -3145728, %r41;
	mov.b64 	%fd9, {%r40, %r27};
	cvt.rn.f32.f64	%f2, %fd9;
	// inline asm
	lg2.approx.ftz.f32 %f1,%f2;
	// inline asm
	mul.f32 	%f4, %f1, 0f3EAAAAAB;
	// inline asm
	ex2.approx.ftz.f32 %f3,%f4;
	// inline asm
	cvt.f64.f32	%fd10, %f3;
	mul.f64 	%fd11, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd11;
	}
	add.s32 	%r30, %r29, 1048576;
	mov.b64 	%fd12, {%r28, %r30};
	fma.rn.f64 	%fd8, %fd12, %fd10, %fd9;
	// inline asm
	rcp.approx.ftz.f64 %fd7,%fd8;
	// inline asm
	neg.f64 	%fd13, %fd8;
	mov.f64 	%fd14, 0d3FF0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd7, %fd14;
	fma.rn.f64 	%fd16, %fd15, %fd15, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd7, %fd7;
	neg.f64 	%fd18, %fd10;
	fma.rn.f64 	%fd19, %fd11, %fd18, %fd9;
	mul.f64 	%fd20, %fd17, %fd19;
	fma.rn.f64 	%fd21, %fd10, %fd20, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd21;
	}
	sub.s32 	%r33, %r26, %r43;
	shl.b32 	%r34, %r33, 20;
	add.s32 	%r35, %r32, %r34;
	mov.b64 	%fd22, {%r31, %r35};
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd22;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd22;
	}
	and.b32  	%r38, %r3, -2147483648;
	or.b32  	%r39, %r37, %r38;
	mov.b64 	%fd23, {%r36, %r39};
	bra.uni 	BB38_6;

BB38_2:
	add.f64 	%fd23, %fd1, %fd1;

BB38_6:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd23;

BB38_7:
	ret;
}

	// .globl	vec_ceil
.visible .entry vec_ceil(
	.param .u32 vec_ceil_param_0,
	.param .u64 vec_ceil_param_1,
	.param .u64 vec_ceil_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_ceil_param_0];
	ld.param.u64 	%rd1, [vec_ceil_param_1];
	ld.param.u64 	%rd2, [vec_ceil_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB39_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rpi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB39_2:
	ret;
}

	// .globl	vec_cos
.visible .entry vec_cos(
	.param .u32 vec_cos_param_0,
	.param .u64 vec_cos_param_1,
	.param .u64 vec_cos_param_2
)
{
	.local .align 4 .b8 	__local_depot40[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<17>;


	mov.u64 	%rd16, __local_depot40;
	cvta.local.u64 	%SP, %rd16;
	ld.param.u32 	%r6, [vec_cos_param_0];
	ld.param.u64 	%rd3, [vec_cos_param_1];
	ld.param.u64 	%rd4, [vec_cos_param_2];
	add.u64 	%rd5, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd5;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r15;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB40_10;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd2, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd39, [%rd8];
	abs.f64 	%fd14, %fd39;
	setp.neu.f64	%p2, %fd14, 0d7FF0000000000000;
	@%p2 bra 	BB40_3;

	mov.f64 	%fd15, 0d0000000000000000;
	mul.rn.f64 	%fd39, %fd39, %fd15;

BB40_3:
	mul.f64 	%fd16, %fd39, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r21, %fd16;
	st.local.u32 	[%rd1], %r21;
	cvt.rn.f64.s32	%fd17, %r21;
	neg.f64 	%fd18, %fd17;
	mov.f64 	%fd19, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd20, %fd18, %fd19, %fd39;
	mov.f64 	%fd21, 0d3C91A62633145C00;
	fma.rn.f64 	%fd22, %fd18, %fd21, %fd20;
	mov.f64 	%fd23, 0d397B839A252049C0;
	fma.rn.f64 	%fd40, %fd18, %fd23, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd39;
	}
	and.b32  	%r17, %r16, 2145386496;
	setp.lt.u32	%p3, %r17, 1105199104;
	@%p3 bra 	BB40_5;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd40, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r21, [%rd1];

BB40_5:
	add.s32 	%r5, %r21, 1;
	and.b32  	%r18, %r5, 1;
	shl.b32 	%r19, %r18, 3;
	setp.eq.s32	%p4, %r18, 0;
	selp.f64	%fd24, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd10, %r19, 8;
	mov.u64 	%rd11, __cudart_sin_cos_coeffs;
	add.s64 	%rd12, %rd10, %rd11;
	ld.const.f64 	%fd25, [%rd12+8];
	mul.rn.f64 	%fd7, %fd40, %fd40;
	fma.rn.f64 	%fd26, %fd24, %fd7, %fd25;
	ld.const.f64 	%fd27, [%rd12+16];
	fma.rn.f64 	%fd28, %fd26, %fd7, %fd27;
	ld.const.f64 	%fd29, [%rd12+24];
	fma.rn.f64 	%fd30, %fd28, %fd7, %fd29;
	ld.const.f64 	%fd31, [%rd12+32];
	fma.rn.f64 	%fd32, %fd30, %fd7, %fd31;
	ld.const.f64 	%fd33, [%rd12+40];
	fma.rn.f64 	%fd34, %fd32, %fd7, %fd33;
	ld.const.f64 	%fd35, [%rd12+48];
	fma.rn.f64 	%fd8, %fd34, %fd7, %fd35;
	fma.rn.f64 	%fd41, %fd8, %fd40, %fd40;
	@%p4 bra 	BB40_7;

	mov.f64 	%fd36, 0d3FF0000000000000;
	fma.rn.f64 	%fd41, %fd8, %fd7, %fd36;

BB40_7:
	and.b32  	%r20, %r5, 2;
	setp.eq.s32	%p5, %r20, 0;
	@%p5 bra 	BB40_9;

	mov.f64 	%fd37, 0d0000000000000000;
	mov.f64 	%fd38, 0dBFF0000000000000;
	fma.rn.f64 	%fd41, %fd41, %fd38, %fd37;

BB40_9:
	cvta.to.global.u64 	%rd13, %rd3;
	shl.b64 	%rd14, %rd2, 3;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f64 	[%rd15], %fd41;

BB40_10:
	ret;
}

	// .globl	vec_cosh
.visible .entry vec_cosh(
	.param .u32 vec_cosh_param_0,
	.param .u64 vec_cosh_param_1,
	.param .u64 vec_cosh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<46>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [vec_cosh_param_0];
	ld.param.u64 	%rd2, [vec_cosh_param_1];
	ld.param.u64 	%rd3, [vec_cosh_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB41_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	and.b32  	%r13, %r12, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r14, %r13};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd2;
	}
	setp.lt.u32	%p2, %r15, 1082536911;
	@%p2 bra 	BB41_3;
	bra.uni 	BB41_2;

BB41_3:
	mov.f64 	%fd8, 0d4338000000000000;
	mov.f64 	%fd9, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd10, %fd2, %fd9, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd10;
	}
	mov.f64 	%fd11, 0dC338000000000000;
	add.rn.f64 	%fd12, %fd10, %fd11;
	mov.f64 	%fd13, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd14, %fd12, %fd13, %fd2;
	mov.f64 	%fd15, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd16, %fd12, %fd15, %fd14;
	mov.f64 	%fd17, 0d3E928AF3FCA213EA;
	mov.f64 	%fd18, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd19, %fd18, %fd16, %fd17;
	mov.f64 	%fd20, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd21, %fd19, %fd16, %fd20;
	mov.f64 	%fd22, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd23, %fd21, %fd16, %fd22;
	mov.f64 	%fd24, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd25, %fd23, %fd16, %fd24;
	mov.f64 	%fd26, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd27, %fd25, %fd16, %fd26;
	mov.f64 	%fd28, 0d3F81111111122322;
	fma.rn.f64 	%fd29, %fd27, %fd16, %fd28;
	mov.f64 	%fd30, 0d3FA55555555502A1;
	fma.rn.f64 	%fd31, %fd29, %fd16, %fd30;
	mov.f64 	%fd32, 0d3FC5555555555511;
	fma.rn.f64 	%fd33, %fd31, %fd16, %fd32;
	mov.f64 	%fd34, 0d3FE000000000000B;
	fma.rn.f64 	%fd35, %fd33, %fd16, %fd34;
	mov.f64 	%fd36, 0d3FF0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd16, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd16, %fd36;
	shl.b32 	%r17, %r16, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd38;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd38;
	}
	add.s32 	%r20, %r17, %r19;
	add.s32 	%r21, %r20, -2097152;
	mov.b64 	%fd7, {%r18, %r21};
	// inline asm
	rcp.approx.ftz.f64 %fd6,%fd7;
	// inline asm
	neg.f64 	%fd39, %fd7;
	fma.rn.f64 	%fd40, %fd39, %fd6, %fd36;
	fma.rn.f64 	%fd41, %fd40, %fd40, %fd40;
	fma.rn.f64 	%fd42, %fd41, %fd6, %fd6;
	mov.f64 	%fd43, 0d3FB0000000000000;
	fma.rn.f64 	%fd45, %fd42, %fd43, %fd7;
	bra.uni 	BB41_4;

BB41_2:
	setp.gtu.f64	%p3, %fd1, 0d7FF0000000000000;
	selp.f64	%fd45, %fd1, 0d7FF0000000000000, %p3;

BB41_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	add.f64 	%fd44, %fd45, %fd45;
	st.global.f64 	[%rd9], %fd44;

BB41_5:
	ret;
}

	// .globl	vec_cospi
.visible .entry vec_cospi(
	.param .u32 vec_cospi_param_0,
	.param .u64 vec_cospi_param_1,
	.param .u64 vec_cospi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r3, [vec_cospi_param_0];
	ld.param.u64 	%rd2, [vec_cospi_param_1];
	ld.param.u64 	%rd3, [vec_cospi_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB42_8;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd35, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd35;
	}
	add.s32 	%r14, %r13, %r13;
	setp.lt.u32	%p2, %r14, -2038431743;
	@%p2 bra 	BB42_3;

	mov.f64 	%fd11, 0d0000000000000000;
	mul.rn.f64 	%fd35, %fd35, %fd11;

BB42_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd35;
	}
	add.s32 	%r16, %r15, 1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd35;
	}
	mov.b64 	%fd12, {%r17, %r16};
	cvt.rni.f64.f64	%fd13, %fd12;
	cvt.rzi.s64.f64	%rd7, %fd13;
	cvt.u32.u64	%r18, %rd7;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FE0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd35;
	mul.f64 	%fd17, %fd16, 0d3CA1A62633145C07;
	mov.f64 	%fd18, 0d400921FB54442D18;
	fma.rn.f64 	%fd19, %fd16, %fd18, %fd17;
	add.s32 	%r2, %r18, 1;
	and.b32  	%r19, %r2, 1;
	shl.b32 	%r20, %r19, 3;
	mul.rn.f64 	%fd4, %fd19, %fd19;
	setp.eq.s32	%p3, %r19, 0;
	selp.f64	%fd20, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p3;
	mul.wide.u32 	%rd8, %r20, 8;
	mov.u64 	%rd9, __cudart_sin_cos_coeffs;
	add.s64 	%rd10, %rd8, %rd9;
	ld.const.f64 	%fd21, [%rd10+8];
	fma.rn.f64 	%fd22, %fd20, %fd4, %fd21;
	ld.const.f64 	%fd23, [%rd10+16];
	fma.rn.f64 	%fd24, %fd22, %fd4, %fd23;
	ld.const.f64 	%fd25, [%rd10+24];
	fma.rn.f64 	%fd26, %fd24, %fd4, %fd25;
	ld.const.f64 	%fd27, [%rd10+32];
	fma.rn.f64 	%fd28, %fd26, %fd4, %fd27;
	ld.const.f64 	%fd29, [%rd10+40];
	fma.rn.f64 	%fd30, %fd28, %fd4, %fd29;
	ld.const.f64 	%fd31, [%rd10+48];
	fma.rn.f64 	%fd5, %fd30, %fd4, %fd31;
	fma.rn.f64 	%fd36, %fd5, %fd19, %fd19;
	@%p3 bra 	BB42_5;

	mov.f64 	%fd32, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd5, %fd4, %fd32;

BB42_5:
	and.b32  	%r21, %r2, 2;
	setp.eq.s32	%p4, %r21, 0;
	@%p4 bra 	BB42_7;

	mov.f64 	%fd33, 0d0000000000000000;
	mov.f64 	%fd34, 0dBFF0000000000000;
	fma.rn.f64 	%fd36, %fd36, %fd34, %fd33;

BB42_7:
	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 3;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.f64 	[%rd13], %fd36;

BB42_8:
	ret;
}

	// .globl	vec_erfc
.visible .entry vec_erfc(
	.param .u32 vec_erfc_param_0,
	.param .u64 vec_erfc_param_1,
	.param .u64 vec_erfc_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<123>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r5, [vec_erfc_param_0];
	ld.param.u64 	%rd3, [vec_erfc_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB43_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 2146435072;
	@%p2 bra 	BB43_3;
	bra.uni 	BB43_2;

BB43_3:
	setp.lt.s32	%p7, %r2, 0;
	mov.b64 	%fd11, {%r4, %r3};
	add.f64 	%fd12, %fd11, 0dC010000000000000;
	add.f64 	%fd8, %fd11, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd7,%fd8;
	// inline asm
	neg.f64 	%fd13, %fd8;
	mov.f64 	%fd14, 0d3FF0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd7, %fd14;
	fma.rn.f64 	%fd16, %fd15, %fd15, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd7, %fd7;
	mul.f64 	%fd18, %fd12, %fd17;
	add.rn.f64 	%fd19, %fd18, %fd14;
	mov.f64 	%fd20, 0dC010000000000000;
	fma.rn.f64 	%fd21, %fd20, %fd19, %fd11;
	neg.f64 	%fd22, %fd18;
	fma.rn.f64 	%fd23, %fd22, %fd11, %fd21;
	fma.rn.f64 	%fd24, %fd17, %fd23, %fd18;
	mov.f64 	%fd25, 0dBE44E1C6FD03D328;
	mov.f64 	%fd26, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	mov.f64 	%fd40, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd41, %fd39, %fd24, %fd40;
	mov.f64 	%fd42, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd43, %fd41, %fd24, %fd42;
	mov.f64 	%fd44, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd45, %fd43, %fd24, %fd44;
	mov.f64 	%fd46, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd47, %fd45, %fd24, %fd46;
	mov.f64 	%fd48, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd49, %fd47, %fd24, %fd48;
	mov.f64 	%fd50, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd51, %fd49, %fd24, %fd50;
	mov.f64 	%fd52, 0dBF9096238568E357;
	fma.rn.f64 	%fd53, %fd51, %fd24, %fd52;
	mov.f64 	%fd54, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd55, %fd53, %fd24, %fd54;
	mov.f64 	%fd56, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd57, %fd55, %fd24, %fd56;
	mov.f64 	%fd58, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd59, %fd57, %fd24, %fd58;
	mov.f64 	%fd60, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd61, %fd59, %fd24, %fd60;
	mov.f64 	%fd62, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd63, %fd61, %fd24, %fd62;
	mov.f64 	%fd64, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd65, %fd63, %fd24, %fd64;
	mov.f64 	%fd66, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd67, %fd65, %fd24, %fd66;
	mov.f64 	%fd68, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd69, %fd67, %fd24, %fd68;
	mov.f64 	%fd70, 0d4000000000000000;
	fma.rn.f64 	%fd10, %fd70, %fd11, %fd14;
	// inline asm
	rcp.approx.ftz.f64 %fd9,%fd10;
	// inline asm
	neg.f64 	%fd71, %fd10;
	fma.rn.f64 	%fd72, %fd71, %fd9, %fd14;
	fma.rn.f64 	%fd73, %fd72, %fd72, %fd72;
	fma.rn.f64 	%fd74, %fd73, %fd9, %fd9;
	mul.f64 	%fd75, %fd69, %fd74;
	mul.f64 	%fd76, %fd75, 0dC000000000000000;
	fma.rn.f64 	%fd77, %fd11, %fd76, %fd69;
	neg.f64 	%fd78, %fd75;
	add.rn.f64 	%fd79, %fd77, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd74, %fd75;
	mul.f64 	%fd81, %fd11, %fd11;
	neg.f64 	%fd82, %fd81;
	mov.f64 	%fd83, 0d4338000000000000;
	mov.f64 	%fd84, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd85, %fd82, %fd84, %fd83;
	mov.f64 	%fd86, 0dC338000000000000;
	add.rn.f64 	%fd87, %fd85, %fd86;
	mov.f64 	%fd88, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd89, %fd87, %fd88, %fd82;
	mov.f64 	%fd90, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd91, %fd87, %fd90, %fd89;
	mov.f64 	%fd92, 0d3E928AF3FCA213EA;
	mov.f64 	%fd93, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0d3F81111111122322;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3FA55555555502A1;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	mov.f64 	%fd107, 0d3FC5555555555511;
	fma.rn.f64 	%fd108, %fd106, %fd91, %fd107;
	mov.f64 	%fd109, 0d3FE000000000000B;
	fma.rn.f64 	%fd110, %fd108, %fd91, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd91, %fd14;
	fma.rn.f64 	%fd112, %fd111, %fd91, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd85;
	}
	shr.u32 	%r16, %r15, 31;
	add.s32 	%r17, %r15, %r16;
	shr.s32 	%r18, %r17, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd112;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd112;
	}
	shl.b32 	%r21, %r18, 20;
	add.s32 	%r22, %r20, %r21;
	mov.b64 	%fd113, {%r19, %r22};
	sub.s32 	%r23, %r15, %r18;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd114, {%r26, %r25};
	mul.f64 	%fd115, %fd113, %fd114;
	neg.f64 	%fd116, %fd11;
	fma.rn.f64 	%fd117, %fd116, %fd11, %fd81;
	fma.rn.f64 	%fd118, %fd115, %fd117, %fd115;
	mul.f64 	%fd119, %fd80, %fd118;
	setp.gt.u32	%p8, %r3, 1077624832;
	selp.f64	%fd120, 0d0000000000000000, %fd119, %p8;
	sub.f64 	%fd121, %fd70, %fd120;
	selp.f64	%fd122, %fd121, %fd120, %p7;
	bra.uni 	BB43_4;

BB43_2:
	setp.lt.s32	%p3, %r2, 0;
	setp.eq.s32	%p4, %r4, 0;
	setp.eq.s32	%p5, %r3, 2146435072;
	and.pred  	%p6, %p5, %p4;
	selp.f64	%fd5, 0d4000000000000000, 0d0000000000000000, %p3;
	add.f64 	%fd6, %fd1, %fd1;
	selp.f64	%fd122, %fd5, %fd6, %p6;

BB43_4:
	ld.param.u64 	%rd10, [vec_erfc_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd122;

BB43_5:
	ret;
}

	// .globl	vec_erfcinv
.visible .entry vec_erfcinv(
	.param .u32 vec_erfcinv_param_0,
	.param .u64 vec_erfcinv_param_1,
	.param .u64 vec_erfcinv_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<66>;
	.reg .f64 	%fd<268>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r13, [vec_erfcinv_param_0];
	ld.param.u64 	%rd3, [vec_erfcinv_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB44_15;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd19, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd19, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB44_13;
	bra.uni 	BB44_2;

BB44_13:
	mul.rn.f64 	%fd174, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd174;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd174;
	}
	shr.u32 	%r45, %r43, 20;
	and.b32  	%r46, %r45, 2046;
	add.s32 	%r47, %r46, 2147482626;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd175, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd176, {%r49, %r48};
	sub.f64 	%fd177, %fd175, %fd176;
	and.b32  	%r50, %r43, -2145386497;
	add.s32 	%r51, %r50, 1071644672;
	mov.b64 	%fd178, {%r44, %r51};
	add.f64 	%fd179, %fd178, 0dBFF0000000000000;
	add.f64 	%fd173, %fd178, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd172,%fd173;
	// inline asm
	neg.f64 	%fd180, %fd173;
	mov.f64 	%fd181, 0d3FF0000000000000;
	fma.rn.f64 	%fd182, %fd180, %fd172, %fd181;
	fma.rn.f64 	%fd183, %fd182, %fd182, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd172, %fd172;
	mul.f64 	%fd185, %fd179, %fd184;
	mov.f64 	%fd186, 0dC000000000000000;
	fma.rn.f64 	%fd187, %fd186, %fd185, %fd179;
	neg.f64 	%fd188, %fd185;
	fma.rn.f64 	%fd189, %fd188, %fd179, %fd187;
	fma.rn.f64 	%fd190, %fd189, %fd184, %fd185;
	mul.f64 	%fd191, %fd190, %fd190;
	mov.f64 	%fd192, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd193, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd194, %fd193, %fd191, %fd192;
	mov.f64 	%fd195, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd196, %fd194, %fd191, %fd195;
	mov.f64 	%fd197, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd198, %fd196, %fd191, %fd197;
	mov.f64 	%fd199, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd200, %fd198, %fd191, %fd199;
	mov.f64 	%fd201, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd202, %fd200, %fd191, %fd201;
	mov.f64 	%fd203, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd204, %fd202, %fd191, %fd203;
	mov.f64 	%fd205, 0d3FC249249209112E;
	fma.rn.f64 	%fd206, %fd204, %fd191, %fd205;
	mov.f64 	%fd207, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd208, %fd206, %fd191, %fd207;
	mov.f64 	%fd209, 0d3FD5555555555535;
	fma.rn.f64 	%fd210, %fd208, %fd191, %fd209;
	mul.f64 	%fd211, %fd191, %fd210;
	fma.rn.f64 	%fd212, %fd211, %fd190, %fd190;
	add.f64 	%fd213, %fd212, %fd212;
	mov.f64 	%fd214, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd215, %fd177, %fd214, %fd213;
	mov.f64 	%fd216, 0dC009000000000000;
	sub.f64 	%fd217, %fd216, %fd215;
	mov.f64 	%fd218, 0dBC08DDF93324D327;
	mov.f64 	%fd219, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd220, %fd219, %fd217, %fd218;
	mov.f64 	%fd221, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd222, %fd220, %fd217, %fd221;
	mov.f64 	%fd223, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd224, %fd222, %fd217, %fd223;
	mov.f64 	%fd225, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd226, %fd224, %fd217, %fd225;
	mov.f64 	%fd227, 0d3C782E11898132E0;
	fma.rn.f64 	%fd228, %fd226, %fd217, %fd227;
	mov.f64 	%fd229, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd230, %fd228, %fd217, %fd229;
	mov.f64 	%fd231, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd232, %fd230, %fd217, %fd231;
	mov.f64 	%fd233, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd234, %fd232, %fd217, %fd233;
	mov.f64 	%fd235, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd236, %fd234, %fd217, %fd235;
	mov.f64 	%fd237, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd238, %fd236, %fd217, %fd237;
	mov.f64 	%fd239, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd240, %fd238, %fd217, %fd239;
	mov.f64 	%fd241, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd242, %fd240, %fd217, %fd241;
	mov.f64 	%fd243, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd244, %fd242, %fd217, %fd243;
	mov.f64 	%fd245, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd246, %fd244, %fd217, %fd245;
	mov.f64 	%fd247, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd248, %fd246, %fd217, %fd247;
	mov.f64 	%fd249, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd250, %fd248, %fd217, %fd249;
	mov.f64 	%fd251, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd252, %fd250, %fd217, %fd251;
	mov.f64 	%fd253, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd254, %fd252, %fd217, %fd253;
	mov.f64 	%fd255, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd256, %fd254, %fd217, %fd255;
	mov.f64 	%fd257, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd258, %fd256, %fd217, %fd257;
	mov.f64 	%fd259, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd260, %fd258, %fd217, %fd259;
	mov.f64 	%fd261, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd262, %fd260, %fd217, %fd261;
	fma.rn.f64 	%fd267, %fd262, %fd2, %fd262;
	bra.uni 	BB44_14;

BB44_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd263, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd263;
	}
	mov.b32 	 %f1, %r62;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd263;
	}
	@%p6 bra 	BB44_4;
	bra.uni 	BB44_3;

BB44_4:
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p7, %r62, 1048575;
	@%p7 bra 	BB44_6;

	mul.f64 	%fd263, %fd263, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd263;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd263;
	}
	mov.u32 	%r64, -1077;

BB44_6:
	add.s32 	%r32, %r62, -1;
	setp.lt.u32	%p8, %r32, 2146435071;
	@%p8 bra 	BB44_8;
	bra.uni 	BB44_7;

BB44_8:
	shr.u32 	%r34, %r62, 20;
	add.s32 	%r65, %r64, %r34;
	and.b32  	%r35, %r62, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd264, {%r63, %r36};
	setp.lt.s32	%p10, %r36, 1073127583;
	@%p10 bra 	BB44_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd264;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd264;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd264, {%r37, %r39};
	add.s32 	%r65, %r65, 1;

BB44_10:
	add.f64 	%fd107, %fd264, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd106,%fd107;
	// inline asm
	neg.f64 	%fd108, %fd107;
	mov.f64 	%fd109, 0d3FF0000000000000;
	fma.rn.f64 	%fd110, %fd108, %fd106, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd110, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd106, %fd106;
	add.f64 	%fd113, %fd264, 0dBFF0000000000000;
	mul.f64 	%fd114, %fd113, %fd112;
	fma.rn.f64 	%fd115, %fd113, %fd112, %fd114;
	mul.f64 	%fd116, %fd115, %fd115;
	mov.f64 	%fd117, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd118, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	mov.f64 	%fd120, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd121, %fd119, %fd116, %fd120;
	mov.f64 	%fd122, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd123, %fd121, %fd116, %fd122;
	mov.f64 	%fd124, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd125, %fd123, %fd116, %fd124;
	mov.f64 	%fd126, 0d3F624924923BE72D;
	fma.rn.f64 	%fd127, %fd125, %fd116, %fd126;
	mov.f64 	%fd128, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd129, %fd127, %fd116, %fd128;
	mov.f64 	%fd130, 0d3FB5555555555554;
	fma.rn.f64 	%fd131, %fd129, %fd116, %fd130;
	sub.f64 	%fd132, %fd113, %fd115;
	add.f64 	%fd133, %fd132, %fd132;
	neg.f64 	%fd134, %fd115;
	fma.rn.f64 	%fd135, %fd134, %fd113, %fd133;
	mul.f64 	%fd136, %fd112, %fd135;
	mul.f64 	%fd137, %fd116, %fd131;
	fma.rn.f64 	%fd138, %fd137, %fd115, %fd136;
	xor.b32  	%r40, %r65, -2147483648;
	mov.u32 	%r41, 1127219200;
	mov.b64 	%fd139, {%r40, %r41};
	mov.u32 	%r42, -2147483648;
	mov.b64 	%fd140, {%r42, %r41};
	sub.f64 	%fd141, %fd139, %fd140;
	mov.f64 	%fd142, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd143, %fd141, %fd142, %fd115;
	neg.f64 	%fd144, %fd141;
	fma.rn.f64 	%fd145, %fd144, %fd142, %fd143;
	sub.f64 	%fd146, %fd145, %fd115;
	sub.f64 	%fd147, %fd138, %fd146;
	mov.f64 	%fd148, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd149, %fd141, %fd148, %fd147;
	add.f64 	%fd265, %fd143, %fd149;
	bra.uni 	BB44_11;

BB44_3:
	shr.u32 	%r23, %r62, 20;
	and.b32  	%r24, %r23, 2046;
	add.s32 	%r25, %r24, 2147482626;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd24, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd25, {%r27, %r26};
	sub.f64 	%fd26, %fd24, %fd25;
	and.b32  	%r28, %r62, -2145386497;
	add.s32 	%r29, %r28, 1071644672;
	mov.b64 	%fd27, {%r63, %r29};
	add.f64 	%fd28, %fd27, 0dBFF0000000000000;
	add.f64 	%fd21, %fd27, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd20,%fd21;
	// inline asm
	neg.f64 	%fd29, %fd21;
	mov.f64 	%fd30, 0d3FF0000000000000;
	fma.rn.f64 	%fd31, %fd29, %fd20, %fd30;
	fma.rn.f64 	%fd32, %fd31, %fd31, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd20, %fd20;
	mul.f64 	%fd34, %fd28, %fd33;
	mov.f64 	%fd35, 0dC000000000000000;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd28;
	neg.f64 	%fd37, %fd34;
	fma.rn.f64 	%fd38, %fd37, %fd28, %fd36;
	fma.rn.f64 	%fd39, %fd38, %fd33, %fd34;
	mul.f64 	%fd40, %fd39, %fd39;
	mov.f64 	%fd41, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd42, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd43, %fd42, %fd40, %fd41;
	mov.f64 	%fd44, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd45, %fd43, %fd40, %fd44;
	mov.f64 	%fd46, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd47, %fd45, %fd40, %fd46;
	mov.f64 	%fd48, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd49, %fd47, %fd40, %fd48;
	mov.f64 	%fd50, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd51, %fd49, %fd40, %fd50;
	mov.f64 	%fd52, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd53, %fd51, %fd40, %fd52;
	mov.f64 	%fd54, 0d3FC249249209112E;
	fma.rn.f64 	%fd55, %fd53, %fd40, %fd54;
	mov.f64 	%fd56, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd57, %fd55, %fd40, %fd56;
	mov.f64 	%fd58, 0d3FD5555555555535;
	fma.rn.f64 	%fd59, %fd57, %fd40, %fd58;
	mul.f64 	%fd60, %fd40, %fd59;
	fma.rn.f64 	%fd61, %fd60, %fd39, %fd39;
	add.f64 	%fd62, %fd61, %fd61;
	mov.f64 	%fd63, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd64, %fd26, %fd63, %fd62;
	neg.f64 	%fd23, %fd64;
	// inline asm
	rsqrt.approx.ftz.f64 %fd22, %fd23;
	// inline asm
	mul.rn.f64 	%fd65, %fd22, %fd22;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd23, %fd66, %fd30;
	mov.f64 	%fd68, 0d3FE0000000000000;
	mov.f64 	%fd69, 0d3FD8000000000000;
	fma.rn.f64 	%fd70, %fd69, %fd67, %fd68;
	mul.rn.f64 	%fd71, %fd67, %fd22;
	fma.rn.f64 	%fd72, %fd70, %fd71, %fd22;
	mov.f64 	%fd73, 0d4000A0E7333839AA;
	mov.f64 	%fd74, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd75, %fd74, %fd72, %fd73;
	mov.f64 	%fd76, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd77, %fd75, %fd72, %fd76;
	mov.f64 	%fd78, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd79, %fd77, %fd72, %fd78;
	mov.f64 	%fd80, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd81, %fd79, %fd72, %fd80;
	mov.f64 	%fd82, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd83, %fd81, %fd72, %fd82;
	mov.f64 	%fd84, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd85, %fd83, %fd72, %fd84;
	mov.f64 	%fd86, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd87, %fd85, %fd72, %fd86;
	add.f64 	%fd88, %fd72, 0d4001E90FF51C2197;
	mov.f64 	%fd89, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd90, %fd88, %fd72, %fd89;
	mov.f64 	%fd91, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd92, %fd90, %fd72, %fd91;
	mov.f64 	%fd93, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd94, %fd92, %fd72, %fd93;
	mov.f64 	%fd95, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd96, %fd94, %fd72, %fd95;
	mov.f64 	%fd97, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd98, %fd96, %fd72, %fd97;
	mov.f64 	%fd99, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd100, %fd98, %fd72, %fd99;
	mov.f64 	%fd101, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd102, %fd100, %fd72, %fd101;
	mul.f64 	%fd103, %fd72, %fd102;
	div.rn.f64 	%fd266, %fd87, %fd103;
	bra.uni 	BB44_12;

BB44_7:
	mov.f64 	%fd104, 0d7FF0000000000000;
	fma.rn.f64 	%fd105, %fd263, %fd104, %fd104;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd263;
	}
	mov.b32 	 %f2, %r33;
	setp.eq.f32	%p9, %f2, 0f00000000;
	selp.f64	%fd265, 0dFFF0000000000000, %fd105, %p9;

BB44_11:
	neg.f64 	%fd150, %fd265;
	rsqrt.approx.f64 	%fd151, %fd150;
	mov.f64 	%fd152, 0d3FFA2013964E259C;
	mov.f64 	%fd153, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd154, %fd153, %fd151, %fd152;
	mov.f64 	%fd155, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd156, %fd154, %fd151, %fd155;
	mov.f64 	%fd157, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd158, %fd156, %fd151, %fd157;
	mov.f64 	%fd159, 0d3F311BD487102E94;
	fma.rn.f64 	%fd160, %fd158, %fd151, %fd159;
	add.f64 	%fd161, %fd151, 0d3FF59895C30BAA54;
	mov.f64 	%fd162, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd163, %fd161, %fd151, %fd162;
	mov.f64 	%fd164, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd165, %fd163, %fd151, %fd164;
	mov.f64 	%fd166, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd167, %fd165, %fd151, %fd166;
	mov.f64 	%fd168, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd169, %fd167, %fd151, %fd168;
	mul.f64 	%fd170, %fd151, %fd169;
	div.rn.f64 	%fd266, %fd160, %fd170;

BB44_12:
	neg.f64 	%fd171, %fd266;
	selp.f64	%fd267, %fd171, %fd266, %p5;

BB44_14:
	mov.u32 	%r61, %tid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r58, %ctaid.x;
	mov.u32 	%r57, %nctaid.x;
	mad.lo.s32 	%r56, %r59, %r60, %r61;
	mov.u32 	%r55, %tid.x;
	mov.u32 	%r54, %ntid.x;
	mad.lo.s32 	%r53, %r56, %r57, %r58;
	mad.lo.s32 	%r52, %r53, %r54, %r55;
	cvt.s64.s32	%rd11, %r52;
	ld.param.u64 	%rd10, [vec_erfcinv_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd267;

BB44_15:
	ret;
}

	// .globl	vec_erfcx
.visible .entry vec_erfcx(
	.param .u32 vec_erfcx_param_0,
	.param .u64 vec_erfcx_param_1,
	.param .u64 vec_erfcx_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<140>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r6, [vec_erfcx_param_0];
	ld.param.u64 	%rd3, [vec_erfcx_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB45_10;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.b32 	 %f2, %r2;
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p2, %f3, 0f40400000;
	@%p2 bra 	BB45_3;
	bra.uni 	BB45_2;

BB45_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd1;
	}
	and.b32  	%r17, %r2, 2147483647;
	mov.b64 	%fd31, {%r16, %r17};
	add.f64 	%fd32, %fd31, 0dC010000000000000;
	add.f64 	%fd28, %fd31, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd27,%fd28;
	// inline asm
	neg.f64 	%fd33, %fd28;
	mov.f64 	%fd34, 0d3FF0000000000000;
	fma.rn.f64 	%fd35, %fd33, %fd27, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd35, %fd35;
	fma.rn.f64 	%fd37, %fd36, %fd27, %fd27;
	mul.f64 	%fd38, %fd32, %fd37;
	add.rn.f64 	%fd39, %fd38, %fd34;
	mov.f64 	%fd40, 0dC010000000000000;
	fma.rn.f64 	%fd41, %fd40, %fd39, %fd31;
	neg.f64 	%fd42, %fd38;
	fma.rn.f64 	%fd43, %fd42, %fd31, %fd41;
	fma.rn.f64 	%fd44, %fd37, %fd43, %fd38;
	mov.f64 	%fd45, 0dBE44E1C6FD03D328;
	mov.f64 	%fd46, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd47, %fd46, %fd44, %fd45;
	mov.f64 	%fd48, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd49, %fd47, %fd44, %fd48;
	mov.f64 	%fd50, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd51, %fd49, %fd44, %fd50;
	mov.f64 	%fd52, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd53, %fd51, %fd44, %fd52;
	mov.f64 	%fd54, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd55, %fd53, %fd44, %fd54;
	mov.f64 	%fd56, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd57, %fd55, %fd44, %fd56;
	mov.f64 	%fd58, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd59, %fd57, %fd44, %fd58;
	mov.f64 	%fd60, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd61, %fd59, %fd44, %fd60;
	mov.f64 	%fd62, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd63, %fd61, %fd44, %fd62;
	mov.f64 	%fd64, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd65, %fd63, %fd44, %fd64;
	mov.f64 	%fd66, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd67, %fd65, %fd44, %fd66;
	mov.f64 	%fd68, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd69, %fd67, %fd44, %fd68;
	mov.f64 	%fd70, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd71, %fd69, %fd44, %fd70;
	mov.f64 	%fd72, 0dBF9096238568E357;
	fma.rn.f64 	%fd73, %fd71, %fd44, %fd72;
	mov.f64 	%fd74, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd75, %fd73, %fd44, %fd74;
	mov.f64 	%fd76, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd77, %fd75, %fd44, %fd76;
	mov.f64 	%fd78, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd79, %fd77, %fd44, %fd78;
	mov.f64 	%fd80, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd81, %fd79, %fd44, %fd80;
	mov.f64 	%fd82, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd83, %fd81, %fd44, %fd82;
	mov.f64 	%fd84, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd85, %fd83, %fd44, %fd84;
	mov.f64 	%fd86, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd87, %fd85, %fd44, %fd86;
	mov.f64 	%fd88, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd89, %fd87, %fd44, %fd88;
	mov.f64 	%fd90, 0d4000000000000000;
	fma.rn.f64 	%fd30, %fd90, %fd31, %fd34;
	// inline asm
	rcp.approx.ftz.f64 %fd29,%fd30;
	// inline asm
	neg.f64 	%fd91, %fd30;
	fma.rn.f64 	%fd92, %fd91, %fd29, %fd34;
	fma.rn.f64 	%fd93, %fd92, %fd92, %fd92;
	fma.rn.f64 	%fd94, %fd93, %fd29, %fd29;
	mul.f64 	%fd95, %fd89, %fd94;
	mul.f64 	%fd96, %fd95, 0dC000000000000000;
	fma.rn.f64 	%fd97, %fd31, %fd96, %fd89;
	neg.f64 	%fd98, %fd95;
	add.rn.f64 	%fd99, %fd97, %fd98;
	fma.rn.f64 	%fd139, %fd99, %fd94, %fd95;
	bra.uni 	BB45_4;

BB45_2:
	rcp.rn.f64 	%fd13, %fd1;
	mul.f64 	%fd14, %fd13, %fd13;
	mov.f64 	%fd15, 0d401A400000000000;
	mov.f64 	%fd16, 0dC03D880000000000;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0dBFFE000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3FE8000000000000;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0dBFE0000000000000;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3FF0000000000000;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mul.f64 	%fd26, %fd13, 0d3FE20DD750429B6D;
	mul.f64 	%fd139, %fd26, %fd25;

BB45_4:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd1;
	}
	setp.gt.s32	%p3, %r30, -1;
	@%p3 bra 	BB45_9;

	mul.f64 	%fd5, %fd1, %fd1;
	neg.f64 	%fd100, %fd5;
	fma.rn.f64 	%fd6, %fd1, %fd1, %fd100;
	mov.f64 	%fd101, 0d4338000000000000;
	mov.f64 	%fd102, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd103, %fd5, %fd102, %fd101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd103;
	}
	mov.f64 	%fd104, 0dC338000000000000;
	add.rn.f64 	%fd105, %fd103, %fd104;
	mov.f64 	%fd106, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd107, %fd105, %fd106, %fd5;
	mov.f64 	%fd108, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd109, %fd105, %fd108, %fd107;
	mov.f64 	%fd110, 0d3E928AF3FCA213EA;
	mov.f64 	%fd111, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd112, %fd111, %fd109, %fd110;
	mov.f64 	%fd113, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd114, %fd112, %fd109, %fd113;
	mov.f64 	%fd115, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd116, %fd114, %fd109, %fd115;
	mov.f64 	%fd117, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd118, %fd116, %fd109, %fd117;
	mov.f64 	%fd119, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd120, %fd118, %fd109, %fd119;
	mov.f64 	%fd121, 0d3F81111111122322;
	fma.rn.f64 	%fd122, %fd120, %fd109, %fd121;
	mov.f64 	%fd123, 0d3FA55555555502A1;
	fma.rn.f64 	%fd124, %fd122, %fd109, %fd123;
	mov.f64 	%fd125, 0d3FC5555555555511;
	fma.rn.f64 	%fd126, %fd124, %fd109, %fd125;
	mov.f64 	%fd127, 0d3FE000000000000B;
	fma.rn.f64 	%fd128, %fd126, %fd109, %fd127;
	mov.f64 	%fd129, 0d3FF0000000000000;
	fma.rn.f64 	%fd130, %fd128, %fd109, %fd129;
	fma.rn.f64 	%fd131, %fd130, %fd109, %fd129;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd131;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd131;
	}
	shl.b32 	%r18, %r3, 20;
	add.s32 	%r19, %r5, %r18;
	mov.b64 	%fd138, {%r4, %r19};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd5;
	}
	mov.b32 	 %f4, %r20;
	abs.f32 	%f1, %f4;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB45_8;

	setp.lt.f64	%p5, %fd5, 0d0000000000000000;
	add.f64 	%fd132, %fd5, 0d7FF0000000000000;
	selp.f64	%fd138, 0d0000000000000000, %fd132, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB45_8;

	shr.u32 	%r21, %r3, 31;
	add.s32 	%r22, %r3, %r21;
	shr.s32 	%r23, %r22, 1;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, %r5;
	mov.b64 	%fd133, {%r4, %r25};
	sub.s32 	%r26, %r3, %r23;
	shl.b32 	%r27, %r26, 20;
	add.s32 	%r28, %r27, 1072693248;
	mov.u32 	%r29, 0;
	mov.b64 	%fd134, {%r29, %r28};
	mul.f64 	%fd138, %fd133, %fd134;

BB45_8:
	add.f64 	%fd135, %fd138, %fd138;
	fma.rn.f64 	%fd136, %fd135, %fd6, %fd135;
	sub.f64 	%fd137, %fd136, %fd139;
	setp.eq.f64	%p7, %fd135, 0d7FF0000000000000;
	selp.f64	%fd139, %fd135, %fd137, %p7;

BB45_9:
	mov.u32 	%r40, %tid.y;
	mov.u32 	%r39, %ctaid.y;
	mov.u32 	%r38, %ntid.y;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r36, %nctaid.x;
	mad.lo.s32 	%r35, %r38, %r39, %r40;
	mov.u32 	%r34, %tid.x;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r32, %r35, %r36, %r37;
	mad.lo.s32 	%r31, %r32, %r33, %r34;
	cvt.s64.s32	%rd11, %r31;
	ld.param.u64 	%rd10, [vec_erfcx_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd139;

BB45_10:
	ret;
}

	// .globl	vec_erf
.visible .entry vec_erf(
	.param .u32 vec_erf_param_0,
	.param .u64 vec_erf_param_1,
	.param .u64 vec_erf_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<111>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r5, [vec_erf_param_0];
	ld.param.u64 	%rd3, [vec_erf_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB46_9;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd1;
	}
	setp.lt.u32	%p2, %r3, 1072693248;
	@%p2 bra 	BB46_7;
	bra.uni 	BB46_2;

BB46_7:
	mul.f64 	%fd86, %fd1, %fd1;
	mov.f64 	%fd87, 0d3E4D5F4BB7A316F6;
	mov.f64 	%fd88, 0dBE0A83AA3B08FBC2;
	fma.rn.f64 	%fd89, %fd88, %fd86, %fd87;
	mov.f64 	%fd90, 0dBE85BDCE301B3CDF;
	fma.rn.f64 	%fd91, %fd89, %fd86, %fd90;
	mov.f64 	%fd92, 0d3EBB978FADB81BC9;
	fma.rn.f64 	%fd93, %fd91, %fd86, %fd92;
	mov.f64 	%fd94, 0dBEEF4C99D6AE5FB8;
	fma.rn.f64 	%fd95, %fd93, %fd86, %fd94;
	mov.f64 	%fd96, 0d3F1F9A2AF549012E;
	fma.rn.f64 	%fd97, %fd95, %fd86, %fd96;
	mov.f64 	%fd98, 0dBF4C02DAFC636A47;
	fma.rn.f64 	%fd99, %fd97, %fd86, %fd98;
	mov.f64 	%fd100, 0d3F7565BCCF619AC0;
	fma.rn.f64 	%fd101, %fd99, %fd86, %fd100;
	mov.f64 	%fd102, 0dBF9B82CE311E321A;
	fma.rn.f64 	%fd103, %fd101, %fd86, %fd102;
	mov.f64 	%fd104, 0d3FBCE2F21A04075C;
	fma.rn.f64 	%fd105, %fd103, %fd86, %fd104;
	mov.f64 	%fd106, 0dBFD812746B0379B4;
	fma.rn.f64 	%fd107, %fd105, %fd86, %fd106;
	mov.f64 	%fd108, 0d3FF20DD750429B6D;
	fma.rn.f64 	%fd109, %fd107, %fd86, %fd108;
	mul.f64 	%fd110, %fd1, %fd109;
	bra.uni 	BB46_8;

BB46_2:
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB46_6;
	bra.uni 	BB46_3;

BB46_6:
	mov.b64 	%fd8, {%r4, %r3};
	mov.f64 	%fd9, 0dBCF1384CE38C616A;
	mov.f64 	%fd10, 0d3C8B9C2B870030E8;
	fma.rn.f64 	%fd11, %fd10, %fd8, %fd9;
	mov.f64 	%fd12, 0d3D4458AE9746C2FD;
	fma.rn.f64 	%fd13, %fd11, %fd8, %fd12;
	mov.f64 	%fd14, 0dBD8E4A44D4F1AB56;
	fma.rn.f64 	%fd15, %fd13, %fd8, %fd14;
	mov.f64 	%fd16, 0d3DCFDF15265C58EE;
	fma.rn.f64 	%fd17, %fd15, %fd8, %fd16;
	mov.f64 	%fd18, 0dBE0933832F358D51;
	fma.rn.f64 	%fd19, %fd17, %fd8, %fd18;
	mov.f64 	%fd20, 0d3E3F136D3F719446;
	fma.rn.f64 	%fd21, %fd19, %fd8, %fd20;
	mov.f64 	%fd22, 0dBE6E94C2FE151B3B;
	fma.rn.f64 	%fd23, %fd21, %fd8, %fd22;
	mov.f64 	%fd24, 0d3E985A70310EE0A8;
	fma.rn.f64 	%fd25, %fd23, %fd8, %fd24;
	mov.f64 	%fd26, 0dBEBF944DA1520B74;
	fma.rn.f64 	%fd27, %fd25, %fd8, %fd26;
	mov.f64 	%fd28, 0d3EE09F503825C543;
	fma.rn.f64 	%fd29, %fd27, %fd8, %fd28;
	mov.f64 	%fd30, 0dBEFBEEFE9F949E59;
	fma.rn.f64 	%fd31, %fd29, %fd8, %fd30;
	mov.f64 	%fd32, 0d3F11D785C6E28857;
	fma.rn.f64 	%fd33, %fd31, %fd8, %fd32;
	mov.f64 	%fd34, 0dBF1D866B223048C7;
	fma.rn.f64 	%fd35, %fd33, %fd8, %fd34;
	mov.f64 	%fd36, 0d3EF258F0847E8908;
	fma.rn.f64 	%fd37, %fd35, %fd8, %fd36;
	mov.f64 	%fd38, 0d3F429CFC58DBB776;
	fma.rn.f64 	%fd39, %fd37, %fd8, %fd38;
	mov.f64 	%fd40, 0dBF5BE16D3F71F3C5;
	fma.rn.f64 	%fd41, %fd39, %fd8, %fd40;
	mov.f64 	%fd42, 0d3F2E8BDA60326B1A;
	fma.rn.f64 	%fd43, %fd41, %fd8, %fd42;
	mov.f64 	%fd44, 0d3F938FB20B0988A6;
	fma.rn.f64 	%fd45, %fd43, %fd8, %fd44;
	mov.f64 	%fd46, 0dBFBA4E3A80F64E33;
	fma.rn.f64 	%fd47, %fd45, %fd8, %fd46;
	mov.f64 	%fd48, 0dBFE45F3E88093928;
	fma.rn.f64 	%fd49, %fd47, %fd8, %fd48;
	mov.f64 	%fd50, 0dBFF20DD599CAEEA0;
	fma.rn.f64 	%fd51, %fd49, %fd8, %fd50;
	mov.f64 	%fd52, 0dBE883BE1E31CE133;
	fma.rn.f64 	%fd53, %fd51, %fd8, %fd52;
	mov.f64 	%fd54, 0d4338000000000000;
	mov.f64 	%fd55, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd56, %fd53, %fd55, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd56;
	}
	mov.f64 	%fd57, 0dC338000000000000;
	add.rn.f64 	%fd58, %fd56, %fd57;
	mov.f64 	%fd59, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd60, %fd58, %fd59, %fd53;
	mov.f64 	%fd61, 0d3E928AF3FCA213EA;
	mov.f64 	%fd62, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0d3F81111111122322;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0d3FA55555555502A1;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3FC5555555555511;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	mov.f64 	%fd78, 0d3FE000000000000B;
	fma.rn.f64 	%fd79, %fd77, %fd60, %fd78;
	mov.f64 	%fd80, 0d3FF0000000000000;
	fma.rn.f64 	%fd81, %fd79, %fd60, %fd80;
	fma.rn.f64 	%fd82, %fd81, %fd60, %fd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd82;
	}
	shl.b32 	%r21, %r19, 20;
	add.s32 	%r22, %r20, %r21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd82;
	}
	mov.b64 	%fd83, {%r23, %r22};
	sub.f64 	%fd84, %fd80, %fd83;
	setp.gt.u32	%p7, %r3, 1075294207;
	selp.f64	%fd85, 0d3FF0000000000000, %fd84, %p7;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd85;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd85;
	}
	and.b32  	%r26, %r2, -2147483648;
	or.b32  	%r27, %r25, %r26;
	mov.b64 	%fd110, {%r24, %r27};
	bra.uni 	BB46_8;

BB46_3:
	setp.eq.s32	%p4, %r3, 2146435072;
	setp.eq.s32	%p5, %r4, 0;
	and.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB46_5;
	bra.uni 	BB46_4;

BB46_5:
	mov.f64 	%fd7, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd7;
	}
	and.b32  	%r16, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd7;
	}
	or.b32  	%r18, %r17, %r16;
	mov.b64 	%fd110, {%r15, %r18};
	bra.uni 	BB46_8;

BB46_4:
	add.f64 	%fd110, %fd1, %fd1;

BB46_8:
	mov.u32 	%r37, %tid.y;
	mov.u32 	%r36, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r33, %nctaid.x;
	mad.lo.s32 	%r32, %r35, %r36, %r37;
	mov.u32 	%r31, %tid.x;
	mov.u32 	%r30, %ntid.x;
	mad.lo.s32 	%r29, %r32, %r33, %r34;
	mad.lo.s32 	%r28, %r29, %r30, %r31;
	cvt.s64.s32	%rd11, %r28;
	ld.param.u64 	%rd10, [vec_erf_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd110;

BB46_9:
	ret;
}

	// .globl	vec_erfinv
.visible .entry vec_erfinv(
	.param .u32 vec_erfinv_param_0,
	.param .u64 vec_erfinv_param_1,
	.param .u64 vec_erfinv_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<34>;
	.reg .f64 	%fd<173>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r2, [vec_erfinv_param_0];
	ld.param.u64 	%rd3, [vec_erfinv_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB47_10;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	mov.b32 	 %f1, %r12;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p2, %f2, 0f3FF00000;
	@%p2 bra 	BB47_4;
	bra.uni 	BB47_2;

BB47_4:
	neg.f64 	%fd12, %fd1;
	mov.f64 	%fd13, 0d3FF0000000000000;
	fma.rn.f64 	%fd14, %fd1, %fd12, %fd13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd14;
	}
	shr.u32 	%r15, %r13, 20;
	and.b32  	%r16, %r15, 2046;
	add.s32 	%r17, %r16, 2147482626;
	mov.u32 	%r18, 1127219200;
	mov.b64 	%fd15, {%r17, %r18};
	mov.u32 	%r19, -2147483648;
	mov.b64 	%fd16, {%r19, %r18};
	sub.f64 	%fd17, %fd15, %fd16;
	and.b32  	%r20, %r13, -2145386497;
	add.s32 	%r21, %r20, 1071644672;
	mov.b64 	%fd18, {%r14, %r21};
	add.f64 	%fd19, %fd18, 0dBFF0000000000000;
	add.f64 	%fd11, %fd18, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd10,%fd11;
	// inline asm
	neg.f64 	%fd20, %fd11;
	fma.rn.f64 	%fd21, %fd20, %fd10, %fd13;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd10, %fd10;
	mul.f64 	%fd24, %fd19, %fd23;
	mov.f64 	%fd25, 0dC000000000000000;
	fma.rn.f64 	%fd26, %fd25, %fd24, %fd19;
	neg.f64 	%fd27, %fd24;
	fma.rn.f64 	%fd28, %fd27, %fd19, %fd26;
	fma.rn.f64 	%fd29, %fd28, %fd23, %fd24;
	mul.f64 	%fd30, %fd29, %fd29;
	mov.f64 	%fd31, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd32, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd33, %fd32, %fd30, %fd31;
	mov.f64 	%fd34, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd35, %fd33, %fd30, %fd34;
	mov.f64 	%fd36, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd37, %fd35, %fd30, %fd36;
	mov.f64 	%fd38, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd39, %fd37, %fd30, %fd38;
	mov.f64 	%fd40, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd41, %fd39, %fd30, %fd40;
	mov.f64 	%fd42, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd43, %fd41, %fd30, %fd42;
	mov.f64 	%fd44, 0d3FC249249209112E;
	fma.rn.f64 	%fd45, %fd43, %fd30, %fd44;
	mov.f64 	%fd46, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd47, %fd45, %fd30, %fd46;
	mov.f64 	%fd48, 0d3FD5555555555535;
	fma.rn.f64 	%fd49, %fd47, %fd30, %fd48;
	mul.f64 	%fd50, %fd30, %fd49;
	fma.rn.f64 	%fd51, %fd50, %fd29, %fd29;
	add.f64 	%fd52, %fd51, %fd51;
	mov.f64 	%fd53, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd4, %fd17, %fd53, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd4;
	}
	setp.lt.u32	%p5, %r22, -1072103424;
	@%p5 bra 	BB47_8;
	bra.uni 	BB47_5;

BB47_8:
	mov.f64 	%fd125, 0dC009000000000000;
	sub.f64 	%fd126, %fd125, %fd4;
	mov.f64 	%fd127, 0dBC08DDF93324D327;
	mov.f64 	%fd128, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd129, %fd128, %fd126, %fd127;
	mov.f64 	%fd130, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd131, %fd129, %fd126, %fd130;
	mov.f64 	%fd132, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd133, %fd131, %fd126, %fd132;
	mov.f64 	%fd134, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd135, %fd133, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C782E11898132E0;
	fma.rn.f64 	%fd137, %fd135, %fd126, %fd136;
	mov.f64 	%fd138, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd139, %fd137, %fd126, %fd138;
	mov.f64 	%fd140, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd141, %fd139, %fd126, %fd140;
	mov.f64 	%fd142, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd143, %fd141, %fd126, %fd142;
	mov.f64 	%fd144, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd145, %fd143, %fd126, %fd144;
	mov.f64 	%fd146, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd147, %fd145, %fd126, %fd146;
	mov.f64 	%fd148, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd149, %fd147, %fd126, %fd148;
	mov.f64 	%fd150, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd151, %fd149, %fd126, %fd150;
	mov.f64 	%fd152, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd153, %fd151, %fd126, %fd152;
	mov.f64 	%fd154, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd155, %fd153, %fd126, %fd154;
	mov.f64 	%fd156, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd157, %fd155, %fd126, %fd156;
	mov.f64 	%fd158, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd159, %fd157, %fd126, %fd158;
	mov.f64 	%fd160, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd161, %fd159, %fd126, %fd160;
	mov.f64 	%fd162, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd163, %fd161, %fd126, %fd162;
	mov.f64 	%fd164, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd165, %fd163, %fd126, %fd164;
	mov.f64 	%fd166, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd167, %fd165, %fd126, %fd166;
	mov.f64 	%fd168, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd169, %fd167, %fd126, %fd168;
	mov.f64 	%fd170, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd8, %fd169, %fd126, %fd170;
	mov.f64 	%fd172, %fd8;
	bra.uni 	BB47_9;

BB47_2:
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p3, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd172, %fd1;
	@%p3 bra 	BB47_9;

	setp.eq.f64	%p4, %fd2, 0d3FF0000000000000;
	selp.f64	%fd3, 0d7FF0000000000000, 0dFFF8000000000000, %p4;
	mov.f64 	%fd172, %fd3;
	bra.uni 	BB47_9;

BB47_5:
	neg.f64 	%fd54, %fd4;
	sqrt.rn.f64 	%fd5, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd5;
	}
	setp.lt.s32	%p6, %r23, 1074790400;
	@%p6 bra 	BB47_7;
	bra.uni 	BB47_6;

BB47_7:
	add.f64 	%fd88, %fd5, 0dC00A000000000000;
	mov.f64 	%fd89, 0d3E785CBE52878635;
	mov.f64 	%fd90, 0d3E23040F87DBD932;
	fma.rn.f64 	%fd91, %fd90, %fd88, %fd89;
	mov.f64 	%fd92, 0dBE92777453DD3955;
	fma.rn.f64 	%fd93, %fd91, %fd88, %fd92;
	mov.f64 	%fd94, 0d3E5395ABCD554C6C;
	fma.rn.f64 	%fd95, %fd93, %fd88, %fd94;
	mov.f64 	%fd96, 0d3EB936388A3790AD;
	fma.rn.f64 	%fd97, %fd95, %fd88, %fd96;
	mov.f64 	%fd98, 0dBED0D5DB812B5083;
	fma.rn.f64 	%fd99, %fd97, %fd88, %fd98;
	mov.f64 	%fd100, 0d3EC8860CD5D652F6;
	fma.rn.f64 	%fd101, %fd99, %fd88, %fd100;
	mov.f64 	%fd102, 0d3EEA29A0CACDFB23;
	fma.rn.f64 	%fd103, %fd101, %fd88, %fd102;
	mov.f64 	%fd104, 0dBF08CEF1F80281F2;
	fma.rn.f64 	%fd105, %fd103, %fd88, %fd104;
	mov.f64 	%fd106, 0d3F11E684D0B9188A;
	fma.rn.f64 	%fd107, %fd105, %fd88, %fd106;
	mov.f64 	%fd108, 0d3EF932CD54C8A222;
	fma.rn.f64 	%fd109, %fd107, %fd88, %fd108;
	mov.f64 	%fd110, 0dBF37448A89EF8AA3;
	fma.rn.f64 	%fd111, %fd109, %fd88, %fd110;
	mov.f64 	%fd112, 0d3F4F3CC55AD40C25;
	fma.rn.f64 	%fd113, %fd111, %fd88, %fd112;
	mov.f64 	%fd114, 0dBF5BA924132F38B1;
	fma.rn.f64 	%fd115, %fd113, %fd88, %fd114;
	mov.f64 	%fd116, 0d3F6468EECA533CF8;
	fma.rn.f64 	%fd117, %fd115, %fd88, %fd116;
	mov.f64 	%fd118, 0dBF6EBADABB891BBD;
	fma.rn.f64 	%fd119, %fd117, %fd88, %fd118;
	mov.f64 	%fd120, 0d3F75FFCFE5B76AFC;
	fma.rn.f64 	%fd121, %fd119, %fd88, %fd120;
	mov.f64 	%fd122, 0d3FF0158A6D641D39;
	fma.rn.f64 	%fd123, %fd121, %fd88, %fd122;
	mov.f64 	%fd124, 0d4008ABCC380D5A48;
	fma.rn.f64 	%fd7, %fd123, %fd88, %fd124;
	mov.f64 	%fd172, %fd7;
	bra.uni 	BB47_9;

BB47_6:
	add.f64 	%fd55, %fd5, 0dC014000000000000;
	mov.f64 	%fd56, 0dBDF18FEEC0E38727;
	mov.f64 	%fd57, 0dBDBDCEC3A7785389;
	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
	mov.f64 	%fd59, 0d3E19E6BF2DDA45E3;
	fma.rn.f64 	%fd60, %fd58, %fd55, %fd59;
	mov.f64 	%fd61, 0dBE30468FB24E2F5F;
	fma.rn.f64 	%fd62, %fd60, %fd55, %fd61;
	mov.f64 	%fd63, 0d3E405AC6A8FBA182;
	fma.rn.f64 	%fd64, %fd62, %fd55, %fd63;
	mov.f64 	%fd65, 0dBE50102E495FB9C0;
	fma.rn.f64 	%fd66, %fd64, %fd55, %fd65;
	mov.f64 	%fd67, 0d3E5F4C20E1334AF8;
	fma.rn.f64 	%fd68, %fd66, %fd55, %fd67;
	mov.f64 	%fd69, 0dBE722D220FDF9C3E;
	fma.rn.f64 	%fd70, %fd68, %fd55, %fd69;
	mov.f64 	%fd71, 0d3E8EBC8BB824CB54;
	fma.rn.f64 	%fd72, %fd70, %fd55, %fd71;
	mov.f64 	%fd73, 0dBEB0A8D40EA372CC;
	fma.rn.f64 	%fd74, %fd72, %fd55, %fd73;
	mov.f64 	%fd75, 0d3ED2FBD29D093D2B;
	fma.rn.f64 	%fd76, %fd74, %fd55, %fd75;
	mov.f64 	%fd77, 0dBEF4A3497E1E0FAC;
	fma.rn.f64 	%fd78, %fd76, %fd55, %fd77;
	mov.f64 	%fd79, 0d3F13EBF4EB00938F;
	fma.rn.f64 	%fd80, %fd78, %fd55, %fd79;
	mov.f64 	%fd81, 0dBF2C2F36A8FC5D53;
	fma.rn.f64 	%fd82, %fd80, %fd55, %fd81;
	mov.f64 	%fd83, 0dBF222EA5DF04047C;
	fma.rn.f64 	%fd84, %fd82, %fd55, %fd83;
	mov.f64 	%fd85, 0d3FF02A30D1FBA0DC;
	fma.rn.f64 	%fd86, %fd84, %fd55, %fd85;
	mov.f64 	%fd87, 0d4013664DDD1AD7FB;
	fma.rn.f64 	%fd6, %fd86, %fd55, %fd87;
	mov.f64 	%fd172, %fd6;

BB47_9:
	mov.f64 	%fd9, %fd172;
	mov.u32 	%r33, %tid.y;
	mov.u32 	%r32, %ctaid.y;
	mov.u32 	%r31, %ntid.y;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r29, %nctaid.x;
	mad.lo.s32 	%r28, %r31, %r32, %r33;
	mov.u32 	%r27, %tid.x;
	mov.u32 	%r26, %ntid.x;
	mad.lo.s32 	%r25, %r28, %r29, %r30;
	mad.lo.s32 	%r24, %r25, %r26, %r27;
	cvt.s64.s32	%rd11, %r24;
	ld.param.u64 	%rd10, [vec_erfinv_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	mul.f64 	%fd171, %fd1, %fd9;
	st.global.f64 	[%rd9], %fd171;

BB47_10:
	ret;
}

	// .globl	vec_exp10
.visible .entry vec_exp10(
	.param .u32 vec_exp10_param_0,
	.param .u64 vec_exp10_param_1,
	.param .u64 vec_exp10_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<46>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r6, [vec_exp10_param_0];
	ld.param.u64 	%rd2, [vec_exp10_param_1];
	ld.param.u64 	%rd3, [vec_exp10_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB48_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d400A934F0979A371;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd8;
	}
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFD34413509F79FF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0d3C49DC1DA994FD21;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	mul.f64 	%fd15, %fd14, 0dBCAF48AD494EA3E9;
	mov.f64 	%fd16, 0d40026BB1BBB55516;
	fma.rn.f64 	%fd17, %fd14, %fd16, %fd15;
	mov.f64 	%fd18, 0d3E928AF3FCA213EA;
	mov.f64 	%fd19, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F81111111122322;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3FA55555555502A1;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555511;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FE000000000000B;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FF0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	fma.rn.f64 	%fd39, %fd38, %fd17, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd39;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd39;
	}
	shl.b32 	%r16, %r2, 20;
	add.s32 	%r17, %r4, %r16;
	mov.b64 	%fd45, {%r3, %r17};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f40733A71;
	@%p2 bra 	BB48_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd40, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd41, %fd1;
	setp.gtu.f64	%p4, %fd41, 0d7FF0000000000000;
	add.f64 	%fd42, %fd1, %fd1;
	selp.f64	%fd45, %fd42, %fd40, %p4;
	setp.geu.f32	%p5, %f1, 0f407439B8;
	@%p5 bra 	BB48_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd43, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd44, {%r26, %r25};
	mul.f64 	%fd45, %fd43, %fd44;

BB48_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd45;

BB48_5:
	ret;
}

	// .globl	vec_exp2
.visible .entry vec_exp2(
	.param .u32 vec_exp2_param_0,
	.param .u64 vec_exp2_param_1,
	.param .u64 vec_exp2_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r6, [vec_exp2_param_0];
	ld.param.u64 	%rd2, [vec_exp2_param_1];
	ld.param.u64 	%rd3, [vec_exp2_param_2];
	mov.u32 	%r7, %tid.x;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r7;
	setp.ge.s32	%p1, %r1, %r6;
	@%p1 bra 	BB49_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	mov.f64 	%fd6, 0d4338000000000000;
	add.rn.f64 	%fd7, %fd1, %fd6;
	mov.f64 	%fd8, 0dC338000000000000;
	add.rn.f64 	%fd9, %fd7, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd7;
	}
	sub.f64 	%fd10, %fd1, %fd9;
	mul.f64 	%fd11, %fd10, 0d3C7ABC9E3B39803F;
	mov.f64 	%fd12, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd13, %fd10, %fd12, %fd11;
	mov.f64 	%fd14, 0d3E928AF3FCA213EA;
	mov.f64 	%fd15, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd16, %fd15, %fd13, %fd14;
	mov.f64 	%fd17, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd18, %fd16, %fd13, %fd17;
	mov.f64 	%fd19, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd20, %fd18, %fd13, %fd19;
	mov.f64 	%fd21, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd22, %fd20, %fd13, %fd21;
	mov.f64 	%fd23, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd24, %fd22, %fd13, %fd23;
	mov.f64 	%fd25, 0d3F81111111122322;
	fma.rn.f64 	%fd26, %fd24, %fd13, %fd25;
	mov.f64 	%fd27, 0d3FA55555555502A1;
	fma.rn.f64 	%fd28, %fd26, %fd13, %fd27;
	mov.f64 	%fd29, 0d3FC5555555555511;
	fma.rn.f64 	%fd30, %fd28, %fd13, %fd29;
	mov.f64 	%fd31, 0d3FE000000000000B;
	fma.rn.f64 	%fd32, %fd30, %fd13, %fd31;
	mov.f64 	%fd33, 0d3FF0000000000000;
	fma.rn.f64 	%fd34, %fd32, %fd13, %fd33;
	fma.rn.f64 	%fd35, %fd34, %fd13, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd35;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd35;
	}
	shl.b32 	%r16, %r2, 20;
	add.s32 	%r17, %r4, %r16;
	mov.b64 	%fd41, {%r3, %r17};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd1;
	}
	mov.b32 	 %f2, %r5;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f408FF000;
	@%p2 bra 	BB49_4;

	setp.lt.s32	%p3, %r5, 0;
	selp.f64	%fd36, 0d0000000000000000, 0d7FF0000000000000, %p3;
	abs.f64 	%fd37, %fd1;
	setp.gtu.f64	%p4, %fd37, 0d7FF0000000000000;
	add.f64 	%fd38, %fd1, %fd1;
	selp.f64	%fd41, %fd38, %fd36, %p4;
	setp.geu.f32	%p5, %f1, 0f4090CC00;
	@%p5 bra 	BB49_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd39, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd40, {%r26, %r25};
	mul.f64 	%fd41, %fd39, %fd40;

BB49_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd41;

BB49_5:
	ret;
}

	// .globl	vec_exp
.visible .entry vec_exp(
	.param .u32 vec_exp_param_0,
	.param .u64 vec_exp_param_1,
	.param .u64 vec_exp_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r5, [vec_exp_param_0];
	ld.param.u64 	%rd2, [vec_exp_param_1];
	ld.param.u64 	%rd3, [vec_exp_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB50_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd8;
	}
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	mov.f64 	%fd15, 0d3E928AF3FCA213EA;
	mov.f64 	%fd16, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd17, %fd16, %fd14, %fd15;
	mov.f64 	%fd18, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd19, %fd17, %fd14, %fd18;
	mov.f64 	%fd20, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd21, %fd19, %fd14, %fd20;
	mov.f64 	%fd22, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd23, %fd21, %fd14, %fd22;
	mov.f64 	%fd24, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd25, %fd23, %fd14, %fd24;
	mov.f64 	%fd26, 0d3F81111111122322;
	fma.rn.f64 	%fd27, %fd25, %fd14, %fd26;
	mov.f64 	%fd28, 0d3FA55555555502A1;
	fma.rn.f64 	%fd29, %fd27, %fd14, %fd28;
	mov.f64 	%fd30, 0d3FC5555555555511;
	fma.rn.f64 	%fd31, %fd29, %fd14, %fd30;
	mov.f64 	%fd32, 0d3FE000000000000B;
	fma.rn.f64 	%fd33, %fd31, %fd14, %fd32;
	mov.f64 	%fd34, 0d3FF0000000000000;
	fma.rn.f64 	%fd35, %fd33, %fd14, %fd34;
	fma.rn.f64 	%fd36, %fd35, %fd14, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd36;
	}
	shl.b32 	%r15, %r2, 20;
	add.s32 	%r16, %r4, %r15;
	mov.b64 	%fd40, {%r3, %r16};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd1;
	}
	mov.b32 	 %f2, %r17;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p2, %f1, 0f4086232B;
	@%p2 bra 	BB50_4;

	setp.lt.f64	%p3, %fd1, 0d0000000000000000;
	add.f64 	%fd37, %fd1, 0d7FF0000000000000;
	selp.f64	%fd40, 0d0000000000000000, %fd37, %p3;
	setp.geu.f32	%p4, %f1, 0f40874800;
	@%p4 bra 	BB50_4;

	shr.u32 	%r18, %r2, 31;
	add.s32 	%r19, %r2, %r18;
	shr.s32 	%r20, %r19, 1;
	shl.b32 	%r21, %r20, 20;
	add.s32 	%r22, %r21, %r4;
	mov.b64 	%fd38, {%r3, %r22};
	sub.s32 	%r23, %r2, %r20;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, 1072693248;
	mov.u32 	%r26, 0;
	mov.b64 	%fd39, {%r26, %r25};
	mul.f64 	%fd40, %fd38, %fd39;

BB50_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd40;

BB50_5:
	ret;
}

	// .globl	vec_expm1
.visible .entry vec_expm1(
	.param .u32 vec_expm1_param_0,
	.param .u64 vec_expm1_param_1,
	.param .u64 vec_expm1_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<48>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r3, [vec_expm1_param_0];
	ld.param.u64 	%rd2, [vec_expm1_param_1];
	ld.param.u64 	%rd3, [vec_expm1_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB51_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.lt.u32	%p2, %r2, 1082535491;
	setp.lt.s32	%p3, %r2, -1068859392;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB51_3;
	bra.uni 	BB51_2;

BB51_3:
	mov.f64 	%fd8, 0d4338000000000000;
	mov.f64 	%fd9, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd10, %fd1, %fd9, %fd8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd10;
	}
	mov.f64 	%fd11, 0dC338000000000000;
	add.rn.f64 	%fd12, %fd10, %fd11;
	mov.f64 	%fd13, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd14, %fd12, %fd13, %fd1;
	mov.f64 	%fd15, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd16, %fd12, %fd15, %fd14;
	add.s32 	%r14, %r2, %r2;
	setp.lt.u32	%p7, %r14, 2142496327;
	selp.b32	%r15, 0, %r13, %p7;
	selp.f64	%fd17, %fd1, %fd16, %p7;
	mov.f64 	%fd18, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd19, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3F8111111110F74D;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FA555555555554D;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FC5555555555557;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FE0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	mul.f64 	%fd39, %fd17, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd17, %fd17;
	setp.eq.s32	%p8, %r15, 1024;
	selp.b32	%r16, -1, 0, %p8;
	add.s32 	%r17, %r16, %r15;
	shl.b32 	%r18, %r17, 20;
	add.s32 	%r19, %r18, 1072693248;
	mov.u32 	%r20, 0;
	mov.b64 	%fd41, {%r20, %r19};
	mov.u32 	%r21, 1072693248;
	mov.b64 	%fd42, {%r20, %r21};
	sub.f64 	%fd43, %fd41, %fd42;
	fma.rn.f64 	%fd44, %fd40, %fd41, %fd43;
	add.f64 	%fd45, %fd44, %fd44;
	selp.f64	%fd46, %fd45, %fd44, %p8;
	setp.eq.s32	%p9, %r14, 0;
	selp.f64	%fd47, %fd17, %fd46, %p9;
	bra.uni 	BB51_4;

BB51_2:
	setp.lt.s32	%p5, %r2, 0;
	selp.f64	%fd5, 0dBFF0000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd6, %fd1;
	setp.gtu.f64	%p6, %fd6, 0d7FF0000000000000;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd47, %fd7, %fd5, %p6;

BB51_4:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd47;

BB51_5:
	ret;
}

	// .globl	vec_fabs
.visible .entry vec_fabs(
	.param .u32 vec_fabs_param_0,
	.param .u64 vec_fabs_param_1,
	.param .u64 vec_fabs_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_fabs_param_0];
	ld.param.u64 	%rd1, [vec_fabs_param_1];
	ld.param.u64 	%rd2, [vec_fabs_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB52_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB52_2:
	ret;
}

	// .globl	vec_floor
.visible .entry vec_floor(
	.param .u32 vec_floor_param_0,
	.param .u64 vec_floor_param_1,
	.param .u64 vec_floor_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_floor_param_0];
	ld.param.u64 	%rd1, [vec_floor_param_1];
	ld.param.u64 	%rd2, [vec_floor_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB53_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rmi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB53_2:
	ret;
}

	// .globl	vec_j0
.visible .entry vec_j0(
	.param .u32 vec_j0_param_0,
	.param .u64 vec_j0_param_1,
	.param .u64 vec_j0_param_2
)
{
	.local .align 4 .b8 	__local_depot54[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<218>;
	.reg .b64 	%rd<22>;


	mov.u64 	%rd21, __local_depot54;
	cvta.local.u64 	%SP, %rd21;
	ld.param.u32 	%r9, [vec_j0_param_0];
	ld.param.u64 	%rd4, [vec_j0_param_1];
	ld.param.u64 	%rd5, [vec_j0_param_2];
	add.u64 	%rd6, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd6;
	add.u64 	%rd7, %SP, 4;
	cvta.to.local.u64 	%rd2, %rd7;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB54_20;

	cvta.to.global.u64 	%rd8, %rd5;
	cvt.s64.s32	%rd3, %r1;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd25, [%rd10];
	abs.f64 	%fd1, %fd25;
	setp.gtu.f64	%p2, %fd1, 0d400FB319F277BBE5;
	@%p2 bra 	BB54_3;
	bra.uni 	BB54_2;

BB54_3:
	setp.gtu.f64	%p3, %fd1, 0d401C58FD1A62F5EC;
	@%p3 bra 	BB54_5;
	bra.uni 	BB54_4;

BB54_5:
	setp.gtu.f64	%p4, %fd1, 0d402471FCB6A7A8C0;
	@%p4 bra 	BB54_7;
	bra.uni 	BB54_6;

BB54_7:
	abs.f64 	%fd132, %fd1;
	mov.f64 	%fd217, 0d0000000000000000;
	setp.eq.f64	%p5, %fd132, 0d7FF0000000000000;
	@%p5 bra 	BB54_19;

	// inline asm
	rcp.approx.ftz.f64 %fd133,%fd1;
	// inline asm
	neg.f64 	%fd135, %fd1;
	mov.f64 	%fd136, 0d3FF0000000000000;
	fma.rn.f64 	%fd137, %fd135, %fd133, %fd136;
	fma.rn.f64 	%fd138, %fd137, %fd137, %fd137;
	fma.rn.f64 	%fd139, %fd138, %fd133, %fd133;
	mul.f64 	%fd140, %fd139, %fd139;
	mov.f64 	%fd141, 0d409927467A655012;
	mov.f64 	%fd142, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd143, %fd142, %fd140, %fd141;
	mov.f64 	%fd144, 0dC05751787E247BD4;
	fma.rn.f64 	%fd145, %fd143, %fd140, %fd144;
	mov.f64 	%fd146, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd147, %fd145, %fd140, %fd146;
	mov.f64 	%fd148, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd149, %fd147, %fd140, %fd148;
	mov.f64 	%fd150, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd151, %fd149, %fd140, %fd150;
	mov.f64 	%fd152, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd153, %fd151, %fd140, %fd152;
	mov.f64 	%fd154, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd155, %fd153, %fd140, %fd154;
	mov.f64 	%fd156, 0d410ECD4523B12B84;
	mov.f64 	%fd157, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd158, %fd157, %fd140, %fd156;
	mov.f64 	%fd159, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd160, %fd158, %fd140, %fd159;
	mov.f64 	%fd161, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd162, %fd160, %fd140, %fd161;
	mov.f64 	%fd163, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd164, %fd162, %fd140, %fd163;
	mov.f64 	%fd165, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd166, %fd164, %fd140, %fd165;
	mov.f64 	%fd167, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd168, %fd166, %fd140, %fd167;
	mov.f64 	%fd169, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd170, %fd168, %fd140, %fd169;
	mov.f64 	%fd171, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd172, %fd170, %fd140, %fd171;
	fma.rn.f64 	%fd5, %fd172, %fd139, %fd1;
	rsqrt.approx.f64 	%fd173, %fd1;
	mul.f64 	%fd174, %fd173, 0d3FE9884533D43651;
	mul.f64 	%fd6, %fd155, %fd174;
	mul.f64 	%fd175, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r27, %fd175;
	st.local.u32 	[%rd2], %r27;
	cvt.rn.f64.s32	%fd176, %r27;
	neg.f64 	%fd177, %fd176;
	mov.f64 	%fd178, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd179, %fd177, %fd178, %fd5;
	mov.f64 	%fd180, 0d3C91A62633145C00;
	fma.rn.f64 	%fd181, %fd177, %fd180, %fd179;
	mov.f64 	%fd182, 0d397B839A252049C0;
	fma.rn.f64 	%fd213, %fd177, %fd182, %fd181;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd5;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p6, %r20, 1105199104;
	@%p6 bra 	BB54_10;

	add.u64 	%rd20, %SP, 4;
	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd20;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd213, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r27, [%rd2];

BB54_10:
	and.b32  	%r21, %r27, 3;
	cvt.rn.f64.s32	%fd183, %r21;
	add.f64 	%fd184, %fd213, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd214, %fd183, 0d3FF921FB54442D18, %fd184;
	abs.f64 	%fd185, %fd214;
	setp.neu.f64	%p7, %fd185, 0d7FF0000000000000;
	@%p7 bra 	BB54_12;

	mov.f64 	%fd186, 0d0000000000000000;
	mul.rn.f64 	%fd214, %fd214, %fd186;

BB54_12:
	mov.f64 	%fd212, 0d397B839A252049C0;
	mov.f64 	%fd211, 0d3C91A62633145C00;
	mov.f64 	%fd210, 0d3FF921FB54442D18;
	mul.f64 	%fd187, %fd214, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r28, %fd187;
	st.local.u32 	[%rd1], %r28;
	cvt.rn.f64.s32	%fd188, %r28;
	neg.f64 	%fd189, %fd188;
	fma.rn.f64 	%fd191, %fd189, %fd210, %fd214;
	fma.rn.f64 	%fd193, %fd189, %fd211, %fd191;
	fma.rn.f64 	%fd215, %fd189, %fd212, %fd193;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd214;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p8, %r23, 1105199104;
	@%p8 bra 	BB54_14;

	add.u64 	%rd19, %SP, 0;
	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd214;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd215, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r28, [%rd1];

BB54_14:
	add.s32 	%r8, %r28, 1;
	and.b32  	%r24, %r8, 1;
	shl.b32 	%r25, %r24, 3;
	setp.eq.s32	%p9, %r24, 0;
	selp.f64	%fd195, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p9;
	mul.wide.u32 	%rd13, %r25, 8;
	mov.u64 	%rd14, __cudart_sin_cos_coeffs;
	add.s64 	%rd15, %rd13, %rd14;
	ld.const.f64 	%fd196, [%rd15+8];
	mul.rn.f64 	%fd16, %fd215, %fd215;
	fma.rn.f64 	%fd197, %fd195, %fd16, %fd196;
	ld.const.f64 	%fd198, [%rd15+16];
	fma.rn.f64 	%fd199, %fd197, %fd16, %fd198;
	ld.const.f64 	%fd200, [%rd15+24];
	fma.rn.f64 	%fd201, %fd199, %fd16, %fd200;
	ld.const.f64 	%fd202, [%rd15+32];
	fma.rn.f64 	%fd203, %fd201, %fd16, %fd202;
	ld.const.f64 	%fd204, [%rd15+40];
	fma.rn.f64 	%fd205, %fd203, %fd16, %fd204;
	ld.const.f64 	%fd206, [%rd15+48];
	fma.rn.f64 	%fd17, %fd205, %fd16, %fd206;
	fma.rn.f64 	%fd216, %fd17, %fd215, %fd215;
	@%p9 bra 	BB54_16;

	fma.rn.f64 	%fd216, %fd17, %fd16, %fd136;

BB54_16:
	and.b32  	%r26, %r8, 2;
	setp.eq.s32	%p10, %r26, 0;
	@%p10 bra 	BB54_18;

	mov.f64 	%fd208, 0d0000000000000000;
	mov.f64 	%fd209, 0dBFF0000000000000;
	fma.rn.f64 	%fd216, %fd216, %fd209, %fd208;

BB54_18:
	mul.f64 	%fd217, %fd6, %fd216;
	bra.uni 	BB54_19;

BB54_2:
	add.f64 	%fd26, %fd1, 0dC0033D152E971B40;
	add.f64 	%fd27, %fd26, 0d3CA0F539D7DA258E;
	mov.f64 	%fd28, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd29, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	mov.f64 	%fd41, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd42, %fd40, %fd27, %fd41;
	mov.f64 	%fd43, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd44, %fd42, %fd27, %fd43;
	mov.f64 	%fd45, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd46, %fd44, %fd27, %fd45;
	mov.f64 	%fd47, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd48, %fd46, %fd27, %fd47;
	mov.f64 	%fd49, 0d3F315382BA096A62;
	fma.rn.f64 	%fd50, %fd48, %fd27, %fd49;
	mov.f64 	%fd51, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd52, %fd50, %fd27, %fd51;
	mov.f64 	%fd53, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd54, %fd52, %fd27, %fd53;
	mov.f64 	%fd55, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd56, %fd54, %fd27, %fd55;
	mov.f64 	%fd57, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd58, %fd56, %fd27, %fd57;
	mov.f64 	%fd59, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd60, %fd58, %fd27, %fd59;
	mul.f64 	%fd217, %fd27, %fd60;
	bra.uni 	BB54_19;

BB54_4:
	add.f64 	%fd61, %fd1, 0dC016148F5B2C2E45;
	add.f64 	%fd62, %fd61, 0dBC975054CD60A517;
	mov.f64 	%fd63, 0d3CF83FD1F333EB61;
	mov.f64 	%fd64, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd65, %fd64, %fd62, %fd63;
	mov.f64 	%fd66, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd67, %fd65, %fd62, %fd66;
	mov.f64 	%fd68, 0dBD7846076D004627;
	fma.rn.f64 	%fd69, %fd67, %fd62, %fd68;
	mov.f64 	%fd70, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd71, %fd69, %fd62, %fd70;
	mov.f64 	%fd72, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd73, %fd71, %fd62, %fd72;
	mov.f64 	%fd74, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd75, %fd73, %fd62, %fd74;
	mov.f64 	%fd76, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd77, %fd75, %fd62, %fd76;
	mov.f64 	%fd78, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd79, %fd77, %fd62, %fd78;
	mov.f64 	%fd80, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd81, %fd79, %fd62, %fd80;
	mov.f64 	%fd82, 0dBF0863F481A37337;
	fma.rn.f64 	%fd83, %fd81, %fd62, %fd82;
	mov.f64 	%fd84, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd85, %fd83, %fd62, %fd84;
	mov.f64 	%fd86, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd87, %fd85, %fd62, %fd86;
	mov.f64 	%fd88, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd89, %fd87, %fd62, %fd88;
	mov.f64 	%fd90, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd91, %fd89, %fd62, %fd90;
	mov.f64 	%fd92, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd93, %fd91, %fd62, %fd92;
	mov.f64 	%fd94, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd95, %fd93, %fd62, %fd94;
	mul.f64 	%fd217, %fd62, %fd95;
	bra.uni 	BB54_19;

BB54_6:
	add.f64 	%fd96, %fd1, 0dC0214EB56CCCDECA;
	add.f64 	%fd97, %fd96, 0d3CB51970714C7C25;
	mov.f64 	%fd98, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd99, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd100, %fd99, %fd97, %fd98;
	mov.f64 	%fd101, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd102, %fd100, %fd97, %fd101;
	mov.f64 	%fd103, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd104, %fd102, %fd97, %fd103;
	mov.f64 	%fd105, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd106, %fd104, %fd97, %fd105;
	mov.f64 	%fd107, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd108, %fd106, %fd97, %fd107;
	mov.f64 	%fd109, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd110, %fd108, %fd97, %fd109;
	mov.f64 	%fd111, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd112, %fd110, %fd97, %fd111;
	mov.f64 	%fd113, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd114, %fd112, %fd97, %fd113;
	mov.f64 	%fd115, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd116, %fd114, %fd97, %fd115;
	mov.f64 	%fd117, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd118, %fd116, %fd97, %fd117;
	mov.f64 	%fd119, 0d3F1D79605276949A;
	fma.rn.f64 	%fd120, %fd118, %fd97, %fd119;
	mov.f64 	%fd121, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd122, %fd120, %fd97, %fd121;
	mov.f64 	%fd123, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd124, %fd122, %fd97, %fd123;
	mov.f64 	%fd125, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd126, %fd124, %fd97, %fd125;
	mov.f64 	%fd127, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd128, %fd126, %fd97, %fd127;
	mov.f64 	%fd129, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd130, %fd128, %fd97, %fd129;
	mul.f64 	%fd217, %fd97, %fd130;

BB54_19:
	cvta.to.global.u64 	%rd16, %rd4;
	shl.b64 	%rd17, %rd3, 3;
	add.s64 	%rd18, %rd16, %rd17;
	st.global.f64 	[%rd18], %fd217;

BB54_20:
	ret;
}

	// .globl	vec_j1
.visible .entry vec_j1(
	.param .u32 vec_j1_param_0,
	.param .u64 vec_j1_param_1,
	.param .u64 vec_j1_param_2
)
{
	.local .align 4 .b8 	__local_depot55[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<217>;
	.reg .b64 	%rd<22>;


	mov.u64 	%rd21, __local_depot55;
	cvta.local.u64 	%SP, %rd21;
	ld.param.u32 	%r9, [vec_j1_param_0];
	ld.param.u64 	%rd3, [vec_j1_param_1];
	ld.param.u64 	%rd4, [vec_j1_param_2];
	add.u64 	%rd5, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd5;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB55_20;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd2, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d400353AABAD7B784;
	@%p2 bra 	BB55_3;
	bra.uni 	BB55_2;

BB55_3:
	setp.gtu.f64	%p3, %fd2, 0d4015B1D0574614EA;
	@%p3 bra 	BB55_5;
	bra.uni 	BB55_4;

BB55_5:
	setp.gtu.f64	%p4, %fd2, 0d40213065E54C1AA9;
	@%p4 bra 	BB55_7;
	bra.uni 	BB55_6;

BB55_7:
	abs.f64 	%fd124, %fd2;
	mov.f64 	%fd216, 0d0000000000000000;
	setp.eq.f64	%p5, %fd124, 0d7FF0000000000000;
	@%p5 bra 	BB55_19;

	add.u64 	%rd9, %SP, 4;
	cvta.to.local.u64 	%rd10, %rd9;
	// inline asm
	rcp.approx.ftz.f64 %fd125,%fd2;
	// inline asm
	neg.f64 	%fd127, %fd2;
	mov.f64 	%fd128, 0d3FF0000000000000;
	fma.rn.f64 	%fd129, %fd127, %fd125, %fd128;
	fma.rn.f64 	%fd130, %fd129, %fd129, %fd129;
	fma.rn.f64 	%fd131, %fd130, %fd125, %fd125;
	mul.f64 	%fd132, %fd131, %fd131;
	mov.f64 	%fd133, 0dC099C06322A3F8BE;
	mov.f64 	%fd134, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd135, %fd134, %fd132, %fd133;
	mov.f64 	%fd136, 0d405B89354DA77324;
	fma.rn.f64 	%fd137, %fd135, %fd132, %fd136;
	mov.f64 	%fd138, 0dC01E352294653188;
	fma.rn.f64 	%fd139, %fd137, %fd132, %fd138;
	mov.f64 	%fd140, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd141, %fd139, %fd132, %fd140;
	mov.f64 	%fd142, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd143, %fd141, %fd132, %fd142;
	mov.f64 	%fd144, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd145, %fd143, %fd132, %fd144;
	mov.f64 	%fd146, 0d3FF00000000068CC;
	fma.rn.f64 	%fd147, %fd145, %fd132, %fd146;
	mov.f64 	%fd148, 0d415A30AC6857BEE0;
	mov.f64 	%fd149, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd150, %fd149, %fd132, %fd148;
	mov.f64 	%fd151, 0dC11764222AD7C910;
	fma.rn.f64 	%fd152, %fd150, %fd132, %fd151;
	mov.f64 	%fd153, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd154, %fd152, %fd132, %fd153;
	mov.f64 	%fd155, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd156, %fd154, %fd132, %fd155;
	mov.f64 	%fd157, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd158, %fd156, %fd132, %fd157;
	mov.f64 	%fd159, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd160, %fd158, %fd132, %fd159;
	mov.f64 	%fd161, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd162, %fd160, %fd132, %fd161;
	mov.f64 	%fd163, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd164, %fd162, %fd132, %fd163;
	mov.f64 	%fd165, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd166, %fd164, %fd132, %fd165;
	fma.rn.f64 	%fd6, %fd166, %fd131, %fd2;
	rsqrt.approx.f64 	%fd167, %fd2;
	mul.f64 	%fd168, %fd167, 0d3FE9884533D43651;
	mul.f64 	%fd7, %fd147, %fd168;
	mul.f64 	%fd169, %fd6, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r27, %fd169;
	st.local.u32 	[%rd10], %r27;
	cvt.rn.f64.s32	%fd170, %r27;
	neg.f64 	%fd171, %fd170;
	mov.f64 	%fd172, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd173, %fd171, %fd172, %fd6;
	mov.f64 	%fd174, 0d3C91A62633145C00;
	fma.rn.f64 	%fd175, %fd171, %fd174, %fd173;
	mov.f64 	%fd176, 0d397B839A252049C0;
	fma.rn.f64 	%fd212, %fd171, %fd176, %fd175;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd6;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p6, %r20, 1105199104;
	@%p6 bra 	BB55_10;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd6;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd9;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd212, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r27, [%rd10];

BB55_10:
	and.b32  	%r21, %r27, 3;
	cvt.rn.f64.s32	%fd177, %r21;
	add.f64 	%fd178, %fd212, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd213, %fd177, 0d3FF921FB54442D18, %fd178;
	abs.f64 	%fd179, %fd213;
	setp.neu.f64	%p7, %fd179, 0d7FF0000000000000;
	@%p7 bra 	BB55_12;

	mov.f64 	%fd180, 0d0000000000000000;
	mul.rn.f64 	%fd213, %fd213, %fd180;

BB55_12:
	mov.f64 	%fd210, 0d397B839A252049C0;
	mov.f64 	%fd209, 0d3C91A62633145C00;
	mov.f64 	%fd208, 0d3FF921FB54442D18;
	mul.f64 	%fd181, %fd213, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r28, %fd181;
	st.local.u32 	[%rd1], %r28;
	cvt.rn.f64.s32	%fd182, %r28;
	neg.f64 	%fd183, %fd182;
	fma.rn.f64 	%fd185, %fd183, %fd208, %fd213;
	fma.rn.f64 	%fd187, %fd183, %fd209, %fd185;
	fma.rn.f64 	%fd214, %fd183, %fd210, %fd187;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd213;
	}
	and.b32  	%r23, %r22, 2145386496;
	setp.lt.u32	%p8, %r23, 1105199104;
	@%p8 bra 	BB55_14;

	add.u64 	%rd20, %SP, 0;
	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd213;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd20;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd214, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r28, [%rd1];

BB55_14:
	add.s32 	%r8, %r28, 1;
	and.b32  	%r24, %r8, 1;
	shl.b32 	%r25, %r24, 3;
	setp.eq.s32	%p9, %r24, 0;
	selp.f64	%fd189, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p9;
	mul.wide.u32 	%rd14, %r25, 8;
	mov.u64 	%rd15, __cudart_sin_cos_coeffs;
	add.s64 	%rd16, %rd14, %rd15;
	ld.const.f64 	%fd190, [%rd16+8];
	mul.rn.f64 	%fd17, %fd214, %fd214;
	fma.rn.f64 	%fd191, %fd189, %fd17, %fd190;
	ld.const.f64 	%fd192, [%rd16+16];
	fma.rn.f64 	%fd193, %fd191, %fd17, %fd192;
	ld.const.f64 	%fd194, [%rd16+24];
	fma.rn.f64 	%fd195, %fd193, %fd17, %fd194;
	ld.const.f64 	%fd196, [%rd16+32];
	fma.rn.f64 	%fd197, %fd195, %fd17, %fd196;
	ld.const.f64 	%fd198, [%rd16+40];
	fma.rn.f64 	%fd199, %fd197, %fd17, %fd198;
	ld.const.f64 	%fd200, [%rd16+48];
	fma.rn.f64 	%fd18, %fd199, %fd17, %fd200;
	fma.rn.f64 	%fd215, %fd18, %fd214, %fd214;
	@%p9 bra 	BB55_16;

	mov.f64 	%fd211, 0d3FF0000000000000;
	fma.rn.f64 	%fd215, %fd18, %fd17, %fd211;

BB55_16:
	and.b32  	%r26, %r8, 2;
	setp.eq.s32	%p10, %r26, 0;
	@%p10 bra 	BB55_18;

	mov.f64 	%fd202, 0d0000000000000000;
	mov.f64 	%fd203, 0dBFF0000000000000;
	fma.rn.f64 	%fd215, %fd215, %fd203, %fd202;

BB55_18:
	mul.f64 	%fd216, %fd7, %fd215;
	bra.uni 	BB55_19;

BB55_2:
	mov.f64 	%fd26, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd27, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd28, %fd27, %fd2, %fd26;
	mov.f64 	%fd29, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd30, %fd28, %fd2, %fd29;
	mov.f64 	%fd31, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd32, %fd30, %fd2, %fd31;
	mov.f64 	%fd33, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd34, %fd32, %fd2, %fd33;
	mov.f64 	%fd35, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd36, %fd34, %fd2, %fd35;
	mov.f64 	%fd37, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd38, %fd36, %fd2, %fd37;
	mov.f64 	%fd39, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd40, %fd38, %fd2, %fd39;
	mov.f64 	%fd41, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd42, %fd40, %fd2, %fd41;
	mov.f64 	%fd43, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd44, %fd42, %fd2, %fd43;
	mov.f64 	%fd45, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd46, %fd44, %fd2, %fd45;
	mov.f64 	%fd47, 0d3F65555555506982;
	fma.rn.f64 	%fd48, %fd46, %fd2, %fd47;
	mov.f64 	%fd49, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd50, %fd48, %fd2, %fd49;
	mov.f64 	%fd51, 0dBFB0000000000065;
	fma.rn.f64 	%fd52, %fd50, %fd2, %fd51;
	mov.f64 	%fd53, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd54, %fd52, %fd2, %fd53;
	mov.f64 	%fd55, 0d3FE0000000000000;
	fma.rn.f64 	%fd56, %fd54, %fd2, %fd55;
	mul.f64 	%fd216, %fd2, %fd56;
	bra.uni 	BB55_19;

BB55_4:
	add.f64 	%fd57, %fd2, 0dC00EA75575AF6F09;
	add.f64 	%fd58, %fd57, 0d3CA60155A9D1B256;
	mov.f64 	%fd59, 0d3D41011A1DF02DAD;
	mov.f64 	%fd60, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd61, %fd60, %fd58, %fd59;
	mov.f64 	%fd62, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd63, %fd61, %fd58, %fd62;
	mov.f64 	%fd64, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd65, %fd63, %fd58, %fd64;
	mov.f64 	%fd66, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd67, %fd65, %fd58, %fd66;
	mov.f64 	%fd68, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd69, %fd67, %fd58, %fd68;
	mov.f64 	%fd70, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd71, %fd69, %fd58, %fd70;
	mov.f64 	%fd72, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd73, %fd71, %fd58, %fd72;
	mov.f64 	%fd74, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd75, %fd73, %fd58, %fd74;
	mov.f64 	%fd76, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd77, %fd75, %fd58, %fd76;
	mov.f64 	%fd78, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd79, %fd77, %fd58, %fd78;
	mov.f64 	%fd80, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd81, %fd79, %fd58, %fd80;
	mov.f64 	%fd82, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd83, %fd81, %fd58, %fd82;
	mov.f64 	%fd84, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd85, %fd83, %fd58, %fd84;
	mov.f64 	%fd86, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd87, %fd85, %fd58, %fd86;
	mov.f64 	%fd88, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd89, %fd87, %fd58, %fd88;
	mul.f64 	%fd216, %fd58, %fd89;
	bra.uni 	BB55_19;

BB55_6:
	add.f64 	%fd90, %fd2, 0dC01C0FF5F3B47250;
	add.f64 	%fd91, %fd90, 0d3C9B226D9D243827;
	mov.f64 	%fd92, 0dBD40E8363DB649A9;
	mov.f64 	%fd93, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd94, %fd93, %fd91, %fd92;
	mov.f64 	%fd95, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd96, %fd94, %fd91, %fd95;
	mov.f64 	%fd97, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd98, %fd96, %fd91, %fd97;
	mov.f64 	%fd99, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd100, %fd98, %fd91, %fd99;
	mov.f64 	%fd101, 0dBE34462EED732A23;
	fma.rn.f64 	%fd102, %fd100, %fd91, %fd101;
	mov.f64 	%fd103, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd104, %fd102, %fd91, %fd103;
	mov.f64 	%fd105, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd106, %fd104, %fd91, %fd105;
	mov.f64 	%fd107, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd108, %fd106, %fd91, %fd107;
	mov.f64 	%fd109, 0dBF0796A751B32693;
	fma.rn.f64 	%fd110, %fd108, %fd91, %fd109;
	mov.f64 	%fd111, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd112, %fd110, %fd91, %fd111;
	mov.f64 	%fd113, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd114, %fd112, %fd91, %fd113;
	mov.f64 	%fd115, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd116, %fd114, %fd91, %fd115;
	mov.f64 	%fd117, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd118, %fd116, %fd91, %fd117;
	mov.f64 	%fd119, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd120, %fd118, %fd91, %fd119;
	mov.f64 	%fd121, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd122, %fd120, %fd91, %fd121;
	mul.f64 	%fd216, %fd91, %fd122;

BB55_19:
	cvta.to.global.u64 	%rd17, %rd3;
	neg.f64 	%fd204, %fd216;
	setp.lt.f64	%p11, %fd1, 0d0000000000000000;
	selp.f64	%fd205, %fd204, %fd216, %p11;
	mul.f64 	%fd206, %fd1, 0d3FE0000000000000;
	setp.lt.f64	%p12, %fd2, 0d39B4484BFEEBC2A0;
	selp.f64	%fd207, %fd206, %fd205, %p12;
	shl.b64 	%rd18, %rd2, 3;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.f64 	[%rd19], %fd207;

BB55_20:
	ret;
}

	// .globl	vec_lgamma
.visible .entry vec_lgamma(
	.param .u32 vec_lgamma_param_0,
	.param .u64 vec_lgamma_param_1,
	.param .u64 vec_lgamma_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<45>;
	.reg .f64 	%fd<105>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r14, [vec_lgamma_param_0];
	ld.param.u64 	%rd2, [vec_lgamma_param_1];
	ld.param.u64 	%rd3, [vec_lgamma_param_2];
	mov.u32 	%r15, %tid.x;
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %tid.y;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %nctaid.x;
	mov.u32 	%r21, %ctaid.x;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %ntid.x;
	mad.lo.s32 	%r1, %r22, %r23, %r15;
	setp.ge.s32	%p1, %r1, %r14;
	@%p1 bra 	BB56_20;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	abs.f64 	%fd101, %fd1;
	setp.gtu.f64	%p2, %fd101, 0d7FF0000000000000;
	@%p2 bra 	BB56_18;
	bra.uni 	BB56_2;

BB56_18:
	add.f64 	%fd104, %fd1, %fd1;
	bra.uni 	BB56_19;

BB56_2:
	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd101;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_lgamma_pos, 
	(
	param0
	);
	ld.param.f64	%fd3, [retval0+0];
	
	//{
	}// Callseq End 18
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd1;
	}
	setp.gt.s32	%p3, %r24, -1;
	mov.f64 	%fd104, %fd3;
	@%p3 bra 	BB56_19;

	cvt.rzi.f64.f64	%fd25, %fd101;
	setp.eq.f64	%p4, %fd101, %fd25;
	mov.f64 	%fd24, 0d7FF0000000000000;
	mov.f64 	%fd104, %fd24;
	@%p4 bra 	BB56_19;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd101;
	}
	setp.lt.s32	%p5, %r2, 1006632960;
	@%p5 bra 	BB56_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd101;
	}
	add.s32 	%r26, %r2, 1048576;
	mov.b64 	%fd26, {%r25, %r26};
	cvt.rni.f64.f64	%fd27, %fd26;
	cvt.rzi.s64.f64	%rd7, %fd27;
	cvt.u32.u64	%r3, %rd7;
	neg.f64 	%fd28, %fd27;
	mov.f64 	%fd29, 0d3FE0000000000000;
	fma.rn.f64 	%fd30, %fd28, %fd29, %fd101;
	mul.f64 	%fd31, %fd30, 0d3CA1A62633145C07;
	mov.f64 	%fd32, 0d400921FB54442D18;
	fma.rn.f64 	%fd33, %fd30, %fd32, %fd31;
	and.b64  	%rd8, %rd7, 1;
	mul.rn.f64 	%fd4, %fd33, %fd33;
	setp.eq.b64	%p6, %rd8, 1;
	not.pred 	%p7, %p6;
	selp.f64	%fd34, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p7;
	shl.b64 	%rd9, %rd8, 6;
	mov.u64 	%rd10, __cudart_sin_cos_coeffs;
	add.s64 	%rd11, %rd9, %rd10;
	ld.const.f64 	%fd35, [%rd11+8];
	fma.rn.f64 	%fd36, %fd34, %fd4, %fd35;
	ld.const.f64 	%fd37, [%rd11+16];
	fma.rn.f64 	%fd38, %fd36, %fd4, %fd37;
	ld.const.f64 	%fd39, [%rd11+24];
	fma.rn.f64 	%fd40, %fd38, %fd4, %fd39;
	ld.const.f64 	%fd41, [%rd11+32];
	fma.rn.f64 	%fd42, %fd40, %fd4, %fd41;
	ld.const.f64 	%fd43, [%rd11+40];
	fma.rn.f64 	%fd44, %fd42, %fd4, %fd43;
	ld.const.f64 	%fd45, [%rd11+48];
	fma.rn.f64 	%fd5, %fd44, %fd4, %fd45;
	fma.rn.f64 	%fd100, %fd5, %fd33, %fd33;
	@%p7 bra 	BB56_7;

	mov.f64 	%fd46, 0d3FF0000000000000;
	fma.rn.f64 	%fd100, %fd5, %fd4, %fd46;

BB56_7:
	and.b32  	%r27, %r3, 2;
	setp.eq.s32	%p8, %r27, 0;
	@%p8 bra 	BB56_9;

	mov.f64 	%fd47, 0d0000000000000000;
	mov.f64 	%fd48, 0dBFF0000000000000;
	fma.rn.f64 	%fd100, %fd100, %fd48, %fd47;

BB56_9:
	abs.f64 	%fd49, %fd100;
	mul.f64 	%fd50, %fd101, %fd49;
	div.rn.f64 	%fd101, %fd32, %fd50;

BB56_10:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd101;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd101;
	}
	mov.u32 	%r43, -1023;
	setp.gt.s32	%p9, %r41, 1048575;
	@%p9 bra 	BB56_12;

	mul.f64 	%fd101, %fd101, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd101;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd101;
	}
	mov.u32 	%r43, -1077;

BB56_12:
	add.s32 	%r30, %r41, -1;
	setp.lt.u32	%p10, %r30, 2146435071;
	@%p10 bra 	BB56_14;
	bra.uni 	BB56_13;

BB56_14:
	shr.u32 	%r32, %r41, 20;
	add.s32 	%r44, %r43, %r32;
	and.b32  	%r33, %r41, -2146435073;
	or.b32  	%r34, %r33, 1072693248;
	mov.b64 	%fd102, {%r42, %r34};
	setp.lt.s32	%p12, %r34, 1073127583;
	@%p12 bra 	BB56_16;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd102;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd102;
	}
	add.s32 	%r37, %r36, -1048576;
	mov.b64 	%fd102, {%r35, %r37};
	add.s32 	%r44, %r44, 1;

BB56_16:
	add.f64 	%fd55, %fd102, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd54,%fd55;
	// inline asm
	neg.f64 	%fd56, %fd55;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd54, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd54, %fd54;
	add.f64 	%fd61, %fd102, 0dBFF0000000000000;
	mul.f64 	%fd62, %fd61, %fd60;
	fma.rn.f64 	%fd63, %fd61, %fd60, %fd62;
	mul.f64 	%fd64, %fd63, %fd63;
	mov.f64 	%fd65, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd66, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd67, %fd66, %fd64, %fd65;
	mov.f64 	%fd68, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd69, %fd67, %fd64, %fd68;
	mov.f64 	%fd70, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd71, %fd69, %fd64, %fd70;
	mov.f64 	%fd72, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd73, %fd71, %fd64, %fd72;
	mov.f64 	%fd74, 0d3F624924923BE72D;
	fma.rn.f64 	%fd75, %fd73, %fd64, %fd74;
	mov.f64 	%fd76, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd77, %fd75, %fd64, %fd76;
	mov.f64 	%fd78, 0d3FB5555555555554;
	fma.rn.f64 	%fd79, %fd77, %fd64, %fd78;
	sub.f64 	%fd80, %fd61, %fd63;
	add.f64 	%fd81, %fd80, %fd80;
	neg.f64 	%fd82, %fd63;
	fma.rn.f64 	%fd83, %fd82, %fd61, %fd81;
	mul.f64 	%fd84, %fd60, %fd83;
	mul.f64 	%fd85, %fd64, %fd79;
	fma.rn.f64 	%fd86, %fd85, %fd63, %fd84;
	xor.b32  	%r38, %r44, -2147483648;
	mov.u32 	%r39, 1127219200;
	mov.b64 	%fd87, {%r38, %r39};
	mov.u32 	%r40, -2147483648;
	mov.b64 	%fd88, {%r40, %r39};
	sub.f64 	%fd89, %fd87, %fd88;
	mov.f64 	%fd90, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd91, %fd89, %fd90, %fd63;
	neg.f64 	%fd92, %fd89;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	sub.f64 	%fd94, %fd93, %fd63;
	sub.f64 	%fd95, %fd86, %fd94;
	mov.f64 	%fd96, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd97, %fd89, %fd96, %fd95;
	add.f64 	%fd103, %fd91, %fd97;
	bra.uni 	BB56_17;

BB56_13:
	mov.f64 	%fd52, 0d7FF0000000000000;
	fma.rn.f64 	%fd53, %fd101, %fd52, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd101;
	}
	mov.b32 	 %f1, %r31;
	setp.eq.f32	%p11, %f1, 0f00000000;
	selp.f64	%fd103, 0dFFF0000000000000, %fd53, %p11;

BB56_17:
	sub.f64 	%fd98, %fd103, %fd3;
	neg.f64 	%fd99, %fd103;
	selp.f64	%fd104, %fd99, %fd98, %p5;

BB56_19:
	cvta.to.global.u64 	%rd12, %rd2;
	shl.b64 	%rd13, %rd1, 3;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd104;

BB56_20:
	ret;
}

	// .globl	vec_log10
.visible .entry vec_log10(
	.param .u32 vec_log10_param_0,
	.param .u64 vec_log10_param_1,
	.param .u64 vec_log10_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r12, [vec_log10_param_0];
	ld.param.u64 	%rd2, [vec_log10_param_1];
	ld.param.u64 	%rd3, [vec_log10_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB57_9;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd59, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB57_3;

	mul.f64 	%fd59, %fd59, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1077;

BB57_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB57_5;
	bra.uni 	BB57_4;

BB57_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB57_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB57_7:
	add.f64 	%fd13, %fd60, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, 1127219200;
	mov.b64 	%fd45, {%r32, %r33};
	mov.u32 	%r34, -2147483648;
	mov.b64 	%fd46, {%r34, %r33};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd61, %fd49, %fd55;
	bra.uni 	BB57_8;

BB57_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd59, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd59;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd11, %p4;

BB57_8:
	cvta.to.global.u64 	%rd7, %rd2;
	mul.f64 	%fd56, %fd61, 0d3C695355BAAAFAD3;
	mov.f64 	%fd57, 0d3FDBCB7B1526E50E;
	fma.rn.f64 	%fd58, %fd61, %fd57, %fd56;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd58;

BB57_9:
	ret;
}

	// .globl	vec_log1p
.visible .entry vec_log1p(
	.param .u32 vec_log1p_param_0,
	.param .u64 vec_log1p_param_1,
	.param .u64 vec_log1p_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<40>;
	.reg .f64 	%fd<84>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r12, [vec_log1p_param_0];
	ld.param.u64 	%rd2, [vec_log1p_param_1];
	ld.param.u64 	%rd3, [vec_log1p_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB58_11;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd1;
	}
	setp.lt.u32	%p2, %r22, 1071994197;
	setp.lt.s32	%p3, %r22, -1076258407;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB58_9;
	bra.uni 	BB58_2;

BB58_9:
	add.f64 	%fd58, %fd1, 0d4000000000000000;
	div.rn.f64 	%fd59, %fd1, %fd58;
	mul.f64 	%fd60, %fd1, %fd59;
	neg.f64 	%fd61, %fd60;
	sub.f64 	%fd62, %fd1, %fd60;
	mul.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3ED087FFCEB2DC44;
	mov.f64 	%fd65, 0d3EB372FB2FBE14B5;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3EF3B9FF890F468C;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0d3F17457EFD51BAF8;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F3C71C8DE3CE825;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0d3F6249248FA4661F;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3F899999999D70C4;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0d3FB5555555555462;
	fma.rn.f64 	%fd78, %fd76, %fd63, %fd77;
	mul.f64 	%fd79, %fd63, %fd78;
	fma.rn.f64 	%fd80, %fd79, %fd62, %fd61;
	add.f64 	%fd83, %fd1, %fd80;
	bra.uni 	BB58_10;

BB58_2:
	add.f64 	%fd81, %fd1, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd81;
	}
	mov.u32 	%r38, -1023;
	setp.gt.s32	%p5, %r36, 1048575;
	@%p5 bra 	BB58_4;

	mul.f64 	%fd81, %fd81, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd81;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd81;
	}
	mov.u32 	%r38, -1077;

BB58_4:
	add.s32 	%r25, %r36, -1;
	setp.lt.u32	%p6, %r25, 2146435071;
	@%p6 bra 	BB58_6;
	bra.uni 	BB58_5;

BB58_6:
	shr.u32 	%r27, %r36, 20;
	add.s32 	%r39, %r38, %r27;
	and.b32  	%r28, %r36, -2146435073;
	or.b32  	%r29, %r28, 1072693248;
	mov.b64 	%fd82, {%r37, %r29};
	setp.lt.s32	%p8, %r29, 1073127583;
	@%p8 bra 	BB58_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r30, %temp}, %fd82;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd82;
	}
	add.s32 	%r32, %r31, -1048576;
	mov.b64 	%fd82, {%r30, %r32};
	add.s32 	%r39, %r39, 1;

BB58_8:
	add.f64 	%fd15, %fd82, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd14,%fd15;
	// inline asm
	neg.f64 	%fd16, %fd15;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd16, %fd14, %fd17;
	fma.rn.f64 	%fd19, %fd18, %fd18, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd14, %fd14;
	add.f64 	%fd21, %fd82, 0dBFF0000000000000;
	mul.f64 	%fd22, %fd21, %fd20;
	fma.rn.f64 	%fd23, %fd21, %fd20, %fd22;
	mul.f64 	%fd24, %fd23, %fd23;
	mov.f64 	%fd25, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd26, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F624924923BE72D;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	mov.f64 	%fd38, 0d3FB5555555555554;
	fma.rn.f64 	%fd39, %fd37, %fd24, %fd38;
	sub.f64 	%fd40, %fd21, %fd23;
	add.f64 	%fd41, %fd40, %fd40;
	neg.f64 	%fd42, %fd23;
	fma.rn.f64 	%fd43, %fd42, %fd21, %fd41;
	mul.f64 	%fd44, %fd20, %fd43;
	mul.f64 	%fd45, %fd24, %fd39;
	fma.rn.f64 	%fd46, %fd45, %fd23, %fd44;
	xor.b32  	%r33, %r39, -2147483648;
	mov.u32 	%r34, 1127219200;
	mov.b64 	%fd47, {%r33, %r34};
	mov.u32 	%r35, -2147483648;
	mov.b64 	%fd48, {%r35, %r34};
	sub.f64 	%fd49, %fd47, %fd48;
	mov.f64 	%fd50, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd51, %fd49, %fd50, %fd23;
	neg.f64 	%fd52, %fd49;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	sub.f64 	%fd54, %fd53, %fd23;
	sub.f64 	%fd55, %fd46, %fd54;
	mov.f64 	%fd56, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd57, %fd49, %fd56, %fd55;
	add.f64 	%fd83, %fd51, %fd57;
	bra.uni 	BB58_10;

BB58_5:
	mov.f64 	%fd12, 0d7FF0000000000000;
	fma.rn.f64 	%fd13, %fd81, %fd12, %fd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd81;
	}
	mov.b32 	 %f1, %r26;
	setp.eq.f32	%p7, %f1, 0f00000000;
	selp.f64	%fd83, 0dFFF0000000000000, %fd13, %p7;

BB58_10:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd83;

BB58_11:
	ret;
}

	// .globl	vec_log2
.visible .entry vec_log2(
	.param .u32 vec_log2_param_0,
	.param .u64 vec_log2_param_1,
	.param .u64 vec_log2_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r12, [vec_log2_param_0];
	ld.param.u64 	%rd2, [vec_log2_param_1];
	ld.param.u64 	%rd3, [vec_log2_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB59_9;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd59, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB59_3;

	mul.f64 	%fd59, %fd59, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd59;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd59;
	}
	mov.u32 	%r37, -1077;

BB59_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB59_5;
	bra.uni 	BB59_4;

BB59_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB59_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB59_7:
	add.f64 	%fd13, %fd60, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, 1127219200;
	mov.b64 	%fd45, {%r32, %r33};
	mov.u32 	%r34, -2147483648;
	mov.b64 	%fd46, {%r34, %r33};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd61, %fd49, %fd55;
	bra.uni 	BB59_8;

BB59_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd59, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd59;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd11, %p4;

BB59_8:
	cvta.to.global.u64 	%rd7, %rd2;
	mul.f64 	%fd56, %fd61, 0d3C7777D0FFDA0D24;
	mov.f64 	%fd57, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd58, %fd61, %fd57, %fd56;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd58;

BB59_9:
	ret;
}

	// .globl	vec_logb
.visible .entry vec_logb(
	.param .u32 vec_logb_param_0,
	.param .u64 vec_logb_param_1,
	.param .u64 vec_logb_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r3, [vec_logb_param_0];
	ld.param.u64 	%rd2, [vec_logb_param_1];
	ld.param.u64 	%rd3, [vec_logb_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB60_9;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d7FF0000000000000;
	@%p2 bra 	BB60_7;
	bra.uni 	BB60_2;

BB60_7:
	add.f64 	%fd8, %fd1, %fd1;
	bra.uni 	BB60_8;

BB60_2:
	setp.eq.f64	%p3, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd8, %fd2;
	@%p3 bra 	BB60_8;

	setp.eq.f64	%p4, %fd2, 0d0000000000000000;
	mov.f64 	%fd7, 0dFFF0000000000000;
	mov.f64 	%fd8, %fd7;
	@%p4 bra 	BB60_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd2;
	}
	setp.gt.u32	%p5, %r2, 1048575;
	@%p5 bra 	BB60_6;
	bra.uni 	BB60_5;

BB60_6:
	shr.u32 	%r17, %r2, 20;
	add.s32 	%r18, %r17, -1023;
	cvt.rn.f64.s32	%fd8, %r18;
	bra.uni 	BB60_8;

BB60_5:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd2;
	}
	cvt.u64.u32	%rd7, %r2;
	shl.b64 	%rd8, %rd7, 32;
	cvt.u64.u32	%rd9, %r13;
	or.b64  	%rd10, %rd8, %rd9;
	clz.b64 	%r14, %rd10;
	mov.u32 	%r15, -1011;
	sub.s32 	%r16, %r15, %r14;
	cvt.rn.f64.s32	%fd8, %r16;

BB60_8:
	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 3;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.f64 	[%rd13], %fd8;

BB60_9:
	ret;
}

	// .globl	vec_log
.visible .entry vec_log(
	.param .u32 vec_log_param_0,
	.param .u64 vec_log_param_1,
	.param .u64 vec_log_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<59>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r12, [vec_log_param_0];
	ld.param.u64 	%rd2, [vec_log_param_1];
	ld.param.u64 	%rd3, [vec_log_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %nctaid.x;
	mov.u32 	%r19, %ctaid.x;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r1, %r20, %r21, %r13;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB61_9;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd56, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd56;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p2, %r35, 1048575;
	@%p2 bra 	BB61_3;

	mul.f64 	%fd56, %fd56, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd56;
	}
	mov.u32 	%r37, -1077;

BB61_3:
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p3, %r24, 2146435071;
	@%p3 bra 	BB61_5;
	bra.uni 	BB61_4;

BB61_5:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd57, {%r36, %r28};
	setp.lt.s32	%p5, %r28, 1073127583;
	@%p5 bra 	BB61_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd57;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd57;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd57, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB61_7:
	add.f64 	%fd13, %fd57, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd12,%fd13;
	// inline asm
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd14, %fd12, %fd15;
	fma.rn.f64 	%fd17, %fd16, %fd16, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd12, %fd12;
	add.f64 	%fd19, %fd57, 0dBFF0000000000000;
	mul.f64 	%fd20, %fd19, %fd18;
	fma.rn.f64 	%fd21, %fd19, %fd18, %fd20;
	mul.f64 	%fd22, %fd21, %fd21;
	mov.f64 	%fd23, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd24, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mov.f64 	%fd26, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd27, %fd25, %fd22, %fd26;
	mov.f64 	%fd28, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd29, %fd27, %fd22, %fd28;
	mov.f64 	%fd30, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd31, %fd29, %fd22, %fd30;
	mov.f64 	%fd32, 0d3F624924923BE72D;
	fma.rn.f64 	%fd33, %fd31, %fd22, %fd32;
	mov.f64 	%fd34, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd35, %fd33, %fd22, %fd34;
	mov.f64 	%fd36, 0d3FB5555555555554;
	fma.rn.f64 	%fd37, %fd35, %fd22, %fd36;
	sub.f64 	%fd38, %fd19, %fd21;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd21;
	fma.rn.f64 	%fd41, %fd40, %fd19, %fd39;
	mul.f64 	%fd42, %fd18, %fd41;
	mul.f64 	%fd43, %fd22, %fd37;
	fma.rn.f64 	%fd44, %fd43, %fd21, %fd42;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, 1127219200;
	mov.b64 	%fd45, {%r32, %r33};
	mov.u32 	%r34, -2147483648;
	mov.b64 	%fd46, {%r34, %r33};
	sub.f64 	%fd47, %fd45, %fd46;
	mov.f64 	%fd48, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd49, %fd47, %fd48, %fd21;
	neg.f64 	%fd50, %fd47;
	fma.rn.f64 	%fd51, %fd50, %fd48, %fd49;
	sub.f64 	%fd52, %fd51, %fd21;
	sub.f64 	%fd53, %fd44, %fd52;
	mov.f64 	%fd54, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd55, %fd47, %fd54, %fd53;
	add.f64 	%fd58, %fd49, %fd55;
	bra.uni 	BB61_8;

BB61_4:
	mov.f64 	%fd10, 0d7FF0000000000000;
	fma.rn.f64 	%fd11, %fd56, %fd10, %fd10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd56;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f64	%fd58, 0dFFF0000000000000, %fd11, %p4;

BB61_8:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd58;

BB61_9:
	ret;
}

	// .globl	vec_normcdf
.visible .entry vec_normcdf(
	.param .u32 vec_normcdf_param_0,
	.param .u64 vec_normcdf_param_1,
	.param .u64 vec_normcdf_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<144>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r5, [vec_normcdf_param_0];
	ld.param.u64 	%rd3, [vec_normcdf_param_2];
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %tid.y;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r6;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB62_9;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd142, [%rd6];
	abs.f64 	%fd12, %fd142;
	setp.leu.f64	%p2, %fd12, 0d4043400000000000;
	@%p2 bra 	BB62_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd142;
	}
	and.b32  	%r16, %r15, -2147483648;
	mov.f64 	%fd13, 0d4043400000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd13;
	}
	and.b32  	%r18, %r17, 2147483647;
	or.b32  	%r19, %r18, %r16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd13;
	}
	mov.b64 	%fd142, {%r20, %r19};

BB62_3:
	mov.f64 	%fd14, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd4, %fd142, %fd14;
	neg.f64 	%fd15, %fd4;
	fma.rn.f64 	%fd16, %fd142, %fd14, %fd15;
	mov.f64 	%fd17, 0d3C8BDD3413B26456;
	fma.rn.f64 	%fd5, %fd142, %fd17, %fd16;
	add.rn.f64 	%fd6, %fd4, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd6;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd6;
	}
	setp.lt.u32	%p3, %r3, 2146435072;
	@%p3 bra 	BB62_5;
	bra.uni 	BB62_4;

BB62_5:
	setp.lt.s32	%p8, %r2, 0;
	mov.b64 	%fd24, {%r4, %r3};
	add.f64 	%fd25, %fd24, 0dC010000000000000;
	add.f64 	%fd21, %fd24, 0d4010000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd20,%fd21;
	// inline asm
	neg.f64 	%fd26, %fd21;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd20, %fd27;
	fma.rn.f64 	%fd29, %fd28, %fd28, %fd28;
	fma.rn.f64 	%fd30, %fd29, %fd20, %fd20;
	mul.f64 	%fd31, %fd25, %fd30;
	add.rn.f64 	%fd32, %fd31, %fd27;
	mov.f64 	%fd33, 0dC010000000000000;
	fma.rn.f64 	%fd34, %fd33, %fd32, %fd24;
	neg.f64 	%fd35, %fd31;
	fma.rn.f64 	%fd36, %fd35, %fd24, %fd34;
	fma.rn.f64 	%fd37, %fd30, %fd36, %fd31;
	mov.f64 	%fd38, 0dBE44E1C6FD03D328;
	mov.f64 	%fd39, 0dBDF8774AD4E0BFD7;
	fma.rn.f64 	%fd40, %fd39, %fd37, %fd38;
	mov.f64 	%fd41, 0dBE4330149F7A56B6;
	fma.rn.f64 	%fd42, %fd40, %fd37, %fd41;
	mov.f64 	%fd43, 0d3E7BEDDED8376273;
	fma.rn.f64 	%fd44, %fd42, %fd37, %fd43;
	mov.f64 	%fd45, 0d3E6F9254C3ABF22B;
	fma.rn.f64 	%fd46, %fd44, %fd37, %fd45;
	mov.f64 	%fd47, 0dBEAB9068C2148CF0;
	fma.rn.f64 	%fd48, %fd46, %fd37, %fd47;
	mov.f64 	%fd49, 0d3E94C6454DB34009;
	fma.rn.f64 	%fd50, %fd48, %fd37, %fd49;
	mov.f64 	%fd51, 0d3ED7F1C378F2311D;
	fma.rn.f64 	%fd52, %fd50, %fd37, %fd51;
	mov.f64 	%fd53, 0dBEE78E051C6D5C58;
	fma.rn.f64 	%fd54, %fd52, %fd37, %fd53;
	mov.f64 	%fd55, 0dBEF995B4EAD14A90;
	fma.rn.f64 	%fd56, %fd54, %fd37, %fd55;
	mov.f64 	%fd57, 0d3F23BE27CF0A29B2;
	fma.rn.f64 	%fd58, %fd56, %fd37, %fd57;
	mov.f64 	%fd59, 0dBF2A1DEF3E81672E;
	fma.rn.f64 	%fd60, %fd58, %fd37, %fd59;
	mov.f64 	%fd61, 0dBF48D4ABE68C1713;
	fma.rn.f64 	%fd62, %fd60, %fd37, %fd61;
	mov.f64 	%fd63, 0d3F749C67210DD6B4;
	fma.rn.f64 	%fd64, %fd62, %fd37, %fd63;
	mov.f64 	%fd65, 0dBF9096238568E357;
	fma.rn.f64 	%fd66, %fd64, %fd37, %fd65;
	mov.f64 	%fd67, 0d3FA3079EDF8C2DC9;
	fma.rn.f64 	%fd68, %fd66, %fd37, %fd67;
	mov.f64 	%fd69, 0dBFB0FB06DFF601FC;
	fma.rn.f64 	%fd70, %fd68, %fd37, %fd69;
	mov.f64 	%fd71, 0d3FB7FEE004DFBCDC;
	fma.rn.f64 	%fd72, %fd70, %fd37, %fd71;
	mov.f64 	%fd73, 0dBFB9DDB23C3DB8C6;
	fma.rn.f64 	%fd74, %fd72, %fd37, %fd73;
	mov.f64 	%fd75, 0d3FB16ECEFCFA5FDA;
	fma.rn.f64 	%fd76, %fd74, %fd37, %fd75;
	mov.f64 	%fd77, 0d3F8F7F5DF66FB6D6;
	fma.rn.f64 	%fd78, %fd76, %fd37, %fd77;
	mov.f64 	%fd79, 0dBFC1DF1AD154A29D;
	fma.rn.f64 	%fd80, %fd78, %fd37, %fd79;
	mov.f64 	%fd81, 0d3FF3BA5916E9FD7F;
	fma.rn.f64 	%fd82, %fd80, %fd37, %fd81;
	mov.f64 	%fd83, 0d4000000000000000;
	fma.rn.f64 	%fd23, %fd83, %fd24, %fd27;
	// inline asm
	rcp.approx.ftz.f64 %fd22,%fd23;
	// inline asm
	neg.f64 	%fd84, %fd23;
	fma.rn.f64 	%fd85, %fd84, %fd22, %fd27;
	fma.rn.f64 	%fd86, %fd85, %fd85, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd22, %fd22;
	mul.f64 	%fd88, %fd82, %fd87;
	mul.f64 	%fd89, %fd88, 0dC000000000000000;
	fma.rn.f64 	%fd90, %fd24, %fd89, %fd82;
	neg.f64 	%fd91, %fd88;
	add.rn.f64 	%fd92, %fd90, %fd91;
	fma.rn.f64 	%fd93, %fd92, %fd87, %fd88;
	mul.f64 	%fd94, %fd24, %fd24;
	neg.f64 	%fd95, %fd94;
	mov.f64 	%fd96, 0d4338000000000000;
	mov.f64 	%fd97, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd98, %fd95, %fd97, %fd96;
	mov.f64 	%fd99, 0dC338000000000000;
	add.rn.f64 	%fd100, %fd98, %fd99;
	mov.f64 	%fd101, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd102, %fd100, %fd101, %fd95;
	mov.f64 	%fd103, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd104, %fd100, %fd103, %fd102;
	mov.f64 	%fd105, 0d3E928AF3FCA213EA;
	mov.f64 	%fd106, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd107, %fd106, %fd104, %fd105;
	mov.f64 	%fd108, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd109, %fd107, %fd104, %fd108;
	mov.f64 	%fd110, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd111, %fd109, %fd104, %fd110;
	mov.f64 	%fd112, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd113, %fd111, %fd104, %fd112;
	mov.f64 	%fd114, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd115, %fd113, %fd104, %fd114;
	mov.f64 	%fd116, 0d3F81111111122322;
	fma.rn.f64 	%fd117, %fd115, %fd104, %fd116;
	mov.f64 	%fd118, 0d3FA55555555502A1;
	fma.rn.f64 	%fd119, %fd117, %fd104, %fd118;
	mov.f64 	%fd120, 0d3FC5555555555511;
	fma.rn.f64 	%fd121, %fd119, %fd104, %fd120;
	mov.f64 	%fd122, 0d3FE000000000000B;
	fma.rn.f64 	%fd123, %fd121, %fd104, %fd122;
	fma.rn.f64 	%fd124, %fd123, %fd104, %fd27;
	fma.rn.f64 	%fd125, %fd124, %fd104, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r21, %temp}, %fd98;
	}
	shr.u32 	%r22, %r21, 31;
	add.s32 	%r23, %r21, %r22;
	shr.s32 	%r24, %r23, 1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r25, %temp}, %fd125;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd125;
	}
	shl.b32 	%r27, %r24, 20;
	add.s32 	%r28, %r26, %r27;
	mov.b64 	%fd126, {%r25, %r28};
	sub.s32 	%r29, %r21, %r24;
	shl.b32 	%r30, %r29, 20;
	add.s32 	%r31, %r30, 1072693248;
	mov.u32 	%r32, 0;
	mov.b64 	%fd127, {%r32, %r31};
	mul.f64 	%fd128, %fd126, %fd127;
	neg.f64 	%fd129, %fd24;
	fma.rn.f64 	%fd130, %fd129, %fd24, %fd94;
	fma.rn.f64 	%fd131, %fd128, %fd130, %fd128;
	mul.f64 	%fd132, %fd93, %fd131;
	setp.gt.u32	%p9, %r3, 1077624832;
	selp.f64	%fd133, 0d0000000000000000, %fd132, %p9;
	sub.f64 	%fd134, %fd83, %fd133;
	selp.f64	%fd143, %fd134, %fd133, %p8;
	bra.uni 	BB62_6;

BB62_4:
	setp.lt.s32	%p4, %r2, 0;
	setp.eq.s32	%p5, %r4, 0;
	setp.eq.s32	%p6, %r3, 2146435072;
	and.pred  	%p7, %p6, %p5;
	selp.f64	%fd18, 0d4000000000000000, 0d0000000000000000, %p4;
	add.f64 	%fd19, %fd6, %fd6;
	selp.f64	%fd143, %fd18, %fd19, %p7;

BB62_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd142;
	}
	setp.lt.u32	%p10, %r33, -1074790399;
	@%p10 bra 	BB62_8;

	mov.f64 	%fd141, 0dBFE6A09E667F3BCD;
	mul.rn.f64 	%fd140, %fd142, %fd141;
	sub.f64 	%fd135, %fd140, %fd6;
	add.rn.f64 	%fd136, %fd135, %fd5;
	mul.f64 	%fd137, %fd6, 0dC000000000000000;
	mul.f64 	%fd138, %fd137, %fd143;
	fma.rn.f64 	%fd143, %fd138, %fd136, %fd143;

BB62_8:
	mov.u32 	%r43, %tid.y;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r39, %nctaid.x;
	mad.lo.s32 	%r38, %r41, %r42, %r43;
	mov.u32 	%r37, %tid.x;
	mov.u32 	%r36, %ntid.x;
	mad.lo.s32 	%r35, %r38, %r39, %r40;
	mad.lo.s32 	%r34, %r35, %r36, %r37;
	cvt.s64.s32	%rd11, %r34;
	ld.param.u64 	%rd10, [vec_normcdf_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	mul.f64 	%fd139, %fd143, 0d3FE0000000000000;
	st.global.f64 	[%rd9], %fd139;

BB62_9:
	ret;
}

	// .globl	vec_normcdfinv
.visible .entry vec_normcdfinv(
	.param .u32 vec_normcdfinv_param_0,
	.param .u64 vec_normcdfinv_param_1,
	.param .u64 vec_normcdfinv_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<66>;
	.reg .f64 	%fd<274>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r13, [vec_normcdfinv_param_0];
	ld.param.u64 	%rd3, [vec_normcdfinv_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB63_15;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd19, [%rd6];
	add.f64 	%fd1, %fd19, %fd19;
	neg.f64 	%fd2, %fd1;
	mov.f64 	%fd20, 0d4000000000000000;
	add.rn.f64 	%fd3, %fd20, %fd2;
	setp.le.f64	%p2, %fd1, 0d3FFFFC0B65AA4E0E;
	setp.ge.f64	%p3, %fd1, 0d3F4FA4D2AD8F904D;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB63_13;
	bra.uni 	BB63_2;

BB63_13:
	mul.rn.f64 	%fd175, %fd3, %fd1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd175;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd175;
	}
	shr.u32 	%r45, %r43, 20;
	and.b32  	%r46, %r45, 2046;
	add.s32 	%r47, %r46, 2147482626;
	mov.u32 	%r48, 1127219200;
	mov.b64 	%fd176, {%r47, %r48};
	mov.u32 	%r49, -2147483648;
	mov.b64 	%fd177, {%r49, %r48};
	sub.f64 	%fd178, %fd176, %fd177;
	and.b32  	%r50, %r43, -2145386497;
	add.s32 	%r51, %r50, 1071644672;
	mov.b64 	%fd179, {%r44, %r51};
	add.f64 	%fd180, %fd179, 0dBFF0000000000000;
	add.f64 	%fd174, %fd179, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd173,%fd174;
	// inline asm
	neg.f64 	%fd181, %fd174;
	mov.f64 	%fd182, 0d3FF0000000000000;
	fma.rn.f64 	%fd183, %fd181, %fd173, %fd182;
	fma.rn.f64 	%fd184, %fd183, %fd183, %fd183;
	fma.rn.f64 	%fd185, %fd184, %fd173, %fd173;
	mul.f64 	%fd186, %fd180, %fd185;
	mov.f64 	%fd187, 0dC000000000000000;
	fma.rn.f64 	%fd188, %fd187, %fd186, %fd180;
	neg.f64 	%fd189, %fd186;
	fma.rn.f64 	%fd190, %fd189, %fd180, %fd188;
	fma.rn.f64 	%fd191, %fd190, %fd185, %fd186;
	mul.f64 	%fd192, %fd191, %fd191;
	mov.f64 	%fd193, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd194, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd195, %fd194, %fd192, %fd193;
	mov.f64 	%fd196, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd197, %fd195, %fd192, %fd196;
	mov.f64 	%fd198, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd199, %fd197, %fd192, %fd198;
	mov.f64 	%fd200, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd201, %fd199, %fd192, %fd200;
	mov.f64 	%fd202, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd203, %fd201, %fd192, %fd202;
	mov.f64 	%fd204, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd205, %fd203, %fd192, %fd204;
	mov.f64 	%fd206, 0d3FC249249209112E;
	fma.rn.f64 	%fd207, %fd205, %fd192, %fd206;
	mov.f64 	%fd208, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd209, %fd207, %fd192, %fd208;
	mov.f64 	%fd210, 0d3FD5555555555535;
	fma.rn.f64 	%fd211, %fd209, %fd192, %fd210;
	mul.f64 	%fd212, %fd192, %fd211;
	fma.rn.f64 	%fd213, %fd212, %fd191, %fd191;
	add.f64 	%fd214, %fd213, %fd213;
	mov.f64 	%fd215, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd216, %fd178, %fd215, %fd214;
	mov.f64 	%fd217, 0dC009000000000000;
	sub.f64 	%fd218, %fd217, %fd216;
	mov.f64 	%fd219, 0dBC08DDF93324D327;
	mov.f64 	%fd220, 0dBBB135D2E746E627;
	fma.rn.f64 	%fd221, %fd220, %fd218, %fd219;
	mov.f64 	%fd222, 0d3C37B83EEF0B7C9F;
	fma.rn.f64 	%fd223, %fd221, %fd218, %fd222;
	mov.f64 	%fd224, 0d3C69BA72CD589B91;
	fma.rn.f64 	%fd225, %fd223, %fd218, %fd224;
	mov.f64 	%fd226, 0dBCA33689090A6B96;
	fma.rn.f64 	%fd227, %fd225, %fd218, %fd226;
	mov.f64 	%fd228, 0d3C782E11898132E0;
	fma.rn.f64 	%fd229, %fd227, %fd218, %fd228;
	mov.f64 	%fd230, 0d3CFDE4ACFD9E26BA;
	fma.rn.f64 	%fd231, %fd229, %fd218, %fd230;
	mov.f64 	%fd232, 0dBD26D33EED66C487;
	fma.rn.f64 	%fd233, %fd231, %fd218, %fd232;
	mov.f64 	%fd234, 0dBD36F2167040D8E2;
	fma.rn.f64 	%fd235, %fd233, %fd218, %fd234;
	mov.f64 	%fd236, 0d3D872A22C2D77E20;
	fma.rn.f64 	%fd237, %fd235, %fd218, %fd236;
	mov.f64 	%fd238, 0dBDAC8859C4E5C0AF;
	fma.rn.f64 	%fd239, %fd237, %fd218, %fd238;
	mov.f64 	%fd240, 0dBDCDC583D118A561;
	fma.rn.f64 	%fd241, %fd239, %fd218, %fd240;
	mov.f64 	%fd242, 0d3E120F47CCF46B3C;
	fma.rn.f64 	%fd243, %fd241, %fd218, %fd242;
	mov.f64 	%fd244, 0dBE31A9E38DC84D60;
	fma.rn.f64 	%fd245, %fd243, %fd218, %fd244;
	mov.f64 	%fd246, 0dBE5F36CD6D3D46A9;
	fma.rn.f64 	%fd247, %fd245, %fd218, %fd246;
	mov.f64 	%fd248, 0d3E9C6B4F5D03B787;
	fma.rn.f64 	%fd249, %fd247, %fd218, %fd248;
	mov.f64 	%fd250, 0dBEB6E8A5434AE8A2;
	fma.rn.f64 	%fd251, %fd249, %fd218, %fd250;
	mov.f64 	%fd252, 0dBEED1D1F7B8736F6;
	fma.rn.f64 	%fd253, %fd251, %fd218, %fd252;
	mov.f64 	%fd254, 0d3F2879C2A212F024;
	fma.rn.f64 	%fd255, %fd253, %fd218, %fd254;
	mov.f64 	%fd256, 0dBF4845769484FCA8;
	fma.rn.f64 	%fd257, %fd255, %fd218, %fd256;
	mov.f64 	%fd258, 0dBF78B6C33114F909;
	fma.rn.f64 	%fd259, %fd257, %fd218, %fd258;
	mov.f64 	%fd260, 0d3FCEBD80D9B13E28;
	fma.rn.f64 	%fd261, %fd259, %fd218, %fd260;
	mov.f64 	%fd262, 0d3FFA755E7C99AE86;
	fma.rn.f64 	%fd263, %fd261, %fd218, %fd262;
	fma.rn.f64 	%fd273, %fd263, %fd2, %fd263;
	bra.uni 	BB63_14;

BB63_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	setp.gt.s32	%p5, %r2, 1072693247;
	selp.f64	%fd269, %fd3, %fd1, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd269;
	}
	mov.b32 	 %f1, %r62;
	setp.ltu.f32	%p6, %f1, 0f2B2BFF2F;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd269;
	}
	@%p6 bra 	BB63_4;
	bra.uni 	BB63_3;

BB63_4:
	mov.u32 	%r64, -1023;
	setp.gt.s32	%p7, %r62, 1048575;
	@%p7 bra 	BB63_6;

	mul.f64 	%fd269, %fd269, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd269;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd269;
	}
	mov.u32 	%r64, -1077;

BB63_6:
	add.s32 	%r32, %r62, -1;
	setp.lt.u32	%p8, %r32, 2146435071;
	@%p8 bra 	BB63_8;
	bra.uni 	BB63_7;

BB63_8:
	shr.u32 	%r34, %r62, 20;
	add.s32 	%r65, %r64, %r34;
	and.b32  	%r35, %r62, -2146435073;
	or.b32  	%r36, %r35, 1072693248;
	mov.b64 	%fd270, {%r63, %r36};
	setp.lt.s32	%p10, %r36, 1073127583;
	@%p10 bra 	BB63_10;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd270;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd270;
	}
	add.s32 	%r39, %r38, -1048576;
	mov.b64 	%fd270, {%r37, %r39};
	add.s32 	%r65, %r65, 1;

BB63_10:
	add.f64 	%fd108, %fd270, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd107,%fd108;
	// inline asm
	neg.f64 	%fd109, %fd108;
	mov.f64 	%fd110, 0d3FF0000000000000;
	fma.rn.f64 	%fd111, %fd109, %fd107, %fd110;
	fma.rn.f64 	%fd112, %fd111, %fd111, %fd111;
	fma.rn.f64 	%fd113, %fd112, %fd107, %fd107;
	add.f64 	%fd114, %fd270, 0dBFF0000000000000;
	mul.f64 	%fd115, %fd114, %fd113;
	fma.rn.f64 	%fd116, %fd114, %fd113, %fd115;
	mul.f64 	%fd117, %fd116, %fd116;
	mov.f64 	%fd118, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd119, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd120, %fd119, %fd117, %fd118;
	mov.f64 	%fd121, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd122, %fd120, %fd117, %fd121;
	mov.f64 	%fd123, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd124, %fd122, %fd117, %fd123;
	mov.f64 	%fd125, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd126, %fd124, %fd117, %fd125;
	mov.f64 	%fd127, 0d3F624924923BE72D;
	fma.rn.f64 	%fd128, %fd126, %fd117, %fd127;
	mov.f64 	%fd129, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd130, %fd128, %fd117, %fd129;
	mov.f64 	%fd131, 0d3FB5555555555554;
	fma.rn.f64 	%fd132, %fd130, %fd117, %fd131;
	sub.f64 	%fd133, %fd114, %fd116;
	add.f64 	%fd134, %fd133, %fd133;
	neg.f64 	%fd135, %fd116;
	fma.rn.f64 	%fd136, %fd135, %fd114, %fd134;
	mul.f64 	%fd137, %fd113, %fd136;
	mul.f64 	%fd138, %fd117, %fd132;
	fma.rn.f64 	%fd139, %fd138, %fd116, %fd137;
	xor.b32  	%r40, %r65, -2147483648;
	mov.u32 	%r41, 1127219200;
	mov.b64 	%fd140, {%r40, %r41};
	mov.u32 	%r42, -2147483648;
	mov.b64 	%fd141, {%r42, %r41};
	sub.f64 	%fd142, %fd140, %fd141;
	mov.f64 	%fd143, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd144, %fd142, %fd143, %fd116;
	neg.f64 	%fd145, %fd142;
	fma.rn.f64 	%fd146, %fd145, %fd143, %fd144;
	sub.f64 	%fd147, %fd146, %fd116;
	sub.f64 	%fd148, %fd139, %fd147;
	mov.f64 	%fd149, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd150, %fd142, %fd149, %fd148;
	add.f64 	%fd271, %fd144, %fd150;
	bra.uni 	BB63_11;

BB63_3:
	shr.u32 	%r23, %r62, 20;
	and.b32  	%r24, %r23, 2046;
	add.s32 	%r25, %r24, 2147482626;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd25, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd26, {%r27, %r26};
	sub.f64 	%fd27, %fd25, %fd26;
	and.b32  	%r28, %r62, -2145386497;
	add.s32 	%r29, %r28, 1071644672;
	mov.b64 	%fd28, {%r63, %r29};
	add.f64 	%fd29, %fd28, 0dBFF0000000000000;
	add.f64 	%fd22, %fd28, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd21,%fd22;
	// inline asm
	neg.f64 	%fd30, %fd22;
	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd32, %fd30, %fd21, %fd31;
	fma.rn.f64 	%fd33, %fd32, %fd32, %fd32;
	fma.rn.f64 	%fd34, %fd33, %fd21, %fd21;
	mul.f64 	%fd35, %fd29, %fd34;
	mov.f64 	%fd36, 0dC000000000000000;
	fma.rn.f64 	%fd37, %fd36, %fd35, %fd29;
	neg.f64 	%fd38, %fd35;
	fma.rn.f64 	%fd39, %fd38, %fd29, %fd37;
	fma.rn.f64 	%fd40, %fd39, %fd34, %fd35;
	mul.f64 	%fd41, %fd40, %fd40;
	mov.f64 	%fd42, 0d3FA55CF59CDC5D89;
	mov.f64 	%fd43, 0d3FB5C5C218C775C9;
	fma.rn.f64 	%fd44, %fd43, %fd41, %fd42;
	mov.f64 	%fd45, 0d3FAEFD18CF6EBB9C;
	fma.rn.f64 	%fd46, %fd44, %fd41, %fd45;
	mov.f64 	%fd47, 0d3FB10682EDCB8D1B;
	fma.rn.f64 	%fd48, %fd46, %fd41, %fd47;
	mov.f64 	%fd49, 0d3FB3B1DD3AC7FC96;
	fma.rn.f64 	%fd50, %fd48, %fd41, %fd49;
	mov.f64 	%fd51, 0d3FB745CB459B54A6;
	fma.rn.f64 	%fd52, %fd50, %fd41, %fd51;
	mov.f64 	%fd53, 0d3FBC71C741A0669F;
	fma.rn.f64 	%fd54, %fd52, %fd41, %fd53;
	mov.f64 	%fd55, 0d3FC249249209112E;
	fma.rn.f64 	%fd56, %fd54, %fd41, %fd55;
	mov.f64 	%fd57, 0d3FC99999999A06C1;
	fma.rn.f64 	%fd58, %fd56, %fd41, %fd57;
	mov.f64 	%fd59, 0d3FD5555555555535;
	fma.rn.f64 	%fd60, %fd58, %fd41, %fd59;
	mul.f64 	%fd61, %fd41, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd40, %fd40;
	add.f64 	%fd63, %fd62, %fd62;
	mov.f64 	%fd64, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd65, %fd27, %fd64, %fd63;
	neg.f64 	%fd24, %fd65;
	// inline asm
	rsqrt.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	mul.rn.f64 	%fd66, %fd23, %fd23;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd68, %fd24, %fd67, %fd31;
	mov.f64 	%fd69, 0d3FE0000000000000;
	mov.f64 	%fd70, 0d3FD8000000000000;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	mul.rn.f64 	%fd72, %fd68, %fd23;
	fma.rn.f64 	%fd73, %fd71, %fd72, %fd23;
	mov.f64 	%fd74, 0d4000A0E7333839AA;
	mov.f64 	%fd75, 0d3FEBE9222591AFAB;
	fma.rn.f64 	%fd76, %fd75, %fd73, %fd74;
	mov.f64 	%fd77, 0d4008768CF7E57D5C;
	fma.rn.f64 	%fd78, %fd76, %fd73, %fd77;
	mov.f64 	%fd79, 0d400B77E7E28DA583;
	fma.rn.f64 	%fd80, %fd78, %fd73, %fd79;
	mov.f64 	%fd81, 0d3FF34F26A4F99CF9;
	fma.rn.f64 	%fd82, %fd80, %fd73, %fd81;
	mov.f64 	%fd83, 0d3FC1F674ADB019ED;
	fma.rn.f64 	%fd84, %fd82, %fd73, %fd83;
	mov.f64 	%fd85, 0d3F75DDAE9506431D;
	fma.rn.f64 	%fd86, %fd84, %fd73, %fd85;
	mov.f64 	%fd87, 0d3F0ADA49AA32489C;
	fma.rn.f64 	%fd88, %fd86, %fd73, %fd87;
	add.f64 	%fd89, %fd73, 0d4001E90FF51C2197;
	mov.f64 	%fd90, 0d40111EA3A7CF3820;
	fma.rn.f64 	%fd91, %fd89, %fd73, %fd90;
	mov.f64 	%fd92, 0d4011A0E4A4749594;
	fma.rn.f64 	%fd93, %fd91, %fd73, %fd92;
	mov.f64 	%fd94, 0d400D4E977D38C14D;
	fma.rn.f64 	%fd95, %fd93, %fd73, %fd94;
	mov.f64 	%fd96, 0d3FF37FD567EC0D5F;
	fma.rn.f64 	%fd97, %fd95, %fd73, %fd96;
	mov.f64 	%fd98, 0d3FC1FB9D7F676033;
	fma.rn.f64 	%fd99, %fd97, %fd73, %fd98;
	mov.f64 	%fd100, 0d3F75DDCDF98946E4;
	fma.rn.f64 	%fd101, %fd99, %fd73, %fd100;
	mov.f64 	%fd102, 0d3F0ADA42D79D8DBB;
	fma.rn.f64 	%fd103, %fd101, %fd73, %fd102;
	mul.f64 	%fd104, %fd73, %fd103;
	div.rn.f64 	%fd272, %fd88, %fd104;
	bra.uni 	BB63_12;

BB63_7:
	mov.f64 	%fd105, 0d7FF0000000000000;
	fma.rn.f64 	%fd106, %fd269, %fd105, %fd105;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd269;
	}
	mov.b32 	 %f2, %r33;
	setp.eq.f32	%p9, %f2, 0f00000000;
	selp.f64	%fd271, 0dFFF0000000000000, %fd106, %p9;

BB63_11:
	neg.f64 	%fd151, %fd271;
	rsqrt.approx.f64 	%fd152, %fd151;
	mov.f64 	%fd153, 0d3FFA2013964E259C;
	mov.f64 	%fd154, 0d3FE8E2101C71B0BF;
	fma.rn.f64 	%fd155, %fd154, %fd152, %fd153;
	mov.f64 	%fd156, 0d3FDABFE90921BE68;
	fma.rn.f64 	%fd157, %fd155, %fd152, %fd156;
	mov.f64 	%fd158, 0d3F97E41314DE00D4;
	fma.rn.f64 	%fd159, %fd157, %fd152, %fd158;
	mov.f64 	%fd160, 0d3F311BD487102E94;
	fma.rn.f64 	%fd161, %fd159, %fd152, %fd160;
	add.f64 	%fd162, %fd152, 0d3FF59895C30BAA54;
	mov.f64 	%fd163, 0d3FFAE8E5956A143F;
	fma.rn.f64 	%fd164, %fd162, %fd152, %fd163;
	mov.f64 	%fd165, 0d3FDACCE85FF7383D;
	fma.rn.f64 	%fd166, %fd164, %fd152, %fd165;
	mov.f64 	%fd167, 0d3F97E43B6CAC34FE;
	fma.rn.f64 	%fd168, %fd166, %fd152, %fd167;
	mov.f64 	%fd169, 0d3F311BD08289EB12;
	fma.rn.f64 	%fd170, %fd168, %fd152, %fd169;
	mul.f64 	%fd171, %fd152, %fd170;
	div.rn.f64 	%fd272, %fd161, %fd171;

BB63_12:
	neg.f64 	%fd172, %fd272;
	selp.f64	%fd273, %fd172, %fd272, %p5;

BB63_14:
	mov.u32 	%r61, %tid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r58, %ctaid.x;
	mov.u32 	%r57, %nctaid.x;
	mad.lo.s32 	%r56, %r59, %r60, %r61;
	mov.u32 	%r55, %tid.x;
	mov.u32 	%r54, %ntid.x;
	mad.lo.s32 	%r53, %r56, %r57, %r58;
	mad.lo.s32 	%r52, %r53, %r54, %r55;
	cvt.s64.s32	%rd11, %r52;
	ld.param.u64 	%rd10, [vec_normcdfinv_param_1];
	cvta.to.global.u64 	%rd7, %rd10;
	mul.f64 	%fd264, %fd273, 0dBCA21165F626CDD5;
	mov.f64 	%fd265, 0dBFF6A09E667F3BCC;
	fma.rn.f64 	%fd266, %fd265, %fd273, %fd264;
	mov.f64 	%fd267, 0d0000000000000000;
	add.rn.f64 	%fd268, %fd266, %fd267;
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd268;

BB63_15:
	ret;
}

	// .globl	vec_rcbrt
.visible .entry vec_rcbrt(
	.param .u32 vec_rcbrt_param_0,
	.param .u64 vec_rcbrt_param_1,
	.param .u64 vec_rcbrt_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<31>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r13, [vec_rcbrt_param_0];
	ld.param.u64 	%rd2, [vec_rcbrt_param_1];
	ld.param.u64 	%rd3, [vec_rcbrt_param_2];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p1, %r1, %r13;
	@%p1 bra 	BB64_7;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	and.b32  	%r39, %r3, 2147483647;
	setp.neu.f64	%p2, %fd1, 0d0000000000000000;
	setp.lt.u32	%p3, %r39, 2146435072;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB64_3;
	bra.uni 	BB64_2;

BB64_3:
	shr.u32 	%r40, %r39, 20;
	mov.u32 	%r41, 0;
	setp.ne.s32	%p6, %r40, 0;
	@%p6 bra 	BB64_5;

	mov.b64 	%fd8, {%r38, %r39};
	mul.f64 	%fd9, %fd8, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd9;
	}
	shr.u32 	%r40, %r39, 20;
	mov.u32 	%r41, 18;

BB64_5:
	add.s32 	%r26, %r40, -1022;
	cvt.rn.f32.s32	%f5, %r26;
	mul.f32 	%f6, %f5, 0f3EAAAAAB;
	cvt.rni.s32.f32	%r27, %f6;
	mad.lo.s32 	%r28, %r27, -3145728, %r39;
	mov.b64 	%fd10, {%r38, %r28};
	cvt.rn.f32.f64	%f2, %fd10;
	// inline asm
	lg2.approx.ftz.f32 %f1,%f2;
	// inline asm
	mul.f32 	%f4, %f1, 0fBEAAAAAB;
	// inline asm
	ex2.approx.ftz.f32 %f3,%f4;
	// inline asm
	cvt.f64.f32	%fd11, %f3;
	mul.rn.f64 	%fd12, %fd11, %fd11;
	mul.rn.f64 	%fd13, %fd10, %fd11;
	neg.f64 	%fd14, %fd13;
	mov.f64 	%fd15, 0d3FF0000000000000;
	fma.rn.f64 	%fd16, %fd12, %fd14, %fd15;
	fma.rn.f64 	%fd17, %fd10, %fd11, %fd14;
	neg.f64 	%fd18, %fd17;
	fma.rn.f64 	%fd19, %fd12, %fd18, %fd16;
	neg.f64 	%fd20, %fd12;
	fma.rn.f64 	%fd21, %fd11, %fd11, %fd20;
	neg.f64 	%fd22, %fd21;
	fma.rn.f64 	%fd23, %fd13, %fd22, %fd19;
	mov.f64 	%fd24, 0d3FD5555555555555;
	mov.f64 	%fd25, 0d3FCC71C71C71C71C;
	fma.rn.f64 	%fd26, %fd25, %fd23, %fd24;
	mul.rn.f64 	%fd27, %fd23, %fd11;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd28;
	}
	sub.s32 	%r31, %r41, %r27;
	shl.b32 	%r32, %r31, 20;
	add.s32 	%r33, %r30, %r32;
	mov.b64 	%fd30, {%r29, %r33};
	bra.uni 	BB64_6;

BB64_2:
	xor.b32  	%r23, %r39, 2146435072;
	mov.b64 	%fd5, {%r38, %r23};
	abs.f64 	%fd6, %fd1;
	setp.gtu.f64	%p5, %fd6, 0d7FF0000000000000;
	add.f64 	%fd7, %fd1, %fd1;
	selp.f64	%fd30, %fd7, %fd5, %p5;

BB64_6:
	cvta.to.global.u64 	%rd7, %rd2;
	and.b32  	%r34, %r3, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd30;
	}
	or.b32  	%r36, %r35, %r34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r37, %temp}, %fd30;
	}
	mov.b64 	%fd29, {%r37, %r36};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd29;

BB64_7:
	ret;
}

	// .globl	vec_rint
.visible .entry vec_rint(
	.param .u32 vec_rint_param_0,
	.param .u64 vec_rint_param_1,
	.param .u64 vec_rint_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_rint_param_0];
	ld.param.u64 	%rd1, [vec_rint_param_1];
	ld.param.u64 	%rd2, [vec_rint_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB65_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rni.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB65_2:
	ret;
}

	// .globl	vec_round
.visible .entry vec_round(
	.param .u32 vec_round_param_0,
	.param .u64 vec_round_param_1,
	.param .u64 vec_round_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r2, [vec_round_param_0];
	ld.param.u64 	%rd2, [vec_round_param_1];
	ld.param.u64 	%rd3, [vec_round_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB66_4;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd8, [%rd6];
	abs.f64 	%fd2, %fd8;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB66_3;

	add.f64 	%fd5, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd6, %fd5;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd7, 0d0000000000000000, %fd6, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd8;
	}
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r13, %r15;
	mov.b64 	%fd8, {%r12, %r16};

BB66_3:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd8;

BB66_4:
	ret;
}

	// .globl	vec_rsqrt
.visible .entry vec_rsqrt(
	.param .u32 vec_rsqrt_param_0,
	.param .u64 vec_rsqrt_param_1,
	.param .u64 vec_rsqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_rsqrt_param_0];
	ld.param.u64 	%rd1, [vec_rsqrt_param_1];
	ld.param.u64 	%rd2, [vec_rsqrt_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB67_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	rsqrt.approx.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB67_2:
	ret;
}

	// .globl	vec_sin
.visible .entry vec_sin(
	.param .u32 vec_sin_param_0,
	.param .u64 vec_sin_param_1,
	.param .u64 vec_sin_param_2
)
{
	.local .align 4 .b8 	__local_depot68[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<42>;
	.reg .b64 	%rd<17>;


	mov.u64 	%rd16, __local_depot68;
	cvta.local.u64 	%SP, %rd16;
	ld.param.u32 	%r5, [vec_sin_param_0];
	ld.param.u64 	%rd3, [vec_sin_param_1];
	ld.param.u64 	%rd4, [vec_sin_param_2];
	add.u64 	%rd5, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd5;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB68_10;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd2, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd39, [%rd8];
	abs.f64 	%fd14, %fd39;
	setp.neu.f64	%p2, %fd14, 0d7FF0000000000000;
	@%p2 bra 	BB68_3;

	mov.f64 	%fd15, 0d0000000000000000;
	mul.rn.f64 	%fd39, %fd39, %fd15;

BB68_3:
	mul.f64 	%fd16, %fd39, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r20, %fd16;
	st.local.u32 	[%rd1], %r20;
	cvt.rn.f64.s32	%fd17, %r20;
	neg.f64 	%fd18, %fd17;
	mov.f64 	%fd19, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd20, %fd18, %fd19, %fd39;
	mov.f64 	%fd21, 0d3C91A62633145C00;
	fma.rn.f64 	%fd22, %fd18, %fd21, %fd20;
	mov.f64 	%fd23, 0d397B839A252049C0;
	fma.rn.f64 	%fd40, %fd18, %fd23, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd39;
	}
	and.b32  	%r16, %r15, 2145386496;
	setp.lt.u32	%p3, %r16, 1105199104;
	@%p3 bra 	BB68_5;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd39;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd40, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r20, [%rd1];

BB68_5:
	and.b32  	%r17, %r20, 1;
	shl.b32 	%r18, %r17, 3;
	setp.eq.s32	%p4, %r17, 0;
	selp.f64	%fd24, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd10, %r18, 8;
	mov.u64 	%rd11, __cudart_sin_cos_coeffs;
	add.s64 	%rd12, %rd10, %rd11;
	ld.const.f64 	%fd25, [%rd12+8];
	mul.rn.f64 	%fd7, %fd40, %fd40;
	fma.rn.f64 	%fd26, %fd24, %fd7, %fd25;
	ld.const.f64 	%fd27, [%rd12+16];
	fma.rn.f64 	%fd28, %fd26, %fd7, %fd27;
	ld.const.f64 	%fd29, [%rd12+24];
	fma.rn.f64 	%fd30, %fd28, %fd7, %fd29;
	ld.const.f64 	%fd31, [%rd12+32];
	fma.rn.f64 	%fd32, %fd30, %fd7, %fd31;
	ld.const.f64 	%fd33, [%rd12+40];
	fma.rn.f64 	%fd34, %fd32, %fd7, %fd33;
	ld.const.f64 	%fd35, [%rd12+48];
	fma.rn.f64 	%fd8, %fd34, %fd7, %fd35;
	fma.rn.f64 	%fd41, %fd8, %fd40, %fd40;
	@%p4 bra 	BB68_7;

	mov.f64 	%fd36, 0d3FF0000000000000;
	fma.rn.f64 	%fd41, %fd8, %fd7, %fd36;

BB68_7:
	and.b32  	%r19, %r20, 2;
	setp.eq.s32	%p5, %r19, 0;
	@%p5 bra 	BB68_9;

	mov.f64 	%fd37, 0d0000000000000000;
	mov.f64 	%fd38, 0dBFF0000000000000;
	fma.rn.f64 	%fd41, %fd41, %fd38, %fd37;

BB68_9:
	cvta.to.global.u64 	%rd13, %rd3;
	shl.b64 	%rd14, %rd2, 3;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f64 	[%rd15], %fd41;

BB68_10:
	ret;
}

	// .globl	vec_sinh
.visible .entry vec_sinh(
	.param .u32 vec_sinh_param_0,
	.param .u64 vec_sinh_param_1,
	.param .u64 vec_sinh_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<68>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r3, [vec_sinh_param_0];
	ld.param.u64 	%rd2, [vec_sinh_param_1];
	ld.param.u64 	%rd3, [vec_sinh_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB69_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd5, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd5;
	}
	and.b32  	%r13, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd5;
	}
	mov.b64 	%fd1, {%r14, %r13};
	setp.lt.u32	%p2, %r13, 1072693248;
	@%p2 bra 	BB69_3;
	bra.uni 	BB69_2;

BB69_3:
	mul.f64 	%fd51, %fd1, %fd1;
	mov.f64 	%fd52, 0d3DE611A561D87DEF;
	mov.f64 	%fd53, 0d3D6B4C75AB274C53;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3E5AE64671B18F5C;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0d3EC71DE3A465B1E4;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F2A01A01A02899D;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0d3F811111111110A6;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3FC5555555555556;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mul.f64 	%fd65, %fd51, %fd64;
	fma.rn.f64 	%fd67, %fd65, %fd1, %fd1;
	bra.uni 	BB69_4;

BB69_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd1;
	}
	mov.f64 	%fd6, 0d4338000000000000;
	mov.f64 	%fd7, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd8;
	}
	add.s32 	%r17, %r16, -1;
	mov.f64 	%fd9, 0dC338000000000000;
	add.rn.f64 	%fd10, %fd8, %fd9;
	mov.f64 	%fd11, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd12, %fd10, %fd11, %fd1;
	mov.f64 	%fd13, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd14, %fd10, %fd13, %fd12;
	add.s32 	%r18, %r15, %r15;
	setp.lt.u32	%p3, %r18, 2142496327;
	selp.b32	%r19, 0, %r17, %p3;
	selp.f64	%fd15, %fd1, %fd14, %p3;
	mov.f64 	%fd16, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd17, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd18, %fd17, %fd15, %fd16;
	mov.f64 	%fd19, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd20, %fd18, %fd15, %fd19;
	mov.f64 	%fd21, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd22, %fd20, %fd15, %fd21;
	mov.f64 	%fd23, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd24, %fd22, %fd15, %fd23;
	mov.f64 	%fd25, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd26, %fd24, %fd15, %fd25;
	mov.f64 	%fd27, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd28, %fd26, %fd15, %fd27;
	mov.f64 	%fd29, 0d3F8111111110F74D;
	fma.rn.f64 	%fd30, %fd28, %fd15, %fd29;
	mov.f64 	%fd31, 0d3FA555555555554D;
	fma.rn.f64 	%fd32, %fd30, %fd15, %fd31;
	mov.f64 	%fd33, 0d3FC5555555555557;
	fma.rn.f64 	%fd34, %fd32, %fd15, %fd33;
	mov.f64 	%fd35, 0d3FE0000000000000;
	fma.rn.f64 	%fd36, %fd34, %fd15, %fd35;
	mul.f64 	%fd37, %fd15, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd15, %fd15;
	setp.eq.s32	%p4, %r19, 1024;
	selp.b32	%r20, -1, 0, %p4;
	add.s32 	%r21, %r20, %r19;
	shl.b32 	%r22, %r21, 20;
	add.s32 	%r23, %r22, 1072693248;
	mov.u32 	%r24, 0;
	mov.b64 	%fd39, {%r24, %r23};
	mov.u32 	%r25, 1071644672;
	mov.b64 	%fd40, {%r24, %r25};
	sub.f64 	%fd41, %fd39, %fd40;
	fma.rn.f64 	%fd42, %fd38, %fd39, %fd41;
	add.f64 	%fd43, %fd42, %fd42;
	selp.f64	%fd44, %fd43, %fd42, %p4;
	setp.eq.s32	%p5, %r18, 0;
	selp.f64	%fd45, %fd15, %fd44, %p5;
	mov.f64 	%fd46, 0d3FF0000000000000;
	mov.f64 	%fd47, 0d4000000000000000;
	fma.rn.f64 	%fd48, %fd47, %fd45, %fd46;
	div.rn.f64 	%fd49, %fd45, %fd48;
	add.f64 	%fd50, %fd49, %fd45;
	setp.ltu.f64	%p6, %fd1, 0d408633CE8FB9F87E;
	selp.f64	%fd67, %fd50, 0d7FF0000000000000, %p6;

BB69_4:
	cvta.to.global.u64 	%rd7, %rd2;
	and.b32  	%r26, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd67;
	}
	or.b32  	%r28, %r27, %r26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd67;
	}
	mov.b64 	%fd66, {%r29, %r28};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd66;

BB69_5:
	ret;
}

	// .globl	vec_sinpi
.visible .entry vec_sinpi(
	.param .u32 vec_sinpi_param_0,
	.param .u64 vec_sinpi_param_1,
	.param .u64 vec_sinpi_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [vec_sinpi_param_0];
	ld.param.u64 	%rd2, [vec_sinpi_param_1];
	ld.param.u64 	%rd3, [vec_sinpi_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB70_8;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd1;
	}
	add.s32 	%r15, %r14, 1048576;
	mov.b64 	%fd11, {%r13, %r15};
	cvt.rni.f64.f64	%fd12, %fd11;
	cvt.rzi.s64.f64	%rd7, %fd12;
	cvt.u32.u64	%r2, %rd7;
	neg.f64 	%fd13, %fd12;
	mov.f64 	%fd14, 0d3FE0000000000000;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd1;
	mul.f64 	%fd16, %fd15, 0d3CA1A62633145C07;
	mov.f64 	%fd17, 0d400921FB54442D18;
	fma.rn.f64 	%fd18, %fd15, %fd17, %fd16;
	and.b64  	%rd8, %rd7, 1;
	mul.rn.f64 	%fd2, %fd18, %fd18;
	setp.eq.b64	%p2, %rd8, 1;
	not.pred 	%p3, %p2;
	selp.f64	%fd19, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p3;
	shl.b64 	%rd9, %rd8, 6;
	mov.u64 	%rd10, __cudart_sin_cos_coeffs;
	add.s64 	%rd11, %rd9, %rd10;
	ld.const.f64 	%fd20, [%rd11+8];
	fma.rn.f64 	%fd21, %fd19, %fd2, %fd20;
	ld.const.f64 	%fd22, [%rd11+16];
	fma.rn.f64 	%fd23, %fd21, %fd2, %fd22;
	ld.const.f64 	%fd24, [%rd11+24];
	fma.rn.f64 	%fd25, %fd23, %fd2, %fd24;
	ld.const.f64 	%fd26, [%rd11+32];
	fma.rn.f64 	%fd27, %fd25, %fd2, %fd26;
	ld.const.f64 	%fd28, [%rd11+40];
	fma.rn.f64 	%fd29, %fd27, %fd2, %fd28;
	ld.const.f64 	%fd30, [%rd11+48];
	fma.rn.f64 	%fd3, %fd29, %fd2, %fd30;
	fma.rn.f64 	%fd36, %fd3, %fd18, %fd18;
	@%p3 bra 	BB70_3;

	mov.f64 	%fd31, 0d3FF0000000000000;
	fma.rn.f64 	%fd36, %fd3, %fd2, %fd31;

BB70_3:
	and.b32  	%r16, %r2, 2;
	setp.eq.s32	%p4, %r16, 0;
	@%p4 bra 	BB70_5;

	mov.f64 	%fd32, 0d0000000000000000;
	mov.f64 	%fd33, 0dBFF0000000000000;
	fma.rn.f64 	%fd36, %fd36, %fd33, %fd32;

BB70_5:
	cvt.rzi.f64.f64	%fd34, %fd1;
	setp.neu.f64	%p5, %fd34, %fd1;
	@%p5 bra 	BB70_7;

	mov.f64 	%fd35, 0d0000000000000000;
	mul.rn.f64 	%fd36, %fd1, %fd35;

BB70_7:
	cvta.to.global.u64 	%rd12, %rd2;
	shl.b64 	%rd13, %rd1, 3;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd36;

BB70_8:
	ret;
}

	// .globl	vec_sqrt
.visible .entry vec_sqrt(
	.param .u32 vec_sqrt_param_0,
	.param .u64 vec_sqrt_param_1,
	.param .u64 vec_sqrt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_sqrt_param_0];
	ld.param.u64 	%rd1, [vec_sqrt_param_1];
	ld.param.u64 	%rd2, [vec_sqrt_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB71_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	sqrt.rn.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB71_2:
	ret;
}

	// .globl	vec_tan
.visible .entry vec_tan(
	.param .u32 vec_tan_param_0,
	.param .u64 vec_tan_param_1,
	.param .u64 vec_tan_param_2
)
{
	.local .align 4 .b8 	__local_depot72[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<5>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<67>;
	.reg .b64 	%rd<14>;


	mov.u64 	%rd13, __local_depot72;
	cvta.local.u64 	%SP, %rd13;
	ld.param.u32 	%r5, [vec_tan_param_0];
	ld.param.u64 	%rd3, [vec_tan_param_1];
	ld.param.u64 	%rd4, [vec_tan_param_2];
	add.u64 	%rd5, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd5;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r14;
	setp.ge.s32	%p1, %r1, %r5;
	@%p1 bra 	BB72_8;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd2, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd64, [%rd8];
	abs.f64 	%fd11, %fd64;
	setp.neu.f64	%p2, %fd11, 0d7FF0000000000000;
	@%p2 bra 	BB72_3;

	mov.f64 	%fd12, 0d0000000000000000;
	mul.rn.f64 	%fd64, %fd64, %fd12;

BB72_3:
	mul.f64 	%fd13, %fd64, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r18, %fd13;
	st.local.u32 	[%rd1], %r18;
	cvt.rn.f64.s32	%fd14, %r18;
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd17, %fd15, %fd16, %fd64;
	mov.f64 	%fd18, 0d3C91A62633145C00;
	fma.rn.f64 	%fd19, %fd15, %fd18, %fd17;
	mov.f64 	%fd20, 0d397B839A252049C0;
	fma.rn.f64 	%fd65, %fd15, %fd20, %fd19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd64;
	}
	and.b32  	%r16, %r15, 2145386496;
	setp.lt.u32	%p3, %r16, 1105199104;
	@%p3 bra 	BB72_5;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd64;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd65, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r18, [%rd1];

BB72_5:
	mul.f64 	%fd21, %fd65, %fd65;
	mov.f64 	%fd22, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd23, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd24, %fd23, %fd21, %fd22;
	mov.f64 	%fd25, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd26, %fd24, %fd21, %fd25;
	mov.f64 	%fd27, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd28, %fd26, %fd21, %fd27;
	mov.f64 	%fd29, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd30, %fd28, %fd21, %fd29;
	mov.f64 	%fd31, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd32, %fd30, %fd21, %fd31;
	mov.f64 	%fd33, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd34, %fd32, %fd21, %fd33;
	mov.f64 	%fd35, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd36, %fd34, %fd21, %fd35;
	mov.f64 	%fd37, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd38, %fd36, %fd21, %fd37;
	mov.f64 	%fd39, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd40, %fd38, %fd21, %fd39;
	mov.f64 	%fd41, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd42, %fd40, %fd21, %fd41;
	mov.f64 	%fd43, 0d3F9664F485D25660;
	fma.rn.f64 	%fd44, %fd42, %fd21, %fd43;
	mov.f64 	%fd45, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd46, %fd44, %fd21, %fd45;
	mov.f64 	%fd47, 0d3FC11111111105D2;
	fma.rn.f64 	%fd48, %fd46, %fd21, %fd47;
	mov.f64 	%fd49, 0d3FD555555555555E;
	fma.rn.f64 	%fd50, %fd48, %fd21, %fd49;
	mul.f64 	%fd7, %fd21, %fd50;
	fma.rn.f64 	%fd66, %fd7, %fd65, %fd65;
	and.b32  	%r17, %r18, 1;
	setp.eq.b32	%p4, %r17, 1;
	@!%p4 bra 	BB72_7;
	bra.uni 	BB72_6;

BB72_6:
	sub.f64 	%fd53, %fd66, %fd65;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd7, %fd65, %fd54;
	// inline asm
	rcp.approx.ftz.f64 %fd51,%fd66;
	// inline asm
	neg.f64 	%fd56, %fd66;
	mov.f64 	%fd57, 0d3FF0000000000000;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	fma.rn.f64 	%fd59, %fd58, %fd58, %fd58;
	fma.rn.f64 	%fd60, %fd59, %fd51, %fd51;
	neg.f64 	%fd61, %fd60;
	fma.rn.f64 	%fd62, %fd66, %fd61, %fd57;
	fma.rn.f64 	%fd63, %fd61, %fd55, %fd62;
	fma.rn.f64 	%fd66, %fd63, %fd61, %fd61;

BB72_7:
	cvta.to.global.u64 	%rd10, %rd3;
	shl.b64 	%rd11, %rd2, 3;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd66;

BB72_8:
	ret;
}

	// .globl	vec_tanh
.visible .entry vec_tanh(
	.param .u32 vec_tanh_param_0,
	.param .u64 vec_tanh_param_1,
	.param .u64 vec_tanh_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<74>;
	.reg .b64 	%rd<10>;


	ld.param.u32 	%r4, [vec_tanh_param_0];
	ld.param.u64 	%rd2, [vec_tanh_param_1];
	ld.param.u64 	%rd3, [vec_tanh_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB73_5;

	cvta.to.global.u64 	%rd4, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd1;
	}
	mov.b64 	%fd2, {%r14, %r3};
	setp.ltu.f64	%p2, %fd2, 0d3FE1C7A398201CD6;
	@%p2 bra 	BB73_3;
	bra.uni 	BB73_2;

BB73_3:
	mul.f64 	%fd51, %fd1, %fd1;
	mov.f64 	%fd52, 0dBF2B9093D89F0E23;
	mov.f64 	%fd53, 0d3F0ABFFC9B5786C4;
	fma.rn.f64 	%fd54, %fd53, %fd51, %fd52;
	mov.f64 	%fd55, 0d3F42FA2744C30B61;
	fma.rn.f64 	%fd56, %fd54, %fd51, %fd55;
	mov.f64 	%fd57, 0dBF57CF3B9C1E491D;
	fma.rn.f64 	%fd58, %fd56, %fd51, %fd57;
	mov.f64 	%fd59, 0d3F6D6C61D450119A;
	fma.rn.f64 	%fd60, %fd58, %fd51, %fd59;
	mov.f64 	%fd61, 0dBF8226DDD44294F5;
	fma.rn.f64 	%fd62, %fd60, %fd51, %fd61;
	mov.f64 	%fd63, 0d3F9664F45C2B04A6;
	fma.rn.f64 	%fd64, %fd62, %fd51, %fd63;
	mov.f64 	%fd65, 0dBFABA1BA1AD70754;
	fma.rn.f64 	%fd66, %fd64, %fd51, %fd65;
	mov.f64 	%fd67, 0d3FC111111110295E;
	fma.rn.f64 	%fd68, %fd66, %fd51, %fd67;
	mov.f64 	%fd69, 0dBFD555555555549F;
	fma.rn.f64 	%fd70, %fd68, %fd51, %fd69;
	mul.f64 	%fd71, %fd51, %fd70;
	fma.rn.f64 	%fd73, %fd71, %fd1, %fd1;
	bra.uni 	BB73_4;

BB73_2:
	add.f64 	%fd8, %fd2, %fd2;
	mov.f64 	%fd9, 0d4338000000000000;
	mov.f64 	%fd10, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd11, %fd8, %fd10, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd11;
	}
	mov.f64 	%fd12, 0dC338000000000000;
	add.rn.f64 	%fd13, %fd11, %fd12;
	mov.f64 	%fd14, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd15, %fd13, %fd14, %fd8;
	mov.f64 	%fd16, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd17, %fd13, %fd16, %fd15;
	mov.f64 	%fd18, 0d3E5AF86D8EBD13CD;
	mov.f64 	%fd19, 0d3E21F4076ACD15B6;
	fma.rn.f64 	%fd20, %fd19, %fd17, %fd18;
	mov.f64 	%fd21, 0d3E927E5092BA033D;
	fma.rn.f64 	%fd22, %fd20, %fd17, %fd21;
	mov.f64 	%fd23, 0d3EC71DDE6C5F9DA1;
	fma.rn.f64 	%fd24, %fd22, %fd17, %fd23;
	mov.f64 	%fd25, 0d3EFA01A018D034E6;
	fma.rn.f64 	%fd26, %fd24, %fd17, %fd25;
	mov.f64 	%fd27, 0d3F2A01A01B3B6940;
	fma.rn.f64 	%fd28, %fd26, %fd17, %fd27;
	mov.f64 	%fd29, 0d3F56C16C16C1B5DD;
	fma.rn.f64 	%fd30, %fd28, %fd17, %fd29;
	mov.f64 	%fd31, 0d3F8111111110F74D;
	fma.rn.f64 	%fd32, %fd30, %fd17, %fd31;
	mov.f64 	%fd33, 0d3FA555555555554D;
	fma.rn.f64 	%fd34, %fd32, %fd17, %fd33;
	mov.f64 	%fd35, 0d3FC5555555555557;
	fma.rn.f64 	%fd36, %fd34, %fd17, %fd35;
	mov.f64 	%fd37, 0d3FE0000000000000;
	fma.rn.f64 	%fd38, %fd36, %fd17, %fd37;
	mul.f64 	%fd39, %fd17, %fd38;
	fma.rn.f64 	%fd40, %fd39, %fd17, %fd17;
	shl.b32 	%r16, %r15, 20;
	add.s32 	%r17, %r16, 1072693248;
	mov.u32 	%r18, 0;
	mov.b64 	%fd41, {%r18, %r17};
	fma.rn.f64 	%fd42, %fd40, %fd41, %fd41;
	add.f64 	%fd7, %fd42, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd6,%fd7;
	// inline asm
	neg.f64 	%fd43, %fd7;
	mov.f64 	%fd44, 0d3FF0000000000000;
	fma.rn.f64 	%fd45, %fd43, %fd6, %fd44;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd45;
	fma.rn.f64 	%fd47, %fd46, %fd6, %fd6;
	neg.f64 	%fd48, %fd47;
	mov.f64 	%fd49, 0d4000000000000000;
	fma.rn.f64 	%fd50, %fd49, %fd48, %fd44;
	setp.gt.u32	%p3, %r3, 1077936127;
	selp.f64	%fd73, 0d3FF0000000000000, %fd50, %p3;

BB73_4:
	cvta.to.global.u64 	%rd7, %rd2;
	and.b32  	%r19, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd73;
	}
	or.b32  	%r21, %r20, %r19;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd73;
	}
	mov.b64 	%fd72, {%r22, %r21};
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f64 	[%rd9], %fd72;

BB73_5:
	ret;
}

	// .globl	vec_tgamma
.visible .entry vec_tgamma(
	.param .u32 vec_tgamma_param_0,
	.param .u64 vec_tgamma_param_1,
	.param .u64 vec_tgamma_param_2
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<97>;
	.reg .f64 	%fd<427>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r23, [vec_tgamma_param_0];
	ld.param.u64 	%rd3, [vec_tgamma_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r28, %r25, %r26, %r27;
	mov.u32 	%r29, %nctaid.x;
	mov.u32 	%r30, %ctaid.x;
	mad.lo.s32 	%r31, %r28, %r29, %r30;
	mov.u32 	%r32, %ntid.x;
	mad.lo.s32 	%r33, %r31, %r32, %r24;
	setp.ge.s32	%p1, %r33, %r23;
	@%p1 bra 	BB74_44;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r33, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	setp.ltu.f64	%p2, %fd1, 0d0000000000000000;
	@%p2 bra 	BB74_18;
	bra.uni 	BB74_2;

BB74_18:
	setp.lt.f64	%p15, %fd1, 0d0000000000000000;
	@%p15 bra 	BB74_20;
	bra.uni 	BB74_19;

BB74_20:
	cvt.rzi.f64.f64	%fd266, %fd1;
	setp.eq.f64	%p16, %fd266, %fd1;
	mov.f64 	%fd426, 0dFFF8000000000000;
	@%p16 bra 	BB74_43;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd1;
	}
	setp.lt.u32	%p17, %r16, -1072693248;
	@%p17 bra 	BB74_34;
	bra.uni 	BB74_22;

BB74_34:
	setp.lt.u32	%p26, %r16, -1072955392;
	mov.f64 	%fd422, %fd1;
	mov.f64 	%fd421, %fd1;
	@%p26 bra 	BB74_36;

	fma.rn.f64 	%fd422, %fd1, %fd1, %fd1;
	add.f64 	%fd421, %fd1, 0d3FF0000000000000;

BB74_36:
	mov.f64 	%fd419, %fd421;
	mov.f64 	%fd420, %fd422;
	setp.lt.u32	%p27, %r16, -1073479680;
	@%p27 bra 	BB74_38;

	fma.rn.f64 	%fd420, %fd419, %fd420, %fd420;
	add.f64 	%fd419, %fd419, 0d3FF0000000000000;

BB74_38:
	mov.f64 	%fd417, %fd419;
	mov.f64 	%fd418, %fd420;
	setp.lt.u32	%p28, %r16, -1074266112;
	@%p28 bra 	BB74_40;

	fma.rn.f64 	%fd418, %fd417, %fd418, %fd418;
	add.f64 	%fd417, %fd417, 0d3FF0000000000000;

BB74_40:
	mov.f64 	%fd415, %fd417;
	mov.f64 	%fd416, %fd418;
	setp.lt.u32	%p29, %r16, -1075838976;
	@%p29 bra 	BB74_42;

	fma.rn.f64 	%fd416, %fd415, %fd416, %fd416;
	add.f64 	%fd415, %fd415, 0d3FF0000000000000;

BB74_42:
	mov.f64 	%fd367, 0dBE8AF7049AE8A594;
	mov.f64 	%fd368, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd369, %fd368, %fd415, %fd367;
	mov.f64 	%fd370, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd371, %fd369, %fd415, %fd370;
	mov.f64 	%fd372, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd373, %fd371, %fd415, %fd372;
	mov.f64 	%fd374, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd375, %fd373, %fd415, %fd374;
	mov.f64 	%fd376, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd377, %fd375, %fd415, %fd376;
	mov.f64 	%fd378, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd379, %fd377, %fd415, %fd378;
	mov.f64 	%fd380, 0dBF531711365A3E26;
	fma.rn.f64 	%fd381, %fd379, %fd415, %fd380;
	mov.f64 	%fd382, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd383, %fd381, %fd415, %fd382;
	mov.f64 	%fd384, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd385, %fd383, %fd415, %fd384;
	mov.f64 	%fd386, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd387, %fd385, %fd415, %fd386;
	mov.f64 	%fd388, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd389, %fd387, %fd415, %fd388;
	mov.f64 	%fd390, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd391, %fd389, %fd415, %fd390;
	mov.f64 	%fd392, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd393, %fd391, %fd415, %fd392;
	mov.f64 	%fd394, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd395, %fd393, %fd415, %fd394;
	mov.f64 	%fd396, 0d3FF0000000000000;
	fma.rn.f64 	%fd397, %fd395, %fd415, %fd396;
	mul.f64 	%fd398, %fd416, %fd397;
	rcp.rn.f64 	%fd426, %fd398;
	bra.uni 	BB74_43;

BB74_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd1;
	}
	setp.lt.s32	%p3, %r93, 1074790400;
	@%p3 bra 	BB74_11;
	bra.uni 	BB74_3;

BB74_11:
	mov.f64 	%fd401, 0d3FF0000000000000;
	setp.lt.s32	%p10, %r93, 1074528256;
	mov.f64 	%fd425, %fd1;
	@%p10 bra 	BB74_13;

	mov.f64 	%fd226, 0dBFF0000000000000;
	mov.f64 	%fd227, 0d3FF0000000000000;
	fma.rn.f64 	%fd401, %fd1, %fd227, %fd226;
	add.f64 	%fd13, %fd1, 0dBFF0000000000000;
	mov.f64 	%fd425, %fd13;

BB74_13:
	mov.f64 	%fd407, %fd425;
	mov.f64 	%fd424, %fd407;
	setp.lt.s32	%p11, %r93, 1074003968;
	@%p11 bra 	BB74_15;

	neg.f64 	%fd228, %fd401;
	fma.rn.f64 	%fd401, %fd424, %fd401, %fd228;
	add.f64 	%fd424, %fd424, 0dBFF0000000000000;

BB74_15:
	mov.f64 	%fd423, %fd424;
	setp.lt.s32	%p12, %r93, 1073217536;
	@%p12 bra 	BB74_17;

	neg.f64 	%fd229, %fd401;
	fma.rn.f64 	%fd401, %fd423, %fd401, %fd229;
	add.f64 	%fd423, %fd423, 0dBFF0000000000000;

BB74_17:
	add.f64 	%fd230, %fd423, 0dBFF0000000000000;
	setp.gt.s32	%p13, %r93, 1071644671;
	selp.f64	%fd231, %fd230, %fd423, %p13;
	mov.f64 	%fd232, 0dBE8AF7049AE8A594;
	mov.f64 	%fd233, 0d3E381B4960FCF5C9;
	fma.rn.f64 	%fd234, %fd233, %fd231, %fd232;
	mov.f64 	%fd235, 0d3EB301D46D4B22F5;
	fma.rn.f64 	%fd236, %fd234, %fd231, %fd235;
	mov.f64 	%fd237, 0dBEB50272AEED0FC4;
	fma.rn.f64 	%fd238, %fd236, %fd231, %fd237;
	mov.f64 	%fd239, 0dBEF51CE1A40516F8;
	fma.rn.f64 	%fd240, %fd238, %fd231, %fd239;
	mov.f64 	%fd241, 0d3F20C8AA7419084C;
	fma.rn.f64 	%fd242, %fd240, %fd231, %fd241;
	mov.f64 	%fd243, 0dBF2C3650196BAD8A;
	fma.rn.f64 	%fd244, %fd242, %fd231, %fd243;
	mov.f64 	%fd245, 0dBF531711365A3E26;
	fma.rn.f64 	%fd246, %fd244, %fd231, %fd245;
	mov.f64 	%fd247, 0d3F7D919C52A7DF35;
	fma.rn.f64 	%fd248, %fd246, %fd231, %fd247;
	mov.f64 	%fd249, 0dBF83B4AF28386F4D;
	fma.rn.f64 	%fd250, %fd248, %fd231, %fd249;
	mov.f64 	%fd251, 0dBFA59AF103C37B4D;
	fma.rn.f64 	%fd252, %fd250, %fd231, %fd251;
	mov.f64 	%fd253, 0d3FC5512320B439EF;
	fma.rn.f64 	%fd254, %fd252, %fd231, %fd253;
	mov.f64 	%fd255, 0dBFA5815E8FA26F4F;
	fma.rn.f64 	%fd256, %fd254, %fd231, %fd255;
	mov.f64 	%fd257, 0dBFE4FCF4026AFA2B;
	fma.rn.f64 	%fd258, %fd256, %fd231, %fd257;
	mov.f64 	%fd259, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd260, %fd258, %fd231, %fd259;
	mov.f64 	%fd261, 0d3FF0000000000000;
	fma.rn.f64 	%fd262, %fd260, %fd231, %fd261;
	mul.f64 	%fd263, %fd1, %fd262;
	setp.lt.s32	%p14, %r93, 1071644672;
	selp.f64	%fd264, %fd263, %fd262, %p14;
	div.rn.f64 	%fd426, %fd401, %fd264;
	bra.uni 	BB74_43;

BB74_19:
	add.f64 	%fd426, %fd1, %fd1;
	bra.uni 	BB74_43;

BB74_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd1;
	}
	shr.u32 	%r94, %r93, 20;
	setp.ne.s32	%p4, %r94, 0;
	@%p4 bra 	BB74_5;

	mul.f64 	%fd62, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd62;
	}
	shr.u32 	%r44, %r93, 20;
	add.s32 	%r94, %r44, -54;

BB74_5:
	add.s32 	%r95, %r94, -1023;
	and.b32  	%r45, %r93, -2146435073;
	or.b32  	%r46, %r45, 1072693248;
	mov.b64 	%fd399, {%r92, %r46};
	setp.lt.u32	%p5, %r46, 1073127583;
	@%p5 bra 	BB74_7;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd399;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd399;
	}
	add.s32 	%r49, %r48, -1048576;
	mov.b64 	%fd399, {%r47, %r49};
	add.s32 	%r95, %r94, -1022;

BB74_7:
	add.f64 	%fd64, %fd399, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd63,%fd64;
	// inline asm
	neg.f64 	%fd65, %fd64;
	mov.f64 	%fd66, 0d3FF0000000000000;
	fma.rn.f64 	%fd67, %fd65, %fd63, %fd66;
	fma.rn.f64 	%fd68, %fd67, %fd67, %fd67;
	fma.rn.f64 	%fd69, %fd68, %fd63, %fd63;
	add.f64 	%fd70, %fd399, 0dBFF0000000000000;
	mul.f64 	%fd71, %fd70, %fd69;
	fma.rn.f64 	%fd72, %fd70, %fd69, %fd71;
	mul.f64 	%fd73, %fd72, %fd72;
	mov.f64 	%fd74, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd75, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd76, %fd75, %fd73, %fd74;
	mov.f64 	%fd77, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd78, %fd76, %fd73, %fd77;
	mov.f64 	%fd79, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd80, %fd78, %fd73, %fd79;
	mov.f64 	%fd81, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd82, %fd80, %fd73, %fd81;
	mov.f64 	%fd83, 0d3F6249249242B910;
	fma.rn.f64 	%fd84, %fd82, %fd73, %fd83;
	mov.f64 	%fd85, 0d3F89999999999DFB;
	fma.rn.f64 	%fd86, %fd84, %fd73, %fd85;
	sub.f64 	%fd87, %fd70, %fd72;
	add.f64 	%fd88, %fd87, %fd87;
	neg.f64 	%fd89, %fd72;
	fma.rn.f64 	%fd90, %fd89, %fd70, %fd88;
	mul.f64 	%fd91, %fd69, %fd90;
	fma.rn.f64 	%fd92, %fd73, %fd86, 0d3FB5555555555555;
	mov.f64 	%fd93, 0d3FB5555555555555;
	sub.f64 	%fd94, %fd93, %fd92;
	fma.rn.f64 	%fd95, %fd73, %fd86, %fd94;
	add.f64 	%fd96, %fd95, 0d0000000000000000;
	add.f64 	%fd97, %fd96, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd98, %fd92, %fd97;
	sub.f64 	%fd99, %fd92, %fd98;
	add.f64 	%fd100, %fd97, %fd99;
	mul.rn.f64 	%fd101, %fd72, %fd72;
	neg.f64 	%fd102, %fd101;
	fma.rn.f64 	%fd103, %fd72, %fd72, %fd102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd91;
	}
	add.s32 	%r52, %r51, 1048576;
	mov.b64 	%fd104, {%r50, %r52};
	fma.rn.f64 	%fd105, %fd72, %fd104, %fd103;
	mul.rn.f64 	%fd106, %fd101, %fd72;
	neg.f64 	%fd107, %fd106;
	fma.rn.f64 	%fd108, %fd101, %fd72, %fd107;
	fma.rn.f64 	%fd109, %fd101, %fd91, %fd108;
	fma.rn.f64 	%fd110, %fd105, %fd72, %fd109;
	mul.rn.f64 	%fd111, %fd98, %fd106;
	neg.f64 	%fd112, %fd111;
	fma.rn.f64 	%fd113, %fd98, %fd106, %fd112;
	fma.rn.f64 	%fd114, %fd98, %fd110, %fd113;
	fma.rn.f64 	%fd115, %fd100, %fd106, %fd114;
	add.f64 	%fd116, %fd111, %fd115;
	sub.f64 	%fd117, %fd111, %fd116;
	add.f64 	%fd118, %fd115, %fd117;
	add.f64 	%fd119, %fd72, %fd116;
	sub.f64 	%fd120, %fd72, %fd119;
	add.f64 	%fd121, %fd116, %fd120;
	add.f64 	%fd122, %fd118, %fd121;
	add.f64 	%fd123, %fd91, %fd122;
	add.f64 	%fd124, %fd119, %fd123;
	sub.f64 	%fd125, %fd119, %fd124;
	add.f64 	%fd126, %fd123, %fd125;
	xor.b32  	%r53, %r95, -2147483648;
	mov.u32 	%r54, 1127219200;
	mov.b64 	%fd127, {%r53, %r54};
	mov.u32 	%r55, -2147483648;
	mov.b64 	%fd128, {%r55, %r54};
	sub.f64 	%fd129, %fd127, %fd128;
	mov.f64 	%fd130, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd131, %fd129, %fd130, %fd124;
	neg.f64 	%fd132, %fd129;
	fma.rn.f64 	%fd133, %fd132, %fd130, %fd131;
	sub.f64 	%fd134, %fd133, %fd124;
	sub.f64 	%fd135, %fd126, %fd134;
	mov.f64 	%fd136, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd137, %fd129, %fd136, %fd135;
	add.f64 	%fd138, %fd131, %fd137;
	sub.f64 	%fd139, %fd131, %fd138;
	add.f64 	%fd140, %fd137, %fd139;
	add.f64 	%fd141, %fd1, 0dBFE0000000000000;
	mul.rn.f64 	%fd142, %fd138, %fd141;
	neg.f64 	%fd143, %fd142;
	fma.rn.f64 	%fd144, %fd138, %fd141, %fd143;
	fma.rn.f64 	%fd145, %fd140, %fd141, %fd144;
	add.f64 	%fd146, %fd142, %fd145;
	sub.f64 	%fd147, %fd142, %fd146;
	add.f64 	%fd148, %fd145, %fd147;
	sub.f64 	%fd149, %fd146, %fd1;
	sub.f64 	%fd150, %fd146, %fd149;
	sub.f64 	%fd151, %fd150, %fd1;
	add.f64 	%fd152, %fd148, %fd151;
	add.f64 	%fd5, %fd149, %fd152;
	sub.f64 	%fd153, %fd149, %fd5;
	add.f64 	%fd6, %fd152, %fd153;
	mov.f64 	%fd154, 0d4338000000000000;
	mov.f64 	%fd155, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd156, %fd5, %fd155, %fd154;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd156;
	}
	mov.f64 	%fd157, 0dC338000000000000;
	add.rn.f64 	%fd158, %fd156, %fd157;
	mov.f64 	%fd159, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd160, %fd158, %fd159, %fd5;
	mov.f64 	%fd161, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd162, %fd158, %fd161, %fd160;
	mov.f64 	%fd163, 0d3E928AF3FCA213EA;
	mov.f64 	%fd164, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd165, %fd164, %fd162, %fd163;
	mov.f64 	%fd166, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd167, %fd165, %fd162, %fd166;
	mov.f64 	%fd168, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd169, %fd167, %fd162, %fd168;
	mov.f64 	%fd170, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd171, %fd169, %fd162, %fd170;
	mov.f64 	%fd172, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd173, %fd171, %fd162, %fd172;
	mov.f64 	%fd174, 0d3F81111111122322;
	fma.rn.f64 	%fd175, %fd173, %fd162, %fd174;
	mov.f64 	%fd176, 0d3FA55555555502A1;
	fma.rn.f64 	%fd177, %fd175, %fd162, %fd176;
	mov.f64 	%fd178, 0d3FC5555555555511;
	fma.rn.f64 	%fd179, %fd177, %fd162, %fd178;
	mov.f64 	%fd180, 0d3FE000000000000B;
	fma.rn.f64 	%fd181, %fd179, %fd162, %fd180;
	fma.rn.f64 	%fd182, %fd181, %fd162, %fd66;
	fma.rn.f64 	%fd183, %fd182, %fd162, %fd66;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd183;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd183;
	}
	shl.b32 	%r56, %r13, 20;
	add.s32 	%r57, %r15, %r56;
	mov.b64 	%fd400, {%r14, %r57};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd5;
	}
	mov.b32 	 %f2, %r58;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p6, %f1, 0f4086232B;
	@%p6 bra 	BB74_10;

	setp.lt.f64	%p7, %fd5, 0d0000000000000000;
	add.f64 	%fd184, %fd5, 0d7FF0000000000000;
	selp.f64	%fd400, 0d0000000000000000, %fd184, %p7;
	setp.geu.f32	%p8, %f1, 0f40874800;
	@%p8 bra 	BB74_10;

	shr.u32 	%r59, %r13, 31;
	add.s32 	%r60, %r13, %r59;
	shr.s32 	%r61, %r60, 1;
	shl.b32 	%r62, %r61, 20;
	add.s32 	%r63, %r62, %r15;
	mov.b64 	%fd185, {%r14, %r63};
	sub.s32 	%r64, %r13, %r61;
	shl.b32 	%r65, %r64, 20;
	add.s32 	%r66, %r65, 1072693248;
	mov.u32 	%r67, 0;
	mov.b64 	%fd186, {%r67, %r66};
	mul.f64 	%fd400, %fd185, %fd186;

BB74_10:
	fma.rn.f64 	%fd189, %fd6, %fd400, %fd400;
	mul.f64 	%fd190, %fd189, 0dBCAA6A0D6F814637;
	mov.f64 	%fd191, 0d40040D931FF62706;
	fma.rn.f64 	%fd192, %fd189, %fd191, %fd190;
	// inline asm
	rcp.approx.ftz.f64 %fd187,%fd1;
	// inline asm
	neg.f64 	%fd193, %fd1;
	fma.rn.f64 	%fd195, %fd193, %fd187, %fd66;
	fma.rn.f64 	%fd196, %fd195, %fd195, %fd195;
	fma.rn.f64 	%fd197, %fd196, %fd187, %fd187;
	mov.f64 	%fd198, 0d3F73C25DA81303D5;
	mov.f64 	%fd199, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd200, %fd199, %fd197, %fd198;
	mov.f64 	%fd201, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd202, %fd200, %fd197, %fd201;
	mov.f64 	%fd203, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd204, %fd202, %fd197, %fd203;
	mov.f64 	%fd205, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd206, %fd204, %fd197, %fd205;
	mov.f64 	%fd207, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd208, %fd206, %fd197, %fd207;
	mov.f64 	%fd209, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd210, %fd208, %fd197, %fd209;
	mov.f64 	%fd211, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd212, %fd210, %fd197, %fd211;
	mov.f64 	%fd213, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd214, %fd212, %fd197, %fd213;
	mov.f64 	%fd215, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd216, %fd214, %fd197, %fd215;
	mov.f64 	%fd217, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd218, %fd216, %fd197, %fd217;
	mov.f64 	%fd219, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd220, %fd218, %fd197, %fd219;
	mov.f64 	%fd221, 0d3FB5555555555556;
	fma.rn.f64 	%fd222, %fd220, %fd197, %fd221;
	mul.f64 	%fd223, %fd197, %fd222;
	fma.rn.f64 	%fd224, %fd223, %fd192, %fd192;
	setp.ltu.f64	%p9, %fd1, 0d406573FAE561F648;
	selp.f64	%fd426, %fd224, 0d7FF0000000000000, %p9;

BB74_43:
	mov.u32 	%r91, %tid.y;
	mov.u32 	%r90, %ctaid.y;
	mov.u32 	%r89, %ntid.y;
	mov.u32 	%r88, %ctaid.x;
	mov.u32 	%r87, %nctaid.x;
	mad.lo.s32 	%r86, %r89, %r90, %r91;
	mov.u32 	%r85, %tid.x;
	mov.u32 	%r84, %ntid.x;
	mad.lo.s32 	%r83, %r86, %r87, %r88;
	mad.lo.s32 	%r82, %r83, %r84, %r85;
	cvt.s64.s32	%rd16, %r82;
	ld.param.u64 	%rd15, [vec_tgamma_param_1];
	cvta.to.global.u64 	%rd12, %rd15;
	shl.b64 	%rd13, %rd16, 3;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd426;

BB74_44:
	ret;

BB74_22:
	setp.lt.u32	%p18, %r16, -1066983424;
	@%p18 bra 	BB74_24;
	bra.uni 	BB74_23;

BB74_24:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd1;
	}
	add.s32 	%r69, %r16, 1048576;
	mov.b64 	%fd271, {%r68, %r69};
	cvt.rni.f64.f64	%fd272, %fd271;
	cvt.rzi.s64.f64	%rd7, %fd272;
	cvt.u32.u64	%r17, %rd7;
	neg.f64 	%fd273, %fd272;
	mov.f64 	%fd274, 0d3FE0000000000000;
	fma.rn.f64 	%fd275, %fd273, %fd274, %fd1;
	mul.f64 	%fd276, %fd275, 0d3CA1A62633145C07;
	mov.f64 	%fd277, 0d400921FB54442D18;
	fma.rn.f64 	%fd278, %fd275, %fd277, %fd276;
	and.b64  	%rd8, %rd7, 1;
	mul.rn.f64 	%fd27, %fd278, %fd278;
	setp.eq.b64	%p20, %rd8, 1;
	not.pred 	%p21, %p20;
	selp.f64	%fd279, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p21;
	shl.b64 	%rd9, %rd8, 6;
	mov.u64 	%rd10, __cudart_sin_cos_coeffs;
	add.s64 	%rd11, %rd9, %rd10;
	ld.const.f64 	%fd280, [%rd11+8];
	fma.rn.f64 	%fd281, %fd279, %fd27, %fd280;
	ld.const.f64 	%fd282, [%rd11+16];
	fma.rn.f64 	%fd283, %fd281, %fd27, %fd282;
	ld.const.f64 	%fd284, [%rd11+24];
	fma.rn.f64 	%fd285, %fd283, %fd27, %fd284;
	ld.const.f64 	%fd286, [%rd11+32];
	fma.rn.f64 	%fd287, %fd285, %fd27, %fd286;
	ld.const.f64 	%fd288, [%rd11+40];
	fma.rn.f64 	%fd289, %fd287, %fd27, %fd288;
	ld.const.f64 	%fd290, [%rd11+48];
	fma.rn.f64 	%fd28, %fd289, %fd27, %fd290;
	fma.rn.f64 	%fd402, %fd28, %fd278, %fd278;
	@%p21 bra 	BB74_26;

	mov.f64 	%fd291, 0d3FF0000000000000;
	fma.rn.f64 	%fd402, %fd28, %fd27, %fd291;

BB74_26:
	and.b32  	%r70, %r17, 2;
	setp.eq.s32	%p22, %r70, 0;
	@%p22 bra 	BB74_28;

	mov.f64 	%fd292, 0d0000000000000000;
	mov.f64 	%fd293, 0dBFF0000000000000;
	fma.rn.f64 	%fd402, %fd402, %fd293, %fd292;

BB74_28:
	abs.f64 	%fd295, %fd1;
	// inline asm
	rcp.approx.ftz.f64 %fd294,%fd295;
	// inline asm
	neg.f64 	%fd296, %fd295;
	mov.f64 	%fd297, 0d3FF0000000000000;
	fma.rn.f64 	%fd298, %fd296, %fd294, %fd297;
	fma.rn.f64 	%fd299, %fd298, %fd298, %fd298;
	fma.rn.f64 	%fd300, %fd299, %fd294, %fd294;
	mov.f64 	%fd301, 0d3F73C25DA81303D5;
	mov.f64 	%fd302, 0dBF64BEE47C38A637;
	fma.rn.f64 	%fd303, %fd302, %fd300, %fd301;
	mov.f64 	%fd304, 0dBF6B7C37A96CFC72;
	fma.rn.f64 	%fd305, %fd303, %fd300, %fd304;
	mov.f64 	%fd306, 0d3F35A85ABDE1E324;
	fma.rn.f64 	%fd307, %fd305, %fd300, %fd306;
	mov.f64 	%fd308, 0d3F4A8B28F07B3F05;
	fma.rn.f64 	%fd309, %fd307, %fd300, %fd308;
	mov.f64 	%fd310, 0dBF0A15D1D45A282F;
	fma.rn.f64 	%fd311, %fd309, %fd300, %fd310;
	mov.f64 	%fd312, 0dBF4367D3468CB5BE;
	fma.rn.f64 	%fd313, %fd311, %fd300, %fd312;
	mov.f64 	%fd314, 0d3F12471B0E9F1005;
	fma.rn.f64 	%fd315, %fd313, %fd300, %fd314;
	mov.f64 	%fd316, 0d3F49B1004744D5C4;
	fma.rn.f64 	%fd317, %fd315, %fd300, %fd316;
	mov.f64 	%fd318, 0dBF2E13CE69AB4B7F;
	fma.rn.f64 	%fd319, %fd317, %fd300, %fd318;
	mov.f64 	%fd320, 0dBF65F7268ECF8A01;
	fma.rn.f64 	%fd321, %fd319, %fd300, %fd320;
	mov.f64 	%fd322, 0d3F6C71C71C71ACE0;
	fma.rn.f64 	%fd323, %fd321, %fd300, %fd322;
	mov.f64 	%fd324, 0d3FB5555555555556;
	fma.rn.f64 	%fd325, %fd323, %fd300, %fd324;
	mul.f64 	%fd326, %fd300, %fd325;
	fma.rn.f64 	%fd35, %fd326, %fd402, %fd402;
	mov.f64 	%fd327, 0d4338000000000000;
	mov.f64 	%fd328, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd329, %fd295, %fd328, %fd327;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd329;
	}
	mov.f64 	%fd330, 0dC338000000000000;
	add.rn.f64 	%fd331, %fd329, %fd330;
	mov.f64 	%fd332, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd333, %fd331, %fd332, %fd295;
	mov.f64 	%fd334, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd335, %fd331, %fd334, %fd333;
	mov.f64 	%fd336, 0d3E928AF3FCA213EA;
	mov.f64 	%fd337, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd338, %fd337, %fd335, %fd336;
	mov.f64 	%fd339, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd340, %fd338, %fd335, %fd339;
	mov.f64 	%fd341, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd342, %fd340, %fd335, %fd341;
	mov.f64 	%fd343, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd344, %fd342, %fd335, %fd343;
	mov.f64 	%fd345, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd346, %fd344, %fd335, %fd345;
	mov.f64 	%fd347, 0d3F81111111122322;
	fma.rn.f64 	%fd348, %fd346, %fd335, %fd347;
	mov.f64 	%fd349, 0d3FA55555555502A1;
	fma.rn.f64 	%fd350, %fd348, %fd335, %fd349;
	mov.f64 	%fd351, 0d3FC5555555555511;
	fma.rn.f64 	%fd352, %fd350, %fd335, %fd351;
	mov.f64 	%fd353, 0d3FE000000000000B;
	fma.rn.f64 	%fd354, %fd352, %fd335, %fd353;
	fma.rn.f64 	%fd355, %fd354, %fd335, %fd297;
	fma.rn.f64 	%fd403, %fd355, %fd335, %fd297;
	abs.s32 	%r71, %r18;
	setp.lt.s32	%p23, %r71, 1023;
	@%p23 bra 	BB74_30;
	bra.uni 	BB74_29;

BB74_30:
	shl.b32 	%r77, %r18, 20;
	add.s32 	%r96, %r77, 1072693248;
	bra.uni 	BB74_31;

BB74_23:
	cvt.rmi.f64.f64	%fd267, %fd1;
	mul.f64 	%fd268, %fd267, 0d3FE0000000000000;
	cvt.rmi.f64.f64	%fd269, %fd268;
	fma.rn.f64 	%fd270, %fd269, 0dC000000000000000, %fd267;
	setp.eq.f64	%p19, %fd270, 0d3FF0000000000000;
	selp.f64	%fd426, 0d8000000000000000, 0d0000000000000000, %p19;
	bra.uni 	BB74_43;

BB74_29:
	add.s32 	%r72, %r18, 2046;
	shl.b32 	%r73, %r72, 19;
	and.b32  	%r74, %r73, -1048576;
	shl.b32 	%r75, %r72, 20;
	sub.s32 	%r96, %r75, %r74;
	mov.u32 	%r76, 0;
	mov.b64 	%fd356, {%r76, %r74};
	mul.f64 	%fd403, %fd403, %fd356;

BB74_31:
	mul.f64 	%fd357, %fd295, %fd35;
	mov.u32 	%r78, 0;
	mov.b64 	%fd358, {%r78, %r96};
	mul.f64 	%fd359, %fd403, %fd358;
	mul.f64 	%fd360, %fd359, 0dBC9A6A0D6F814637;
	mov.f64 	%fd361, 0d3FF40D931FF62706;
	fma.rn.f64 	%fd362, %fd359, %fd361, %fd360;
	div.rn.f64 	%fd39, %fd362, %fd357;
	add.f64 	%fd404, %fd295, 0dBFE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd295;
	}
	setp.lt.s32	%p24, %r22, 1080157184;
	@%p24 bra 	BB74_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd404;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd404;
	}
	add.s32 	%r81, %r80, -1048576;
	mov.b64 	%fd404, {%r79, %r81};

BB74_33:
	neg.f64 	%fd363, %fd404;
	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd295;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd363;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd364, [retval0+0];
	
	//{
	}// Callseq End 21
	mul.f64 	%fd365, %fd39, %fd364;
	setp.gt.s32	%p25, %r22, 1080157183;
	selp.f64	%fd366, %fd365, %fd39, %p25;
	mul.f64 	%fd426, %fd364, %fd366;
	bra.uni 	BB74_43;
}

	// .globl	vec_trunc
.visible .entry vec_trunc(
	.param .u32 vec_trunc_param_0,
	.param .u64 vec_trunc_param_1,
	.param .u64 vec_trunc_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_trunc_param_0];
	ld.param.u64 	%rd1, [vec_trunc_param_1];
	ld.param.u64 	%rd2, [vec_trunc_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB75_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rzi.f64.f64	%fd2, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f64 	[%rd7], %fd2;

BB75_2:
	ret;
}

	// .globl	vec_y0
.visible .entry vec_y0(
	.param .u32 vec_y0_param_0,
	.param .u64 vec_y0_param_1,
	.param .u64 vec_y0_param_2
)
{
	.local .align 4 .b8 	__local_depot76[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<26>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<83>;
	.reg .f64 	%fd<545>;
	.reg .b64 	%rd<32>;


	mov.u64 	%rd31, __local_depot76;
	cvta.local.u64 	%SP, %rd31;
	ld.param.u32 	%r26, [vec_y0_param_0];
	ld.param.u64 	%rd1, [vec_y0_param_1];
	ld.param.u64 	%rd2, [vec_y0_param_2];
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r30, %r27, %r28, %r29;
	mov.u32 	%r31, %nctaid.x;
	mov.u32 	%r32, %ctaid.x;
	mad.lo.s32 	%r33, %r30, %r31, %r32;
	mov.u32 	%r34, %ntid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r1, %r33, %r34, %r35;
	setp.ge.s32	%p1, %r1, %r26;
	@%p1 bra 	BB76_47;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, 0d3FE97F4A8F9D3F28;
	@%p2 bra 	BB76_28;
	bra.uni 	BB76_2;

BB76_28:
	setp.gtu.f64	%p16, %fd2, 0d4000347C4AB37B18;
	@%p16 bra 	BB76_30;
	bra.uni 	BB76_29;

BB76_30:
	setp.gtu.f64	%p17, %fd2, 0d40161663B5D9A628;
	@%p17 bra 	BB76_32;
	bra.uni 	BB76_31;

BB76_32:
	setp.gtu.f64	%p18, %fd2, 0d40214EF30C0C06ED;
	@%p18 bra 	BB76_34;
	bra.uni 	BB76_33;

BB76_34:
	abs.f64 	%fd443, %fd2;
	mov.f64 	%fd544, 0d0000000000000000;
	setp.eq.f64	%p19, %fd443, 0d7FF0000000000000;
	@%p19 bra 	BB76_46;

	// inline asm
	rcp.approx.ftz.f64 %fd444,%fd2;
	// inline asm
	neg.f64 	%fd446, %fd2;
	mov.f64 	%fd447, 0d3FF0000000000000;
	fma.rn.f64 	%fd448, %fd446, %fd444, %fd447;
	fma.rn.f64 	%fd449, %fd448, %fd448, %fd448;
	fma.rn.f64 	%fd450, %fd449, %fd444, %fd444;
	mul.f64 	%fd451, %fd450, %fd450;
	mov.f64 	%fd452, 0d4093F56A049CDDE7;
	mov.f64 	%fd453, 0dC0C5E91E6AC3AD03;
	fma.rn.f64 	%fd454, %fd453, %fd451, %fd452;
	mov.f64 	%fd455, 0dC05572D39DFB8433;
	fma.rn.f64 	%fd456, %fd454, %fd451, %fd455;
	mov.f64 	%fd457, 0d4016A6041CAA59E5;
	fma.rn.f64 	%fd458, %fd456, %fd451, %fd457;
	mov.f64 	%fd459, 0dBFE155E3A0493880;
	fma.rn.f64 	%fd460, %fd458, %fd451, %fd459;
	mov.f64 	%fd461, 0d3FBA7FB92F417F7F;
	fma.rn.f64 	%fd462, %fd460, %fd451, %fd461;
	mov.f64 	%fd463, 0dBFAFFFFFB12E32F5;
	fma.rn.f64 	%fd464, %fd462, %fd451, %fd463;
	mov.f64 	%fd465, 0d3FEFFFFFFFFECED5;
	fma.rn.f64 	%fd466, %fd464, %fd451, %fd465;
	mov.f64 	%fd467, 0dC15709C79AAC5813;
	mov.f64 	%fd468, 0d418A86A64BE101DC;
	fma.rn.f64 	%fd469, %fd468, %fd451, %fd467;
	mov.f64 	%fd470, 0d41142A31C980A287;
	fma.rn.f64 	%fd471, %fd469, %fd451, %fd470;
	mov.f64 	%fd472, 0dC0C9CBE68930485D;
	fma.rn.f64 	%fd473, %fd471, %fd451, %fd472;
	mov.f64 	%fd474, 0d407F583E14E8A4E8;
	fma.rn.f64 	%fd475, %fd473, %fd451, %fd474;
	mov.f64 	%fd476, 0dC0374A629C650680;
	fma.rn.f64 	%fd477, %fd475, %fd451, %fd476;
	mov.f64 	%fd478, 0d3FFA32A7AF17FAE9;
	fma.rn.f64 	%fd479, %fd477, %fd451, %fd478;
	mov.f64 	%fd480, 0dBFCAD32497785CD6;
	fma.rn.f64 	%fd481, %fd479, %fd451, %fd480;
	mov.f64 	%fd482, 0d3FB0AAAA9FB75F7B;
	fma.rn.f64 	%fd483, %fd481, %fd451, %fd482;
	mov.f64 	%fd484, 0dBFBFFFFFFFFE320F;
	fma.rn.f64 	%fd485, %fd483, %fd451, %fd484;
	fma.rn.f64 	%fd40, %fd485, %fd450, %fd2;
	rsqrt.approx.f64 	%fd486, %fd2;
	mul.f64 	%fd487, %fd486, 0d3FE9884533D43651;
	mul.f64 	%fd41, %fd466, %fd487;
	mul.f64 	%fd488, %fd40, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r81, %fd488;
	add.u64 	%rd17, %SP, 0;
	cvta.to.local.u64 	%rd18, %rd17;
	st.local.u32 	[%rd18], %r81;
	cvt.rn.f64.s32	%fd489, %r81;
	neg.f64 	%fd490, %fd489;
	mov.f64 	%fd491, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd492, %fd490, %fd491, %fd40;
	mov.f64 	%fd493, 0d3C91A62633145C00;
	fma.rn.f64 	%fd494, %fd490, %fd493, %fd492;
	mov.f64 	%fd495, 0d397B839A252049C0;
	fma.rn.f64 	%fd540, %fd490, %fd495, %fd494;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd40;
	}
	and.b32  	%r58, %r57, 2145386496;
	setp.lt.u32	%p20, %r58, 1105199104;
	@%p20 bra 	BB76_37;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd40;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd540, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r81, [%rd18];

BB76_37:
	and.b32  	%r59, %r81, 3;
	cvt.rn.f64.s32	%fd496, %r59;
	add.f64 	%fd497, %fd540, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd541, %fd496, 0d3FF921FB54442D18, %fd497;
	abs.f64 	%fd498, %fd541;
	setp.neu.f64	%p21, %fd498, 0d7FF0000000000000;
	@%p21 bra 	BB76_39;

	mov.f64 	%fd499, 0d0000000000000000;
	mul.rn.f64 	%fd541, %fd541, %fd499;

BB76_39:
	mov.f64 	%fd530, 0d397B839A252049C0;
	mov.f64 	%fd529, 0d3C91A62633145C00;
	mov.f64 	%fd528, 0d3FF921FB54442D18;
	mul.f64 	%fd500, %fd541, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r82, %fd500;
	st.local.u32 	[%rd18], %r82;
	cvt.rn.f64.s32	%fd501, %r82;
	neg.f64 	%fd502, %fd501;
	fma.rn.f64 	%fd504, %fd502, %fd528, %fd541;
	fma.rn.f64 	%fd506, %fd502, %fd529, %fd504;
	fma.rn.f64 	%fd542, %fd502, %fd530, %fd506;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd541;
	}
	and.b32  	%r61, %r60, 2145386496;
	setp.lt.u32	%p22, %r61, 1105199104;
	@%p22 bra 	BB76_41;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd541;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd542, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r82, [%rd18];

BB76_41:
	add.s32 	%r25, %r82, 1;
	and.b32  	%r62, %r25, 1;
	shl.b32 	%r63, %r62, 3;
	setp.eq.s32	%p23, %r62, 0;
	selp.f64	%fd508, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p23;
	mul.wide.u32 	%rd25, %r63, 8;
	mov.u64 	%rd26, __cudart_sin_cos_coeffs;
	add.s64 	%rd27, %rd25, %rd26;
	ld.const.f64 	%fd509, [%rd27+8];
	mul.rn.f64 	%fd51, %fd542, %fd542;
	fma.rn.f64 	%fd510, %fd508, %fd51, %fd509;
	ld.const.f64 	%fd511, [%rd27+16];
	fma.rn.f64 	%fd512, %fd510, %fd51, %fd511;
	ld.const.f64 	%fd513, [%rd27+24];
	fma.rn.f64 	%fd514, %fd512, %fd51, %fd513;
	ld.const.f64 	%fd515, [%rd27+32];
	fma.rn.f64 	%fd516, %fd514, %fd51, %fd515;
	ld.const.f64 	%fd517, [%rd27+40];
	fma.rn.f64 	%fd518, %fd516, %fd51, %fd517;
	ld.const.f64 	%fd519, [%rd27+48];
	fma.rn.f64 	%fd52, %fd518, %fd51, %fd519;
	fma.rn.f64 	%fd543, %fd52, %fd542, %fd542;
	@%p23 bra 	BB76_43;

	mov.f64 	%fd531, 0d3FF0000000000000;
	fma.rn.f64 	%fd543, %fd52, %fd51, %fd531;

BB76_43:
	and.b32  	%r64, %r25, 2;
	setp.eq.s32	%p24, %r64, 0;
	@%p24 bra 	BB76_45;

	mov.f64 	%fd521, 0d0000000000000000;
	mov.f64 	%fd522, 0dBFF0000000000000;
	fma.rn.f64 	%fd543, %fd543, %fd522, %fd521;

BB76_45:
	mul.f64 	%fd544, %fd41, %fd543;
	bra.uni 	BB76_46;

BB76_2:
	mul.f64 	%fd60, %fd2, %fd2;
	mov.f64 	%fd61, 0dBD13098C51C18514;
	mov.f64 	%fd62, 0d3C8EFBD0A1B77C65;
	fma.rn.f64 	%fd63, %fd62, %fd60, %fd61;
	mov.f64 	%fd64, 0d3D923102D2F5F2F5;
	fma.rn.f64 	%fd65, %fd63, %fd60, %fd64;
	mov.f64 	%fd66, 0dBE0A5F2DEE7D526E;
	fma.rn.f64 	%fd67, %fd65, %fd60, %fd66;
	mov.f64 	%fd68, 0d3E7BB77E758B38AF;
	fma.rn.f64 	%fd69, %fd67, %fd60, %fd68;
	mov.f64 	%fd70, 0dBEE3D1A206EC4F36;
	fma.rn.f64 	%fd71, %fd69, %fd60, %fd70;
	mov.f64 	%fd72, 0d3F4183DCD3ED6294;
	fma.rn.f64 	%fd73, %fd71, %fd60, %fd72;
	mov.f64 	%fd74, 0dBF903921CF04F123;
	fma.rn.f64 	%fd75, %fd73, %fd60, %fd74;
	mov.f64 	%fd76, 0d3FC5DB69D7753176;
	fma.rn.f64 	%fd77, %fd75, %fd60, %fd76;
	add.f64 	%fd78, %fd60, 0dBFDBA96740000000;
	add.f64 	%fd79, %fd78, 0d3E15A30C80000000;
	mul.f64 	%fd3, %fd79, %fd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r76, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd2;
	}
	mov.u32 	%r77, -1023;
	setp.gt.s32	%p3, %r75, 1048575;
	mov.f64 	%fd532, %fd2;
	@%p3 bra 	BB76_4;

	mul.f64 	%fd4, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd4;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r76, %temp}, %fd4;
	}
	mov.u32 	%r77, -1077;
	mov.f64 	%fd532, %fd4;

BB76_4:
	mov.f64 	%fd5, %fd532;
	add.s32 	%r38, %r75, -1;
	setp.lt.u32	%p4, %r38, 2146435071;
	@%p4 bra 	BB76_6;
	bra.uni 	BB76_5;

BB76_6:
	shr.u32 	%r40, %r75, 20;
	add.s32 	%r78, %r77, %r40;
	and.b32  	%r41, %r75, -2146435073;
	or.b32  	%r42, %r41, 1072693248;
	mov.b64 	%fd533, {%r76, %r42};
	setp.lt.s32	%p6, %r42, 1073127583;
	@%p6 bra 	BB76_8;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd533;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd533;
	}
	add.s32 	%r45, %r44, -1048576;
	mov.b64 	%fd533, {%r43, %r45};
	add.s32 	%r78, %r78, 1;

BB76_8:
	add.f64 	%fd83, %fd533, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd82,%fd83;
	// inline asm
	neg.f64 	%fd84, %fd83;
	mov.f64 	%fd85, 0d3FF0000000000000;
	fma.rn.f64 	%fd86, %fd84, %fd82, %fd85;
	fma.rn.f64 	%fd87, %fd86, %fd86, %fd86;
	fma.rn.f64 	%fd88, %fd87, %fd82, %fd82;
	add.f64 	%fd89, %fd533, 0dBFF0000000000000;
	mul.f64 	%fd90, %fd89, %fd88;
	fma.rn.f64 	%fd91, %fd89, %fd88, %fd90;
	mul.f64 	%fd92, %fd91, %fd91;
	mov.f64 	%fd93, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd94, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd95, %fd94, %fd92, %fd93;
	mov.f64 	%fd96, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd97, %fd95, %fd92, %fd96;
	mov.f64 	%fd98, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd99, %fd97, %fd92, %fd98;
	mov.f64 	%fd100, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd101, %fd99, %fd92, %fd100;
	mov.f64 	%fd102, 0d3F624924923BE72D;
	fma.rn.f64 	%fd103, %fd101, %fd92, %fd102;
	mov.f64 	%fd104, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd105, %fd103, %fd92, %fd104;
	mov.f64 	%fd106, 0d3FB5555555555554;
	fma.rn.f64 	%fd107, %fd105, %fd92, %fd106;
	sub.f64 	%fd108, %fd89, %fd91;
	add.f64 	%fd109, %fd108, %fd108;
	neg.f64 	%fd110, %fd91;
	fma.rn.f64 	%fd111, %fd110, %fd89, %fd109;
	mul.f64 	%fd112, %fd88, %fd111;
	mul.f64 	%fd113, %fd92, %fd107;
	fma.rn.f64 	%fd114, %fd113, %fd91, %fd112;
	xor.b32  	%r46, %r78, -2147483648;
	mov.u32 	%r47, 1127219200;
	mov.b64 	%fd115, {%r46, %r47};
	mov.u32 	%r48, -2147483648;
	mov.b64 	%fd116, {%r48, %r47};
	sub.f64 	%fd117, %fd115, %fd116;
	mov.f64 	%fd118, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd119, %fd117, %fd118, %fd91;
	neg.f64 	%fd120, %fd117;
	fma.rn.f64 	%fd121, %fd120, %fd118, %fd119;
	sub.f64 	%fd122, %fd121, %fd91;
	sub.f64 	%fd123, %fd114, %fd122;
	mov.f64 	%fd124, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd125, %fd117, %fd124, %fd123;
	add.f64 	%fd534, %fd119, %fd125;
	bra.uni 	BB76_9;

BB76_29:
	add.f64 	%fd311, %fd2, 0dBFEC982EB8D417EA;
	add.f64 	%fd312, %fd311, 0dBC7EA9D270347F83;
	mov.f64 	%fd313, 0d3F3D054B05D3C52D;
	mov.f64 	%fd314, 0dBF01630132D75FC3;
	fma.rn.f64 	%fd315, %fd314, %fd312, %fd313;
	mov.f64 	%fd316, 0dBF66DAC0B314B2E5;
	fma.rn.f64 	%fd317, %fd315, %fd312, %fd316;
	mov.f64 	%fd318, 0d3F86A5D1DE76263F;
	fma.rn.f64 	%fd319, %fd317, %fd312, %fd318;
	mov.f64 	%fd320, 0dBF9FD16652824592;
	fma.rn.f64 	%fd321, %fd319, %fd312, %fd320;
	mov.f64 	%fd322, 0d3FB0F69A9CC79FBD;
	fma.rn.f64 	%fd323, %fd321, %fd312, %fd322;
	mov.f64 	%fd324, 0dBFBCCE40EF15583E;
	fma.rn.f64 	%fd325, %fd323, %fd312, %fd324;
	mov.f64 	%fd326, 0d3FC446B11780E4FC;
	fma.rn.f64 	%fd327, %fd325, %fd312, %fd326;
	mov.f64 	%fd328, 0dBFC89AE7E19621F7;
	fma.rn.f64 	%fd329, %fd327, %fd312, %fd328;
	mov.f64 	%fd330, 0d3FCACBA1B38EF7B8;
	fma.rn.f64 	%fd331, %fd329, %fd312, %fd330;
	mov.f64 	%fd332, 0dBFCB4166A03BBFA5;
	fma.rn.f64 	%fd333, %fd331, %fd312, %fd332;
	mov.f64 	%fd334, 0d3FCACCA4D5D4889A;
	fma.rn.f64 	%fd335, %fd333, %fd312, %fd334;
	mov.f64 	%fd336, 0dBFCA1455932B9392;
	fma.rn.f64 	%fd337, %fd335, %fd312, %fd336;
	mov.f64 	%fd338, 0d3FC96D8DB8D844EC;
	fma.rn.f64 	%fd339, %fd337, %fd312, %fd338;
	mov.f64 	%fd340, 0dBFC8F7FB77522EDF;
	fma.rn.f64 	%fd341, %fd339, %fd312, %fd340;
	mov.f64 	%fd342, 0d3FC8C0926ABC9AB0;
	fma.rn.f64 	%fd343, %fd341, %fd312, %fd342;
	mov.f64 	%fd344, 0dBFC8D35B8FEA468C;
	fma.rn.f64 	%fd345, %fd343, %fd312, %fd344;
	mov.f64 	%fd346, 0d3FC9424B8A0C8F94;
	fma.rn.f64 	%fd347, %fd345, %fd312, %fd346;
	mov.f64 	%fd348, 0dBFCA396A7F3403EF;
	fma.rn.f64 	%fd349, %fd347, %fd312, %fd348;
	mov.f64 	%fd350, 0d3FCC068086C37055;
	fma.rn.f64 	%fd351, %fd349, %fd312, %fd350;
	mov.f64 	%fd352, 0dBFCCF18E6A4C5C4E;
	fma.rn.f64 	%fd353, %fd351, %fd312, %fd352;
	mov.f64 	%fd354, 0d3FCC3B1338AF4239;
	fma.rn.f64 	%fd355, %fd353, %fd312, %fd354;
	mov.f64 	%fd356, 0dBFDF7E38A46D70DB;
	fma.rn.f64 	%fd357, %fd355, %fd312, %fd356;
	mov.f64 	%fd358, 0d3FEC24371844B88A;
	fma.rn.f64 	%fd359, %fd357, %fd312, %fd358;
	mul.f64 	%fd544, %fd312, %fd359;
	bra.uni 	BB76_46;

BB76_5:
	mov.f64 	%fd80, 0d7FF0000000000000;
	fma.rn.f64 	%fd81, %fd5, %fd80, %fd80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd5;
	}
	mov.b32 	 %f1, %r39;
	setp.eq.f32	%p5, %f1, 0f00000000;
	selp.f64	%fd534, 0dFFF0000000000000, %fd81, %p5;

BB76_9:
	abs.f64 	%fd12, %fd2;
	setp.gtu.f64	%p7, %fd12, 0d400FB319F277BBE5;
	@%p7 bra 	BB76_11;
	bra.uni 	BB76_10;

BB76_11:
	setp.gtu.f64	%p8, %fd12, 0d401C58FD1A62F5EC;
	@%p8 bra 	BB76_13;
	bra.uni 	BB76_12;

BB76_13:
	setp.gtu.f64	%p9, %fd12, 0d402471FCB6A7A8C0;
	@%p9 bra 	BB76_15;
	bra.uni 	BB76_14;

BB76_15:
	abs.f64 	%fd232, %fd12;
	mov.f64 	%fd539, 0d0000000000000000;
	setp.eq.f64	%p10, %fd232, 0d7FF0000000000000;
	@%p10 bra 	BB76_27;

	// inline asm
	rcp.approx.ftz.f64 %fd233,%fd12;
	// inline asm
	neg.f64 	%fd235, %fd12;
	mov.f64 	%fd236, 0d3FF0000000000000;
	fma.rn.f64 	%fd237, %fd235, %fd233, %fd236;
	fma.rn.f64 	%fd238, %fd237, %fd237, %fd237;
	fma.rn.f64 	%fd239, %fd238, %fd233, %fd233;
	mul.f64 	%fd240, %fd239, %fd239;
	mov.f64 	%fd241, 0d409927467A655012;
	mov.f64 	%fd242, 0dC0D115CB8C11A9DC;
	fma.rn.f64 	%fd243, %fd242, %fd240, %fd241;
	mov.f64 	%fd244, 0dC05751787E247BD4;
	fma.rn.f64 	%fd245, %fd243, %fd240, %fd244;
	mov.f64 	%fd246, 0d401704C4E5FC36B2;
	fma.rn.f64 	%fd247, %fd245, %fd240, %fd246;
	mov.f64 	%fd248, 0dBFE15B747A2FD531;
	fma.rn.f64 	%fd249, %fd247, %fd240, %fd248;
	mov.f64 	%fd250, 0d3FBA7FEACF6CB79B;
	fma.rn.f64 	%fd251, %fd249, %fd240, %fd250;
	mov.f64 	%fd252, 0dBFAFFFFFEDDCF548;
	fma.rn.f64 	%fd253, %fd251, %fd240, %fd252;
	mov.f64 	%fd254, 0d3FEFFFFFFFFFC9E5;
	fma.rn.f64 	%fd255, %fd253, %fd240, %fd254;
	mov.f64 	%fd256, 0d410ECD4523B12B84;
	mov.f64 	%fd257, 0dC14602FE1C34685E;
	fma.rn.f64 	%fd258, %fd257, %fd240, %fd256;
	mov.f64 	%fd259, 0dC0C7A2FC1972F05A;
	fma.rn.f64 	%fd260, %fd258, %fd240, %fd259;
	mov.f64 	%fd261, 0d407EBA131F7E5BEB;
	fma.rn.f64 	%fd262, %fd260, %fd240, %fd261;
	mov.f64 	%fd263, 0dC0373B92E6E7CC7D;
	fma.rn.f64 	%fd264, %fd262, %fd240, %fd263;
	mov.f64 	%fd265, 0d3FFA31BEE63A2F08;
	fma.rn.f64 	%fd266, %fd264, %fd240, %fd265;
	mov.f64 	%fd267, 0dBFCAD320104D5D05;
	fma.rn.f64 	%fd268, %fd266, %fd240, %fd267;
	mov.f64 	%fd269, 0d3FB0AAAA9C76D07E;
	fma.rn.f64 	%fd270, %fd268, %fd240, %fd269;
	mov.f64 	%fd271, 0dBFBFFFFFFFFDACEC;
	fma.rn.f64 	%fd272, %fd270, %fd240, %fd271;
	fma.rn.f64 	%fd16, %fd272, %fd239, %fd12;
	rsqrt.approx.f64 	%fd273, %fd12;
	mul.f64 	%fd274, %fd273, 0d3FE9884533D43651;
	mul.f64 	%fd17, %fd255, %fd274;
	mul.f64 	%fd275, %fd16, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r79, %fd275;
	add.u64 	%rd6, %SP, 0;
	cvta.to.local.u64 	%rd7, %rd6;
	st.local.u32 	[%rd7], %r79;
	cvt.rn.f64.s32	%fd276, %r79;
	neg.f64 	%fd277, %fd276;
	mov.f64 	%fd278, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd279, %fd277, %fd278, %fd16;
	mov.f64 	%fd280, 0d3C91A62633145C00;
	fma.rn.f64 	%fd281, %fd277, %fd280, %fd279;
	mov.f64 	%fd282, 0d397B839A252049C0;
	fma.rn.f64 	%fd535, %fd277, %fd282, %fd281;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd16;
	}
	and.b32  	%r50, %r49, 2145386496;
	setp.lt.u32	%p11, %r50, 1105199104;
	@%p11 bra 	BB76_18;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd16;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd6;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd535, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r79, [%rd7];

BB76_18:
	and.b32  	%r51, %r79, 3;
	cvt.rn.f64.s32	%fd283, %r51;
	add.f64 	%fd284, %fd535, 0dBFE921FB54442D18;
	fma.rn.f64 	%fd536, %fd283, 0d3FF921FB54442D18, %fd284;
	abs.f64 	%fd285, %fd536;
	setp.neu.f64	%p12, %fd285, 0d7FF0000000000000;
	@%p12 bra 	BB76_20;

	mov.f64 	%fd286, 0d0000000000000000;
	mul.rn.f64 	%fd536, %fd536, %fd286;

BB76_20:
	mov.f64 	%fd526, 0d397B839A252049C0;
	mov.f64 	%fd525, 0d3C91A62633145C00;
	mov.f64 	%fd524, 0d3FF921FB54442D18;
	mul.f64 	%fd287, %fd536, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r80, %fd287;
	st.local.u32 	[%rd7], %r80;
	cvt.rn.f64.s32	%fd288, %r80;
	neg.f64 	%fd289, %fd288;
	fma.rn.f64 	%fd291, %fd289, %fd524, %fd536;
	fma.rn.f64 	%fd293, %fd289, %fd525, %fd291;
	fma.rn.f64 	%fd537, %fd289, %fd526, %fd293;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd536;
	}
	and.b32  	%r53, %r52, 2145386496;
	setp.lt.u32	%p13, %r53, 1105199104;
	@%p13 bra 	BB76_22;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd536;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd6;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd537, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r80, [%rd7];

BB76_22:
	add.s32 	%r18, %r80, 1;
	and.b32  	%r54, %r18, 1;
	shl.b32 	%r55, %r54, 3;
	setp.eq.s32	%p14, %r54, 0;
	selp.f64	%fd295, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p14;
	mul.wide.u32 	%rd14, %r55, 8;
	mov.u64 	%rd15, __cudart_sin_cos_coeffs;
	add.s64 	%rd16, %rd14, %rd15;
	ld.const.f64 	%fd296, [%rd16+8];
	mul.rn.f64 	%fd27, %fd537, %fd537;
	fma.rn.f64 	%fd297, %fd295, %fd27, %fd296;
	ld.const.f64 	%fd298, [%rd16+16];
	fma.rn.f64 	%fd299, %fd297, %fd27, %fd298;
	ld.const.f64 	%fd300, [%rd16+24];
	fma.rn.f64 	%fd301, %fd299, %fd27, %fd300;
	ld.const.f64 	%fd302, [%rd16+32];
	fma.rn.f64 	%fd303, %fd301, %fd27, %fd302;
	ld.const.f64 	%fd304, [%rd16+40];
	fma.rn.f64 	%fd305, %fd303, %fd27, %fd304;
	ld.const.f64 	%fd306, [%rd16+48];
	fma.rn.f64 	%fd28, %fd305, %fd27, %fd306;
	fma.rn.f64 	%fd538, %fd28, %fd537, %fd537;
	@%p14 bra 	BB76_24;

	mov.f64 	%fd527, 0d3FF0000000000000;
	fma.rn.f64 	%fd538, %fd28, %fd27, %fd527;

BB76_24:
	and.b32  	%r56, %r18, 2;
	setp.eq.s32	%p15, %r56, 0;
	@%p15 bra 	BB76_26;

	mov.f64 	%fd308, 0d0000000000000000;
	mov.f64 	%fd309, 0dBFF0000000000000;
	fma.rn.f64 	%fd538, %fd538, %fd309, %fd308;

BB76_26:
	mul.f64 	%fd539, %fd17, %fd538;
	bra.uni 	BB76_27;

BB76_10:
	add.f64 	%fd126, %fd12, 0dC0033D152E971B40;
	add.f64 	%fd127, %fd126, 0d3CA0F539D7DA258E;
	mov.f64 	%fd128, 0dBCFCF8F9A8C294BC;
	mov.f64 	%fd129, 0dBCC0D18564C48C61;
	fma.rn.f64 	%fd130, %fd129, %fd127, %fd128;
	mov.f64 	%fd131, 0d3D3FAB983CAE498B;
	fma.rn.f64 	%fd132, %fd130, %fd127, %fd131;
	mov.f64 	%fd133, 0d3D7CD7C018579B88;
	fma.rn.f64 	%fd134, %fd132, %fd127, %fd133;
	mov.f64 	%fd135, 0dBDBBDD2342D64FDD;
	fma.rn.f64 	%fd136, %fd134, %fd127, %fd135;
	mov.f64 	%fd137, 0dBDF5C2D9416B1E2B;
	fma.rn.f64 	%fd138, %fd136, %fd127, %fd137;
	mov.f64 	%fd139, 0d3E32951D73174DD5;
	fma.rn.f64 	%fd140, %fd138, %fd127, %fd139;
	mov.f64 	%fd141, 0d3E67FF99802CAEB5;
	fma.rn.f64 	%fd142, %fd140, %fd127, %fd141;
	mov.f64 	%fd143, 0dBEA1CCE305C4C9F7;
	fma.rn.f64 	%fd144, %fd142, %fd127, %fd143;
	mov.f64 	%fd145, 0dBED232C77E29E1BB;
	fma.rn.f64 	%fd146, %fd144, %fd127, %fd145;
	mov.f64 	%fd147, 0d3F06ED3B9F0EF757;
	fma.rn.f64 	%fd148, %fd146, %fd127, %fd147;
	mov.f64 	%fd149, 0d3F315382BA096A62;
	fma.rn.f64 	%fd150, %fd148, %fd127, %fd149;
	mov.f64 	%fd151, 0dBF61F992590D1AE4;
	fma.rn.f64 	%fd152, %fd150, %fd127, %fd151;
	mov.f64 	%fd153, 0dBF81BB1CBE1A465F;
	fma.rn.f64 	%fd154, %fd152, %fd127, %fd153;
	mov.f64 	%fd155, 0d3FACFAE864368D84;
	fma.rn.f64 	%fd156, %fd154, %fd127, %fd155;
	mov.f64 	%fd157, 0d3FBBA1DEEA0294A3;
	fma.rn.f64 	%fd158, %fd156, %fd127, %fd157;
	mov.f64 	%fd159, 0dBFE09CDB36551280;
	fma.rn.f64 	%fd160, %fd158, %fd127, %fd159;
	mul.f64 	%fd539, %fd127, %fd160;
	bra.uni 	BB76_27;

BB76_31:
	add.f64 	%fd360, %fd2, 0dC00FA9534D98569C;
	add.f64 	%fd361, %fd360, 0d3C9F06AE7804384E;
	mov.f64 	%fd362, 0dBCD2434958151AC7;
	mov.f64 	%fd363, 0dBCDAEA62AC8BDA68;
	fma.rn.f64 	%fd364, %fd363, %fd361, %fd362;
	mov.f64 	%fd365, 0d3D11C24A40D33FE1;
	fma.rn.f64 	%fd366, %fd364, %fd361, %fd365;
	mov.f64 	%fd367, 0d3D237CD62FA08CA4;
	fma.rn.f64 	%fd368, %fd366, %fd361, %fd367;
	mov.f64 	%fd369, 0dBD43902E0298C52A;
	fma.rn.f64 	%fd370, %fd368, %fd361, %fd369;
	mov.f64 	%fd371, 0dBD1DDAAD11CAB40F;
	fma.rn.f64 	%fd372, %fd370, %fd361, %fd371;
	mov.f64 	%fd373, 0dBD5209D9F06D7DE4;
	fma.rn.f64 	%fd374, %fd372, %fd361, %fd373;
	mov.f64 	%fd375, 0d3D8BB9F464468E1A;
	fma.rn.f64 	%fd376, %fd374, %fd361, %fd375;
	mov.f64 	%fd377, 0dBDA8F67B07D1B440;
	fma.rn.f64 	%fd378, %fd376, %fd361, %fd377;
	mov.f64 	%fd379, 0d3DC7C8D60F9EAECF;
	fma.rn.f64 	%fd380, %fd378, %fd361, %fd379;
	mov.f64 	%fd381, 0dBDE9703405B49A8D;
	fma.rn.f64 	%fd382, %fd380, %fd361, %fd381;
	mov.f64 	%fd383, 0d3E0A6B64E76417E4;
	fma.rn.f64 	%fd384, %fd382, %fd361, %fd383;
	mov.f64 	%fd385, 0dBE2F6B5AFB2F1359;
	fma.rn.f64 	%fd386, %fd384, %fd361, %fd385;
	mov.f64 	%fd387, 0d3E54526B71C21EC1;
	fma.rn.f64 	%fd388, %fd386, %fd361, %fd387;
	mov.f64 	%fd389, 0dBE5776DBCBBC8E1D;
	fma.rn.f64 	%fd390, %fd388, %fd361, %fd389;
	mov.f64 	%fd391, 0dBE93B211FC2DF90E;
	fma.rn.f64 	%fd392, %fd390, %fd361, %fd391;
	mov.f64 	%fd393, 0dBED486372E8562DC;
	fma.rn.f64 	%fd394, %fd392, %fd361, %fd393;
	mov.f64 	%fd395, 0d3F0AB2C1FBC3A254;
	fma.rn.f64 	%fd396, %fd394, %fd361, %fd395;
	mov.f64 	%fd397, 0d3F299827653353B8;
	fma.rn.f64 	%fd398, %fd396, %fd361, %fd397;
	mov.f64 	%fd399, 0dBF61E32BC4ED7084;
	fma.rn.f64 	%fd400, %fd398, %fd361, %fd399;
	mov.f64 	%fd401, 0dBF7C116FDC599A09;
	fma.rn.f64 	%fd402, %fd400, %fd361, %fd401;
	mov.f64 	%fd403, 0d3FADF6D59BF50C77;
	fma.rn.f64 	%fd404, %fd402, %fd361, %fd403;
	mov.f64 	%fd405, 0d3FAA09C92903680B;
	fma.rn.f64 	%fd406, %fd404, %fd361, %fd405;
	mov.f64 	%fd407, 0dBFD9C34256A12A0B;
	fma.rn.f64 	%fd408, %fd406, %fd361, %fd407;
	mul.f64 	%fd544, %fd361, %fd408;
	bra.uni 	BB76_46;

BB76_12:
	add.f64 	%fd161, %fd12, 0dC016148F5B2C2E45;
	add.f64 	%fd162, %fd161, 0dBC975054CD60A517;
	mov.f64 	%fd163, 0d3CF83FD1F333EB61;
	mov.f64 	%fd164, 0d3CBCB0A8F126B343;
	fma.rn.f64 	%fd165, %fd164, %fd162, %fd163;
	mov.f64 	%fd166, 0dBD4100E33E3FB413;
	fma.rn.f64 	%fd167, %fd165, %fd162, %fd166;
	mov.f64 	%fd168, 0dBD7846076D004627;
	fma.rn.f64 	%fd169, %fd167, %fd162, %fd168;
	mov.f64 	%fd170, 0d3DBE2F1D4F90720D;
	fma.rn.f64 	%fd171, %fd169, %fd162, %fd170;
	mov.f64 	%fd172, 0d3DF1D03B1E4A119B;
	fma.rn.f64 	%fd173, %fd171, %fd162, %fd172;
	mov.f64 	%fd174, 0dBE341D72B1B3BCE9;
	fma.rn.f64 	%fd175, %fd173, %fd162, %fd174;
	mov.f64 	%fd176, 0dBE62DA37CE2A9EF8;
	fma.rn.f64 	%fd177, %fd175, %fd162, %fd176;
	mov.f64 	%fd178, 0d3EA32E6D9974F763;
	fma.rn.f64 	%fd179, %fd177, %fd162, %fd178;
	mov.f64 	%fd180, 0d3ECAD77D744A1879;
	fma.rn.f64 	%fd181, %fd179, %fd162, %fd180;
	mov.f64 	%fd182, 0dBF0863F481A37337;
	fma.rn.f64 	%fd183, %fd181, %fd162, %fd182;
	mov.f64 	%fd184, 0dBF26F641F418F0F4;
	fma.rn.f64 	%fd185, %fd183, %fd162, %fd184;
	mov.f64 	%fd186, 0d3F627E31FE9A969E;
	fma.rn.f64 	%fd187, %fd185, %fd162, %fd186;
	mov.f64 	%fd188, 0d3F72F7FFE9025628;
	fma.rn.f64 	%fd189, %fd187, %fd162, %fd188;
	mov.f64 	%fd190, 0dBFAB2150CB41E8BF;
	fma.rn.f64 	%fd191, %fd189, %fd162, %fd190;
	mov.f64 	%fd192, 0dBF9F8F72E7A848DE;
	fma.rn.f64 	%fd193, %fd191, %fd162, %fd192;
	mov.f64 	%fd194, 0d3FD5C6E60A097823;
	fma.rn.f64 	%fd195, %fd193, %fd162, %fd194;
	mul.f64 	%fd539, %fd162, %fd195;
	bra.uni 	BB76_27;

BB76_33:
	add.f64 	%fd409, %fd2, 0dC01C581DC4E72103;
	add.f64 	%fd410, %fd409, 0d3C99774A495F56CF;
	mov.f64 	%fd411, 0dBD3F443BB4F53D75;
	mov.f64 	%fd412, 0d3CF1CB3ABA718B8E;
	fma.rn.f64 	%fd413, %fd412, %fd410, %fd411;
	mov.f64 	%fd414, 0dBD770F737BD6A786;
	fma.rn.f64 	%fd415, %fd413, %fd410, %fd414;
	mov.f64 	%fd416, 0d3DBF0E9A20459E14;
	fma.rn.f64 	%fd417, %fd415, %fd410, %fd416;
	mov.f64 	%fd418, 0d3DEFA6B137D5E108;
	fma.rn.f64 	%fd419, %fd417, %fd410, %fd418;
	mov.f64 	%fd420, 0dBE344296729FB7FA;
	fma.rn.f64 	%fd421, %fd419, %fd410, %fd420;
	mov.f64 	%fd422, 0dBE60A2813A80DFAA;
	fma.rn.f64 	%fd423, %fd421, %fd410, %fd422;
	mov.f64 	%fd424, 0d3EA34AA737A83EB4;
	fma.rn.f64 	%fd425, %fd423, %fd410, %fd424;
	mov.f64 	%fd426, 0d3EC6A9227332D03C;
	fma.rn.f64 	%fd427, %fd425, %fd410, %fd426;
	mov.f64 	%fd428, 0dBF08177E4F93C81E;
	fma.rn.f64 	%fd429, %fd427, %fd410, %fd428;
	mov.f64 	%fd430, 0dBF226DD71E391775;
	fma.rn.f64 	%fd431, %fd429, %fd410, %fd430;
	mov.f64 	%fd432, 0d3F61D35E85FD7B22;
	fma.rn.f64 	%fd433, %fd431, %fd410, %fd432;
	mov.f64 	%fd434, 0d3F6B2F14A955285C;
	fma.rn.f64 	%fd435, %fd433, %fd410, %fd434;
	mov.f64 	%fd436, 0dBFA8969C64CBF388;
	fma.rn.f64 	%fd437, %fd435, %fd410, %fd436;
	mov.f64 	%fd438, 0dBF95AEF611FC4D5A;
	fma.rn.f64 	%fd439, %fd437, %fd410, %fd438;
	mov.f64 	%fd440, 0d3FD334CCA0697A5A;
	fma.rn.f64 	%fd441, %fd439, %fd410, %fd440;
	mul.f64 	%fd544, %fd410, %fd441;
	bra.uni 	BB76_46;

BB76_14:
	add.f64 	%fd196, %fd12, 0dC0214EB56CCCDECA;
	add.f64 	%fd197, %fd196, 0d3CB51970714C7C25;
	mov.f64 	%fd198, 0dBCF4B3A71AAAC629;
	mov.f64 	%fd199, 0dBCBDB7FFCF659E24;
	fma.rn.f64 	%fd200, %fd199, %fd197, %fd198;
	mov.f64 	%fd201, 0d3D417EC150ECDCE7;
	fma.rn.f64 	%fd202, %fd200, %fd197, %fd201;
	mov.f64 	%fd203, 0d3D7438F5EA1D10B2;
	fma.rn.f64 	%fd204, %fd202, %fd197, %fd203;
	mov.f64 	%fd205, 0dBDBEDAE7EC2C9E87;
	fma.rn.f64 	%fd206, %fd204, %fd197, %fd205;
	mov.f64 	%fd207, 0dBDECADD2C4B91F58;
	fma.rn.f64 	%fd208, %fd206, %fd197, %fd207;
	mov.f64 	%fd209, 0d3E34582C8EE12204;
	fma.rn.f64 	%fd210, %fd208, %fd197, %fd209;
	mov.f64 	%fd211, 0d3E5CEDA451DD20F8;
	fma.rn.f64 	%fd212, %fd210, %fd197, %fd211;
	mov.f64 	%fd213, 0dBEA30E8CC3165E2F;
	fma.rn.f64 	%fd214, %fd212, %fd197, %fd213;
	mov.f64 	%fd215, 0dBEC3324842BB1A2E;
	fma.rn.f64 	%fd216, %fd214, %fd197, %fd215;
	mov.f64 	%fd217, 0d3F07800BC54FBDDB;
	fma.rn.f64 	%fd218, %fd216, %fd197, %fd217;
	mov.f64 	%fd219, 0d3F1D79605276949A;
	fma.rn.f64 	%fd220, %fd218, %fd197, %fd219;
	mov.f64 	%fd221, 0dBF60E0D60385A629;
	fma.rn.f64 	%fd222, %fd220, %fd197, %fd221;
	mov.f64 	%fd223, 0dBF648E63600D82F3;
	fma.rn.f64 	%fd224, %fd222, %fd197, %fd223;
	mov.f64 	%fd225, 0d3FA68B984EC6493A;
	fma.rn.f64 	%fd226, %fd224, %fd197, %fd225;
	mov.f64 	%fd227, 0d3F900F7FCF183E0B;
	fma.rn.f64 	%fd228, %fd226, %fd197, %fd227;
	mov.f64 	%fd229, 0dBFD15F7977A772D4;
	fma.rn.f64 	%fd230, %fd228, %fd197, %fd229;
	mul.f64 	%fd539, %fd197, %fd230;

BB76_27:
	mul.f64 	%fd310, %fd534, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd544, %fd310, %fd539, %fd3;

BB76_46:
	setp.lt.f64	%p25, %fd1, 0d0000000000000000;
	selp.f64	%fd523, 0dFFF8000000000000, %fd544, %p25;
	cvta.to.global.u64 	%rd28, %rd1;
	add.s64 	%rd30, %rd28, %rd4;
	st.global.f64 	[%rd30], %fd523;

BB76_47:
	ret;
}

	// .globl	vec_y1
.visible .entry vec_y1(
	.param .u32 vec_y1_param_0,
	.param .u64 vec_y1_param_1,
	.param .u64 vec_y1_param_2
)
{
	.local .align 4 .b8 	__local_depot77[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<93>;
	.reg .f64 	%fd<539>;
	.reg .b64 	%rd<34>;


	mov.u64 	%rd33, __local_depot77;
	cvta.local.u64 	%SP, %rd33;
	ld.param.u32 	%r25, [vec_y1_param_0];
	ld.param.u64 	%rd1, [vec_y1_param_1];
	ld.param.u64 	%rd2, [vec_y1_param_2];
	mov.u32 	%r26, %ntid.y;
	mov.u32 	%r27, %ctaid.y;
	mov.u32 	%r28, %tid.y;
	mad.lo.s32 	%r29, %r26, %r27, %r28;
	mov.u32 	%r30, %nctaid.x;
	mov.u32 	%r31, %ctaid.x;
	mad.lo.s32 	%r32, %r29, %r30, %r31;
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r35, %r32, %r33, %r34;
	setp.ge.s32	%p1, %r35, %r25;
	@%p1 bra 	BB77_51;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r35, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p2, %fd2, 0d000730D67819E8D2;
	@%p2 bra 	BB77_47;
	bra.uni 	BB77_2;

BB77_47:
	mov.f64 	%fd516, 0dBFE45F306DC9C883;
	div.rn.f64 	%fd538, %fd516, %fd2;
	bra.uni 	BB77_48;

BB77_2:
	setp.gtu.f64	%p3, %fd2, 0d3FF4C6F208132576;
	@%p3 bra 	BB77_29;
	bra.uni 	BB77_3;

BB77_29:
	setp.gtu.f64	%p19, %fd2, 0d4009B510EC2ADC83;
	@%p19 bra 	BB77_31;
	bra.uni 	BB77_30;

BB77_31:
	setp.gtu.f64	%p20, %fd2, 0d401C0D26D5A541CB;
	@%p20 bra 	BB77_33;
	bra.uni 	BB77_32;

BB77_33:
	setp.gtu.f64	%p21, %fd2, 0d4022585C739ACDDD;
	@%p21 bra 	BB77_35;
	bra.uni 	BB77_34;

BB77_35:
	abs.f64 	%fd436, %fd2;
	mov.f64 	%fd538, 0d0000000000000000;
	setp.eq.f64	%p22, %fd436, 0d7FF0000000000000;
	@%p22 bra 	BB77_48;

	// inline asm
	rcp.approx.ftz.f64 %fd437,%fd2;
	// inline asm
	neg.f64 	%fd439, %fd2;
	mov.f64 	%fd440, 0d3FF0000000000000;
	fma.rn.f64 	%fd441, %fd439, %fd437, %fd440;
	fma.rn.f64 	%fd442, %fd441, %fd441, %fd441;
	fma.rn.f64 	%fd443, %fd442, %fd437, %fd437;
	mul.f64 	%fd444, %fd443, %fd443;
	mov.f64 	%fd445, 0dC09C26E89385D5B1;
	mov.f64 	%fd446, 0d40D13DB326ECEBFE;
	fma.rn.f64 	%fd447, %fd446, %fd444, %fd445;
	mov.f64 	%fd448, 0d405C6AB923C6F55E;
	fma.rn.f64 	%fd449, %fd447, %fd444, %fd448;
	mov.f64 	%fd450, 0dC01E61EAF3BD2FA1;
	fma.rn.f64 	%fd451, %fd449, %fd444, %fd450;
	mov.f64 	%fd452, 0d3FE9BF15D9B97DD1;
	fma.rn.f64 	%fd453, %fd451, %fd444, %fd452;
	mov.f64 	%fd454, 0dBFC8BFECF93D7D19;
	fma.rn.f64 	%fd455, %fd453, %fd444, %fd454;
	mov.f64 	%fd456, 0d3FC7FFFFF756AA6C;
	fma.rn.f64 	%fd457, %fd455, %fd444, %fd456;
	mov.f64 	%fd458, 0d3FF0000000003646;
	fma.rn.f64 	%fd459, %fd457, %fd444, %fd458;
	mov.f64 	%fd460, 0d416024E99BA46E7B;
	mov.f64 	%fd461, 0dC1943281A050209C;
	fma.rn.f64 	%fd462, %fd461, %fd444, %fd460;
	mov.f64 	%fd463, 0dC11A6875D7DFBD65;
	fma.rn.f64 	%fd464, %fd462, %fd444, %fd463;
	mov.f64 	%fd465, 0d40D032C041790233;
	fma.rn.f64 	%fd466, %fd464, %fd444, %fd465;
	mov.f64 	%fd467, 0dC0839F895BC22946;
	fma.rn.f64 	%fd468, %fd466, %fd444, %fd467;
	mov.f64 	%fd469, 0d403E77CC78ECD2D8;
	fma.rn.f64 	%fd470, %fd468, %fd444, %fd469;
	mov.f64 	%fd471, 0dC002F368D0117BE9;
	fma.rn.f64 	%fd472, %fd470, %fd444, %fd471;
	mov.f64 	%fd473, 0d3FD7BCC786009A25;
	fma.rn.f64 	%fd474, %fd472, %fd444, %fd473;
	mov.f64 	%fd475, 0dBFC4FFFFFC51BC7A;
	fma.rn.f64 	%fd476, %fd474, %fd444, %fd475;
	mov.f64 	%fd477, 0d3FD7FFFFFFFFB5EA;
	fma.rn.f64 	%fd478, %fd476, %fd444, %fd477;
	fma.rn.f64 	%fd40, %fd478, %fd443, %fd2;
	rsqrt.approx.f64 	%fd479, %fd2;
	mul.f64 	%fd480, %fd479, 0d3FE9884533D43651;
	mul.f64 	%fd41, %fd459, %fd480;
	mul.f64 	%fd481, %fd40, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r91, %fd481;
	add.u64 	%rd17, %SP, 0;
	cvta.to.local.u64 	%rd18, %rd17;
	st.local.u32 	[%rd18], %r91;
	cvt.rn.f64.s32	%fd482, %r91;
	neg.f64 	%fd483, %fd482;
	mov.f64 	%fd484, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd485, %fd483, %fd484, %fd40;
	mov.f64 	%fd486, 0d3C91A62633145C00;
	fma.rn.f64 	%fd487, %fd483, %fd486, %fd485;
	mov.f64 	%fd488, 0d397B839A252049C0;
	fma.rn.f64 	%fd534, %fd483, %fd488, %fd487;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd40;
	}
	and.b32  	%r68, %r67, 2145386496;
	setp.lt.u32	%p23, %r68, 1105199104;
	@%p23 bra 	BB77_38;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd40;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd534, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r91, [%rd18];

BB77_38:
	and.b32  	%r69, %r91, 3;
	cvt.rn.f64.s32	%fd489, %r69;
	add.f64 	%fd490, %fd534, 0dC00F6A7A2955385E;
	fma.rn.f64 	%fd535, %fd489, 0d3FF921FB54442D18, %fd490;
	abs.f64 	%fd491, %fd535;
	setp.neu.f64	%p24, %fd491, 0d7FF0000000000000;
	@%p24 bra 	BB77_40;

	mov.f64 	%fd492, 0d0000000000000000;
	mul.rn.f64 	%fd535, %fd535, %fd492;

BB77_40:
	mov.f64 	%fd524, 0d397B839A252049C0;
	mov.f64 	%fd523, 0d3C91A62633145C00;
	mov.f64 	%fd522, 0d3FF921FB54442D18;
	mul.f64 	%fd493, %fd535, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r92, %fd493;
	st.local.u32 	[%rd18], %r92;
	cvt.rn.f64.s32	%fd494, %r92;
	neg.f64 	%fd495, %fd494;
	fma.rn.f64 	%fd497, %fd495, %fd522, %fd535;
	fma.rn.f64 	%fd499, %fd495, %fd523, %fd497;
	fma.rn.f64 	%fd536, %fd495, %fd524, %fd499;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd535;
	}
	and.b32  	%r71, %r70, 2145386496;
	setp.lt.u32	%p25, %r71, 1105199104;
	@%p25 bra 	BB77_42;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd535;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd536, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r92, [%rd18];

BB77_42:
	add.s32 	%r24, %r92, 1;
	and.b32  	%r72, %r24, 1;
	shl.b32 	%r73, %r72, 3;
	setp.eq.s32	%p26, %r72, 0;
	selp.f64	%fd501, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p26;
	mul.wide.u32 	%rd25, %r73, 8;
	mov.u64 	%rd26, __cudart_sin_cos_coeffs;
	add.s64 	%rd27, %rd25, %rd26;
	ld.const.f64 	%fd502, [%rd27+8];
	mul.rn.f64 	%fd51, %fd536, %fd536;
	fma.rn.f64 	%fd503, %fd501, %fd51, %fd502;
	ld.const.f64 	%fd504, [%rd27+16];
	fma.rn.f64 	%fd505, %fd503, %fd51, %fd504;
	ld.const.f64 	%fd506, [%rd27+24];
	fma.rn.f64 	%fd507, %fd505, %fd51, %fd506;
	ld.const.f64 	%fd508, [%rd27+32];
	fma.rn.f64 	%fd509, %fd507, %fd51, %fd508;
	ld.const.f64 	%fd510, [%rd27+40];
	fma.rn.f64 	%fd511, %fd509, %fd51, %fd510;
	ld.const.f64 	%fd512, [%rd27+48];
	fma.rn.f64 	%fd52, %fd511, %fd51, %fd512;
	fma.rn.f64 	%fd537, %fd52, %fd536, %fd536;
	@%p26 bra 	BB77_44;

	mov.f64 	%fd525, 0d3FF0000000000000;
	fma.rn.f64 	%fd537, %fd52, %fd51, %fd525;

BB77_44:
	and.b32  	%r74, %r24, 2;
	setp.eq.s32	%p27, %r74, 0;
	@%p27 bra 	BB77_46;

	mov.f64 	%fd514, 0d0000000000000000;
	mov.f64 	%fd515, 0dBFF0000000000000;
	fma.rn.f64 	%fd537, %fd537, %fd515, %fd514;

BB77_46:
	mul.f64 	%fd538, %fd41, %fd537;
	bra.uni 	BB77_48;

BB77_3:
	mul.f64 	%fd63, %fd2, %fd2;
	mov.f64 	%fd64, 0dBDCF0B5B1FB7B95E;
	mov.f64 	%fd65, 0d3D5249F90687428C;
	fma.rn.f64 	%fd66, %fd65, %fd63, %fd64;
	mov.f64 	%fd67, 0d3E432E589311FA14;
	fma.rn.f64 	%fd68, %fd66, %fd63, %fd67;
	mov.f64 	%fd69, 0dBEB0A780AA4A92E9;
	fma.rn.f64 	%fd70, %fd68, %fd63, %fd69;
	mov.f64 	%fd71, 0d3F12C7DBFFCAEC2B;
	fma.rn.f64 	%fd72, %fd70, %fd63, %fd71;
	mov.f64 	%fd73, 0dBF6835B97894BA4A;
	fma.rn.f64 	%fd74, %fd72, %fd63, %fd73;
	mov.f64 	%fd75, 0d3FABD3975C75B4A3;
	fma.rn.f64 	%fd76, %fd74, %fd63, %fd75;
	mov.f64 	%fd77, 0dBFC91866143CBC8A;
	fma.rn.f64 	%fd3, %fd76, %fd63, %fd77;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r86, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd2;
	}
	mov.u32 	%r87, -1023;
	setp.gt.s32	%p4, %r85, 1048575;
	mov.f64 	%fd526, %fd2;
	@%p4 bra 	BB77_5;

	mul.f64 	%fd4, %fd2, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd4;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r86, %temp}, %fd4;
	}
	mov.u32 	%r87, -1077;
	mov.f64 	%fd526, %fd4;

BB77_5:
	mov.f64 	%fd5, %fd526;
	add.s32 	%r48, %r85, -1;
	setp.lt.u32	%p5, %r48, 2146435071;
	@%p5 bra 	BB77_7;
	bra.uni 	BB77_6;

BB77_7:
	shr.u32 	%r50, %r85, 20;
	add.s32 	%r88, %r87, %r50;
	and.b32  	%r51, %r85, -2146435073;
	or.b32  	%r52, %r51, 1072693248;
	mov.b64 	%fd527, {%r86, %r52};
	setp.lt.s32	%p7, %r52, 1073127583;
	@%p7 bra 	BB77_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd527;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd527;
	}
	add.s32 	%r55, %r54, -1048576;
	mov.b64 	%fd527, {%r53, %r55};
	add.s32 	%r88, %r88, 1;

BB77_9:
	add.f64 	%fd81, %fd527, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd80,%fd81;
	// inline asm
	neg.f64 	%fd82, %fd81;
	mov.f64 	%fd83, 0d3FF0000000000000;
	fma.rn.f64 	%fd84, %fd82, %fd80, %fd83;
	fma.rn.f64 	%fd85, %fd84, %fd84, %fd84;
	fma.rn.f64 	%fd86, %fd85, %fd80, %fd80;
	add.f64 	%fd87, %fd527, 0dBFF0000000000000;
	mul.f64 	%fd88, %fd87, %fd86;
	fma.rn.f64 	%fd89, %fd87, %fd86, %fd88;
	mul.f64 	%fd90, %fd89, %fd89;
	mov.f64 	%fd91, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd92, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	mov.f64 	%fd94, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd95, %fd93, %fd90, %fd94;
	mov.f64 	%fd96, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd97, %fd95, %fd90, %fd96;
	mov.f64 	%fd98, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd99, %fd97, %fd90, %fd98;
	mov.f64 	%fd100, 0d3F624924923BE72D;
	fma.rn.f64 	%fd101, %fd99, %fd90, %fd100;
	mov.f64 	%fd102, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd103, %fd101, %fd90, %fd102;
	mov.f64 	%fd104, 0d3FB5555555555554;
	fma.rn.f64 	%fd105, %fd103, %fd90, %fd104;
	sub.f64 	%fd106, %fd87, %fd89;
	add.f64 	%fd107, %fd106, %fd106;
	neg.f64 	%fd108, %fd89;
	fma.rn.f64 	%fd109, %fd108, %fd87, %fd107;
	mul.f64 	%fd110, %fd86, %fd109;
	mul.f64 	%fd111, %fd90, %fd105;
	fma.rn.f64 	%fd112, %fd111, %fd89, %fd110;
	xor.b32  	%r56, %r88, -2147483648;
	mov.u32 	%r57, 1127219200;
	mov.b64 	%fd113, {%r56, %r57};
	mov.u32 	%r58, -2147483648;
	mov.b64 	%fd114, {%r58, %r57};
	sub.f64 	%fd115, %fd113, %fd114;
	mov.f64 	%fd116, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd117, %fd115, %fd116, %fd89;
	neg.f64 	%fd118, %fd115;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	sub.f64 	%fd120, %fd119, %fd89;
	sub.f64 	%fd121, %fd112, %fd120;
	mov.f64 	%fd122, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd123, %fd115, %fd122, %fd121;
	add.f64 	%fd528, %fd117, %fd123;
	bra.uni 	BB77_10;

BB77_30:
	add.f64 	%fd310, %fd2, 0dC00193BED4DFF243;
	add.f64 	%fd311, %fd310, 0d3C8BD1E50D219BFD;
	mov.f64 	%fd312, 0d3E4833AAE4D8B975;
	mov.f64 	%fd313, 0dBE2B87B0BE2AA150;
	fma.rn.f64 	%fd314, %fd313, %fd311, %fd312;
	mov.f64 	%fd315, 0dBE44E279B423E68F;
	fma.rn.f64 	%fd316, %fd314, %fd311, %fd315;
	mov.f64 	%fd317, 0d3E129DC6A747EB4F;
	fma.rn.f64 	%fd318, %fd316, %fd311, %fd317;
	mov.f64 	%fd319, 0dBE61D15534496CD8;
	fma.rn.f64 	%fd320, %fd318, %fd311, %fd319;
	mov.f64 	%fd321, 0d3E7EEC8D48FECE00;
	fma.rn.f64 	%fd322, %fd320, %fd311, %fd321;
	mov.f64 	%fd323, 0dBE8D1180AF70A134;
	fma.rn.f64 	%fd324, %fd322, %fd311, %fd323;
	mov.f64 	%fd325, 0d3E9C8386A0EA1388;
	fma.rn.f64 	%fd326, %fd324, %fd311, %fd325;
	mov.f64 	%fd327, 0dBEB01A014E7F3250;
	fma.rn.f64 	%fd328, %fd326, %fd311, %fd327;
	mov.f64 	%fd329, 0d3EC1FB752010A320;
	fma.rn.f64 	%fd330, %fd328, %fd311, %fd329;
	mov.f64 	%fd331, 0dBED3AA0AFF4E332B;
	fma.rn.f64 	%fd332, %fd330, %fd311, %fd331;
	mov.f64 	%fd333, 0d3EE584A6C77F6700;
	fma.rn.f64 	%fd334, %fd332, %fd311, %fd333;
	mov.f64 	%fd335, 0dBEF794C520FC2EBB;
	fma.rn.f64 	%fd336, %fd334, %fd311, %fd335;
	mov.f64 	%fd337, 0d3F09D18D2D35CC71;
	fma.rn.f64 	%fd338, %fd336, %fd311, %fd337;
	mov.f64 	%fd339, 0dBF1C3FB7315C4599;
	fma.rn.f64 	%fd340, %fd338, %fd311, %fd339;
	mov.f64 	%fd341, 0d3F2EEA7ADECCE927;
	fma.rn.f64 	%fd342, %fd340, %fd311, %fd341;
	mov.f64 	%fd343, 0dBF40B2D85257446F;
	fma.rn.f64 	%fd344, %fd342, %fd311, %fd343;
	mov.f64 	%fd345, 0d3F517AB4B1FE5D5B;
	fma.rn.f64 	%fd346, %fd344, %fd311, %fd345;
	mov.f64 	%fd347, 0dBF65429DC6516C0D;
	fma.rn.f64 	%fd348, %fd346, %fd311, %fd347;
	mov.f64 	%fd349, 0d3F7E671C7D0B090B;
	fma.rn.f64 	%fd350, %fd348, %fd311, %fd349;
	mov.f64 	%fd351, 0dBF73A6DEC36FB27C;
	fma.rn.f64 	%fd352, %fd350, %fd311, %fd351;
	mov.f64 	%fd353, 0dBFA0D2AF4E931FD1;
	fma.rn.f64 	%fd354, %fd352, %fd311, %fd353;
	mov.f64 	%fd355, 0dBFBE56F82217B964;
	fma.rn.f64 	%fd356, %fd354, %fd311, %fd355;
	mov.f64 	%fd357, 0d3FE0AA48442F014B;
	fma.rn.f64 	%fd358, %fd356, %fd311, %fd357;
	mul.f64 	%fd538, %fd311, %fd358;
	bra.uni 	BB77_48;

BB77_6:
	mov.f64 	%fd78, 0d7FF0000000000000;
	fma.rn.f64 	%fd79, %fd5, %fd78, %fd78;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd5;
	}
	mov.b32 	 %f1, %r49;
	setp.eq.f32	%p6, %f1, 0f00000000;
	selp.f64	%fd528, 0dFFF0000000000000, %fd79, %p6;

BB77_10:
	abs.f64 	%fd12, %fd2;
	setp.gtu.f64	%p8, %fd12, 0d400353AABAD7B784;
	@%p8 bra 	BB77_12;
	bra.uni 	BB77_11;

BB77_12:
	setp.gtu.f64	%p9, %fd12, 0d4015B1D0574614EA;
	@%p9 bra 	BB77_14;
	bra.uni 	BB77_13;

BB77_14:
	setp.gtu.f64	%p10, %fd12, 0d40213065E54C1AA9;
	@%p10 bra 	BB77_16;
	bra.uni 	BB77_15;

BB77_16:
	abs.f64 	%fd222, %fd12;
	mov.f64 	%fd533, 0d0000000000000000;
	setp.eq.f64	%p11, %fd222, 0d7FF0000000000000;
	@%p11 bra 	BB77_28;

	// inline asm
	rcp.approx.ftz.f64 %fd223,%fd12;
	// inline asm
	neg.f64 	%fd225, %fd12;
	mov.f64 	%fd226, 0d3FF0000000000000;
	fma.rn.f64 	%fd227, %fd225, %fd223, %fd226;
	fma.rn.f64 	%fd228, %fd227, %fd227, %fd227;
	fma.rn.f64 	%fd229, %fd228, %fd223, %fd223;
	mul.f64 	%fd230, %fd229, %fd229;
	mov.f64 	%fd231, 0dC099C06322A3F8BE;
	mov.f64 	%fd232, 0d40CD02EA3F2F6751;
	fma.rn.f64 	%fd233, %fd232, %fd230, %fd231;
	mov.f64 	%fd234, 0d405B89354DA77324;
	fma.rn.f64 	%fd235, %fd233, %fd230, %fd234;
	mov.f64 	%fd236, 0dC01E352294653188;
	fma.rn.f64 	%fd237, %fd235, %fd230, %fd236;
	mov.f64 	%fd238, 0d3FE9BC7DB16BD7A7;
	fma.rn.f64 	%fd239, %fd237, %fd230, %fd238;
	mov.f64 	%fd240, 0dBFC8BFE1C3A4F741;
	fma.rn.f64 	%fd241, %fd239, %fd230, %fd240;
	mov.f64 	%fd242, 0d3FC7FFFFF0D00BE2;
	fma.rn.f64 	%fd243, %fd241, %fd230, %fd242;
	mov.f64 	%fd244, 0d3FF00000000068CC;
	fma.rn.f64 	%fd245, %fd243, %fd230, %fd244;
	mov.f64 	%fd246, 0d415A30AC6857BEE0;
	mov.f64 	%fd247, 0dC18DA26B212FDC9A;
	fma.rn.f64 	%fd248, %fd247, %fd230, %fd246;
	mov.f64 	%fd249, 0dC11764222AD7C910;
	fma.rn.f64 	%fd250, %fd248, %fd230, %fd249;
	mov.f64 	%fd251, 0d40CEB02E0C306857;
	fma.rn.f64 	%fd252, %fd250, %fd230, %fd251;
	mov.f64 	%fd253, 0dC08351859FA2B23B;
	fma.rn.f64 	%fd254, %fd252, %fd230, %fd253;
	mov.f64 	%fd255, 0d403E65A07AF51F42;
	fma.rn.f64 	%fd256, %fd254, %fd230, %fd255;
	mov.f64 	%fd257, 0dC002F2B817F77A57;
	fma.rn.f64 	%fd258, %fd256, %fd230, %fd257;
	mov.f64 	%fd259, 0d3FD7BCC34DA069FD;
	fma.rn.f64 	%fd260, %fd258, %fd230, %fd259;
	mov.f64 	%fd261, 0dBFC4FFFFF8A44463;
	fma.rn.f64 	%fd262, %fd260, %fd230, %fd261;
	mov.f64 	%fd263, 0d3FD7FFFFFFFF5CD7;
	fma.rn.f64 	%fd264, %fd262, %fd230, %fd263;
	fma.rn.f64 	%fd16, %fd264, %fd229, %fd12;
	rsqrt.approx.f64 	%fd265, %fd12;
	mul.f64 	%fd266, %fd265, 0d3FE9884533D43651;
	mul.f64 	%fd17, %fd245, %fd266;
	mul.f64 	%fd267, %fd16, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r89, %fd267;
	add.u64 	%rd6, %SP, 0;
	cvta.to.local.u64 	%rd7, %rd6;
	st.local.u32 	[%rd7], %r89;
	cvt.rn.f64.s32	%fd268, %r89;
	neg.f64 	%fd269, %fd268;
	mov.f64 	%fd270, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd271, %fd269, %fd270, %fd16;
	mov.f64 	%fd272, 0d3C91A62633145C00;
	fma.rn.f64 	%fd273, %fd269, %fd272, %fd271;
	mov.f64 	%fd274, 0d397B839A252049C0;
	fma.rn.f64 	%fd529, %fd269, %fd274, %fd273;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd16;
	}
	and.b32  	%r60, %r59, 2145386496;
	setp.lt.u32	%p12, %r60, 1105199104;
	@%p12 bra 	BB77_19;

	add.u64 	%rd32, %SP, 0;
	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd16;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd32;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd529, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r89, [%rd7];

BB77_19:
	and.b32  	%r61, %r89, 3;
	cvt.rn.f64.s32	%fd275, %r61;
	add.f64 	%fd276, %fd529, 0dC002D97C7F3321D2;
	fma.rn.f64 	%fd530, %fd275, 0d3FF921FB54442D18, %fd276;
	abs.f64 	%fd277, %fd530;
	setp.neu.f64	%p13, %fd277, 0d7FF0000000000000;
	@%p13 bra 	BB77_21;

	mov.f64 	%fd278, 0d0000000000000000;
	mul.rn.f64 	%fd530, %fd530, %fd278;

BB77_21:
	mov.f64 	%fd519, 0d397B839A252049C0;
	mov.f64 	%fd518, 0d3C91A62633145C00;
	mov.f64 	%fd517, 0d3FF921FB54442D18;
	mul.f64 	%fd279, %fd530, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r90, %fd279;
	st.local.u32 	[%rd7], %r90;
	cvt.rn.f64.s32	%fd280, %r90;
	neg.f64 	%fd281, %fd280;
	fma.rn.f64 	%fd283, %fd281, %fd517, %fd530;
	fma.rn.f64 	%fd285, %fd281, %fd518, %fd283;
	fma.rn.f64 	%fd531, %fd281, %fd519, %fd285;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd530;
	}
	and.b32  	%r63, %r62, 2145386496;
	setp.lt.u32	%p14, %r63, 1105199104;
	@%p14 bra 	BB77_23;

	add.u64 	%rd31, %SP, 0;
	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd530;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd31;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd531, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r90, [%rd7];

BB77_23:
	add.s32 	%r17, %r90, 1;
	and.b32  	%r64, %r17, 1;
	shl.b32 	%r65, %r64, 3;
	setp.eq.s32	%p15, %r64, 0;
	selp.f64	%fd287, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p15;
	mul.wide.u32 	%rd14, %r65, 8;
	mov.u64 	%rd15, __cudart_sin_cos_coeffs;
	add.s64 	%rd16, %rd14, %rd15;
	ld.const.f64 	%fd288, [%rd16+8];
	mul.rn.f64 	%fd27, %fd531, %fd531;
	fma.rn.f64 	%fd289, %fd287, %fd27, %fd288;
	ld.const.f64 	%fd290, [%rd16+16];
	fma.rn.f64 	%fd291, %fd289, %fd27, %fd290;
	ld.const.f64 	%fd292, [%rd16+24];
	fma.rn.f64 	%fd293, %fd291, %fd27, %fd292;
	ld.const.f64 	%fd294, [%rd16+32];
	fma.rn.f64 	%fd295, %fd293, %fd27, %fd294;
	ld.const.f64 	%fd296, [%rd16+40];
	fma.rn.f64 	%fd297, %fd295, %fd27, %fd296;
	ld.const.f64 	%fd298, [%rd16+48];
	fma.rn.f64 	%fd28, %fd297, %fd27, %fd298;
	fma.rn.f64 	%fd532, %fd28, %fd531, %fd531;
	@%p15 bra 	BB77_25;

	mov.f64 	%fd520, 0d3FF0000000000000;
	fma.rn.f64 	%fd532, %fd28, %fd27, %fd520;

BB77_25:
	and.b32  	%r66, %r17, 2;
	setp.eq.s32	%p16, %r66, 0;
	@%p16 bra 	BB77_27;

	mov.f64 	%fd300, 0d0000000000000000;
	mov.f64 	%fd301, 0dBFF0000000000000;
	fma.rn.f64 	%fd532, %fd532, %fd301, %fd300;

BB77_27:
	mul.f64 	%fd533, %fd17, %fd532;
	bra.uni 	BB77_28;

BB77_11:
	mov.f64 	%fd124, 0dBD4DD167A0DC3F55;
	mov.f64 	%fd125, 0d3D020E4ADCDE2AD3;
	fma.rn.f64 	%fd126, %fd125, %fd12, %fd124;
	mov.f64 	%fd127, 0d3D5503F5A491E487;
	fma.rn.f64 	%fd128, %fd126, %fd12, %fd127;
	mov.f64 	%fd129, 0d3DC1F29940C2403A;
	fma.rn.f64 	%fd130, %fd128, %fd12, %fd129;
	mov.f64 	%fd131, 0d3D84CF9302EACDEF;
	fma.rn.f64 	%fd132, %fd130, %fd12, %fd131;
	mov.f64 	%fd133, 0dBE384A53DBBCA436;
	fma.rn.f64 	%fd134, %fd132, %fd12, %fd133;
	mov.f64 	%fd135, 0d3D9779BEE4F63BCC;
	fma.rn.f64 	%fd136, %fd134, %fd12, %fd135;
	mov.f64 	%fd137, 0d3EA6C160E414F3F0;
	fma.rn.f64 	%fd138, %fd136, %fd12, %fd137;
	mov.f64 	%fd139, 0d3D8F3D2F12430699;
	fma.rn.f64 	%fd140, %fd138, %fd12, %fd139;
	mov.f64 	%fd141, 0dBF0C71C72C0CED04;
	fma.rn.f64 	%fd142, %fd140, %fd12, %fd141;
	mov.f64 	%fd143, 0d3D659BCA506F1128;
	fma.rn.f64 	%fd144, %fd142, %fd12, %fd143;
	mov.f64 	%fd145, 0d3F65555555506982;
	fma.rn.f64 	%fd146, %fd144, %fd12, %fd145;
	mov.f64 	%fd147, 0d3D15BA0B425F1BFB;
	fma.rn.f64 	%fd148, %fd146, %fd12, %fd147;
	mov.f64 	%fd149, 0dBFB0000000000065;
	fma.rn.f64 	%fd150, %fd148, %fd12, %fd149;
	mov.f64 	%fd151, 0d3C8729A7253FB679;
	fma.rn.f64 	%fd152, %fd150, %fd12, %fd151;
	mov.f64 	%fd153, 0d3FE0000000000000;
	fma.rn.f64 	%fd154, %fd152, %fd12, %fd153;
	mul.f64 	%fd533, %fd12, %fd154;
	bra.uni 	BB77_28;

BB77_32:
	add.f64 	%fd359, %fd2, 0dC015B7FE4E87B02E;
	add.f64 	%fd360, %fd359, 0dBCBDFE7BAC228E8C;
	mov.f64 	%fd361, 0d3CC69A30996793E2;
	mov.f64 	%fd362, 0d3CBA3C76069F1D8C;
	fma.rn.f64 	%fd363, %fd362, %fd360, %fd361;
	mov.f64 	%fd364, 0dBCDDD8432FE756E7;
	fma.rn.f64 	%fd365, %fd363, %fd360, %fd364;
	mov.f64 	%fd366, 0dBD143158EEE220F7;
	fma.rn.f64 	%fd367, %fd365, %fd360, %fd366;
	mov.f64 	%fd368, 0d3D28D44491230F5A;
	fma.rn.f64 	%fd369, %fd367, %fd360, %fd368;
	mov.f64 	%fd370, 0dBD438842EAF4EDBC;
	fma.rn.f64 	%fd371, %fd369, %fd360, %fd370;
	mov.f64 	%fd372, 0d3D74958DAFBFAF5A;
	fma.rn.f64 	%fd373, %fd371, %fd360, %fd372;
	mov.f64 	%fd374, 0dBD9449A60E664848;
	fma.rn.f64 	%fd375, %fd373, %fd360, %fd374;
	mov.f64 	%fd376, 0d3D838BC8CD594A76;
	fma.rn.f64 	%fd377, %fd375, %fd360, %fd376;
	mov.f64 	%fd378, 0dBDFA798002141323;
	fma.rn.f64 	%fd379, %fd377, %fd360, %fd378;
	mov.f64 	%fd380, 0d3E380B4198956AAA;
	fma.rn.f64 	%fd381, %fd379, %fd360, %fd380;
	mov.f64 	%fd382, 0d3E5B62B5F21BACD4;
	fma.rn.f64 	%fd383, %fd381, %fd360, %fd382;
	mov.f64 	%fd384, 0dBEA255E729FB6AAE;
	fma.rn.f64 	%fd385, %fd383, %fd360, %fd384;
	mov.f64 	%fd386, 0dBEC80618F6BAE5AA;
	fma.rn.f64 	%fd387, %fd385, %fd360, %fd386;
	mov.f64 	%fd388, 0d3F085B940F8E8D36;
	fma.rn.f64 	%fd389, %fd387, %fd360, %fd388;
	mov.f64 	%fd390, 0d3F2337C7E10E14E8;
	fma.rn.f64 	%fd391, %fd389, %fd360, %fd390;
	mov.f64 	%fd392, 0dBF61BE6DB99332CA;
	fma.rn.f64 	%fd393, %fd391, %fd360, %fd392;
	mov.f64 	%fd394, 0dBF710A329E2BE9B8;
	fma.rn.f64 	%fd395, %fd393, %fd360, %fd394;
	mov.f64 	%fd396, 0d3FAA15D92DFE3FCF;
	fma.rn.f64 	%fd397, %fd395, %fd360, %fd396;
	mov.f64 	%fd398, 0d3FA00B9F8571C9BE;
	fma.rn.f64 	%fd399, %fd397, %fd360, %fd398;
	mov.f64 	%fd400, 0dBFD5C7C556F0C19A;
	fma.rn.f64 	%fd401, %fd399, %fd360, %fd400;
	mul.f64 	%fd538, %fd360, %fd401;
	bra.uni 	BB77_48;

BB77_13:
	add.f64 	%fd155, %fd12, 0dC00EA75575AF6F09;
	add.f64 	%fd156, %fd155, 0d3CA60155A9D1B256;
	mov.f64 	%fd157, 0d3D41011A1DF02DAD;
	mov.f64 	%fd158, 0dBCF8D3CDBB60175E;
	fma.rn.f64 	%fd159, %fd158, %fd156, %fd157;
	mov.f64 	%fd160, 0d3D76013AC1E5E222;
	fma.rn.f64 	%fd161, %fd159, %fd156, %fd160;
	mov.f64 	%fd162, 0dBDBEC315D96D5F03;
	fma.rn.f64 	%fd163, %fd161, %fd156, %fd162;
	mov.f64 	%fd164, 0dBDF03BE1B4B57207;
	fma.rn.f64 	%fd165, %fd163, %fd156, %fd164;
	mov.f64 	%fd166, 0d3E345695F8B660F7;
	fma.rn.f64 	%fd167, %fd165, %fd156, %fd166;
	mov.f64 	%fd168, 0d3E617069FCFCFFF4;
	fma.rn.f64 	%fd169, %fd167, %fd156, %fd168;
	mov.f64 	%fd170, 0dBEA33825C36745EB;
	fma.rn.f64 	%fd171, %fd169, %fd156, %fd170;
	mov.f64 	%fd172, 0dBEC9799D4F90931B;
	fma.rn.f64 	%fd173, %fd171, %fd156, %fd172;
	mov.f64 	%fd174, 0d3F083A06E2F7DF13;
	fma.rn.f64 	%fd175, %fd173, %fd156, %fd174;
	mov.f64 	%fd176, 0d3F26E4C2D53A7CF6;
	fma.rn.f64 	%fd177, %fd175, %fd156, %fd176;
	mov.f64 	%fd178, 0dBF624B3409957B1C;
	fma.rn.f64 	%fd179, %fd177, %fd156, %fd178;
	mov.f64 	%fd180, 0dBF7537544C3325DF;
	fma.rn.f64 	%fd181, %fd179, %fd156, %fd180;
	mov.f64 	%fd182, 0d3FAB589D1DA138E2;
	fma.rn.f64 	%fd183, %fd181, %fd156, %fd182;
	mov.f64 	%fd184, 0d3FAAE8A39F51AD13;
	fma.rn.f64 	%fd185, %fd183, %fd156, %fd184;
	mov.f64 	%fd186, 0dBFD9C6CF582CBF7F;
	fma.rn.f64 	%fd187, %fd185, %fd156, %fd186;
	mul.f64 	%fd533, %fd156, %fd187;
	bra.uni 	BB77_28;

BB77_34:
	add.f64 	%fd402, %fd2, 0dC0213127AE6169B4;
	add.f64 	%fd403, %fd402, 0dBCB479CC068D9046;
	mov.f64 	%fd404, 0dBD43515F67644276;
	mov.f64 	%fd405, 0d3CB09CCC22945996;
	fma.rn.f64 	%fd406, %fd405, %fd403, %fd404;
	mov.f64 	%fd407, 0dBD72C5B978E9F5C7;
	fma.rn.f64 	%fd408, %fd406, %fd403, %fd407;
	mov.f64 	%fd409, 0d3DBEC1151613913C;
	fma.rn.f64 	%fd410, %fd408, %fd403, %fd409;
	mov.f64 	%fd411, 0d3DE9E38D13C4A824;
	fma.rn.f64 	%fd412, %fd410, %fd403, %fd411;
	mov.f64 	%fd413, 0dBE341E75E1088EB5;
	fma.rn.f64 	%fd414, %fd412, %fd403, %fd413;
	mov.f64 	%fd415, 0dBE5A384EBB13CFE1;
	fma.rn.f64 	%fd416, %fd414, %fd403, %fd415;
	mov.f64 	%fd417, 0d3EA2BECB27F8C8F8;
	fma.rn.f64 	%fd418, %fd416, %fd403, %fd417;
	mov.f64 	%fd419, 0d3EC176E72B989FD8;
	fma.rn.f64 	%fd420, %fd418, %fd403, %fd419;
	mov.f64 	%fd421, 0dBF06F7BAB102F822;
	fma.rn.f64 	%fd422, %fd420, %fd403, %fd421;
	mov.f64 	%fd423, 0dBF1B50D7E1D278E1;
	fma.rn.f64 	%fd424, %fd422, %fd403, %fd423;
	mov.f64 	%fd425, 0d3F607A678D60004F;
	fma.rn.f64 	%fd426, %fd424, %fd403, %fd425;
	mov.f64 	%fd427, 0d3F63CED2A2E69115;
	fma.rn.f64 	%fd428, %fd426, %fd403, %fd427;
	mov.f64 	%fd429, 0dBFA6395DFE49FCD4;
	fma.rn.f64 	%fd430, %fd428, %fd403, %fd429;
	mov.f64 	%fd431, 0dBF902B3933CF21B1;
	fma.rn.f64 	%fd432, %fd430, %fd403, %fd431;
	mov.f64 	%fd433, 0d3FD15F993FCEAB5C;
	fma.rn.f64 	%fd434, %fd432, %fd403, %fd433;
	mul.f64 	%fd538, %fd403, %fd434;
	bra.uni 	BB77_48;

BB77_15:
	add.f64 	%fd188, %fd12, 0dC01C0FF5F3B47250;
	add.f64 	%fd189, %fd188, 0d3C9B226D9D243827;
	mov.f64 	%fd190, 0dBD40E8363DB649A9;
	mov.f64 	%fd191, 0d3CF3EB867515FAD6;
	fma.rn.f64 	%fd192, %fd191, %fd189, %fd190;
	mov.f64 	%fd193, 0dBD73B7DD4A6608FB;
	fma.rn.f64 	%fd194, %fd192, %fd189, %fd193;
	mov.f64 	%fd195, 0d3DBEC5E01482C750;
	fma.rn.f64 	%fd196, %fd194, %fd189, %fd195;
	mov.f64 	%fd197, 0d3DEC62BB9E882103;
	fma.rn.f64 	%fd198, %fd196, %fd189, %fd197;
	mov.f64 	%fd199, 0dBE34462EED732A23;
	fma.rn.f64 	%fd200, %fd198, %fd189, %fd199;
	mov.f64 	%fd201, 0dBE5D48DCAD7DC59B;
	fma.rn.f64 	%fd202, %fd200, %fd189, %fd201;
	mov.f64 	%fd203, 0d3EA3026DF29167E9;
	fma.rn.f64 	%fd204, %fd202, %fd189, %fd203;
	mov.f64 	%fd205, 0d3EC4255B0119666C;
	fma.rn.f64 	%fd206, %fd204, %fd189, %fd205;
	mov.f64 	%fd207, 0dBF0796A751B32693;
	fma.rn.f64 	%fd208, %fd206, %fd189, %fd207;
	mov.f64 	%fd209, 0dBF207358BBDBA284;
	fma.rn.f64 	%fd210, %fd208, %fd189, %fd209;
	mov.f64 	%fd211, 0d3F613FBC7D6927B1;
	fma.rn.f64 	%fd212, %fd210, %fd189, %fd211;
	mov.f64 	%fd213, 0d3F69A4B292E3DD75;
	fma.rn.f64 	%fd214, %fd212, %fd189, %fd213;
	mov.f64 	%fd215, 0dBFA80C83BDEEE4FB;
	fma.rn.f64 	%fd216, %fd214, %fd189, %fd215;
	mov.f64 	%fd217, 0dBF95E70DC60362BF;
	fma.rn.f64 	%fd218, %fd216, %fd189, %fd217;
	mov.f64 	%fd219, 0d3FD33518B3874E8A;
	fma.rn.f64 	%fd220, %fd218, %fd189, %fd219;
	mul.f64 	%fd533, %fd189, %fd220;

BB77_28:
	abs.f64 	%fd521, %fd1;
	neg.f64 	%fd302, %fd533;
	setp.lt.f64	%p17, %fd521, 0d0000000000000000;
	selp.f64	%fd303, %fd302, %fd533, %p17;
	mul.f64 	%fd304, %fd521, 0d3FE0000000000000;
	setp.lt.f64	%p18, %fd12, 0d39B4484BFEEBC2A0;
	selp.f64	%fd305, %fd304, %fd303, %p18;
	mov.f64 	%fd306, 0dBFF0000000000000;
	div.rn.f64 	%fd307, %fd306, %fd521;
	fma.rn.f64 	%fd308, %fd528, %fd305, %fd307;
	mul.f64 	%fd309, %fd308, 0d3FE45F306DC9C883;
	fma.rn.f64 	%fd538, %fd521, %fd3, %fd309;

BB77_48:
	setp.gtu.f64	%p28, %fd1, 0d0000000000000000;
	@%p28 bra 	BB77_50;

	setp.eq.f64	%p29, %fd1, 0d0000000000000000;
	selp.f64	%fd538, 0dFFF0000000000000, 0dFFF8000000000000, %p29;

BB77_50:
	cvta.to.global.u64 	%rd28, %rd1;
	add.s64 	%rd30, %rd28, %rd4;
	st.global.f64 	[%rd30], %fd538;

BB77_51:
	ret;
}

	// .globl	vec_copysign
.visible .entry vec_copysign(
	.param .u32 vec_copysign_param_0,
	.param .u64 vec_copysign_param_1,
	.param .u64 vec_copysign_param_2,
	.param .u64 vec_copysign_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_copysign_param_0];
	ld.param.u64 	%rd1, [vec_copysign_param_1];
	ld.param.u64 	%rd2, [vec_copysign_param_2];
	ld.param.u64 	%rd3, [vec_copysign_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB78_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd6];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd2;
	}
	and.b32  	%r15, %r12, -2147483648;
	and.b32  	%r16, %r14, 2147483647;
	or.b32  	%r17, %r16, %r15;
	mov.b64 	%fd3, {%r13, %r17};
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB78_2:
	ret;
}

	// .globl	vec_fdim
.visible .entry vec_fdim(
	.param .u32 vec_fdim_param_0,
	.param .u64 vec_fdim_param_1,
	.param .u64 vec_fdim_param_2,
	.param .u64 vec_fdim_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fdim_param_0];
	ld.param.u64 	%rd1, [vec_fdim_param_1];
	ld.param.u64 	%rd2, [vec_fdim_param_2];
	ld.param.u64 	%rd3, [vec_fdim_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB79_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	sub.f64 	%fd3, %fd2, %fd1;
	setp.gtu.f64	%p2, %fd2, %fd1;
	selp.f64	%fd4, %fd3, 0d0000000000000000, %p2;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd4;

BB79_2:
	ret;
}

	// .globl	vec_fdivide
.visible .entry vec_fdivide(
	.param .u32 vec_fdivide_param_0,
	.param .u64 vec_fdivide_param_1,
	.param .u64 vec_fdivide_param_2,
	.param .u64 vec_fdivide_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fdivide_param_0];
	ld.param.u64 	%rd1, [vec_fdivide_param_1];
	ld.param.u64 	%rd2, [vec_fdivide_param_2];
	ld.param.u64 	%rd3, [vec_fdivide_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB80_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB80_2:
	ret;
}

	// .globl	vec_fmax
.visible .entry vec_fmax(
	.param .u32 vec_fmax_param_0,
	.param .u64 vec_fmax_param_1,
	.param .u64 vec_fmax_param_2,
	.param .u64 vec_fmax_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fmax_param_0];
	ld.param.u64 	%rd1, [vec_fmax_param_1];
	ld.param.u64 	%rd2, [vec_fmax_param_2];
	ld.param.u64 	%rd3, [vec_fmax_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB81_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	max.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB81_2:
	ret;
}

	// .globl	vec_fmin
.visible .entry vec_fmin(
	.param .u32 vec_fmin_param_0,
	.param .u64 vec_fmin_param_1,
	.param .u64 vec_fmin_param_2,
	.param .u64 vec_fmin_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_fmin_param_0];
	ld.param.u64 	%rd1, [vec_fmin_param_1];
	ld.param.u64 	%rd2, [vec_fmin_param_2];
	ld.param.u64 	%rd3, [vec_fmin_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB82_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd1, [%rd8];
	ld.global.f64 	%fd2, [%rd6];
	min.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd3;

BB82_2:
	ret;
}

	// .globl	vec_fmod
.visible .entry vec_fmod(
	.param .u32 vec_fmod_param_0,
	.param .u64 vec_fmod_param_1,
	.param .u64 vec_fmod_param_2,
	.param .u64 vec_fmod_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<34>;
	.reg .f64 	%fd<36>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r9, [vec_fmod_param_0];
	ld.param.u64 	%rd2, [vec_fmod_param_1];
	ld.param.u64 	%rd3, [vec_fmod_param_2];
	ld.param.u64 	%rd4, [vec_fmod_param_3];
	mov.u32 	%r10, %tid.x;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %nctaid.x;
	mov.u32 	%r16, %ctaid.x;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %ntid.x;
	mad.lo.s32 	%r1, %r17, %r18, %r10;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB83_18;

	cvta.to.global.u64 	%rd5, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r19, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r20, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd9];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd2;
	}
	and.b32  	%r32, %r21, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd2;
	}
	mov.b64 	%fd34, {%r20, %r19};
	mov.b64 	%fd4, {%r33, %r32};
	setp.gt.u32	%p2, %r19, 2146435071;
	setp.gt.u32	%p3, %r32, 2146435071;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB83_14;
	bra.uni 	BB83_2;

BB83_14:
	setp.gtu.f64	%p17, %fd34, 0d7FF0000000000000;
	setp.gtu.f64	%p18, %fd4, 0d7FF0000000000000;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	BB83_16;
	bra.uni 	BB83_15;

BB83_16:
	add.f64 	%fd35, %fd1, %fd2;
	bra.uni 	BB83_17;

BB83_2:
	setp.eq.f64	%p5, %fd4, 0d0000000000000000;
	mov.f64 	%fd21, 0dFFF8000000000000;
	mov.f64 	%fd35, %fd21;
	@%p5 bra 	BB83_17;

	setp.ltu.f64	%p6, %fd34, %fd4;
	mov.f64 	%fd35, %fd1;
	@%p6 bra 	BB83_17;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd4;
	}
	mov.f64 	%fd30, 0d0000000000000000;
	setp.gt.s32	%p7, %r22, 1048575;
	@%p7 bra 	BB83_9;

	setp.geu.f64	%p8, %fd4, %fd34;
	mov.f64 	%fd31, %fd4;
	@%p8 bra 	BB83_8;

	mov.f64 	%fd32, %fd4;

BB83_7:
	add.f64 	%fd32, %fd32, %fd32;
	setp.lt.f64	%p9, %fd32, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd32;
	}
	setp.lt.s32	%p10, %r23, 1048576;
	and.pred  	%p11, %p9, %p10;
	mov.f64 	%fd26, %fd32;
	mov.f64 	%fd31, %fd26;
	@%p11 bra 	BB83_7;

BB83_8:
	mov.f64 	%fd27, %fd31;
	mov.f64 	%fd30, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd30;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd30;
	}

BB83_9:
	mov.f64 	%fd29, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd34;
	}
	setp.lt.s32	%p12, %r24, 1048576;
	@%p12 bra 	BB83_11;

	and.b32  	%r25, %r32, 1048575;
	and.b32  	%r26, %r2, 2146435072;
	or.b32  	%r27, %r25, %r26;
	mov.b64 	%fd29, {%r33, %r27};

BB83_11:
	mul.f64 	%fd23, %fd29, 0d3FE0000000000000;
	setp.gt.f64	%p13, %fd29, %fd34;
	selp.f64	%fd33, %fd23, %fd29, %p13;
	setp.ltu.f64	%p14, %fd33, %fd4;
	@%p14 bra 	BB83_13;

BB83_12:
	sub.f64 	%fd24, %fd34, %fd33;
	setp.ltu.f64	%p15, %fd34, %fd33;
	selp.f64	%fd34, %fd34, %fd24, %p15;
	mul.f64 	%fd33, %fd33, 0d3FE0000000000000;
	setp.ge.f64	%p16, %fd33, %fd4;
	@%p16 bra 	BB83_12;

BB83_13:
	and.b32  	%r28, %r2, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd34;
	}
	or.b32  	%r30, %r29, %r28;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd34;
	}
	mov.b64 	%fd35, {%r31, %r30};
	bra.uni 	BB83_17;

BB83_15:
	setp.eq.f64	%p20, %fd34, 0d7FF0000000000000;
	selp.f64	%fd35, 0dFFF8000000000000, %fd1, %p20;

BB83_17:
	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 3;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd35;

BB83_18:
	ret;
}

	// .globl	vec_hypot
.visible .entry vec_hypot(
	.param .u32 vec_hypot_param_0,
	.param .u64 vec_hypot_param_1,
	.param .u64 vec_hypot_param_2,
	.param .u64 vec_hypot_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r2, [vec_hypot_param_0];
	ld.param.u64 	%rd1, [vec_hypot_param_1];
	ld.param.u64 	%rd2, [vec_hypot_param_2];
	ld.param.u64 	%rd3, [vec_hypot_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB84_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd3, [%rd6];
	abs.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd8];
	abs.f64 	%fd6, %fd5;
	mov.b64 	 %rd9, %fd6;
	mov.b64 	 %rd10, %fd4;
	min.u64 	%rd11, %rd9, %rd10;
	mov.b64 	 %fd7, %rd11;
	max.u64 	%rd12, %rd10, %rd9;
	mov.b64 	 %fd8, %rd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd8;
	}
	and.b32  	%r13, %r12, -4194304;
	mov.u32 	%r14, 2144337920;
	sub.s32 	%r15, %r14, %r13;
	mov.u32 	%r16, 0;
	mov.b64 	%fd9, {%r16, %r15};
	mul.f64 	%fd10, %fd7, %fd9;
	mul.f64 	%fd11, %fd8, %fd9;
	mul.f64 	%fd12, %fd10, %fd10;
	fma.rn.f64 	%fd13, %fd11, %fd11, %fd12;
	mov.f64 	%fd14, 0d7FEFFFFFFFFFFFFF;
	min.f64 	%fd2, %fd13, %fd14;
	// inline asm
	rsqrt.approx.ftz.f64 %fd1, %fd2;
	// inline asm
	mul.rn.f64 	%fd15, %fd1, %fd1;
	neg.f64 	%fd16, %fd15;
	mov.f64 	%fd17, 0d3FF0000000000000;
	fma.rn.f64 	%fd18, %fd2, %fd16, %fd17;
	mov.f64 	%fd19, 0d3FE0000000000000;
	mov.f64 	%fd20, 0d3FD8000000000000;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mul.rn.f64 	%fd22, %fd18, %fd1;
	fma.rn.f64 	%fd23, %fd21, %fd22, %fd1;
	mul.f64 	%fd24, %fd13, %fd23;
	add.s32 	%r17, %r13, 1048576;
	mov.b64 	%fd25, {%r16, %r17};
	mul.f64 	%fd26, %fd24, %fd25;
	setp.eq.f64	%p2, %fd7, 0d0000000000000000;
	selp.f64	%fd27, %fd8, %fd26, %p2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd7;
	}
	mov.f64 	%fd28, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd28;
	}
	setp.lt.u32	%p3, %r18, %r19;
	selp.f64	%fd29, %fd27, %fd7, %p3;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd5;
	st.global.f64 	[%rd14], %fd29;

BB84_2:
	ret;
}

	// .globl	vec_nextafter
.visible .entry vec_nextafter(
	.param .u32 vec_nextafter_param_0,
	.param .u64 vec_nextafter_param_1,
	.param .u64 vec_nextafter_param_2,
	.param .u64 vec_nextafter_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<26>;


	ld.param.u32 	%r2, [vec_nextafter_param_0];
	ld.param.u64 	%rd3, [vec_nextafter_param_1];
	ld.param.u64 	%rd4, [vec_nextafter_param_2];
	ld.param.u64 	%rd5, [vec_nextafter_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB85_9;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd5;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	mov.b64 	 %rd2, %fd1;
	ld.global.f64 	%fd10, [%rd10];
	abs.f64 	%fd7, %fd1;
	setp.gtu.f64	%p2, %fd7, 0d7FF0000000000000;
	@%p2 bra 	BB85_7;

	abs.f64 	%fd8, %fd10;
	setp.gtu.f64	%p3, %fd8, 0d7FF0000000000000;
	@%p3 bra 	BB85_7;
	bra.uni 	BB85_3;

BB85_7:
	add.f64 	%fd10, %fd1, %fd10;

BB85_8:
	cvta.to.global.u64 	%rd23, %rd3;
	shl.b64 	%rd24, %rd1, 3;
	add.s64 	%rd25, %rd23, %rd24;
	st.global.f64 	[%rd25], %fd10;

BB85_9:
	ret;

BB85_3:
	mov.b64 	 %rd11, %fd10;
	or.b64  	%rd12, %rd11, %rd2;
	and.b64  	%rd13, %rd12, 9223372036854775807;
	setp.eq.s64	%p4, %rd13, 0;
	@%p4 bra 	BB85_8;

	shl.b64 	%rd14, %rd2, 1;
	setp.eq.s64	%p5, %rd14, 0;
	@%p5 bra 	BB85_6;
	bra.uni 	BB85_5;

BB85_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd10;
	}
	and.b32  	%r13, %r12, -2147483648;
	mov.f64 	%fd9, 0d0000000000000001;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd9;
	}
	or.b32  	%r15, %r14, %r13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r16, %temp}, %fd9;
	}
	mov.b64 	%fd10, {%r16, %r15};
	bra.uni 	BB85_8;

BB85_5:
	setp.lt.f64	%p6, %fd1, %fd10;
	setp.lt.f64	%p7, %fd1, 0d0000000000000000;
	and.pred  	%p8, %p6, %p7;
	selp.s64	%rd15, -1, 0, %p8;
	add.s64 	%rd16, %rd15, %rd2;
	setp.gt.f64	%p9, %fd1, 0d0000000000000000;
	and.pred  	%p10, %p6, %p9;
	selp.u64	%rd17, 1, 0, %p10;
	add.s64 	%rd18, %rd16, %rd17;
	setp.gt.f64	%p11, %fd1, %fd10;
	and.pred  	%p12, %p11, %p7;
	selp.u64	%rd19, 1, 0, %p12;
	add.s64 	%rd20, %rd18, %rd19;
	and.pred  	%p13, %p11, %p9;
	selp.s64	%rd21, -1, 0, %p13;
	add.s64 	%rd22, %rd20, %rd21;
	mov.b64 	 %fd10, %rd22;
	bra.uni 	BB85_8;
}

	// .globl	vec_pow
.visible .entry vec_pow(
	.param .u32 vec_pow_param_0,
	.param .u64 vec_pow_param_1,
	.param .u64 vec_pow_param_2,
	.param .u64 vec_pow_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r4, [vec_pow_param_0];
	ld.param.u64 	%rd3, [vec_pow_param_1];
	ld.param.u64 	%rd4, [vec_pow_param_2];
	ld.param.u64 	%rd5, [vec_pow_param_3];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p2, %r1, %r4;
	@%p2 bra 	BB86_15;

	cvta.to.global.u64 	%rd6, %rd4;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd5;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd10];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd2;
	}
	bfe.u32 	%r14, %r3, 20, 11;
	add.s32 	%r15, %r14, -1012;
	mov.b64 	 %rd11, %fd2;
	shl.b64 	%rd2, %rd11, %r15;
	setp.eq.s64	%p3, %rd2, -9223372036854775808;
	abs.f64 	%fd3, %fd1;
	// Callseq Start 30
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd2;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd23, [retval0+0];
	
	//{
	}// Callseq End 30
	setp.lt.s32	%p4, %r2, 0;
	and.pred  	%p1, %p4, %p3;
	@!%p1 bra 	BB86_3;
	bra.uni 	BB86_2;

BB86_2:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd23;
	}
	xor.b32  	%r17, %r16, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd23;
	}
	mov.b64 	%fd23, {%r18, %r17};

BB86_3:
	mov.f64 	%fd22, %fd23;
	setp.eq.f64	%p5, %fd1, 0d0000000000000000;
	@%p5 bra 	BB86_6;
	bra.uni 	BB86_4;

BB86_6:
	selp.b32	%r19, %r2, 0, %p3;
	or.b32  	%r20, %r19, 2146435072;
	setp.lt.s32	%p9, %r3, 0;
	selp.b32	%r21, %r20, %r19, %p9;
	mov.u32 	%r22, 0;
	mov.b64 	%fd22, {%r22, %r21};
	bra.uni 	BB86_7;

BB86_4:
	setp.gt.s32	%p6, %r2, -1;
	@%p6 bra 	BB86_7;

	cvt.rzi.f64.f64	%fd15, %fd2;
	setp.neu.f64	%p7, %fd15, %fd2;
	selp.f64	%fd22, 0dFFF8000000000000, %fd22, %p7;

BB86_7:
	mov.f64 	%fd9, %fd22;
	add.f64 	%fd10, %fd1, %fd2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd10;
	}
	and.b32  	%r24, %r23, 2146435072;
	setp.ne.s32	%p10, %r24, 2146435072;
	mov.f64 	%fd21, %fd9;
	@%p10 bra 	BB86_14;

	setp.gtu.f64	%p11, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd21, %fd10;
	@%p11 bra 	BB86_14;

	abs.f64 	%fd11, %fd2;
	setp.gtu.f64	%p12, %fd11, 0d7FF0000000000000;
	mov.f64 	%fd20, %fd10;
	mov.f64 	%fd21, %fd20;
	@%p12 bra 	BB86_14;

	setp.eq.f64	%p13, %fd11, 0d7FF0000000000000;
	@%p13 bra 	BB86_13;
	bra.uni 	BB86_11;

BB86_13:
	setp.gt.f64	%p15, %fd3, 0d3FF0000000000000;
	selp.b32	%r31, 2146435072, 0, %p15;
	xor.b32  	%r32, %r31, 2146435072;
	setp.lt.s32	%p16, %r3, 0;
	selp.b32	%r33, %r32, %r31, %p16;
	setp.eq.f64	%p17, %fd1, 0dBFF0000000000000;
	selp.b32	%r34, 1072693248, %r33, %p17;
	mov.u32 	%r35, 0;
	mov.b64 	%fd21, {%r35, %r34};
	bra.uni 	BB86_14;

BB86_11:
	setp.neu.f64	%p14, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd21, %fd9;
	@%p14 bra 	BB86_14;

	shr.s32 	%r25, %r3, 31;
	and.b32  	%r26, %r25, -2146435072;
	add.s32 	%r27, %r26, 2146435072;
	or.b32  	%r28, %r27, -2147483648;
	selp.b32	%r29, %r28, %r27, %p1;
	mov.u32 	%r30, 0;
	mov.b64 	%fd21, {%r30, %r29};

BB86_14:
	setp.eq.f64	%p18, %fd2, 0d0000000000000000;
	setp.eq.f64	%p19, %fd1, 0d3FF0000000000000;
	or.pred  	%p20, %p19, %p18;
	selp.f64	%fd16, 0d3FF0000000000000, %fd21, %p20;
	cvta.to.global.u64 	%rd12, %rd3;
	shl.b64 	%rd13, %rd1, 3;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd16;

BB86_15:
	ret;
}

	// .globl	vec_remainder
.visible .entry vec_remainder(
	.param .u32 vec_remainder_param_0,
	.param .u64 vec_remainder_param_1,
	.param .u64 vec_remainder_param_2,
	.param .u64 vec_remainder_param_3
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<46>;
	.reg .f64 	%fd<39>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r13, [vec_remainder_param_0];
	ld.param.u64 	%rd2, [vec_remainder_param_1];
	ld.param.u64 	%rd3, [vec_remainder_param_2];
	ld.param.u64 	%rd4, [vec_remainder_param_3];
	mov.u32 	%r14, %tid.x;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %nctaid.x;
	mov.u32 	%r20, %ctaid.x;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r1, %r21, %r22, %r14;
	setp.ge.s32	%p3, %r1, %r13;
	@%p3 bra 	BB87_21;

	cvta.to.global.u64 	%rd5, %rd3;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd4;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r23, %r2, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd1;
	}
	ld.global.f64 	%fd2, [%rd9];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd2;
	}
	and.b32  	%r42, %r25, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd2;
	}
	mov.b64 	%fd37, {%r24, %r23};
	mov.b64 	%fd4, {%r43, %r42};
	setp.gt.u32	%p4, %r23, 2146435071;
	setp.gt.u32	%p5, %r42, 2146435071;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB87_17;
	bra.uni 	BB87_2;

BB87_17:
	setp.gtu.f64	%p23, %fd37, 0d7FF0000000000000;
	setp.gtu.f64	%p24, %fd4, 0d7FF0000000000000;
	or.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB87_19;
	bra.uni 	BB87_18;

BB87_19:
	add.f64 	%fd38, %fd1, %fd2;
	bra.uni 	BB87_20;

BB87_2:
	setp.eq.f64	%p7, %fd4, 0d0000000000000000;
	mov.f64 	%fd38, 0dFFF8000000000000;
	@%p7 bra 	BB87_20;

	setp.ltu.f64	%p8, %fd37, %fd4;
	mov.u32 	%r45, 0;
	@%p8 bra 	BB87_14;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd4;
	}
	mov.f64 	%fd33, 0d0000000000000000;
	setp.gt.s32	%p9, %r27, 1048575;
	@%p9 bra 	BB87_9;

	setp.geu.f64	%p10, %fd4, %fd37;
	mov.f64 	%fd34, %fd4;
	@%p10 bra 	BB87_8;

	mov.f64 	%fd35, %fd4;

BB87_7:
	add.f64 	%fd35, %fd35, %fd35;
	setp.lt.f64	%p11, %fd35, %fd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd35;
	}
	setp.lt.s32	%p12, %r28, 1048576;
	and.pred  	%p13, %p11, %p12;
	mov.f64 	%fd29, %fd35;
	mov.f64 	%fd34, %fd29;
	@%p13 bra 	BB87_7;

BB87_8:
	mov.f64 	%fd30, %fd34;
	mov.f64 	%fd33, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd33;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd33;
	}

BB87_9:
	mov.f64 	%fd32, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd37;
	}
	setp.lt.s32	%p14, %r29, 1048576;
	@%p14 bra 	BB87_11;

	and.b32  	%r30, %r42, 1048575;
	and.b32  	%r31, %r2, 2146435072;
	or.b32  	%r32, %r30, %r31;
	mov.b64 	%fd32, {%r43, %r32};

BB87_11:
	mul.f64 	%fd24, %fd32, 0d3FE0000000000000;
	setp.gt.f64	%p15, %fd32, %fd37;
	selp.f64	%fd36, %fd24, %fd32, %p15;
	setp.ltu.f64	%p16, %fd36, %fd4;
	mov.u32 	%r44, -1;
	@%p16 bra 	BB87_14;

BB87_12:
	setp.ltu.f64	%p17, %fd37, %fd36;
	selp.u32	%r35, 1, 0, %p17;
	shl.b32 	%r36, %r44, 1;
	add.s32 	%r44, %r35, %r36;
	sub.f64 	%fd25, %fd37, %fd36;
	selp.f64	%fd37, %fd37, %fd25, %p17;
	mul.f64 	%fd36, %fd36, 0d3FE0000000000000;
	setp.ge.f64	%p18, %fd36, %fd4;
	@%p18 bra 	BB87_12;

	not.b32 	%r37, %r44;
	and.b32  	%r45, %r37, 1;

BB87_14:
	add.f64 	%fd17, %fd37, %fd37;
	setp.gt.f64	%p20, %fd17, %fd4;
	mov.pred 	%p27, -1;
	@%p20 bra 	BB87_16;

	setp.eq.f64	%p21, %fd17, %fd4;
	setp.ne.s32	%p22, %r45, 0;
	and.pred  	%p27, %p21, %p22;

BB87_16:
	sub.f64 	%fd26, %fd37, %fd4;
	selp.f64	%fd27, %fd26, %fd37, %p27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd27;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd27;
	}
	and.b32  	%r40, %r2, -2147483648;
	xor.b32  	%r41, %r38, %r40;
	mov.b64 	%fd38, {%r39, %r41};
	bra.uni 	BB87_20;

BB87_18:
	setp.eq.f64	%p26, %fd37, 0d7FF0000000000000;
	selp.f64	%fd38, 0dFFF8000000000000, %fd1, %p26;

BB87_20:
	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 3;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd38;

BB87_21:
	ret;
}

	// .globl	vec_testkernel
.visible .entry vec_testkernel(
	.param .u32 vec_testkernel_param_0,
	.param .u64 vec_testkernel_param_1,
	.param .u64 vec_testkernel_param_2,
	.param .u64 vec_testkernel_param_3
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<23>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd4, [vec_testkernel_param_1];
	ld.param.u64 	%rd5, [vec_testkernel_param_2];
	ld.param.u64 	%rd6, [vec_testkernel_param_3];
	cvta.to.global.u64 	%rd7, %rd4;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %nctaid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r16;
	cvta.to.global.u64 	%rd8, %rd5;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd1, %rd8, %rd9;
	cvta.to.global.u64 	%rd10, %rd6;
	add.s64 	%rd2, %rd10, %rd9;
	abs.s32 	%r17, %r1;
	and.b32  	%r18, %r17, 1;
	setp.eq.b32	%p1, %r18, 1;
	not.pred 	%p2, %p1;
	selp.f64	%fd1, 0d3FF0000000000000, 0d400921F9F01B866E, %p2;
	shr.u32 	%r2, %r17, 1;
	add.s64 	%rd3, %rd7, %rd9;
	mov.u32 	%r20, 0;

BB88_1:
	ld.global.f64 	%fd11, [%rd2];
	ld.global.f64 	%fd12, [%rd1];
	mul.f64 	%fd2, %fd12, %fd11;
	setp.eq.s32	%p3, %r2, 0;
	mov.f64 	%fd16, 0d400921F9F01B866E;
	mov.u32 	%r21, %r2;
	mov.f64 	%fd21, %fd1;
	mov.f64 	%fd22, %fd1;
	@%p3 bra 	BB88_3;

BB88_2:
	mov.f64 	%fd3, %fd22;
	mov.u32 	%r4, %r21;
	and.b32  	%r19, %r4, 1;
	setp.eq.b32	%p4, %r19, 1;
	not.pred 	%p5, %p4;
	mul.f64 	%fd16, %fd16, %fd16;
	mul.f64 	%fd13, %fd3, %fd16;
	selp.f64	%fd6, %fd3, %fd13, %p5;
	shr.u32 	%r5, %r4, 1;
	setp.ne.s32	%p6, %r5, 0;
	mov.u32 	%r21, %r5;
	mov.f64 	%fd21, %fd6;
	mov.f64 	%fd22, %fd6;
	@%p6 bra 	BB88_2;

BB88_3:
	mov.f64 	%fd20, %fd21;
	setp.gt.s32	%p7, %r1, -1;
	@%p7 bra 	BB88_5;

	rcp.rn.f64 	%fd20, %fd20;

BB88_5:
	sqrt.rn.f64 	%fd14, %fd20;
	add.f64 	%fd15, %fd2, %fd14;
	st.global.f64 	[%rd3], %fd15;
	add.s32 	%r20, %r20, 1;
	setp.lt.s32	%p8, %r20, 100;
	@%p8 bra 	BB88_1;

	ret;
}

	// .globl	vec_computePSF_phase
.visible .entry vec_computePSF_phase(
	.param .u32 vec_computePSF_phase_param_0,
	.param .u64 vec_computePSF_phase_param_1,
	.param .u64 vec_computePSF_phase_param_2,
	.param .u64 vec_computePSF_phase_param_3,
	.param .u64 vec_computePSF_phase_param_4,
	.param .u64 vec_computePSF_phase_param_5,
	.param .u64 vec_computePSF_phase_param_6,
	.param .u64 vec_computePSF_phase_param_7,
	.param .f64 vec_computePSF_phase_param_8,
	.param .f64 vec_computePSF_phase_param_9,
	.param .f64 vec_computePSF_phase_param_10
)
{
	.local .align 4 .b8 	__local_depot89[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<51>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<42>;


	mov.u64 	%rd41, __local_depot89;
	cvta.local.u64 	%SP, %rd41;
	ld.param.u32 	%r9, [vec_computePSF_phase_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_phase_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_phase_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_phase_param_3];
	ld.param.u64 	%rd4, [vec_computePSF_phase_param_4];
	ld.param.u64 	%rd5, [vec_computePSF_phase_param_5];
	ld.param.u64 	%rd6, [vec_computePSF_phase_param_6];
	ld.param.u64 	%rd7, [vec_computePSF_phase_param_7];
	ld.param.f64 	%fd29, [vec_computePSF_phase_param_8];
	ld.param.f64 	%fd30, [vec_computePSF_phase_param_9];
	ld.param.f64 	%fd31, [vec_computePSF_phase_param_10];
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB89_18;

	cvta.to.global.u64 	%rd8, %rd6;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r1, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd32, [%rd11];
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f64 	%fd33, [%rd13];
	mul.f64 	%fd34, %fd33, %fd30;
	fma.rn.f64 	%fd35, %fd32, %fd29, %fd34;
	cvta.to.global.u64 	%rd14, %rd5;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.f64 	%fd36, [%rd15];
	fma.rn.f64 	%fd37, %fd36, %fd31, %fd35;
	cvta.to.global.u64 	%rd16, %rd7;
	add.s64 	%rd17, %rd16, %rd10;
	ld.global.f64 	%fd38, [%rd17];
	add.f64 	%fd1, %fd38, %fd37;
	add.s64 	%rd18, %rd8, %rd10;
	ld.global.f64 	%fd2, [%rd18];
	abs.f64 	%fd3, %fd1;
	setp.neu.f64	%p2, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd93, %fd1;
	@%p2 bra 	BB89_3;

	mov.f64 	%fd39, 0d0000000000000000;
	mul.rn.f64 	%fd4, %fd1, %fd39;
	mov.f64 	%fd93, %fd4;

BB89_3:
	mov.f64 	%fd5, %fd93;
	mul.f64 	%fd40, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r49, %fd40;
	add.u64 	%rd19, %SP, 0;
	cvta.to.local.u64 	%rd20, %rd19;
	st.local.u32 	[%rd20], %r49;
	cvt.rn.f64.s32	%fd41, %r49;
	neg.f64 	%fd42, %fd41;
	mov.f64 	%fd43, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd44, %fd42, %fd43, %fd5;
	mov.f64 	%fd45, 0d3C91A62633145C00;
	fma.rn.f64 	%fd46, %fd42, %fd45, %fd44;
	mov.f64 	%fd47, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd42, %fd47, %fd46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd5;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p3, %r20, 1105199104;
	@%p3 bra 	BB89_5;

	// Callseq Start 31
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 31
	ld.local.u32 	%r49, [%rd20];

BB89_5:
	add.s32 	%r5, %r49, 1;
	and.b32  	%r21, %r5, 1;
	shl.b32 	%r22, %r21, 3;
	setp.eq.s32	%p4, %r21, 0;
	selp.f64	%fd48, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd23, %r22, 8;
	mov.u64 	%rd24, __cudart_sin_cos_coeffs;
	add.s64 	%rd25, %rd23, %rd24;
	ld.const.f64 	%fd49, [%rd25+8];
	mul.rn.f64 	%fd9, %fd89, %fd89;
	fma.rn.f64 	%fd50, %fd48, %fd9, %fd49;
	ld.const.f64 	%fd51, [%rd25+16];
	fma.rn.f64 	%fd52, %fd50, %fd9, %fd51;
	ld.const.f64 	%fd53, [%rd25+24];
	fma.rn.f64 	%fd54, %fd52, %fd9, %fd53;
	ld.const.f64 	%fd55, [%rd25+32];
	fma.rn.f64 	%fd56, %fd54, %fd9, %fd55;
	ld.const.f64 	%fd57, [%rd25+40];
	fma.rn.f64 	%fd58, %fd56, %fd9, %fd57;
	ld.const.f64 	%fd59, [%rd25+48];
	fma.rn.f64 	%fd10, %fd58, %fd9, %fd59;
	fma.rn.f64 	%fd90, %fd10, %fd89, %fd89;
	@%p4 bra 	BB89_7;

	mov.f64 	%fd60, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd10, %fd9, %fd60;

BB89_7:
	and.b32  	%r23, %r5, 2;
	setp.eq.s32	%p5, %r23, 0;
	@%p5 bra 	BB89_9;

	mov.f64 	%fd61, 0d0000000000000000;
	mov.f64 	%fd62, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd62, %fd61;

BB89_9:
	cvta.to.global.u64 	%rd26, %rd1;
	add.s64 	%rd28, %rd26, %rd10;
	mul.f64 	%fd63, %fd2, %fd90;
	st.global.f64 	[%rd28], %fd63;
	ld.global.f64 	%fd16, [%rd18];
	mov.f64 	%fd92, %fd1;
	@%p2 bra 	BB89_11;

	mov.f64 	%fd64, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd1, %fd64;

BB89_11:
	mul.f64 	%fd65, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r50, %fd65;
	add.u64 	%rd31, %SP, 4;
	cvta.to.local.u64 	%rd32, %rd31;
	st.local.u32 	[%rd32], %r50;
	cvt.rn.f64.s32	%fd66, %r50;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd69, %fd67, %fd43, %fd92;
	fma.rn.f64 	%fd71, %fd67, %fd45, %fd69;
	fma.rn.f64 	%fd94, %fd67, %fd47, %fd71;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd92;
	}
	and.b32  	%r35, %r34, 2145386496;
	setp.lt.u32	%p7, %r35, 1105199104;
	@%p7 bra 	BB89_13;

	// Callseq Start 32
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd31;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd94, [retval0+0];
	
	//{
	}// Callseq End 32
	ld.local.u32 	%r50, [%rd32];

BB89_13:
	and.b32  	%r36, %r50, 1;
	shl.b32 	%r37, %r36, 3;
	setp.eq.s32	%p8, %r36, 0;
	selp.f64	%fd73, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p8;
	mul.wide.u32 	%rd35, %r37, 8;
	add.s64 	%rd37, %rd35, %rd24;
	ld.const.f64 	%fd74, [%rd37+8];
	mul.rn.f64 	%fd22, %fd94, %fd94;
	fma.rn.f64 	%fd75, %fd73, %fd22, %fd74;
	ld.const.f64 	%fd76, [%rd37+16];
	fma.rn.f64 	%fd77, %fd75, %fd22, %fd76;
	ld.const.f64 	%fd78, [%rd37+24];
	fma.rn.f64 	%fd79, %fd77, %fd22, %fd78;
	ld.const.f64 	%fd80, [%rd37+32];
	fma.rn.f64 	%fd81, %fd79, %fd22, %fd80;
	ld.const.f64 	%fd82, [%rd37+40];
	fma.rn.f64 	%fd83, %fd81, %fd22, %fd82;
	ld.const.f64 	%fd84, [%rd37+48];
	fma.rn.f64 	%fd23, %fd83, %fd22, %fd84;
	fma.rn.f64 	%fd95, %fd23, %fd94, %fd94;
	@%p8 bra 	BB89_15;

	mov.f64 	%fd85, 0d3FF0000000000000;
	fma.rn.f64 	%fd95, %fd23, %fd22, %fd85;

BB89_15:
	and.b32  	%r38, %r50, 2;
	setp.eq.s32	%p9, %r38, 0;
	@%p9 bra 	BB89_17;

	mov.f64 	%fd86, 0d0000000000000000;
	mov.f64 	%fd87, 0dBFF0000000000000;
	fma.rn.f64 	%fd95, %fd95, %fd87, %fd86;

BB89_17:
	cvta.to.global.u64 	%rd38, %rd2;
	add.s64 	%rd40, %rd38, %rd10;
	mul.f64 	%fd88, %fd16, %fd95;
	st.global.f64 	[%rd40], %fd88;

BB89_18:
	ret;
}

	// .globl	vec_computePSF_phaseN
.visible .entry vec_computePSF_phaseN(
	.param .u32 vec_computePSF_phaseN_param_0,
	.param .u64 vec_computePSF_phaseN_param_1,
	.param .u64 vec_computePSF_phaseN_param_2,
	.param .u64 vec_computePSF_phaseN_param_3,
	.param .u64 vec_computePSF_phaseN_param_4,
	.param .u64 vec_computePSF_phaseN_param_5,
	.param .f64 vec_computePSF_phaseN_param_6,
	.param .f64 vec_computePSF_phaseN_param_7,
	.param .f64 vec_computePSF_phaseN_param_8,
	.param .u64 vec_computePSF_phaseN_param_9,
	.param .u64 vec_computePSF_phaseN_param_10,
	.param .u64 vec_computePSF_phaseN_param_11
)
{
	.local .align 4 .b8 	__local_depot90[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<50>;


	mov.u64 	%rd49, __local_depot90;
	cvta.local.u64 	%SP, %rd49;
	ld.param.u32 	%r9, [vec_computePSF_phaseN_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_phaseN_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_phaseN_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_phaseN_param_3];
	ld.param.u64 	%rd4, [vec_computePSF_phaseN_param_4];
	ld.param.u64 	%rd5, [vec_computePSF_phaseN_param_5];
	ld.param.f64 	%fd29, [vec_computePSF_phaseN_param_6];
	ld.param.f64 	%fd30, [vec_computePSF_phaseN_param_7];
	ld.param.f64 	%fd31, [vec_computePSF_phaseN_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_phaseN_param_9];
	ld.param.u64 	%rd7, [vec_computePSF_phaseN_param_10];
	ld.param.u64 	%rd8, [vec_computePSF_phaseN_param_11];
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB90_18;

	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r1, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f64 	%fd32, [%rd12];
	cvta.to.global.u64 	%rd13, %rd2;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.f64 	%fd33, [%rd14];
	mul.f64 	%fd34, %fd33, %fd30;
	fma.rn.f64 	%fd35, %fd32, %fd29, %fd34;
	cvta.to.global.u64 	%rd15, %rd3;
	add.s64 	%rd16, %rd15, %rd11;
	ld.global.f64 	%fd36, [%rd16];
	fma.rn.f64 	%fd37, %fd36, %fd31, %fd35;
	cvta.to.global.u64 	%rd17, %rd5;
	add.s64 	%rd18, %rd17, %rd11;
	ld.global.f64 	%fd38, [%rd18];
	add.f64 	%fd1, %fd38, %fd37;
	add.s64 	%rd19, %rd9, %rd11;
	ld.global.f64 	%fd2, [%rd19];
	abs.f64 	%fd3, %fd1;
	setp.neu.f64	%p2, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd93, %fd1;
	@%p2 bra 	BB90_3;

	mov.f64 	%fd39, 0d0000000000000000;
	mul.rn.f64 	%fd4, %fd1, %fd39;
	mov.f64 	%fd93, %fd4;

BB90_3:
	mov.f64 	%fd5, %fd93;
	mul.f64 	%fd40, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r51, %fd40;
	add.u64 	%rd20, %SP, 0;
	cvta.to.local.u64 	%rd21, %rd20;
	st.local.u32 	[%rd21], %r51;
	cvt.rn.f64.s32	%fd41, %r51;
	neg.f64 	%fd42, %fd41;
	mov.f64 	%fd43, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd44, %fd42, %fd43, %fd5;
	mov.f64 	%fd45, 0d3C91A62633145C00;
	fma.rn.f64 	%fd46, %fd42, %fd45, %fd44;
	mov.f64 	%fd47, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd42, %fd47, %fd46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd5;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p3, %r20, 1105199104;
	@%p3 bra 	BB90_5;

	// Callseq Start 33
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd20;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 33
	ld.local.u32 	%r51, [%rd21];

BB90_5:
	add.s32 	%r5, %r51, 1;
	and.b32  	%r21, %r5, 1;
	shl.b32 	%r22, %r21, 3;
	setp.eq.s32	%p4, %r21, 0;
	selp.f64	%fd48, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd24, %r22, 8;
	mov.u64 	%rd25, __cudart_sin_cos_coeffs;
	add.s64 	%rd26, %rd24, %rd25;
	ld.const.f64 	%fd49, [%rd26+8];
	mul.rn.f64 	%fd9, %fd89, %fd89;
	fma.rn.f64 	%fd50, %fd48, %fd9, %fd49;
	ld.const.f64 	%fd51, [%rd26+16];
	fma.rn.f64 	%fd52, %fd50, %fd9, %fd51;
	ld.const.f64 	%fd53, [%rd26+24];
	fma.rn.f64 	%fd54, %fd52, %fd9, %fd53;
	ld.const.f64 	%fd55, [%rd26+32];
	fma.rn.f64 	%fd56, %fd54, %fd9, %fd55;
	ld.const.f64 	%fd57, [%rd26+40];
	fma.rn.f64 	%fd58, %fd56, %fd9, %fd57;
	ld.const.f64 	%fd59, [%rd26+48];
	fma.rn.f64 	%fd10, %fd58, %fd9, %fd59;
	fma.rn.f64 	%fd90, %fd10, %fd89, %fd89;
	@%p4 bra 	BB90_7;

	mov.f64 	%fd60, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd10, %fd9, %fd60;

BB90_7:
	and.b32  	%r23, %r5, 2;
	setp.eq.s32	%p5, %r23, 0;
	@%p5 bra 	BB90_9;

	mov.f64 	%fd61, 0d0000000000000000;
	mov.f64 	%fd62, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd62, %fd61;

BB90_9:
	cvta.to.global.u64 	%rd27, %rd6;
	mul.wide.s32 	%rd28, %r1, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.u32 	%r34, [%rd29];
	cvta.to.global.u64 	%rd30, %rd8;
	mul.wide.s32 	%rd31, %r34, 8;
	add.s64 	%rd32, %rd30, %rd31;
	mul.f64 	%fd63, %fd2, %fd90;
	st.global.f64 	[%rd32], %fd63;
	ld.global.f64 	%fd16, [%rd19];
	mov.f64 	%fd92, %fd1;
	@%p2 bra 	BB90_11;

	mov.f64 	%fd64, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd1, %fd64;

BB90_11:
	mul.f64 	%fd65, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r52, %fd65;
	add.u64 	%rd36, %SP, 4;
	cvta.to.local.u64 	%rd37, %rd36;
	st.local.u32 	[%rd37], %r52;
	cvt.rn.f64.s32	%fd66, %r52;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd69, %fd67, %fd43, %fd92;
	fma.rn.f64 	%fd71, %fd67, %fd45, %fd69;
	fma.rn.f64 	%fd94, %fd67, %fd47, %fd71;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd92;
	}
	and.b32  	%r36, %r35, 2145386496;
	setp.lt.u32	%p7, %r36, 1105199104;
	@%p7 bra 	BB90_13;

	// Callseq Start 34
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd94, [retval0+0];
	
	//{
	}// Callseq End 34
	ld.local.u32 	%r52, [%rd37];

BB90_13:
	and.b32  	%r37, %r52, 1;
	shl.b32 	%r38, %r37, 3;
	setp.eq.s32	%p8, %r37, 0;
	selp.f64	%fd73, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p8;
	mul.wide.u32 	%rd40, %r38, 8;
	add.s64 	%rd42, %rd40, %rd25;
	ld.const.f64 	%fd74, [%rd42+8];
	mul.rn.f64 	%fd22, %fd94, %fd94;
	fma.rn.f64 	%fd75, %fd73, %fd22, %fd74;
	ld.const.f64 	%fd76, [%rd42+16];
	fma.rn.f64 	%fd77, %fd75, %fd22, %fd76;
	ld.const.f64 	%fd78, [%rd42+24];
	fma.rn.f64 	%fd79, %fd77, %fd22, %fd78;
	ld.const.f64 	%fd80, [%rd42+32];
	fma.rn.f64 	%fd81, %fd79, %fd22, %fd80;
	ld.const.f64 	%fd82, [%rd42+40];
	fma.rn.f64 	%fd83, %fd81, %fd22, %fd82;
	ld.const.f64 	%fd84, [%rd42+48];
	fma.rn.f64 	%fd23, %fd83, %fd22, %fd84;
	fma.rn.f64 	%fd95, %fd23, %fd94, %fd94;
	@%p8 bra 	BB90_15;

	mov.f64 	%fd85, 0d3FF0000000000000;
	fma.rn.f64 	%fd95, %fd23, %fd22, %fd85;

BB90_15:
	and.b32  	%r39, %r52, 2;
	setp.eq.s32	%p9, %r39, 0;
	@%p9 bra 	BB90_17;

	mov.f64 	%fd86, 0d0000000000000000;
	mov.f64 	%fd87, 0dBFF0000000000000;
	fma.rn.f64 	%fd95, %fd95, %fd87, %fd86;

BB90_17:
	cvta.to.global.u64 	%rd43, %rd7;
	add.s64 	%rd45, %rd43, %rd28;
	ld.global.u32 	%r50, [%rd45];
	mul.wide.s32 	%rd47, %r50, 8;
	add.s64 	%rd48, %rd30, %rd47;
	mul.f64 	%fd88, %fd16, %fd95;
	st.global.f64 	[%rd48], %fd88;

BB90_18:
	ret;
}

	// .globl	vec_computePSF_phaseNwithOil
.visible .entry vec_computePSF_phaseNwithOil(
	.param .u32 vec_computePSF_phaseNwithOil_param_0,
	.param .u64 vec_computePSF_phaseNwithOil_param_1,
	.param .u64 vec_computePSF_phaseNwithOil_param_2,
	.param .u64 vec_computePSF_phaseNwithOil_param_3,
	.param .u64 vec_computePSF_phaseNwithOil_param_4,
	.param .u64 vec_computePSF_phaseNwithOil_param_5,
	.param .u64 vec_computePSF_phaseNwithOil_param_6,
	.param .u64 vec_computePSF_phaseNwithOil_param_7,
	.param .u64 vec_computePSF_phaseNwithOil_param_8,
	.param .f64 vec_computePSF_phaseNwithOil_param_9,
	.param .f64 vec_computePSF_phaseNwithOil_param_10,
	.param .f64 vec_computePSF_phaseNwithOil_param_11,
	.param .f64 vec_computePSF_phaseNwithOil_param_12,
	.param .u64 vec_computePSF_phaseNwithOil_param_13,
	.param .u64 vec_computePSF_phaseNwithOil_param_14,
	.param .u64 vec_computePSF_phaseNwithOil_param_15
)
{
	.local .align 4 .b8 	__local_depot91[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<33>;
	.reg .f64 	%fd<106>;
	.reg .b64 	%rd<56>;


	mov.u64 	%rd55, __local_depot91;
	cvta.local.u64 	%SP, %rd55;
	ld.param.u32 	%r9, [vec_computePSF_phaseNwithOil_param_0];
	ld.param.u64 	%rd4, [vec_computePSF_phaseNwithOil_param_1];
	ld.param.u64 	%rd5, [vec_computePSF_phaseNwithOil_param_2];
	ld.param.u64 	%rd6, [vec_computePSF_phaseNwithOil_param_3];
	ld.param.u64 	%rd7, [vec_computePSF_phaseNwithOil_param_4];
	ld.param.u64 	%rd8, [vec_computePSF_phaseNwithOil_param_5];
	ld.param.u64 	%rd9, [vec_computePSF_phaseNwithOil_param_6];
	ld.param.u64 	%rd10, [vec_computePSF_phaseNwithOil_param_7];
	ld.param.u64 	%rd11, [vec_computePSF_phaseNwithOil_param_8];
	ld.param.f64 	%fd29, [vec_computePSF_phaseNwithOil_param_9];
	ld.param.f64 	%fd30, [vec_computePSF_phaseNwithOil_param_10];
	ld.param.f64 	%fd31, [vec_computePSF_phaseNwithOil_param_11];
	ld.param.f64 	%fd32, [vec_computePSF_phaseNwithOil_param_12];
	ld.param.u64 	%rd12, [vec_computePSF_phaseNwithOil_param_13];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNwithOil_param_14];
	ld.param.u64 	%rd14, [vec_computePSF_phaseNwithOil_param_15];
	add.u64 	%rd15, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd15;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %nctaid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32	%p1, %r1, %r9;
	@%p1 bra 	BB91_18;

	cvta.to.global.u64 	%rd16, %rd6;
	cvt.s64.s32	%rd2, %r1;
	cvta.to.global.u64 	%rd17, %rd4;
	mul.wide.s32 	%rd18, %r1, 8;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.f64 	%fd33, [%rd19];
	cvta.to.global.u64 	%rd20, %rd5;
	add.s64 	%rd21, %rd20, %rd18;
	ld.global.f64 	%fd34, [%rd21];
	mul.f64 	%fd35, %fd34, %fd30;
	fma.rn.f64 	%fd36, %fd33, %fd29, %fd35;
	cvta.to.global.u64 	%rd22, %rd11;
	add.s64 	%rd23, %rd22, %rd18;
	ld.global.f64 	%fd37, [%rd23];
	add.f64 	%fd38, %fd37, %fd36;
	add.s64 	%rd24, %rd16, %rd18;
	ld.global.f64 	%fd39, [%rd24];
	mul.f64 	%fd40, %fd39, %fd31;
	add.f64 	%fd41, %fd38, %fd40;
	cvta.to.global.u64 	%rd25, %rd8;
	add.s64 	%rd26, %rd25, %rd18;
	ld.global.f64 	%fd42, [%rd26];
	mul.f64 	%fd43, %fd42, %fd32;
	sub.f64 	%fd100, %fd41, %fd43;
	cvta.to.global.u64 	%rd27, %rd7;
	add.s64 	%rd28, %rd27, %rd18;
	ld.global.f64 	%fd44, [%rd28];
	fma.rn.f64 	%fd45, %fd40, %fd44, %fd38;
	cvta.to.global.u64 	%rd29, %rd9;
	add.s64 	%rd30, %rd29, %rd18;
	ld.global.f64 	%fd46, [%rd30];
	mul.f64 	%fd47, %fd43, %fd46;
	sub.f64 	%fd103, %fd45, %fd47;
	cvta.to.global.u64 	%rd31, %rd10;
	add.s64 	%rd3, %rd31, %rd18;
	ld.global.f64 	%fd3, [%rd3];
	abs.f64 	%fd48, %fd100;
	setp.neu.f64	%p2, %fd48, 0d7FF0000000000000;
	@%p2 bra 	BB91_3;

	mov.f64 	%fd49, 0d0000000000000000;
	mul.rn.f64 	%fd100, %fd100, %fd49;

BB91_3:
	mul.f64 	%fd50, %fd100, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r31, %fd50;
	st.local.u32 	[%rd1], %r31;
	cvt.rn.f64.s32	%fd51, %r31;
	neg.f64 	%fd52, %fd51;
	mov.f64 	%fd53, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd54, %fd52, %fd53, %fd100;
	mov.f64 	%fd55, 0d3C91A62633145C00;
	fma.rn.f64 	%fd56, %fd52, %fd55, %fd54;
	mov.f64 	%fd57, 0d397B839A252049C0;
	fma.rn.f64 	%fd101, %fd52, %fd57, %fd56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd100;
	}
	and.b32  	%r20, %r19, 2145386496;
	setp.lt.u32	%p3, %r20, 1105199104;
	@%p3 bra 	BB91_5;

	// Callseq Start 35
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd100;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd15;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd101, [retval0+0];
	
	//{
	}// Callseq End 35
	ld.local.u32 	%r31, [%rd1];

BB91_5:
	add.s32 	%r5, %r31, 1;
	and.b32  	%r21, %r5, 1;
	shl.b32 	%r22, %r21, 3;
	setp.eq.s32	%p4, %r21, 0;
	selp.f64	%fd58, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd33, %r22, 8;
	mov.u64 	%rd34, __cudart_sin_cos_coeffs;
	add.s64 	%rd35, %rd33, %rd34;
	ld.const.f64 	%fd59, [%rd35+8];
	mul.rn.f64 	%fd9, %fd101, %fd101;
	fma.rn.f64 	%fd60, %fd58, %fd9, %fd59;
	ld.const.f64 	%fd61, [%rd35+16];
	fma.rn.f64 	%fd62, %fd60, %fd9, %fd61;
	ld.const.f64 	%fd63, [%rd35+24];
	fma.rn.f64 	%fd64, %fd62, %fd9, %fd63;
	ld.const.f64 	%fd65, [%rd35+32];
	fma.rn.f64 	%fd66, %fd64, %fd9, %fd65;
	ld.const.f64 	%fd67, [%rd35+40];
	fma.rn.f64 	%fd68, %fd66, %fd9, %fd67;
	ld.const.f64 	%fd69, [%rd35+48];
	fma.rn.f64 	%fd10, %fd68, %fd9, %fd69;
	fma.rn.f64 	%fd102, %fd10, %fd101, %fd101;
	@%p4 bra 	BB91_7;

	mov.f64 	%fd70, 0d3FF0000000000000;
	fma.rn.f64 	%fd102, %fd10, %fd9, %fd70;

BB91_7:
	and.b32  	%r23, %r5, 2;
	setp.eq.s32	%p5, %r23, 0;
	@%p5 bra 	BB91_9;

	mov.f64 	%fd71, 0d0000000000000000;
	mov.f64 	%fd72, 0dBFF0000000000000;
	fma.rn.f64 	%fd102, %fd102, %fd72, %fd71;

BB91_9:
	cvta.to.global.u64 	%rd36, %rd12;
	shl.b64 	%rd37, %rd2, 2;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.u32 	%r24, [%rd38];
	cvta.to.global.u64 	%rd39, %rd14;
	mul.wide.s32 	%rd40, %r24, 8;
	add.s64 	%rd41, %rd39, %rd40;
	mul.f64 	%fd73, %fd3, %fd102;
	st.global.f64 	[%rd41], %fd73;
	ld.global.f64 	%fd16, [%rd3];
	abs.f64 	%fd74, %fd103;
	setp.neu.f64	%p6, %fd74, 0d7FF0000000000000;
	@%p6 bra 	BB91_11;

	mov.f64 	%fd75, 0d0000000000000000;
	mul.rn.f64 	%fd103, %fd103, %fd75;

BB91_11:
	mul.f64 	%fd76, %fd103, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r32, %fd76;
	add.u64 	%rd42, %SP, 4;
	cvta.to.local.u64 	%rd43, %rd42;
	st.local.u32 	[%rd43], %r32;
	cvt.rn.f64.s32	%fd77, %r32;
	neg.f64 	%fd78, %fd77;
	fma.rn.f64 	%fd80, %fd78, %fd53, %fd103;
	fma.rn.f64 	%fd82, %fd78, %fd55, %fd80;
	fma.rn.f64 	%fd104, %fd78, %fd57, %fd82;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd103;
	}
	and.b32  	%r26, %r25, 2145386496;
	setp.lt.u32	%p7, %r26, 1105199104;
	@%p7 bra 	BB91_13;

	// Callseq Start 36
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd103;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd42;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd104, [retval0+0];
	
	//{
	}// Callseq End 36
	ld.local.u32 	%r32, [%rd43];

BB91_13:
	and.b32  	%r27, %r32, 1;
	shl.b32 	%r28, %r27, 3;
	setp.eq.s32	%p8, %r27, 0;
	selp.f64	%fd84, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p8;
	mul.wide.u32 	%rd46, %r28, 8;
	add.s64 	%rd48, %rd46, %rd34;
	ld.const.f64 	%fd85, [%rd48+8];
	mul.rn.f64 	%fd22, %fd104, %fd104;
	fma.rn.f64 	%fd86, %fd84, %fd22, %fd85;
	ld.const.f64 	%fd87, [%rd48+16];
	fma.rn.f64 	%fd88, %fd86, %fd22, %fd87;
	ld.const.f64 	%fd89, [%rd48+24];
	fma.rn.f64 	%fd90, %fd88, %fd22, %fd89;
	ld.const.f64 	%fd91, [%rd48+32];
	fma.rn.f64 	%fd92, %fd90, %fd22, %fd91;
	ld.const.f64 	%fd93, [%rd48+40];
	fma.rn.f64 	%fd94, %fd92, %fd22, %fd93;
	ld.const.f64 	%fd95, [%rd48+48];
	fma.rn.f64 	%fd23, %fd94, %fd22, %fd95;
	fma.rn.f64 	%fd105, %fd23, %fd104, %fd104;
	@%p8 bra 	BB91_15;

	mov.f64 	%fd96, 0d3FF0000000000000;
	fma.rn.f64 	%fd105, %fd23, %fd22, %fd96;

BB91_15:
	and.b32  	%r29, %r32, 2;
	setp.eq.s32	%p9, %r29, 0;
	@%p9 bra 	BB91_17;

	mov.f64 	%fd97, 0d0000000000000000;
	mov.f64 	%fd98, 0dBFF0000000000000;
	fma.rn.f64 	%fd105, %fd105, %fd98, %fd97;

BB91_17:
	cvta.to.global.u64 	%rd49, %rd13;
	add.s64 	%rd51, %rd49, %rd37;
	ld.global.u32 	%r30, [%rd51];
	mul.wide.s32 	%rd53, %r30, 8;
	add.s64 	%rd54, %rd39, %rd53;
	mul.f64 	%fd99, %fd16, %fd105;
	st.global.f64 	[%rd54], %fd99;

BB91_18:
	ret;
}

	// .globl	vec_computePSF_phaseNMany
.visible .entry vec_computePSF_phaseNMany(
	.param .u32 vec_computePSF_phaseNMany_param_0,
	.param .u32 vec_computePSF_phaseNMany_param_1,
	.param .u32 vec_computePSF_phaseNMany_param_2,
	.param .u64 vec_computePSF_phaseNMany_param_3,
	.param .u64 vec_computePSF_phaseNMany_param_4,
	.param .u64 vec_computePSF_phaseNMany_param_5,
	.param .u64 vec_computePSF_phaseNMany_param_6,
	.param .u64 vec_computePSF_phaseNMany_param_7,
	.param .u64 vec_computePSF_phaseNMany_param_8,
	.param .u64 vec_computePSF_phaseNMany_param_9,
	.param .u64 vec_computePSF_phaseNMany_param_10,
	.param .u64 vec_computePSF_phaseNMany_param_11,
	.param .u32 vec_computePSF_phaseNMany_param_12
)
{
	.local .align 4 .b8 	__local_depot92[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<57>;


	mov.u64 	%rd56, __local_depot92;
	cvta.local.u64 	%SP, %rd56;
	ld.param.u32 	%r15, [vec_computePSF_phaseNMany_param_0];
	ld.param.u32 	%r12, [vec_computePSF_phaseNMany_param_1];
	ld.param.u32 	%r13, [vec_computePSF_phaseNMany_param_2];
	ld.param.u64 	%rd1, [vec_computePSF_phaseNMany_param_3];
	ld.param.u64 	%rd2, [vec_computePSF_phaseNMany_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_phaseNMany_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_phaseNMany_param_6];
	ld.param.u64 	%rd5, [vec_computePSF_phaseNMany_param_7];
	ld.param.u64 	%rd6, [vec_computePSF_phaseNMany_param_8];
	ld.param.u64 	%rd7, [vec_computePSF_phaseNMany_param_9];
	ld.param.u64 	%rd8, [vec_computePSF_phaseNMany_param_10];
	ld.param.u64 	%rd9, [vec_computePSF_phaseNMany_param_11];
	ld.param.u32 	%r14, [vec_computePSF_phaseNMany_param_12];
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %tid.y;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %nctaid.x;
	mov.u32 	%r21, %ctaid.x;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r22, %r23, %r24;
	setp.ge.s32	%p1, %r1, %r15;
	@%p1 bra 	BB92_18;

	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd4;
	rem.s32 	%r2, %r1, %r12;
	cvta.to.global.u64 	%rd12, %rd1;
	mul.wide.s32 	%rd13, %r2, 8;
	add.s64 	%rd14, %rd12, %rd13;
	div.s32 	%r3, %r1, %r12;
	mul.wide.s32 	%rd15, %r3, 8;
	add.s64 	%rd16, %rd10, %rd15;
	ld.global.f64 	%fd29, [%rd16];
	ld.global.f64 	%fd30, [%rd14];
	cvta.to.global.u64 	%rd17, %rd2;
	add.s64 	%rd18, %rd17, %rd13;
	mul.wide.s32 	%rd19, %r14, 8;
	add.s64 	%rd20, %rd16, %rd19;
	ld.global.f64 	%fd31, [%rd20];
	ld.global.f64 	%fd32, [%rd18];
	mul.f64 	%fd33, %fd32, %fd31;
	fma.rn.f64 	%fd34, %fd30, %fd29, %fd33;
	cvta.to.global.u64 	%rd21, %rd3;
	add.s64 	%rd22, %rd21, %rd13;
	add.s64 	%rd23, %rd20, %rd19;
	ld.global.f64 	%fd35, [%rd23];
	ld.global.f64 	%fd36, [%rd22];
	fma.rn.f64 	%fd37, %fd36, %fd35, %fd34;
	cvta.to.global.u64 	%rd24, %rd5;
	add.s64 	%rd25, %rd24, %rd13;
	ld.global.f64 	%fd38, [%rd25];
	add.f64 	%fd1, %fd38, %fd37;
	add.s64 	%rd26, %rd11, %rd13;
	ld.global.f64 	%fd2, [%rd26];
	abs.f64 	%fd3, %fd1;
	setp.neu.f64	%p2, %fd3, 0d7FF0000000000000;
	mov.f64 	%fd93, %fd1;
	@%p2 bra 	BB92_3;

	mov.f64 	%fd39, 0d0000000000000000;
	mul.rn.f64 	%fd4, %fd1, %fd39;
	mov.f64 	%fd93, %fd4;

BB92_3:
	mov.f64 	%fd5, %fd93;
	mul.f64 	%fd40, %fd5, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r39, %fd40;
	add.u64 	%rd27, %SP, 0;
	cvta.to.local.u64 	%rd28, %rd27;
	st.local.u32 	[%rd28], %r39;
	cvt.rn.f64.s32	%fd41, %r39;
	neg.f64 	%fd42, %fd41;
	mov.f64 	%fd43, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd44, %fd42, %fd43, %fd5;
	mov.f64 	%fd45, 0d3C91A62633145C00;
	fma.rn.f64 	%fd46, %fd42, %fd45, %fd44;
	mov.f64 	%fd47, 0d397B839A252049C0;
	fma.rn.f64 	%fd89, %fd42, %fd47, %fd46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd5;
	}
	and.b32  	%r26, %r25, 2145386496;
	setp.lt.u32	%p3, %r26, 1105199104;
	@%p3 bra 	BB92_5;

	// Callseq Start 37
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd27;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd89, [retval0+0];
	
	//{
	}// Callseq End 37
	ld.local.u32 	%r39, [%rd28];

BB92_5:
	add.s32 	%r7, %r39, 1;
	and.b32  	%r27, %r7, 1;
	shl.b32 	%r28, %r27, 3;
	setp.eq.s32	%p4, %r27, 0;
	selp.f64	%fd48, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd31, %r28, 8;
	mov.u64 	%rd32, __cudart_sin_cos_coeffs;
	add.s64 	%rd33, %rd31, %rd32;
	ld.const.f64 	%fd49, [%rd33+8];
	mul.rn.f64 	%fd9, %fd89, %fd89;
	fma.rn.f64 	%fd50, %fd48, %fd9, %fd49;
	ld.const.f64 	%fd51, [%rd33+16];
	fma.rn.f64 	%fd52, %fd50, %fd9, %fd51;
	ld.const.f64 	%fd53, [%rd33+24];
	fma.rn.f64 	%fd54, %fd52, %fd9, %fd53;
	ld.const.f64 	%fd55, [%rd33+32];
	fma.rn.f64 	%fd56, %fd54, %fd9, %fd55;
	ld.const.f64 	%fd57, [%rd33+40];
	fma.rn.f64 	%fd58, %fd56, %fd9, %fd57;
	ld.const.f64 	%fd59, [%rd33+48];
	fma.rn.f64 	%fd10, %fd58, %fd9, %fd59;
	fma.rn.f64 	%fd90, %fd10, %fd89, %fd89;
	@%p4 bra 	BB92_7;

	mov.f64 	%fd60, 0d3FF0000000000000;
	fma.rn.f64 	%fd90, %fd10, %fd9, %fd60;

BB92_7:
	and.b32  	%r29, %r7, 2;
	setp.eq.s32	%p5, %r29, 0;
	@%p5 bra 	BB92_9;

	mov.f64 	%fd61, 0d0000000000000000;
	mov.f64 	%fd62, 0dBFF0000000000000;
	fma.rn.f64 	%fd90, %fd90, %fd62, %fd61;

BB92_9:
	cvta.to.global.u64 	%rd34, %rd7;
	mul.wide.s32 	%rd35, %r2, 4;
	add.s64 	%rd36, %rd34, %rd35;
	mul.lo.s32 	%r8, %r3, %r13;
	ld.global.u32 	%r30, [%rd36];
	add.s32 	%r31, %r30, %r8;
	cvta.to.global.u64 	%rd37, %rd9;
	mul.wide.s32 	%rd38, %r31, 8;
	add.s64 	%rd39, %rd37, %rd38;
	mul.f64 	%fd63, %fd2, %fd90;
	st.global.f64 	[%rd39], %fd63;
	ld.global.f64 	%fd16, [%rd26];
	mov.f64 	%fd92, %fd1;
	@%p2 bra 	BB92_11;

	mov.f64 	%fd64, 0d0000000000000000;
	mul.rn.f64 	%fd92, %fd1, %fd64;

BB92_11:
	mul.f64 	%fd65, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r40, %fd65;
	add.u64 	%rd43, %SP, 4;
	cvta.to.local.u64 	%rd44, %rd43;
	st.local.u32 	[%rd44], %r40;
	cvt.rn.f64.s32	%fd66, %r40;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd69, %fd67, %fd43, %fd92;
	fma.rn.f64 	%fd71, %fd67, %fd45, %fd69;
	fma.rn.f64 	%fd94, %fd67, %fd47, %fd71;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd92;
	}
	and.b32  	%r33, %r32, 2145386496;
	setp.lt.u32	%p7, %r33, 1105199104;
	@%p7 bra 	BB92_13;

	// Callseq Start 38
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd43;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd94, [retval0+0];
	
	//{
	}// Callseq End 38
	ld.local.u32 	%r40, [%rd44];

BB92_13:
	and.b32  	%r34, %r40, 1;
	shl.b32 	%r35, %r34, 3;
	setp.eq.s32	%p8, %r34, 0;
	selp.f64	%fd73, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p8;
	mul.wide.u32 	%rd47, %r35, 8;
	add.s64 	%rd49, %rd47, %rd32;
	ld.const.f64 	%fd74, [%rd49+8];
	mul.rn.f64 	%fd22, %fd94, %fd94;
	fma.rn.f64 	%fd75, %fd73, %fd22, %fd74;
	ld.const.f64 	%fd76, [%rd49+16];
	fma.rn.f64 	%fd77, %fd75, %fd22, %fd76;
	ld.const.f64 	%fd78, [%rd49+24];
	fma.rn.f64 	%fd79, %fd77, %fd22, %fd78;
	ld.const.f64 	%fd80, [%rd49+32];
	fma.rn.f64 	%fd81, %fd79, %fd22, %fd80;
	ld.const.f64 	%fd82, [%rd49+40];
	fma.rn.f64 	%fd83, %fd81, %fd22, %fd82;
	ld.const.f64 	%fd84, [%rd49+48];
	fma.rn.f64 	%fd23, %fd83, %fd22, %fd84;
	fma.rn.f64 	%fd95, %fd23, %fd94, %fd94;
	@%p8 bra 	BB92_15;

	mov.f64 	%fd85, 0d3FF0000000000000;
	fma.rn.f64 	%fd95, %fd23, %fd22, %fd85;

BB92_15:
	and.b32  	%r36, %r40, 2;
	setp.eq.s32	%p9, %r36, 0;
	@%p9 bra 	BB92_17;

	mov.f64 	%fd86, 0d0000000000000000;
	mov.f64 	%fd87, 0dBFF0000000000000;
	fma.rn.f64 	%fd95, %fd95, %fd87, %fd86;

BB92_17:
	cvta.to.global.u64 	%rd50, %rd8;
	add.s64 	%rd52, %rd50, %rd35;
	ld.global.u32 	%r37, [%rd52];
	add.s32 	%r38, %r37, %r8;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd55, %rd37, %rd54;
	mul.f64 	%fd88, %fd16, %fd95;
	st.global.f64 	[%rd55], %fd88;

BB92_18:
	ret;
}

	// .globl	vec_computePSF_phaseNMany_f
.visible .entry vec_computePSF_phaseNMany_f(
	.param .u32 vec_computePSF_phaseNMany_f_param_0,
	.param .u32 vec_computePSF_phaseNMany_f_param_1,
	.param .u32 vec_computePSF_phaseNMany_f_param_2,
	.param .u64 vec_computePSF_phaseNMany_f_param_3,
	.param .u64 vec_computePSF_phaseNMany_f_param_4,
	.param .u64 vec_computePSF_phaseNMany_f_param_5,
	.param .u64 vec_computePSF_phaseNMany_f_param_6,
	.param .u64 vec_computePSF_phaseNMany_f_param_7,
	.param .u64 vec_computePSF_phaseNMany_f_param_8,
	.param .u64 vec_computePSF_phaseNMany_f_param_9,
	.param .u64 vec_computePSF_phaseNMany_f_param_10,
	.param .u64 vec_computePSF_phaseNMany_f_param_11,
	.param .u32 vec_computePSF_phaseNMany_f_param_12
)
{
	.local .align 4 .b8 	__local_depot93[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .f32 	%f<108>;
	.reg .b32 	%r<204>;
	.reg .b64 	%rd<68>;


	mov.u64 	%rd67, __local_depot93;
	cvta.local.u64 	%SP, %rd67;
	ld.param.u32 	%r78, [vec_computePSF_phaseNMany_f_param_0];
	ld.param.u32 	%r75, [vec_computePSF_phaseNMany_f_param_1];
	ld.param.u32 	%r76, [vec_computePSF_phaseNMany_f_param_2];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNMany_f_param_3];
	ld.param.u64 	%rd14, [vec_computePSF_phaseNMany_f_param_4];
	ld.param.u64 	%rd15, [vec_computePSF_phaseNMany_f_param_5];
	ld.param.u64 	%rd16, [vec_computePSF_phaseNMany_f_param_6];
	ld.param.u64 	%rd17, [vec_computePSF_phaseNMany_f_param_7];
	ld.param.u64 	%rd18, [vec_computePSF_phaseNMany_f_param_8];
	ld.param.u64 	%rd19, [vec_computePSF_phaseNMany_f_param_9];
	ld.param.u64 	%rd20, [vec_computePSF_phaseNMany_f_param_10];
	ld.param.u64 	%rd21, [vec_computePSF_phaseNMany_f_param_11];
	ld.param.u32 	%r77, [vec_computePSF_phaseNMany_f_param_12];
	mov.u32 	%r79, %ntid.y;
	mov.u32 	%r80, %ctaid.y;
	mov.u32 	%r81, %tid.y;
	mad.lo.s32 	%r82, %r79, %r80, %r81;
	mov.u32 	%r83, %nctaid.x;
	mov.u32 	%r84, %ctaid.x;
	mad.lo.s32 	%r85, %r82, %r83, %r84;
	mov.u32 	%r86, %ntid.x;
	mov.u32 	%r87, %tid.x;
	mad.lo.s32 	%r1, %r85, %r86, %r87;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB93_46;

	cvta.to.global.u64 	%rd22, %rd18;
	cvta.to.global.u64 	%rd23, %rd16;
	rem.s32 	%r2, %r1, %r75;
	cvta.to.global.u64 	%rd24, %rd13;
	mul.wide.s32 	%rd25, %r2, 4;
	add.s64 	%rd26, %rd24, %rd25;
	div.s32 	%r3, %r1, %r75;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd22, %rd27;
	ld.global.f32 	%f39, [%rd28];
	ld.global.f32 	%f40, [%rd26];
	cvta.to.global.u64 	%rd29, %rd14;
	add.s64 	%rd30, %rd29, %rd25;
	mul.wide.s32 	%rd31, %r77, 4;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.f32 	%f41, [%rd32];
	ld.global.f32 	%f42, [%rd30];
	mul.f32 	%f43, %f42, %f41;
	fma.rn.f32 	%f44, %f40, %f39, %f43;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd33, %rd25;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.f32 	%f45, [%rd35];
	ld.global.f32 	%f46, [%rd34];
	fma.rn.f32 	%f47, %f46, %f45, %f44;
	cvta.to.global.u64 	%rd36, %rd17;
	add.s64 	%rd37, %rd36, %rd25;
	ld.global.f32 	%f48, [%rd37];
	add.f32 	%f1, %f48, %f47;
	add.s64 	%rd38, %rd23, %rd25;
	ld.global.f32 	%f2, [%rd38];
	abs.f32 	%f3, %f1;
	setp.neu.f32	%p2, %f3, 0f7F800000;
	mov.f32 	%f103, %f1;
	@%p2 bra 	BB93_3;

	mov.f32 	%f49, 0f00000000;
	mul.rn.f32 	%f4, %f1, %f49;
	mov.f32 	%f103, %f4;

BB93_3:
	mov.f32 	%f5, %f103;
	mul.f32 	%f50, %f5, 0f3F22F983;
	cvt.rni.s32.f32	%r193, %f50;
	cvt.rn.f32.s32	%f51, %r193;
	neg.f32 	%f52, %f51;
	mov.f32 	%f53, 0f3FC90FDA;
	fma.rn.f32 	%f54, %f52, %f53, %f5;
	mov.f32 	%f55, 0f33A22168;
	fma.rn.f32 	%f56, %f52, %f55, %f54;
	mov.f32 	%f57, 0f27C234C5;
	fma.rn.f32 	%f97, %f52, %f57, %f56;
	abs.f32 	%f58, %f5;
	setp.leu.f32	%p3, %f58, 0f47CE4780;
	@%p3 bra 	BB93_13;

	mov.b32 	 %r5, %f5;
	shl.b32 	%r90, %r5, 8;
	or.b32  	%r6, %r90, -2147483648;
	add.u64 	%rd40, %SP, 0;
	cvta.to.local.u64 	%rd64, %rd40;
	mov.u32 	%r185, 0;
	mov.u64 	%rd63, __cudart_i2opi_f;
	mov.u32 	%r184, -6;

BB93_5:
	.pragma "nounroll";
	ld.const.u32 	%r93, [%rd63];
	// inline asm
	{
	mad.lo.cc.u32   %r91, %r93, %r6, %r185;
	madc.hi.u32     %r185, %r93, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd64], %r91;
	add.s64 	%rd64, %rd64, 4;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r184, %r184, 1;
	setp.ne.s32	%p4, %r184, 0;
	@%p4 bra 	BB93_5;

	bfe.u32 	%r96, %r5, 23, 8;
	add.s32 	%r97, %r96, -128;
	shr.u32 	%r98, %r97, 5;
	and.b32  	%r11, %r5, -2147483648;
	cvta.to.local.u64 	%rd42, %rd40;
	st.local.u32 	[%rd42+24], %r185;
	bfe.u32 	%r12, %r5, 23, 5;
	mov.u32 	%r99, 6;
	sub.s32 	%r100, %r99, %r98;
	mul.wide.s32 	%rd43, %r100, 4;
	add.s64 	%rd6, %rd42, %rd43;
	ld.local.u32 	%r186, [%rd6];
	ld.local.u32 	%r187, [%rd6+-4];
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB93_8;

	mov.u32 	%r101, 32;
	sub.s32 	%r102, %r101, %r12;
	shr.u32 	%r103, %r187, %r102;
	shl.b32 	%r104, %r186, %r12;
	add.s32 	%r186, %r103, %r104;
	ld.local.u32 	%r105, [%rd6+-8];
	shr.u32 	%r106, %r105, %r102;
	shl.b32 	%r107, %r187, %r12;
	add.s32 	%r187, %r106, %r107;

BB93_8:
	shr.u32 	%r108, %r187, 30;
	shl.b32 	%r109, %r186, 2;
	add.s32 	%r188, %r108, %r109;
	shl.b32 	%r20, %r187, 2;
	shr.u32 	%r110, %r188, 31;
	shr.u32 	%r111, %r186, 30;
	add.s32 	%r21, %r110, %r111;
	setp.eq.s32	%p6, %r110, 0;
	mov.u32 	%r189, %r11;
	mov.u32 	%r190, %r20;
	@%p6 bra 	BB93_10;

	not.b32 	%r112, %r188;
	neg.s32 	%r22, %r20;
	setp.eq.s32	%p7, %r20, 0;
	selp.u32	%r113, 1, 0, %p7;
	add.s32 	%r188, %r113, %r112;
	xor.b32  	%r24, %r11, -2147483648;
	mov.u32 	%r189, %r24;
	mov.u32 	%r190, %r22;

BB93_10:
	mov.u32 	%r26, %r189;
	neg.s32 	%r114, %r21;
	setp.eq.s32	%p8, %r11, 0;
	selp.b32	%r193, %r21, %r114, %p8;
	clz.b32 	%r192, %r188;
	setp.eq.s32	%p9, %r192, 0;
	shl.b32 	%r115, %r188, %r192;
	mov.u32 	%r116, 32;
	sub.s32 	%r117, %r116, %r192;
	shr.u32 	%r118, %r190, %r117;
	add.s32 	%r119, %r118, %r115;
	selp.b32	%r30, %r188, %r119, %p9;
	mov.u32 	%r120, -921707870;
	mul.hi.u32 	%r191, %r30, %r120;
	setp.lt.s32	%p10, %r191, 1;
	@%p10 bra 	BB93_12;

	mul.lo.s32 	%r121, %r30, -921707870;
	shr.u32 	%r122, %r121, 31;
	shl.b32 	%r123, %r191, 1;
	add.s32 	%r191, %r122, %r123;
	add.s32 	%r192, %r192, 1;

BB93_12:
	mov.u32 	%r124, 126;
	sub.s32 	%r125, %r124, %r192;
	shl.b32 	%r126, %r125, 23;
	add.s32 	%r127, %r191, 1;
	shr.u32 	%r128, %r127, 7;
	add.s32 	%r129, %r128, 1;
	shr.u32 	%r130, %r129, 1;
	add.s32 	%r131, %r130, %r126;
	or.b32  	%r132, %r131, %r26;
	mov.b32 	 %f97, %r132;

BB93_13:
	mul.rn.f32 	%f9, %f97, %f97;
	add.s32 	%r37, %r193, 1;
	and.b32  	%r38, %r37, 1;
	setp.eq.s32	%p11, %r38, 0;
	@%p11 bra 	BB93_15;

	mov.f32 	%f59, 0fBAB6061A;
	mov.f32 	%f60, 0f37CCF5CE;
	fma.rn.f32 	%f98, %f60, %f9, %f59;
	bra.uni 	BB93_16;

BB93_15:
	mov.f32 	%f61, 0f3C08839E;
	mov.f32 	%f62, 0fB94CA1F9;
	fma.rn.f32 	%f98, %f62, %f9, %f61;

BB93_16:
	@%p11 bra 	BB93_18;

	mov.f32 	%f63, 0f3D2AAAA5;
	fma.rn.f32 	%f64, %f98, %f9, %f63;
	mov.f32 	%f65, 0fBF000000;
	fma.rn.f32 	%f99, %f64, %f9, %f65;
	bra.uni 	BB93_19;

BB93_18:
	mov.f32 	%f66, 0fBE2AAAA3;
	fma.rn.f32 	%f67, %f98, %f9, %f66;
	mov.f32 	%f68, 0f00000000;
	fma.rn.f32 	%f99, %f67, %f9, %f68;

BB93_19:
	fma.rn.f32 	%f100, %f99, %f97, %f97;
	@%p11 bra 	BB93_21;

	mov.f32 	%f69, 0f3F800000;
	fma.rn.f32 	%f100, %f99, %f9, %f69;

BB93_21:
	and.b32  	%r133, %r37, 2;
	setp.eq.s32	%p14, %r133, 0;
	@%p14 bra 	BB93_23;

	mov.f32 	%f70, 0f00000000;
	mov.f32 	%f71, 0fBF800000;
	fma.rn.f32 	%f100, %f100, %f71, %f70;

BB93_23:
	cvta.to.global.u64 	%rd44, %rd19;
	add.s64 	%rd46, %rd44, %rd25;
	mul.lo.s32 	%r39, %r3, %r76;
	ld.global.u32 	%r134, [%rd46];
	add.s32 	%r135, %r134, %r39;
	cvta.to.global.u64 	%rd47, %rd21;
	mul.wide.s32 	%rd48, %r135, 4;
	add.s64 	%rd49, %rd47, %rd48;
	mul.f32 	%f72, %f2, %f100;
	st.global.f32 	[%rd49], %f72;
	ld.global.f32 	%f21, [%rd38];
	mov.f32 	%f102, %f1;
	@%p2 bra 	BB93_25;

	mov.f32 	%f73, 0f00000000;
	mul.rn.f32 	%f102, %f1, %f73;

BB93_25:
	mul.f32 	%f74, %f102, 0f3F22F983;
	cvt.rni.s32.f32	%r203, %f74;
	cvt.rn.f32.s32	%f75, %r203;
	neg.f32 	%f76, %f75;
	fma.rn.f32 	%f78, %f76, %f53, %f102;
	fma.rn.f32 	%f80, %f76, %f55, %f78;
	fma.rn.f32 	%f104, %f76, %f57, %f80;
	abs.f32 	%f82, %f102;
	setp.leu.f32	%p16, %f82, 0f47CE4780;
	@%p16 bra 	BB93_35;

	mov.b32 	 %r41, %f102;
	shr.u32 	%r42, %r41, 23;
	shl.b32 	%r138, %r41, 8;
	or.b32  	%r43, %r138, -2147483648;
	add.u64 	%rd53, %SP, 0;
	cvta.to.local.u64 	%rd66, %rd53;
	mov.u32 	%r195, 0;
	mov.u64 	%rd65, __cudart_i2opi_f;
	mov.u32 	%r194, -6;

BB93_27:
	.pragma "nounroll";
	ld.const.u32 	%r141, [%rd65];
	// inline asm
	{
	mad.lo.cc.u32   %r139, %r141, %r43, %r195;
	madc.hi.u32     %r195, %r141, %r43,  0;
	}
	// inline asm
	st.local.u32 	[%rd66], %r139;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r194, %r194, 1;
	setp.ne.s32	%p17, %r194, 0;
	@%p17 bra 	BB93_27;

	and.b32  	%r144, %r42, 255;
	add.s32 	%r145, %r144, -128;
	shr.u32 	%r146, %r145, 5;
	and.b32  	%r48, %r41, -2147483648;
	cvta.to.local.u64 	%rd55, %rd53;
	st.local.u32 	[%rd55+24], %r195;
	mov.u32 	%r147, 6;
	sub.s32 	%r148, %r147, %r146;
	mul.wide.s32 	%rd56, %r148, 4;
	add.s64 	%rd12, %rd55, %rd56;
	ld.local.u32 	%r196, [%rd12];
	ld.local.u32 	%r197, [%rd12+-4];
	and.b32  	%r51, %r42, 31;
	setp.eq.s32	%p18, %r51, 0;
	@%p18 bra 	BB93_30;

	mov.u32 	%r149, 32;
	sub.s32 	%r150, %r149, %r51;
	shr.u32 	%r151, %r197, %r150;
	shl.b32 	%r152, %r196, %r51;
	add.s32 	%r196, %r151, %r152;
	ld.local.u32 	%r153, [%rd12+-8];
	shr.u32 	%r154, %r153, %r150;
	shl.b32 	%r155, %r197, %r51;
	add.s32 	%r197, %r154, %r155;

BB93_30:
	shr.u32 	%r156, %r197, 30;
	shl.b32 	%r157, %r196, 2;
	add.s32 	%r198, %r156, %r157;
	shl.b32 	%r57, %r197, 2;
	shr.u32 	%r158, %r198, 31;
	shr.u32 	%r159, %r196, 30;
	add.s32 	%r58, %r158, %r159;
	setp.eq.s32	%p19, %r158, 0;
	mov.u32 	%r199, %r48;
	mov.u32 	%r200, %r57;
	@%p19 bra 	BB93_32;

	not.b32 	%r160, %r198;
	neg.s32 	%r59, %r57;
	setp.eq.s32	%p20, %r57, 0;
	selp.u32	%r161, 1, 0, %p20;
	add.s32 	%r198, %r161, %r160;
	xor.b32  	%r61, %r48, -2147483648;
	mov.u32 	%r199, %r61;
	mov.u32 	%r200, %r59;

BB93_32:
	mov.u32 	%r63, %r199;
	neg.s32 	%r162, %r58;
	setp.eq.s32	%p21, %r48, 0;
	selp.b32	%r203, %r58, %r162, %p21;
	clz.b32 	%r202, %r198;
	setp.eq.s32	%p22, %r202, 0;
	shl.b32 	%r163, %r198, %r202;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r202;
	shr.u32 	%r166, %r200, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r67, %r198, %r167, %p22;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r201, %r67, %r168;
	setp.lt.s32	%p23, %r201, 1;
	@%p23 bra 	BB93_34;

	mul.lo.s32 	%r169, %r67, -921707870;
	shr.u32 	%r170, %r169, 31;
	shl.b32 	%r171, %r201, 1;
	add.s32 	%r201, %r170, %r171;
	add.s32 	%r202, %r202, 1;

BB93_34:
	mov.u32 	%r172, 126;
	sub.s32 	%r173, %r172, %r202;
	shl.b32 	%r174, %r173, 23;
	add.s32 	%r175, %r201, 1;
	shr.u32 	%r176, %r175, 7;
	add.s32 	%r177, %r176, 1;
	shr.u32 	%r178, %r177, 1;
	add.s32 	%r179, %r178, %r174;
	or.b32  	%r180, %r179, %r63;
	mov.b32 	 %f104, %r180;

BB93_35:
	mul.rn.f32 	%f27, %f104, %f104;
	and.b32  	%r74, %r203, 1;
	setp.eq.s32	%p24, %r74, 0;
	@%p24 bra 	BB93_37;

	mov.f32 	%f83, 0fBAB6061A;
	mov.f32 	%f84, 0f37CCF5CE;
	fma.rn.f32 	%f105, %f84, %f27, %f83;
	bra.uni 	BB93_38;

BB93_37:
	mov.f32 	%f85, 0f3C08839E;
	mov.f32 	%f86, 0fB94CA1F9;
	fma.rn.f32 	%f105, %f86, %f27, %f85;

BB93_38:
	@%p24 bra 	BB93_40;

	mov.f32 	%f87, 0f3D2AAAA5;
	fma.rn.f32 	%f88, %f105, %f27, %f87;
	mov.f32 	%f89, 0fBF000000;
	fma.rn.f32 	%f106, %f88, %f27, %f89;
	bra.uni 	BB93_41;

BB93_40:
	mov.f32 	%f90, 0fBE2AAAA3;
	fma.rn.f32 	%f91, %f105, %f27, %f90;
	mov.f32 	%f92, 0f00000000;
	fma.rn.f32 	%f106, %f91, %f27, %f92;

BB93_41:
	fma.rn.f32 	%f107, %f106, %f104, %f104;
	@%p24 bra 	BB93_43;

	mov.f32 	%f93, 0f3F800000;
	fma.rn.f32 	%f107, %f106, %f27, %f93;

BB93_43:
	and.b32  	%r181, %r203, 2;
	setp.eq.s32	%p27, %r181, 0;
	@%p27 bra 	BB93_45;

	mov.f32 	%f94, 0f00000000;
	mov.f32 	%f95, 0fBF800000;
	fma.rn.f32 	%f107, %f107, %f95, %f94;

BB93_45:
	cvta.to.global.u64 	%rd57, %rd20;
	add.s64 	%rd59, %rd57, %rd25;
	ld.global.u32 	%r182, [%rd59];
	add.s32 	%r183, %r182, %r39;
	mul.wide.s32 	%rd61, %r183, 4;
	add.s64 	%rd62, %rd47, %rd61;
	mul.f32 	%f96, %f21, %f107;
	st.global.f32 	[%rd62], %f96;

BB93_46:
	ret;
}

	// .globl	vec_computePSF_phaseNManywithOil_f
.visible .entry vec_computePSF_phaseNManywithOil_f(
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_0,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_1,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_2,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_3,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_4,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_5,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_6,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_7,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_8,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_9,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_10,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_11,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_12,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_13,
	.param .u64 vec_computePSF_phaseNManywithOil_f_param_14,
	.param .u32 vec_computePSF_phaseNManywithOil_f_param_15
)
{
	.local .align 4 .b8 	__local_depot94[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .f32 	%f<118>;
	.reg .b32 	%r<204>;
	.reg .b64 	%rd<78>;


	mov.u64 	%rd77, __local_depot94;
	cvta.local.u64 	%SP, %rd77;
	ld.param.u32 	%r78, [vec_computePSF_phaseNManywithOil_f_param_0];
	ld.param.u32 	%r75, [vec_computePSF_phaseNManywithOil_f_param_1];
	ld.param.u32 	%r76, [vec_computePSF_phaseNManywithOil_f_param_2];
	ld.param.u64 	%rd13, [vec_computePSF_phaseNManywithOil_f_param_3];
	ld.param.u64 	%rd14, [vec_computePSF_phaseNManywithOil_f_param_4];
	ld.param.u64 	%rd15, [vec_computePSF_phaseNManywithOil_f_param_5];
	ld.param.u64 	%rd16, [vec_computePSF_phaseNManywithOil_f_param_6];
	ld.param.u64 	%rd17, [vec_computePSF_phaseNManywithOil_f_param_7];
	ld.param.u64 	%rd18, [vec_computePSF_phaseNManywithOil_f_param_8];
	ld.param.u64 	%rd19, [vec_computePSF_phaseNManywithOil_f_param_9];
	ld.param.u64 	%rd20, [vec_computePSF_phaseNManywithOil_f_param_10];
	ld.param.u64 	%rd21, [vec_computePSF_phaseNManywithOil_f_param_11];
	ld.param.u64 	%rd22, [vec_computePSF_phaseNManywithOil_f_param_12];
	ld.param.u64 	%rd23, [vec_computePSF_phaseNManywithOil_f_param_13];
	ld.param.u64 	%rd24, [vec_computePSF_phaseNManywithOil_f_param_14];
	ld.param.u32 	%r77, [vec_computePSF_phaseNManywithOil_f_param_15];
	mov.u32 	%r79, %ntid.y;
	mov.u32 	%r80, %ctaid.y;
	mov.u32 	%r81, %tid.y;
	mad.lo.s32 	%r82, %r79, %r80, %r81;
	mov.u32 	%r83, %nctaid.x;
	mov.u32 	%r84, %ctaid.x;
	mad.lo.s32 	%r85, %r82, %r83, %r84;
	mov.u32 	%r86, %ntid.x;
	mov.u32 	%r87, %tid.x;
	mad.lo.s32 	%r1, %r85, %r86, %r87;
	setp.ge.s32	%p1, %r1, %r78;
	@%p1 bra 	BB94_46;

	cvta.to.global.u64 	%rd25, %rd21;
	cvta.to.global.u64 	%rd26, %rd15;
	rem.s32 	%r2, %r1, %r75;
	cvta.to.global.u64 	%rd27, %rd13;
	mul.wide.s32 	%rd28, %r2, 4;
	add.s64 	%rd29, %rd27, %rd28;
	div.s32 	%r3, %r1, %r75;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd25, %rd30;
	ld.global.f32 	%f39, [%rd31];
	ld.global.f32 	%f40, [%rd29];
	cvta.to.global.u64 	%rd32, %rd14;
	add.s64 	%rd33, %rd32, %rd28;
	mul.wide.s32 	%rd34, %r77, 4;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.f32 	%f41, [%rd35];
	ld.global.f32 	%f42, [%rd33];
	mul.f32 	%f43, %f42, %f41;
	fma.rn.f32 	%f44, %f40, %f39, %f43;
	cvta.to.global.u64 	%rd36, %rd20;
	add.s64 	%rd37, %rd36, %rd28;
	ld.global.f32 	%f45, [%rd37];
	add.f32 	%f46, %f45, %f44;
	add.s64 	%rd38, %rd26, %rd28;
	add.s64 	%rd39, %rd35, %rd34;
	ld.global.f32 	%f47, [%rd39];
	ld.global.f32 	%f48, [%rd38];
	mul.f32 	%f49, %f48, %f47;
	add.f32 	%f50, %f46, %f49;
	cvta.to.global.u64 	%rd40, %rd17;
	add.s64 	%rd41, %rd40, %rd28;
	add.s64 	%rd42, %rd39, %rd34;
	ld.global.f32 	%f51, [%rd42];
	ld.global.f32 	%f52, [%rd41];
	mul.f32 	%f53, %f52, %f51;
	sub.f32 	%f108, %f50, %f53;
	cvta.to.global.u64 	%rd43, %rd16;
	add.s64 	%rd44, %rd43, %rd28;
	ld.global.f32 	%f54, [%rd44];
	fma.rn.f32 	%f55, %f49, %f54, %f46;
	cvta.to.global.u64 	%rd45, %rd18;
	add.s64 	%rd46, %rd45, %rd28;
	ld.global.f32 	%f56, [%rd46];
	mul.f32 	%f57, %f53, %f56;
	sub.f32 	%f113, %f55, %f57;
	cvta.to.global.u64 	%rd47, %rd19;
	add.s64 	%rd48, %rd47, %rd28;
	ld.global.f32 	%f3, [%rd48];
	abs.f32 	%f58, %f108;
	setp.neu.f32	%p2, %f58, 0f7F800000;
	@%p2 bra 	BB94_3;

	mov.f32 	%f59, 0f00000000;
	mul.rn.f32 	%f108, %f108, %f59;

BB94_3:
	mul.f32 	%f60, %f108, 0f3F22F983;
	cvt.rni.s32.f32	%r193, %f60;
	cvt.rn.f32.s32	%f61, %r193;
	neg.f32 	%f62, %f61;
	mov.f32 	%f63, 0f3FC90FDA;
	fma.rn.f32 	%f64, %f62, %f63, %f108;
	mov.f32 	%f65, 0f33A22168;
	fma.rn.f32 	%f66, %f62, %f65, %f64;
	mov.f32 	%f67, 0f27C234C5;
	fma.rn.f32 	%f109, %f62, %f67, %f66;
	abs.f32 	%f68, %f108;
	setp.leu.f32	%p3, %f68, 0f47CE4780;
	@%p3 bra 	BB94_13;

	mov.b32 	 %r5, %f108;
	shl.b32 	%r90, %r5, 8;
	or.b32  	%r6, %r90, -2147483648;
	add.u64 	%rd50, %SP, 0;
	cvta.to.local.u64 	%rd74, %rd50;
	mov.u32 	%r185, 0;
	mov.u64 	%rd73, __cudart_i2opi_f;
	mov.u32 	%r184, -6;

BB94_5:
	.pragma "nounroll";
	ld.const.u32 	%r93, [%rd73];
	// inline asm
	{
	mad.lo.cc.u32   %r91, %r93, %r6, %r185;
	madc.hi.u32     %r185, %r93, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd74], %r91;
	add.s64 	%rd74, %rd74, 4;
	add.s64 	%rd73, %rd73, 4;
	add.s32 	%r184, %r184, 1;
	setp.ne.s32	%p4, %r184, 0;
	@%p4 bra 	BB94_5;

	bfe.u32 	%r96, %r5, 23, 8;
	add.s32 	%r97, %r96, -128;
	shr.u32 	%r98, %r97, 5;
	and.b32  	%r11, %r5, -2147483648;
	cvta.to.local.u64 	%rd52, %rd50;
	st.local.u32 	[%rd52+24], %r185;
	bfe.u32 	%r12, %r5, 23, 5;
	mov.u32 	%r99, 6;
	sub.s32 	%r100, %r99, %r98;
	mul.wide.s32 	%rd53, %r100, 4;
	add.s64 	%rd6, %rd52, %rd53;
	ld.local.u32 	%r186, [%rd6];
	ld.local.u32 	%r187, [%rd6+-4];
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB94_8;

	mov.u32 	%r101, 32;
	sub.s32 	%r102, %r101, %r12;
	shr.u32 	%r103, %r187, %r102;
	shl.b32 	%r104, %r186, %r12;
	add.s32 	%r186, %r103, %r104;
	ld.local.u32 	%r105, [%rd6+-8];
	shr.u32 	%r106, %r105, %r102;
	shl.b32 	%r107, %r187, %r12;
	add.s32 	%r187, %r106, %r107;

BB94_8:
	shr.u32 	%r108, %r187, 30;
	shl.b32 	%r109, %r186, 2;
	add.s32 	%r188, %r108, %r109;
	shl.b32 	%r20, %r187, 2;
	shr.u32 	%r110, %r188, 31;
	shr.u32 	%r111, %r186, 30;
	add.s32 	%r21, %r110, %r111;
	setp.eq.s32	%p6, %r110, 0;
	mov.u32 	%r189, %r11;
	mov.u32 	%r190, %r20;
	@%p6 bra 	BB94_10;

	not.b32 	%r112, %r188;
	neg.s32 	%r22, %r20;
	setp.eq.s32	%p7, %r20, 0;
	selp.u32	%r113, 1, 0, %p7;
	add.s32 	%r188, %r113, %r112;
	xor.b32  	%r24, %r11, -2147483648;
	mov.u32 	%r189, %r24;
	mov.u32 	%r190, %r22;

BB94_10:
	mov.u32 	%r26, %r189;
	neg.s32 	%r114, %r21;
	setp.eq.s32	%p8, %r11, 0;
	selp.b32	%r193, %r21, %r114, %p8;
	clz.b32 	%r192, %r188;
	setp.eq.s32	%p9, %r192, 0;
	shl.b32 	%r115, %r188, %r192;
	mov.u32 	%r116, 32;
	sub.s32 	%r117, %r116, %r192;
	shr.u32 	%r118, %r190, %r117;
	add.s32 	%r119, %r118, %r115;
	selp.b32	%r30, %r188, %r119, %p9;
	mov.u32 	%r120, -921707870;
	mul.hi.u32 	%r191, %r30, %r120;
	setp.lt.s32	%p10, %r191, 1;
	@%p10 bra 	BB94_12;

	mul.lo.s32 	%r121, %r30, -921707870;
	shr.u32 	%r122, %r121, 31;
	shl.b32 	%r123, %r191, 1;
	add.s32 	%r191, %r122, %r123;
	add.s32 	%r192, %r192, 1;

BB94_12:
	mov.u32 	%r124, 126;
	sub.s32 	%r125, %r124, %r192;
	shl.b32 	%r126, %r125, 23;
	add.s32 	%r127, %r191, 1;
	shr.u32 	%r128, %r127, 7;
	add.s32 	%r129, %r128, 1;
	shr.u32 	%r130, %r129, 1;
	add.s32 	%r131, %r130, %r126;
	or.b32  	%r132, %r131, %r26;
	mov.b32 	 %f109, %r132;

BB94_13:
	mul.rn.f32 	%f9, %f109, %f109;
	add.s32 	%r37, %r193, 1;
	and.b32  	%r38, %r37, 1;
	setp.eq.s32	%p11, %r38, 0;
	@%p11 bra 	BB94_15;

	mov.f32 	%f69, 0fBAB6061A;
	mov.f32 	%f70, 0f37CCF5CE;
	fma.rn.f32 	%f110, %f70, %f9, %f69;
	bra.uni 	BB94_16;

BB94_15:
	mov.f32 	%f71, 0f3C08839E;
	mov.f32 	%f72, 0fB94CA1F9;
	fma.rn.f32 	%f110, %f72, %f9, %f71;

BB94_16:
	@%p11 bra 	BB94_18;

	mov.f32 	%f73, 0f3D2AAAA5;
	fma.rn.f32 	%f74, %f110, %f9, %f73;
	mov.f32 	%f75, 0fBF000000;
	fma.rn.f32 	%f111, %f74, %f9, %f75;
	bra.uni 	BB94_19;

BB94_18:
	mov.f32 	%f76, 0fBE2AAAA3;
	fma.rn.f32 	%f77, %f110, %f9, %f76;
	mov.f32 	%f78, 0f00000000;
	fma.rn.f32 	%f111, %f77, %f9, %f78;

BB94_19:
	fma.rn.f32 	%f112, %f111, %f109, %f109;
	@%p11 bra 	BB94_21;

	mov.f32 	%f79, 0f3F800000;
	fma.rn.f32 	%f112, %f111, %f9, %f79;

BB94_21:
	and.b32  	%r133, %r37, 2;
	setp.eq.s32	%p14, %r133, 0;
	@%p14 bra 	BB94_23;

	mov.f32 	%f80, 0f00000000;
	mov.f32 	%f81, 0fBF800000;
	fma.rn.f32 	%f112, %f112, %f81, %f80;

BB94_23:
	cvta.to.global.u64 	%rd54, %rd22;
	add.s64 	%rd56, %rd54, %rd28;
	mul.lo.s32 	%r39, %r3, %r76;
	ld.global.u32 	%r134, [%rd56];
	add.s32 	%r135, %r134, %r39;
	cvta.to.global.u64 	%rd57, %rd24;
	mul.wide.s32 	%rd58, %r135, 4;
	add.s64 	%rd59, %rd57, %rd58;
	mul.f32 	%f82, %f3, %f112;
	st.global.f32 	[%rd59], %f82;
	ld.global.f32 	%f21, [%rd48];
	abs.f32 	%f83, %f113;
	setp.neu.f32	%p15, %f83, 0f7F800000;
	@%p15 bra 	BB94_25;

	mov.f32 	%f84, 0f00000000;
	mul.rn.f32 	%f113, %f113, %f84;

BB94_25:
	mul.f32 	%f85, %f113, 0f3F22F983;
	cvt.rni.s32.f32	%r203, %f85;
	cvt.rn.f32.s32	%f86, %r203;
	neg.f32 	%f87, %f86;
	fma.rn.f32 	%f89, %f87, %f63, %f113;
	fma.rn.f32 	%f91, %f87, %f65, %f89;
	fma.rn.f32 	%f114, %f87, %f67, %f91;
	abs.f32 	%f93, %f113;
	setp.leu.f32	%p16, %f93, 0f47CE4780;
	@%p16 bra 	BB94_35;

	mov.b32 	 %r41, %f113;
	shr.u32 	%r42, %r41, 23;
	shl.b32 	%r138, %r41, 8;
	or.b32  	%r43, %r138, -2147483648;
	add.u64 	%rd63, %SP, 0;
	cvta.to.local.u64 	%rd76, %rd63;
	mov.u32 	%r195, 0;
	mov.u64 	%rd75, __cudart_i2opi_f;
	mov.u32 	%r194, -6;

BB94_27:
	.pragma "nounroll";
	ld.const.u32 	%r141, [%rd75];
	// inline asm
	{
	mad.lo.cc.u32   %r139, %r141, %r43, %r195;
	madc.hi.u32     %r195, %r141, %r43,  0;
	}
	// inline asm
	st.local.u32 	[%rd76], %r139;
	add.s64 	%rd76, %rd76, 4;
	add.s64 	%rd75, %rd75, 4;
	add.s32 	%r194, %r194, 1;
	setp.ne.s32	%p17, %r194, 0;
	@%p17 bra 	BB94_27;

	and.b32  	%r144, %r42, 255;
	add.s32 	%r145, %r144, -128;
	shr.u32 	%r146, %r145, 5;
	and.b32  	%r48, %r41, -2147483648;
	cvta.to.local.u64 	%rd65, %rd63;
	st.local.u32 	[%rd65+24], %r195;
	mov.u32 	%r147, 6;
	sub.s32 	%r148, %r147, %r146;
	mul.wide.s32 	%rd66, %r148, 4;
	add.s64 	%rd12, %rd65, %rd66;
	ld.local.u32 	%r196, [%rd12];
	ld.local.u32 	%r197, [%rd12+-4];
	and.b32  	%r51, %r42, 31;
	setp.eq.s32	%p18, %r51, 0;
	@%p18 bra 	BB94_30;

	mov.u32 	%r149, 32;
	sub.s32 	%r150, %r149, %r51;
	shr.u32 	%r151, %r197, %r150;
	shl.b32 	%r152, %r196, %r51;
	add.s32 	%r196, %r151, %r152;
	ld.local.u32 	%r153, [%rd12+-8];
	shr.u32 	%r154, %r153, %r150;
	shl.b32 	%r155, %r197, %r51;
	add.s32 	%r197, %r154, %r155;

BB94_30:
	shr.u32 	%r156, %r197, 30;
	shl.b32 	%r157, %r196, 2;
	add.s32 	%r198, %r156, %r157;
	shl.b32 	%r57, %r197, 2;
	shr.u32 	%r158, %r198, 31;
	shr.u32 	%r159, %r196, 30;
	add.s32 	%r58, %r158, %r159;
	setp.eq.s32	%p19, %r158, 0;
	mov.u32 	%r199, %r48;
	mov.u32 	%r200, %r57;
	@%p19 bra 	BB94_32;

	not.b32 	%r160, %r198;
	neg.s32 	%r59, %r57;
	setp.eq.s32	%p20, %r57, 0;
	selp.u32	%r161, 1, 0, %p20;
	add.s32 	%r198, %r161, %r160;
	xor.b32  	%r61, %r48, -2147483648;
	mov.u32 	%r199, %r61;
	mov.u32 	%r200, %r59;

BB94_32:
	mov.u32 	%r63, %r199;
	neg.s32 	%r162, %r58;
	setp.eq.s32	%p21, %r48, 0;
	selp.b32	%r203, %r58, %r162, %p21;
	clz.b32 	%r202, %r198;
	setp.eq.s32	%p22, %r202, 0;
	shl.b32 	%r163, %r198, %r202;
	mov.u32 	%r164, 32;
	sub.s32 	%r165, %r164, %r202;
	shr.u32 	%r166, %r200, %r165;
	add.s32 	%r167, %r166, %r163;
	selp.b32	%r67, %r198, %r167, %p22;
	mov.u32 	%r168, -921707870;
	mul.hi.u32 	%r201, %r67, %r168;
	setp.lt.s32	%p23, %r201, 1;
	@%p23 bra 	BB94_34;

	mul.lo.s32 	%r169, %r67, -921707870;
	shr.u32 	%r170, %r169, 31;
	shl.b32 	%r171, %r201, 1;
	add.s32 	%r201, %r170, %r171;
	add.s32 	%r202, %r202, 1;

BB94_34:
	mov.u32 	%r172, 126;
	sub.s32 	%r173, %r172, %r202;
	shl.b32 	%r174, %r173, 23;
	add.s32 	%r175, %r201, 1;
	shr.u32 	%r176, %r175, 7;
	add.s32 	%r177, %r176, 1;
	shr.u32 	%r178, %r177, 1;
	add.s32 	%r179, %r178, %r174;
	or.b32  	%r180, %r179, %r63;
	mov.b32 	 %f114, %r180;

BB94_35:
	mul.rn.f32 	%f27, %f114, %f114;
	and.b32  	%r74, %r203, 1;
	setp.eq.s32	%p24, %r74, 0;
	@%p24 bra 	BB94_37;

	mov.f32 	%f94, 0fBAB6061A;
	mov.f32 	%f95, 0f37CCF5CE;
	fma.rn.f32 	%f115, %f95, %f27, %f94;
	bra.uni 	BB94_38;

BB94_37:
	mov.f32 	%f96, 0f3C08839E;
	mov.f32 	%f97, 0fB94CA1F9;
	fma.rn.f32 	%f115, %f97, %f27, %f96;

BB94_38:
	@%p24 bra 	BB94_40;

	mov.f32 	%f98, 0f3D2AAAA5;
	fma.rn.f32 	%f99, %f115, %f27, %f98;
	mov.f32 	%f100, 0fBF000000;
	fma.rn.f32 	%f116, %f99, %f27, %f100;
	bra.uni 	BB94_41;

BB94_40:
	mov.f32 	%f101, 0fBE2AAAA3;
	fma.rn.f32 	%f102, %f115, %f27, %f101;
	mov.f32 	%f103, 0f00000000;
	fma.rn.f32 	%f116, %f102, %f27, %f103;

BB94_41:
	fma.rn.f32 	%f117, %f116, %f114, %f114;
	@%p24 bra 	BB94_43;

	mov.f32 	%f104, 0f3F800000;
	fma.rn.f32 	%f117, %f116, %f27, %f104;

BB94_43:
	and.b32  	%r181, %r203, 2;
	setp.eq.s32	%p27, %r181, 0;
	@%p27 bra 	BB94_45;

	mov.f32 	%f105, 0f00000000;
	mov.f32 	%f106, 0fBF800000;
	fma.rn.f32 	%f117, %f117, %f106, %f105;

BB94_45:
	cvta.to.global.u64 	%rd67, %rd23;
	add.s64 	%rd69, %rd67, %rd28;
	ld.global.u32 	%r182, [%rd69];
	add.s32 	%r183, %r182, %r39;
	mul.wide.s32 	%rd71, %r183, 4;
	add.s64 	%rd72, %rd57, %rd71;
	mul.f32 	%f107, %f21, %f117;
	st.global.f32 	[%rd72], %f107;

BB94_46:
	ret;
}

	// .globl	vec_thetest
.visible .entry vec_thetest(
	.param .u32 vec_thetest_param_0,
	.param .u64 vec_thetest_param_1
)
{
	.local .align 8 .b8 	__local_depot95[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<10>;


	mov.u64 	%rd9, __local_depot95;
	cvta.local.u64 	%SP, %rd9;
	ld.param.u32 	%r2, [vec_thetest_param_0];
	ld.param.u64 	%rd1, [vec_thetest_param_1];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB95_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	cvt.f64.f32	%fd1, %f1;
	add.u64 	%rd5, %SP, 0;
	cvta.to.local.u64 	%rd6, %rd5;
	st.local.u32 	[%rd6], %r1;
	st.local.f64 	[%rd6+8], %fd1;
	mov.u64 	%rd7, $str1;
	cvta.global.u64 	%rd8, %rd7;
	// Callseq Start 39
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r12, [retval0+0];
	
	//{
	}// Callseq End 39

BB95_2:
	ret;
}

	// .globl	vec_computePSF_signal
.visible .entry vec_computePSF_signal(
	.param .u32 vec_computePSF_signal_param_0,
	.param .u64 vec_computePSF_signal_param_1,
	.param .u64 vec_computePSF_signal_param_2,
	.param .u64 vec_computePSF_signal_param_3,
	.param .f64 vec_computePSF_signal_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_computePSF_signal_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signal_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signal_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_signal_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signal_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB96_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd4, [%rd8];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd7;

BB96_2:
	ret;
}

	// .globl	vec_computePSF_signalN
.visible .entry vec_computePSF_signalN(
	.param .u32 vec_computePSF_signalN_param_0,
	.param .u64 vec_computePSF_signalN_param_1,
	.param .f64 vec_computePSF_signalN_param_2,
	.param .u64 vec_computePSF_signalN_param_3,
	.param .u64 vec_computePSF_signalN_param_4,
	.param .u64 vec_computePSF_signalN_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computePSF_signalN_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalN_param_1];
	ld.param.f64 	%fd1, [vec_computePSF_signalN_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalN_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalN_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalN_param_5];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB97_2;

	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r12, [%rd8];
	mul.wide.s32 	%rd9, %r12, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r13, [%rd12];
	mul.wide.s32 	%rd13, %r13, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd7;

BB97_2:
	ret;
}

	// .globl	vec_computePSF_signalN2
.visible .entry vec_computePSF_signalN2(
	.param .u32 vec_computePSF_signalN2_param_0,
	.param .f64 vec_computePSF_signalN2_param_1,
	.param .u64 vec_computePSF_signalN2_param_2,
	.param .u64 vec_computePSF_signalN2_param_3,
	.param .u64 vec_computePSF_signalN2_param_4,
	.param .u64 vec_computePSF_signalN2_param_5,
	.param .u64 vec_computePSF_signalN2_param_6,
	.param .u64 vec_computePSF_signalN2_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r2, [vec_computePSF_signalN2_param_0];
	ld.param.f64 	%fd1, [vec_computePSF_signalN2_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2_param_5];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2_param_6];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2_param_7];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB98_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%r12, [%rd11];
	mul.wide.s32 	%rd12, %r12, 8;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f64 	%fd2, [%rd13];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r13, [%rd15];
	mul.wide.s32 	%rd16, %r13, 8;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f64 	%fd4, [%rd17];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	ld.global.u32 	%r14, [%rd19];
	mul.wide.s32 	%rd20, %r14, 8;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f64 	[%rd21], %fd7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r15, [%rd23];
	mul.wide.s32 	%rd24, %r15, 8;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u64 	%rd26, 0;
	st.global.u64 	[%rd25], %rd26;

BB98_2:
	ret;
}

	// .globl	vec_computePSF_signalN2Many
.visible .entry vec_computePSF_signalN2Many(
	.param .u32 vec_computePSF_signalN2Many_param_0,
	.param .u32 vec_computePSF_signalN2Many_param_1,
	.param .u32 vec_computePSF_signalN2Many_param_2,
	.param .f64 vec_computePSF_signalN2Many_param_3,
	.param .u64 vec_computePSF_signalN2Many_param_4,
	.param .u64 vec_computePSF_signalN2Many_param_5,
	.param .u64 vec_computePSF_signalN2Many_param_6,
	.param .u64 vec_computePSF_signalN2Many_param_7,
	.param .u64 vec_computePSF_signalN2Many_param_8,
	.param .u64 vec_computePSF_signalN2Many_param_9
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<27>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r4, [vec_computePSF_signalN2Many_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalN2Many_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalN2Many_param_2];
	ld.param.f64 	%fd1, [vec_computePSF_signalN2Many_param_3];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2Many_param_4];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2Many_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2Many_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2Many_param_7];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2Many_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2Many_param_9];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB99_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	rem.s32 	%r14, %r1, %r2;
	mul.wide.s32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	div.s32 	%r15, %r1, %r2;
	mul.lo.s32 	%r16, %r15, %r3;
	ld.global.u32 	%r17, [%rd11];
	add.s32 	%r18, %r17, %r16;
	mul.wide.s32 	%rd12, %r18, 8;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f64 	%fd2, [%rd13];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r19, [%rd15];
	add.s32 	%r20, %r19, %r16;
	mul.wide.s32 	%rd16, %r20, 8;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f64 	%fd4, [%rd17];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	mul.lo.s32 	%r21, %r2, %r15;
	shl.b32 	%r22, %r21, 1;
	ld.global.u32 	%r23, [%rd19];
	add.s32 	%r24, %r23, %r22;
	mul.wide.s32 	%rd20, %r24, 8;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f64 	[%rd21], %fd7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r25, [%rd23];
	add.s32 	%r26, %r25, %r22;
	mul.wide.s32 	%rd24, %r26, 8;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u64 	%rd26, 0;
	st.global.u64 	[%rd25], %rd26;

BB99_2:
	ret;
}

	// .globl	vec_computePSF_signalN2Many_f
.visible .entry vec_computePSF_signalN2Many_f(
	.param .u32 vec_computePSF_signalN2Many_f_param_0,
	.param .u32 vec_computePSF_signalN2Many_f_param_1,
	.param .u32 vec_computePSF_signalN2Many_f_param_2,
	.param .f32 vec_computePSF_signalN2Many_f_param_3,
	.param .u64 vec_computePSF_signalN2Many_f_param_4,
	.param .u64 vec_computePSF_signalN2Many_f_param_5,
	.param .u64 vec_computePSF_signalN2Many_f_param_6,
	.param .u64 vec_computePSF_signalN2Many_f_param_7,
	.param .u64 vec_computePSF_signalN2Many_f_param_8,
	.param .u64 vec_computePSF_signalN2Many_f_param_9
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<26>;


	ld.param.u32 	%r4, [vec_computePSF_signalN2Many_f_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalN2Many_f_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalN2Many_f_param_2];
	ld.param.f32 	%f1, [vec_computePSF_signalN2Many_f_param_3];
	ld.param.u64 	%rd1, [vec_computePSF_signalN2Many_f_param_4];
	ld.param.u64 	%rd2, [vec_computePSF_signalN2Many_f_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalN2Many_f_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalN2Many_f_param_7];
	ld.param.u64 	%rd5, [vec_computePSF_signalN2Many_f_param_8];
	ld.param.u64 	%rd6, [vec_computePSF_signalN2Many_f_param_9];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB100_2;

	cvta.to.global.u64 	%rd7, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd9, %rd1;
	rem.s32 	%r14, %r1, %r2;
	mul.wide.s32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	div.s32 	%r15, %r1, %r2;
	mul.lo.s32 	%r16, %r15, %r3;
	ld.global.u32 	%r17, [%rd11];
	add.s32 	%r18, %r17, %r16;
	mul.wide.s32 	%rd12, %r18, 4;
	add.s64 	%rd13, %rd8, %rd12;
	ld.global.f32 	%f2, [%rd13];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd10;
	ld.global.u32 	%r19, [%rd15];
	add.s32 	%r20, %r19, %r16;
	mul.wide.s32 	%rd16, %r20, 4;
	add.s64 	%rd17, %rd8, %rd16;
	ld.global.f32 	%f4, [%rd17];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	cvta.to.global.u64 	%rd18, %rd4;
	add.s64 	%rd19, %rd18, %rd10;
	mul.lo.s32 	%r21, %r2, %r15;
	shl.b32 	%r22, %r21, 1;
	ld.global.u32 	%r23, [%rd19];
	add.s32 	%r24, %r23, %r22;
	mul.wide.s32 	%rd20, %r24, 4;
	add.s64 	%rd21, %rd7, %rd20;
	st.global.f32 	[%rd21], %f7;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd23, %rd22, %rd10;
	ld.global.u32 	%r25, [%rd23];
	add.s32 	%r26, %r25, %r22;
	mul.wide.s32 	%rd24, %r26, 4;
	add.s64 	%rd25, %rd7, %rd24;
	mov.u32 	%r27, 0;
	st.global.u32 	[%rd25], %r27;

BB100_2:
	ret;
}

	// .globl	vec_computePSF_signalsqrt
.visible .entry vec_computePSF_signalsqrt(
	.param .u32 vec_computePSF_signalsqrt_param_0,
	.param .u64 vec_computePSF_signalsqrt_param_1,
	.param .u64 vec_computePSF_signalsqrt_param_2,
	.param .u64 vec_computePSF_signalsqrt_param_3,
	.param .f64 vec_computePSF_signalsqrt_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r2, [vec_computePSF_signalsqrt_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalsqrt_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signalsqrt_param_2];
	ld.param.u64 	%rd3, [vec_computePSF_signalsqrt_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signalsqrt_param_4];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB101_2;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd2, [%rd6];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd7, %rd3;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f64 	%fd4, [%rd8];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd9, %rd1;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f64 	[%rd10], %fd8;

BB101_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrt
.visible .entry vec_computePSF_signalNsqrt(
	.param .u32 vec_computePSF_signalNsqrt_param_0,
	.param .u64 vec_computePSF_signalNsqrt_param_1,
	.param .u64 vec_computePSF_signalNsqrt_param_2,
	.param .f64 vec_computePSF_signalNsqrt_param_3,
	.param .u64 vec_computePSF_signalNsqrt_param_4,
	.param .u64 vec_computePSF_signalNsqrt_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computePSF_signalNsqrt_param_0];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrt_param_1];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrt_param_2];
	ld.param.f64 	%fd1, [vec_computePSF_signalNsqrt_param_3];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrt_param_4];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrt_param_5];
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.x;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB102_2;

	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r12, [%rd8];
	mul.wide.s32 	%rd9, %r12, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r13, [%rd12];
	mul.wide.s32 	%rd13, %r13, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd8;

BB102_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany
.visible .entry vec_computePSF_signalNsqrtMany(
	.param .u32 vec_computePSF_signalNsqrtMany_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_param_1,
	.param .u64 vec_computePSF_signalNsqrtMany_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_param_3,
	.param .f64 vec_computePSF_signalNsqrtMany_param_4,
	.param .u64 vec_computePSF_signalNsqrtMany_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_param_3];
	ld.param.f64 	%fd1, [vec_computePSF_signalNsqrtMany_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_param_6];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB103_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	sub.s32 	%r14, %r1, %r13;
	shl.b32 	%r15, %r14, 1;
	ld.global.u32 	%r16, [%rd8];
	add.s32 	%r17, %r15, %r16;
	mul.wide.s32 	%rd9, %r17, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r18, [%rd12];
	add.s32 	%r19, %r18, %r15;
	mul.wide.s32 	%rd13, %r19, 8;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f64 	%fd4, [%rd14];
	div.rn.f64 	%fd5, %fd4, %fd1;
	mul.f64 	%fd6, %fd5, %fd5;
	fma.rn.f64 	%fd7, %fd3, %fd3, %fd6;
	sqrt.rn.f64 	%fd8, %fd7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f64 	[%rd17], %fd8;

BB103_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany_f
.visible .entry vec_computePSF_signalNsqrtMany_f(
	.param .u32 vec_computePSF_signalNsqrtMany_f_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_f_param_1,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_3,
	.param .f32 vec_computePSF_signalNsqrtMany_f_param_4,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_f_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_f_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_f_param_1];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_f_param_2];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_f_param_3];
	ld.param.f32 	%f1, [vec_computePSF_signalNsqrtMany_f_param_4];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_f_param_5];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_f_param_6];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB104_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	sub.s32 	%r14, %r1, %r13;
	shl.b32 	%r15, %r14, 1;
	ld.global.u32 	%r16, [%rd8];
	add.s32 	%r17, %r15, %r16;
	mul.wide.s32 	%rd9, %r17, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r18, [%rd12];
	add.s32 	%r19, %r18, %r15;
	mul.wide.s32 	%rd13, %r19, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f32 	%f4, [%rd14];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	sqrt.rn.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f8;

BB104_2:
	ret;
}

	// .globl	vec_computePSF_signalNsqrtMany_fcrop
.visible .entry vec_computePSF_signalNsqrtMany_fcrop(
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_0,
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_1,
	.param .u32 vec_computePSF_signalNsqrtMany_fcrop_param_2,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_3,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_4,
	.param .f32 vec_computePSF_signalNsqrtMany_fcrop_param_5,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_6,
	.param .u64 vec_computePSF_signalNsqrtMany_fcrop_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r4, [vec_computePSF_signalNsqrtMany_fcrop_param_0];
	ld.param.u32 	%r2, [vec_computePSF_signalNsqrtMany_fcrop_param_1];
	ld.param.u32 	%r3, [vec_computePSF_signalNsqrtMany_fcrop_param_2];
	ld.param.u64 	%rd1, [vec_computePSF_signalNsqrtMany_fcrop_param_3];
	ld.param.u64 	%rd2, [vec_computePSF_signalNsqrtMany_fcrop_param_4];
	ld.param.f32 	%f1, [vec_computePSF_signalNsqrtMany_fcrop_param_5];
	ld.param.u64 	%rd3, [vec_computePSF_signalNsqrtMany_fcrop_param_6];
	ld.param.u64 	%rd4, [vec_computePSF_signalNsqrtMany_fcrop_param_7];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB105_2;

	cvta.to.global.u64 	%rd5, %rd2;
	rem.s32 	%r14, %r1, %r2;
	div.s32 	%r15, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	mul.lo.s32 	%r16, %r3, %r15;
	shl.b32 	%r17, %r16, 1;
	ld.global.u32 	%r18, [%rd8];
	add.s32 	%r19, %r18, %r17;
	mul.wide.s32 	%rd9, %r19, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f2, [%rd10];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.u32 	%r20, [%rd12];
	add.s32 	%r21, %r20, %r17;
	mul.wide.s32 	%rd13, %r21, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.f32 	%f4, [%rd14];
	div.rn.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f5;
	fma.rn.f32 	%f7, %f3, %f3, %f6;
	sqrt.rn.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd15, %rd1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.f32 	[%rd17], %f8;

BB105_2:
	ret;
}

	// .globl	vec_mulMany
.visible .entry vec_mulMany(
	.param .u32 vec_mulMany_param_0,
	.param .u32 vec_mulMany_param_1,
	.param .u64 vec_mulMany_param_2,
	.param .u64 vec_mulMany_param_3,
	.param .u64 vec_mulMany_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_mulMany_param_0];
	ld.param.u32 	%r2, [vec_mulMany_param_1];
	ld.param.u64 	%rd1, [vec_mulMany_param_2];
	ld.param.u64 	%rd2, [vec_mulMany_param_3];
	ld.param.u64 	%rd3, [vec_mulMany_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB106_2;

	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd6];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f64 	[%rd11], %fd3;

BB106_2:
	ret;
}

	// .globl	vec_divScalarMany
.visible .entry vec_divScalarMany(
	.param .u32 vec_divScalarMany_param_0,
	.param .u32 vec_divScalarMany_param_1,
	.param .u64 vec_divScalarMany_param_2,
	.param .u64 vec_divScalarMany_param_3,
	.param .u64 vec_divScalarMany_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_divScalarMany_param_0];
	ld.param.u32 	%r2, [vec_divScalarMany_param_1];
	ld.param.u64 	%rd1, [vec_divScalarMany_param_2];
	ld.param.u64 	%rd2, [vec_divScalarMany_param_3];
	ld.param.u64 	%rd3, [vec_divScalarMany_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB107_3;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r13, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	setp.eq.f64	%p2, %fd1, 0d0000000000000000;
	@%p2 bra 	BB107_3;

	cvta.to.global.u64 	%rd7, %rd1;
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	div.rn.f64 	%fd3, %fd2, %fd1;
	add.s64 	%rd11, %rd7, %rd9;
	st.global.f64 	[%rd11], %fd3;

BB107_3:
	ret;
}

	// .globl	vec_mulMany_f
.visible .entry vec_mulMany_f(
	.param .u32 vec_mulMany_f_param_0,
	.param .u32 vec_mulMany_f_param_1,
	.param .u64 vec_mulMany_f_param_2,
	.param .u64 vec_mulMany_f_param_3,
	.param .u64 vec_mulMany_f_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_mulMany_f_param_0];
	ld.param.u32 	%r2, [vec_mulMany_f_param_1];
	ld.param.u64 	%rd1, [vec_mulMany_f_param_2];
	ld.param.u64 	%rd2, [vec_mulMany_f_param_3];
	ld.param.u64 	%rd3, [vec_mulMany_f_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB108_2;

	rem.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f3;

BB108_2:
	ret;
}

	// .globl	vec_computeModelMany1
.visible .entry vec_computeModelMany1(
	.param .u32 vec_computeModelMany1_param_0,
	.param .u32 vec_computeModelMany1_param_1,
	.param .u64 vec_computeModelMany1_param_2,
	.param .u64 vec_computeModelMany1_param_3,
	.param .u64 vec_computeModelMany1_param_4,
	.param .f64 vec_computeModelMany1_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_computeModelMany1_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany1_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany1_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany1_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany1_param_4];
	ld.param.f64 	%fd1, [vec_computeModelMany1_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB109_2;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 8;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r13, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f64 	%fd2, [%rd9];
	ld.global.f64 	%fd3, [%rd6];
	fma.rn.f64 	%fd4, %fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f64 	[%rd11], %fd4;

BB109_2:
	ret;
}

	// .globl	vec_computeModelMany2
.visible .entry vec_computeModelMany2(
	.param .u32 vec_computeModelMany2_param_0,
	.param .u32 vec_computeModelMany2_param_1,
	.param .u64 vec_computeModelMany2_param_2,
	.param .u64 vec_computeModelMany2_param_3,
	.param .u64 vec_computeModelMany2_param_4,
	.param .u64 vec_computeModelMany2_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [vec_computeModelMany2_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany2_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany2_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany2_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany2_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany2_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB110_2;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd7];
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.f64 	%fd3, [%rd12];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd6;
	st.global.f64 	[%rd14], %fd4;

BB110_2:
	ret;
}

	// .globl	vec_computeModelMany3
.visible .entry vec_computeModelMany3(
	.param .u32 vec_computeModelMany3_param_0,
	.param .u32 vec_computeModelMany3_param_1,
	.param .u64 vec_computeModelMany3_param_2,
	.param .u64 vec_computeModelMany3_param_3,
	.param .u64 vec_computeModelMany3_param_4,
	.param .u64 vec_computeModelMany3_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r3, [vec_computeModelMany3_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany3_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany3_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany3_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany3_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany3_param_5];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB111_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd7];
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r14, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd3, [%rd13];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd14, %rd1;
	add.s64 	%rd15, %rd14, %rd6;
	st.global.f64 	[%rd15], %fd4;

BB111_2:
	ret;
}

	// .globl	vec_computeModelMany1_scmos
.visible .entry vec_computeModelMany1_scmos(
	.param .u32 vec_computeModelMany1_scmos_param_0,
	.param .u32 vec_computeModelMany1_scmos_param_1,
	.param .u64 vec_computeModelMany1_scmos_param_2,
	.param .u64 vec_computeModelMany1_scmos_param_3,
	.param .u64 vec_computeModelMany1_scmos_param_4,
	.param .f64 vec_computeModelMany1_scmos_param_5,
	.param .u64 vec_computeModelMany1_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r3, [vec_computeModelMany1_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany1_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany1_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany1_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany1_scmos_param_4];
	ld.param.f64 	%fd1, [vec_computeModelMany1_scmos_param_5];
	ld.param.u64 	%rd4, [vec_computeModelMany1_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB112_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r13, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd2, [%rd10];
	ld.global.f64 	%fd3, [%rd7];
	fma.rn.f64 	%fd4, %fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r14, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd5, [%rd13];
	add.f64 	%fd6, %fd5, %fd4;
	cvta.to.global.u64 	%rd14, %rd1;
	add.s64 	%rd15, %rd14, %rd6;
	st.global.f64 	[%rd15], %fd6;

BB112_2:
	ret;
}

	// .globl	vec_computeModelMany2_scmos
.visible .entry vec_computeModelMany2_scmos(
	.param .u32 vec_computeModelMany2_scmos_param_0,
	.param .u32 vec_computeModelMany2_scmos_param_1,
	.param .u64 vec_computeModelMany2_scmos_param_2,
	.param .u64 vec_computeModelMany2_scmos_param_3,
	.param .u64 vec_computeModelMany2_scmos_param_4,
	.param .u64 vec_computeModelMany2_scmos_param_5,
	.param .u64 vec_computeModelMany2_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r3, [vec_computeModelMany2_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany2_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany2_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany2_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany2_scmos_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany2_scmos_param_5];
	ld.param.u64 	%rd5, [vec_computeModelMany2_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB113_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r13, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	ld.global.f64 	%fd2, [%rd8];
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f64 	%fd3, [%rd13];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd14, %rd5;
	mul.wide.s32 	%rd15, %r14, 8;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f64 	%fd5, [%rd16];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd17, %rd1;
	add.s64 	%rd18, %rd17, %rd7;
	st.global.f64 	[%rd18], %fd6;

BB113_2:
	ret;
}

	// .globl	vec_computeModelMany3_scmos
.visible .entry vec_computeModelMany3_scmos(
	.param .u32 vec_computeModelMany3_scmos_param_0,
	.param .u32 vec_computeModelMany3_scmos_param_1,
	.param .u64 vec_computeModelMany3_scmos_param_2,
	.param .u64 vec_computeModelMany3_scmos_param_3,
	.param .u64 vec_computeModelMany3_scmos_param_4,
	.param .u64 vec_computeModelMany3_scmos_param_5,
	.param .u64 vec_computeModelMany3_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<19>;


	ld.param.u32 	%r3, [vec_computeModelMany3_scmos_param_0];
	ld.param.u32 	%r2, [vec_computeModelMany3_scmos_param_1];
	ld.param.u64 	%rd1, [vec_computeModelMany3_scmos_param_2];
	ld.param.u64 	%rd2, [vec_computeModelMany3_scmos_param_3];
	ld.param.u64 	%rd3, [vec_computeModelMany3_scmos_param_4];
	ld.param.u64 	%rd4, [vec_computeModelMany3_scmos_param_5];
	ld.param.u64 	%rd5, [vec_computeModelMany3_scmos_param_6];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB114_2;

	div.s32 	%r13, %r1, %r2;
	rem.s32 	%r14, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.s32 	%rd10, %r13, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	ld.global.f64 	%fd2, [%rd8];
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r14, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f64 	%fd3, [%rd14];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd16, %rd15, %rd13;
	ld.global.f64 	%fd5, [%rd16];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd17, %rd1;
	add.s64 	%rd18, %rd17, %rd7;
	st.global.f64 	[%rd18], %fd6;

BB114_2:
	ret;
}

	// .globl	vec_divScalarMany_f
.visible .entry vec_divScalarMany_f(
	.param .u32 vec_divScalarMany_f_param_0,
	.param .u32 vec_divScalarMany_f_param_1,
	.param .u64 vec_divScalarMany_f_param_2,
	.param .u64 vec_divScalarMany_f_param_3,
	.param .u64 vec_divScalarMany_f_param_4,
	.param .u64 vec_divScalarMany_f_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r3, [vec_divScalarMany_f_param_0];
	ld.param.u32 	%r2, [vec_divScalarMany_f_param_1];
	ld.param.u64 	%rd1, [vec_divScalarMany_f_param_2];
	ld.param.u64 	%rd2, [vec_divScalarMany_f_param_3];
	ld.param.u64 	%rd3, [vec_divScalarMany_f_param_4];
	ld.param.u64 	%rd4, [vec_divScalarMany_f_param_5];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB115_3;

	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r13, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	setp.eq.f32	%p2, %f1, 0f00000000;
	@%p2 bra 	BB115_3;

	cvta.to.global.u64 	%rd8, %rd2;
	cvta.to.global.u64 	%rd9, %rd1;
	cvta.to.global.u64 	%rd10, %rd3;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f2, [%rd12];
	div.rn.f32 	%f3, %f2, %f1;
	add.s64 	%rd13, %rd9, %rd11;
	st.global.f32 	[%rd13], %f3;
	mul.wide.s32 	%rd14, %r1, 8;
	add.s64 	%rd15, %rd8, %rd14;
	mov.u64 	%rd16, 0;
	st.global.u64 	[%rd15], %rd16;
	ld.global.f32 	%f4, [%rd13];
	cvt.f64.f32	%fd1, %f4;
	st.global.f64 	[%rd15], %fd1;

BB115_3:
	ret;
}

	// .globl	vec_computePoissonLikelihood
.visible .entry vec_computePoissonLikelihood(
	.param .u32 vec_computePoissonLikelihood_param_0,
	.param .u64 vec_computePoissonLikelihood_param_1,
	.param .u64 vec_computePoissonLikelihood_param_2,
	.param .u64 vec_computePoissonLikelihood_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<39>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r12, [vec_computePoissonLikelihood_param_0];
	ld.param.u64 	%rd3, [vec_computePoissonLikelihood_param_1];
	ld.param.u64 	%rd4, [vec_computePoissonLikelihood_param_2];
	ld.param.u64 	%rd5, [vec_computePoissonLikelihood_param_3];
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %nctaid.x;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r1, %r19, %r20, %r21;
	setp.ge.s32	%p1, %r1, %r12;
	@%p1 bra 	BB116_11;

	cvta.to.global.u64 	%rd6, %rd5;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	setp.gt.f64	%p2, %fd1, 0d0000000000000000;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd2, %rd9, %rd7;
	@%p2 bra 	BB116_3;
	bra.uni 	BB116_2;

BB116_3:
	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd1, 3;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd2, [%rd13];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd1;
	}
	mov.u32 	%r37, -1023;
	setp.gt.s32	%p3, %r35, 1048575;
	mov.f64 	%fd59, %fd1;
	@%p3 bra 	BB116_5;

	mul.f64 	%fd3, %fd1, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd3;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd3;
	}
	mov.u32 	%r37, -1077;
	mov.f64 	%fd59, %fd3;

BB116_5:
	mov.f64 	%fd4, %fd59;
	add.s32 	%r24, %r35, -1;
	setp.lt.u32	%p4, %r24, 2146435071;
	@%p4 bra 	BB116_7;
	bra.uni 	BB116_6;

BB116_7:
	shr.u32 	%r26, %r35, 20;
	add.s32 	%r38, %r37, %r26;
	and.b32  	%r27, %r35, -2146435073;
	or.b32  	%r28, %r27, 1072693248;
	mov.b64 	%fd60, {%r36, %r28};
	setp.lt.s32	%p6, %r28, 1073127583;
	@%p6 bra 	BB116_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd60;
	}
	add.s32 	%r31, %r30, -1048576;
	mov.b64 	%fd60, {%r29, %r31};
	add.s32 	%r38, %r38, 1;

BB116_9:
	add.f64 	%fd14, %fd60, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd13,%fd14;
	// inline asm
	neg.f64 	%fd15, %fd14;
	mov.f64 	%fd16, 0d3FF0000000000000;
	fma.rn.f64 	%fd17, %fd15, %fd13, %fd16;
	fma.rn.f64 	%fd18, %fd17, %fd17, %fd17;
	fma.rn.f64 	%fd19, %fd18, %fd13, %fd13;
	add.f64 	%fd20, %fd60, 0dBFF0000000000000;
	mul.f64 	%fd21, %fd20, %fd19;
	fma.rn.f64 	%fd22, %fd20, %fd19, %fd21;
	mul.f64 	%fd23, %fd22, %fd22;
	mov.f64 	%fd24, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd25, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd26, %fd25, %fd23, %fd24;
	mov.f64 	%fd27, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	mov.f64 	%fd29, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd30, %fd28, %fd23, %fd29;
	mov.f64 	%fd31, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd32, %fd30, %fd23, %fd31;
	mov.f64 	%fd33, 0d3F624924923BE72D;
	fma.rn.f64 	%fd34, %fd32, %fd23, %fd33;
	mov.f64 	%fd35, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd36, %fd34, %fd23, %fd35;
	mov.f64 	%fd37, 0d3FB5555555555554;
	fma.rn.f64 	%fd38, %fd36, %fd23, %fd37;
	sub.f64 	%fd39, %fd20, %fd22;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd22;
	fma.rn.f64 	%fd42, %fd41, %fd20, %fd40;
	mul.f64 	%fd43, %fd19, %fd42;
	mul.f64 	%fd44, %fd23, %fd38;
	fma.rn.f64 	%fd45, %fd44, %fd22, %fd43;
	xor.b32  	%r32, %r38, -2147483648;
	mov.u32 	%r33, 1127219200;
	mov.b64 	%fd46, {%r32, %r33};
	mov.u32 	%r34, -2147483648;
	mov.b64 	%fd47, {%r34, %r33};
	sub.f64 	%fd48, %fd46, %fd47;
	mov.f64 	%fd49, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd50, %fd48, %fd49, %fd22;
	neg.f64 	%fd51, %fd48;
	fma.rn.f64 	%fd52, %fd51, %fd49, %fd50;
	sub.f64 	%fd53, %fd52, %fd22;
	sub.f64 	%fd54, %fd45, %fd53;
	mov.f64 	%fd55, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd56, %fd48, %fd55, %fd54;
	add.f64 	%fd61, %fd50, %fd56;
	bra.uni 	BB116_10;

BB116_2:
	mov.u64 	%rd10, 4711630319722168320;
	st.global.u64 	[%rd2], %rd10;
	bra.uni 	BB116_11;

BB116_6:
	mov.f64 	%fd11, 0d7FF0000000000000;
	fma.rn.f64 	%fd12, %fd4, %fd11, %fd11;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd4;
	}
	mov.b32 	 %f1, %r25;
	setp.eq.f32	%p5, %f1, 0f00000000;
	selp.f64	%fd61, 0dFFF0000000000000, %fd12, %p5;

BB116_10:
	mul.f64 	%fd57, %fd2, %fd61;
	sub.f64 	%fd58, %fd1, %fd57;
	st.global.f64 	[%rd2], %fd58;

BB116_11:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundManyReshuffle
.visible .entry vec_addPhotonsAndBackgroundManyReshuffle(
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_0,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_1,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_param_2,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_3,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_4,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundManyReshuffle_param_0];
	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundManyReshuffle_param_1];
	ld.param.u32 	%r4, [vec_addPhotonsAndBackgroundManyReshuffle_param_2];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundManyReshuffle_param_3];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundManyReshuffle_param_4];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundManyReshuffle_param_5];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB117_2;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	div.s32 	%r14, %r1, %r3;
	div.s32 	%r15, %r14, %r4;
	rem.s32 	%r16, %r14, %r4;
	mul.lo.s32 	%r17, %r4, %r3;
	div.s32 	%r18, %r2, %r17;
	mad.lo.s32 	%r19, %r18, %r16, %r15;
	rem.s32 	%r20, %r1, %r3;
	mad.lo.s32 	%r21, %r19, %r3, %r20;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	shl.b32 	%r22, %r14, 1;
	mul.wide.s32 	%rd8, %r22, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	ld.global.f64 	%fd3, [%rd9+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r21, 8;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f64 	[%rd12], %fd4;

BB117_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundManyReshuffle_scmos
.visible .entry vec_addPhotonsAndBackgroundManyReshuffle_scmos(
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_0,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_1,
	.param .u32 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_2,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_3,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_4,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_5,
	.param .u64 vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_0];
	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_1];
	ld.param.u32 	%r4, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_2];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_3];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_4];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_5];
	ld.param.u64 	%rd4, [vec_addPhotonsAndBackgroundManyReshuffle_scmos_param_6];
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB118_2;

	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	div.s32 	%r14, %r1, %r3;
	div.s32 	%r15, %r14, %r4;
	rem.s32 	%r16, %r14, %r4;
	mul.lo.s32 	%r17, %r4, %r3;
	div.s32 	%r18, %r2, %r17;
	mad.lo.s32 	%r19, %r18, %r16, %r15;
	rem.s32 	%r20, %r1, %r3;
	mad.lo.s32 	%r21, %r19, %r3, %r20;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	shl.b32 	%r22, %r14, 1;
	mul.wide.s32 	%rd9, %r22, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd8];
	ld.global.f64 	%fd3, [%rd10+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f64 	%fd5, [%rd12];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd13, %rd1;
	mul.wide.s32 	%rd14, %r21, 8;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f64 	[%rd15], %fd6;

BB118_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany
.visible .entry vec_addPhotonsAndBackgroundMany(
	.param .u32 vec_addPhotonsAndBackgroundMany_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB119_2;

	cvta.to.global.u64 	%rd4, %rd3;
	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd8, %r14, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd1, [%rd9];
	ld.global.f64 	%fd2, [%rd7];
	ld.global.f64 	%fd3, [%rd9+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.f64 	[%rd11], %fd4;

BB119_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany_scmos
.visible .entry vec_addPhotonsAndBackgroundMany_scmos(
	.param .u32 vec_addPhotonsAndBackgroundMany_scmos_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_scmos_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_4,
	.param .u64 vec_addPhotonsAndBackgroundMany_scmos_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_scmos_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_scmos_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_scmos_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_scmos_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_scmos_param_4];
	ld.param.u64 	%rd4, [vec_addPhotonsAndBackgroundMany_scmos_param_5];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB120_2;

	cvta.to.global.u64 	%rd5, %rd3;
	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd9, %r14, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	ld.global.f64 	%fd2, [%rd8];
	ld.global.f64 	%fd3, [%rd10+8];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f64 	%fd5, [%rd12];
	add.f64 	%fd6, %fd4, %fd5;
	cvta.to.global.u64 	%rd13, %rd1;
	add.s64 	%rd14, %rd13, %rd7;
	st.global.f64 	[%rd14], %fd6;

BB120_2:
	ret;
}

	// .globl	vec_addPhotonsAndBackgroundMany_f
.visible .entry vec_addPhotonsAndBackgroundMany_f(
	.param .u32 vec_addPhotonsAndBackgroundMany_f_param_0,
	.param .u32 vec_addPhotonsAndBackgroundMany_f_param_1,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_2,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_3,
	.param .u64 vec_addPhotonsAndBackgroundMany_f_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_addPhotonsAndBackgroundMany_f_param_0];
	ld.param.u32 	%r2, [vec_addPhotonsAndBackgroundMany_f_param_1];
	ld.param.u64 	%rd1, [vec_addPhotonsAndBackgroundMany_f_param_2];
	ld.param.u64 	%rd2, [vec_addPhotonsAndBackgroundMany_f_param_3];
	ld.param.u64 	%rd3, [vec_addPhotonsAndBackgroundMany_f_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB121_2;

	cvta.to.global.u64 	%rd4, %rd3;
	div.s32 	%r13, %r1, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	shl.b32 	%r14, %r13, 1;
	mul.wide.s32 	%rd8, %r14, 4;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	ld.global.f32 	%f3, [%rd9+4];
	fma.rn.f32 	%f4, %f2, %f1, %f3;
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd6;
	st.global.f32 	[%rd11], %f4;

BB121_2:
	ret;
}

	// .globl	vec_double2float
.visible .entry vec_double2float(
	.param .u32 vec_double2float_param_0,
	.param .u64 vec_double2float_param_1,
	.param .u64 vec_double2float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_double2float_param_0];
	ld.param.u64 	%rd1, [vec_double2float_param_1];
	ld.param.u64 	%rd2, [vec_double2float_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB122_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvt.rn.f32.f64	%f1, %fd1;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB122_2:
	ret;
}

	// .globl	vec_float2double
.visible .entry vec_float2double(
	.param .u32 vec_float2double_param_0,
	.param .u64 vec_float2double_param_1,
	.param .u64 vec_float2double_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<12>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r2, [vec_float2double_param_0];
	ld.param.u64 	%rd1, [vec_float2double_param_1];
	ld.param.u64 	%rd2, [vec_float2double_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB123_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvt.f64.f32	%fd1, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB123_2:
	ret;
}

	// .globl	vec_complexeConjugateKernel
.visible .entry vec_complexeConjugateKernel(
	.param .u32 vec_complexeConjugateKernel_param_0,
	.param .u32 vec_complexeConjugateKernel_param_1,
	.param .u64 vec_complexeConjugateKernel_param_2,
	.param .u64 vec_complexeConjugateKernel_param_3,
	.param .u64 vec_complexeConjugateKernel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r3, [vec_complexeConjugateKernel_param_0];
	ld.param.u32 	%r2, [vec_complexeConjugateKernel_param_1];
	ld.param.u64 	%rd1, [vec_complexeConjugateKernel_param_2];
	ld.param.u64 	%rd2, [vec_complexeConjugateKernel_param_3];
	ld.param.u64 	%rd3, [vec_complexeConjugateKernel_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	shl.b32 	%r1, %r13, 1;
	shl.b32 	%r14, %r3, 1;
	setp.ge.s32	%p1, %r1, %r14;
	@%p1 bra 	BB124_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	shl.b32 	%r15, %r2, 1;
	rem.s32 	%r16, %r1, %r15;
	mul.wide.s32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvt.rn.f32.s32	%f1, %r2;
	sqrt.rn.f32 	%f2, %f1;
	ld.global.f32 	%f3, [%rd8];
	div.rn.f32 	%f4, %f3, %f2;
	ld.global.f32 	%f5, [%rd8+4];
	div.rn.f32 	%f6, %f5, %f2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.f32 	%f7, [%rd10+4];
	ld.global.f32 	%f8, [%rd10];
	mul.f32 	%f9, %f4, %f8;
	fma.rn.f32 	%f10, %f6, %f7, %f9;
	mul.f32 	%f11, %f6, %f8;
	mul.f32 	%f12, %f4, %f7;
	sub.f32 	%f13, %f11, %f12;
	add.s64 	%rd11, %rd4, %rd9;
	st.global.f32 	[%rd11+4], %f13;
	st.global.f32 	[%rd11], %f10;

BB124_2:
	ret;
}

	// .globl	vec_makeResultCorrelation
.visible .entry vec_makeResultCorrelation(
	.param .u32 vec_makeResultCorrelation_param_0,
	.param .u32 vec_makeResultCorrelation_param_1,
	.param .u32 vec_makeResultCorrelation_param_2,
	.param .u64 vec_makeResultCorrelation_param_3,
	.param .u64 vec_makeResultCorrelation_param_4,
	.param .u64 vec_makeResultCorrelation_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<18>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r4, [vec_makeResultCorrelation_param_0];
	ld.param.u32 	%r2, [vec_makeResultCorrelation_param_1];
	ld.param.u32 	%r3, [vec_makeResultCorrelation_param_2];
	ld.param.u64 	%rd1, [vec_makeResultCorrelation_param_3];
	ld.param.u64 	%rd2, [vec_makeResultCorrelation_param_4];
	ld.param.u64 	%rd3, [vec_makeResultCorrelation_param_5];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r6, %r7, %r8;
	mov.u32 	%r10, %nctaid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r12, %r9, %r10, %r11;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r1, %r12, %r13, %r5;
	setp.ge.s32	%p1, %r1, %r4;
	@%p1 bra 	BB125_2;

	rem.s32 	%r14, %r1, %r2;
	div.s32 	%r15, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r16, [%rd6];
	mad.lo.s32 	%r17, %r15, %r3, %r16;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r17, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	cvt.f64.f32	%fd1, %f1;
	cvt.rn.f32.s32	%f2, %r3;
	cvt.f64.f32	%fd2, %f2;
	mul.f64 	%fd3, %fd2, 0d3FE0000000000000;
	sqrt.rn.f64 	%fd4, %fd3;
	div.rn.f64 	%fd5, %fd1, %fd4;
	cvt.rn.f32.f64	%f3, %fd5;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f32 	[%rd12], %f3;

BB125_2:
	ret;
}

	// .globl	vec_divScalarFloat
.visible .entry vec_divScalarFloat(
	.param .u32 vec_divScalarFloat_param_0,
	.param .u64 vec_divScalarFloat_param_1,
	.param .u64 vec_divScalarFloat_param_2,
	.param .f32 vec_divScalarFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [vec_divScalarFloat_param_0];
	ld.param.u64 	%rd1, [vec_divScalarFloat_param_1];
	ld.param.u64 	%rd2, [vec_divScalarFloat_param_2];
	ld.param.f32 	%f1, [vec_divScalarFloat_param_3];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r3;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB126_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	div.rn.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

BB126_2:
	ret;
}

	// .globl	vec_computeCRLB
.visible .entry vec_computeCRLB(
	.param .u32 vec_computeCRLB_param_0,
	.param .u32 vec_computeCRLB_param_1,
	.param .u64 vec_computeCRLB_param_2,
	.param .u64 vec_computeCRLB_param_3,
	.param .f64 vec_computeCRLB_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r2, [vec_computeCRLB_param_0];
	ld.param.u32 	%r3, [vec_computeCRLB_param_1];
	ld.param.u64 	%rd2, [vec_computeCRLB_param_2];
	ld.param.u64 	%rd3, [vec_computeCRLB_param_3];
	ld.param.f64 	%fd4, [vec_computeCRLB_param_4];
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %tid.y;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r7, %r8, %r9;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB127_4;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.lo.s32 	%r13, %r3, %r3;
	div.s32 	%r14, %r2, %r13;
	cvta.to.global.u64 	%rd5, %rd2;
	div.s32 	%r15, %r1, %r14;
	div.s32 	%r16, %r15, %r3;
	rem.s32 	%r17, %r15, %r3;
	shl.b32 	%r18, %r16, 1;
	add.s32 	%r19, %r18, 2;
	rem.s32 	%r20, %r1, %r14;
	mad.lo.s32 	%r21, %r19, %r14, %r20;
	mul.wide.s32 	%rd6, %r21, 8;
	add.s64 	%rd7, %rd4, %rd6;
	add.s32 	%r22, %r18, 1;
	mad.lo.s32 	%r23, %r22, %r14, %r20;
	mul.wide.s32 	%rd8, %r23, 8;
	add.s64 	%rd9, %rd4, %rd8;
	ld.global.f64 	%fd5, [%rd9];
	ld.global.f64 	%fd6, [%rd7];
	sub.f64 	%fd7, %fd6, %fd5;
	add.f64 	%fd8, %fd4, %fd4;
	div.rn.f64 	%fd1, %fd7, %fd8;
	shl.b32 	%r24, %r17, 1;
	add.s32 	%r25, %r24, 2;
	mad.lo.s32 	%r26, %r25, %r14, %r20;
	mul.wide.s32 	%rd10, %r26, 8;
	add.s64 	%rd11, %rd4, %rd10;
	add.s32 	%r27, %r24, 1;
	mad.lo.s32 	%r28, %r27, %r14, %r20;
	mul.wide.s32 	%rd12, %r28, 8;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.f64 	%fd9, [%rd13];
	ld.global.f64 	%fd10, [%rd11];
	sub.f64 	%fd11, %fd10, %fd9;
	div.rn.f64 	%fd2, %fd11, %fd8;
	mul.wide.s32 	%rd14, %r20, 8;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.f64 	%fd3, [%rd15];
	setp.gt.f64	%p2, %fd3, 0d0000000000000000;
	mul.wide.s32 	%rd16, %r1, 8;
	add.s64 	%rd1, %rd5, %rd16;
	@%p2 bra 	BB127_3;
	bra.uni 	BB127_2;

BB127_3:
	mul.f64 	%fd12, %fd1, %fd2;
	div.rn.f64 	%fd13, %fd12, %fd3;
	st.global.f64 	[%rd1], %fd13;
	bra.uni 	BB127_4;

BB127_2:
	mov.u64 	%rd17, 4726483295884279808;
	st.global.u64 	[%rd1], %rd17;

BB127_4:
	ret;
}

	// .globl	vec_divCorrelation
.visible .entry vec_divCorrelation(
	.param .u32 vec_divCorrelation_param_0,
	.param .u64 vec_divCorrelation_param_1,
	.param .u32 vec_divCorrelation_param_2,
	.param .u64 vec_divCorrelation_param_3,
	.param .u64 vec_divCorrelation_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r3, [vec_divCorrelation_param_0];
	ld.param.u64 	%rd2, [vec_divCorrelation_param_1];
	ld.param.u32 	%r2, [vec_divCorrelation_param_2];
	ld.param.u64 	%rd3, [vec_divCorrelation_param_3];
	ld.param.u64 	%rd4, [vec_divCorrelation_param_4];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %nctaid.x;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r4;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB128_4;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.lo.s32 	%r13, %r2, %r2;
	rem.s32 	%r14, %r1, %r13;
	div.s32 	%r15, %r1, %r13;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	mul.wide.s32 	%rd10, %r15, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f2, [%rd11];
	ld.global.f32 	%f3, [%rd8];
	mul.f32 	%f1, %f3, %f2;
	setp.gt.f32	%p2, %f1, 0f00000000;
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd1, %rd5, %rd12;
	@%p2 bra 	BB128_3;
	bra.uni 	BB128_2;

BB128_3:
	ld.global.f32 	%f4, [%rd1];
	sqrt.rn.f32 	%f5, %f1;
	div.rn.f32 	%f6, %f4, %f5;
	st.global.f32 	[%rd1], %f6;
	bra.uni 	BB128_4;

BB128_2:
	mov.u32 	%r16, -1082130432;
	st.global.u32 	[%rd1], %r16;

BB128_4:
	ret;
}

	// .globl	_ZN6thrust6system4cuda6detail4cub_11EmptyKernelIvEEvv
.visible .entry _ZN6thrust6system4cuda6detail4cub_11EmptyKernelIvEEvv(

)
{



	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot130[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot130;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB130_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB130_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB130_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB130_3;

BB130_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB130_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB130_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.eq.s32	%p5, %r40, 0;
	selp.b32	%r34, %r32, %r33, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB130_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB130_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB130_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB130_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB130_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB130_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB130_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<49>;
	.reg .f64 	%fd<135>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd12;
	}
	shr.u32 	%r47, %r46, 20;
	setp.ne.s32	%p1, %r47, 0;
	@%p1 bra 	BB131_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd14;
	}
	shr.u32 	%r16, %r46, 20;
	add.s32 	%r47, %r16, -54;

BB131_2:
	add.s32 	%r48, %r47, -1023;
	and.b32  	%r17, %r46, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd133, {%r45, %r18};
	setp.lt.u32	%p2, %r18, 1073127583;
	@%p2 bra 	BB131_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd133;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd133, {%r19, %r21};
	add.s32 	%r48, %r47, -1022;

BB131_4:
	add.f64 	%fd16, %fd133, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd15,%fd16;
	// inline asm
	neg.f64 	%fd17, %fd16;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd15, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd15, %fd15;
	add.f64 	%fd22, %fd133, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r48, -2147483648;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd79, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd80, {%r27, %r26};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	add.s32 	%r29, %r28, %r28;
	setp.gt.u32	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd18;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd134, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	 %f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB131_7;

	setp.lt.f64	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64	%fd134, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB131_7;

	shr.u32 	%r36, %r13, 31;
	add.s32 	%r37, %r13, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r39, %r15;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r13, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd134, %fd130, %fd131;

BB131_7:
	abs.f64 	%fd132, %fd134;
	setp.eq.f64	%p7, %fd132, 0d7FF0000000000000;
	@%p7 bra 	BB131_9;

	fma.rn.f64 	%fd134, %fd134, %fd5, %fd134;

BB131_9:
	st.param.f64	[func_retval0+0], %fd134;
	ret;
}

.func  (.param .b64 func_retval0) __internal_lgamma_pos(
	.param .b64 __internal_lgamma_pos_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<296>;


	ld.param.f64 	%fd26, [__internal_lgamma_pos_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd26;
	}
	setp.gt.s32	%p1, %r54, 1074266111;
	@%p1 bra 	BB132_12;
	bra.uni 	BB132_1;

BB132_12:
	setp.gt.s32	%p8, %r54, 1075838975;
	@%p8 bra 	BB132_14;
	bra.uni 	BB132_13;

BB132_14:
	// inline asm
	rcp.approx.ftz.f64 %fd216,%fd26;
	// inline asm
	neg.f64 	%fd14, %fd26;
	mov.f64 	%fd218, 0d3FF0000000000000;
	fma.rn.f64 	%fd219, %fd14, %fd216, %fd218;
	fma.rn.f64 	%fd220, %fd219, %fd219, %fd219;
	fma.rn.f64 	%fd221, %fd220, %fd216, %fd216;
	mul.f64 	%fd222, %fd221, %fd221;
	mov.f64 	%fd223, 0d3F4B68B992738FBF;
	mov.f64 	%fd224, 0dBF5AC321034783F9;
	fma.rn.f64 	%fd225, %fd224, %fd222, %fd223;
	mov.f64 	%fd226, 0dBF4380D01E4F7B8C;
	fma.rn.f64 	%fd227, %fd225, %fd222, %fd226;
	mov.f64 	%fd228, 0d3F4A019FA29F7264;
	fma.rn.f64 	%fd229, %fd227, %fd222, %fd228;
	mov.f64 	%fd230, 0dBF66C16C16B2ACEC;
	fma.rn.f64 	%fd231, %fd229, %fd222, %fd230;
	mov.f64 	%fd232, 0d3FB5555555555545;
	fma.rn.f64 	%fd233, %fd231, %fd222, %fd232;
	mov.f64 	%fd234, 0d3FED67F1C864BEAE;
	fma.rn.f64 	%fd15, %fd233, %fd221, %fd234;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd26;
	}
	mov.u32 	%r56, -1023;
	setp.gt.s32	%p9, %r54, 1048575;
	mov.f64 	%fd292, %fd26;
	@%p9 bra 	BB132_16;

	mul.f64 	%fd16, %fd26, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd16;
	}
	mov.u32 	%r56, -1077;
	mov.f64 	%fd292, %fd16;

BB132_16:
	mov.f64 	%fd17, %fd292;
	add.s32 	%r36, %r54, -1;
	setp.lt.u32	%p10, %r36, 2146435071;
	@%p10 bra 	BB132_18;
	bra.uni 	BB132_17;

BB132_18:
	shr.u32 	%r38, %r54, 20;
	add.s32 	%r57, %r56, %r38;
	and.b32  	%r39, %r54, -2146435073;
	or.b32  	%r40, %r39, 1072693248;
	mov.b64 	%fd293, {%r55, %r40};
	setp.lt.s32	%p12, %r40, 1073127583;
	@%p12 bra 	BB132_20;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd293;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd293;
	}
	add.s32 	%r43, %r42, -1048576;
	mov.b64 	%fd293, {%r41, %r43};
	add.s32 	%r57, %r57, 1;

BB132_20:
	add.f64 	%fd238, %fd293, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd237,%fd238;
	// inline asm
	neg.f64 	%fd239, %fd238;
	fma.rn.f64 	%fd241, %fd239, %fd237, %fd218;
	fma.rn.f64 	%fd242, %fd241, %fd241, %fd241;
	fma.rn.f64 	%fd243, %fd242, %fd237, %fd237;
	add.f64 	%fd244, %fd293, 0dBFF0000000000000;
	mul.f64 	%fd245, %fd244, %fd243;
	fma.rn.f64 	%fd246, %fd244, %fd243, %fd245;
	mul.f64 	%fd247, %fd246, %fd246;
	mov.f64 	%fd248, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd249, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd250, %fd249, %fd247, %fd248;
	mov.f64 	%fd251, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd252, %fd250, %fd247, %fd251;
	mov.f64 	%fd253, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd254, %fd252, %fd247, %fd253;
	mov.f64 	%fd255, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd256, %fd254, %fd247, %fd255;
	mov.f64 	%fd257, 0d3F624924923BE72D;
	fma.rn.f64 	%fd258, %fd256, %fd247, %fd257;
	mov.f64 	%fd259, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd260, %fd258, %fd247, %fd259;
	mov.f64 	%fd261, 0d3FB5555555555554;
	fma.rn.f64 	%fd262, %fd260, %fd247, %fd261;
	sub.f64 	%fd263, %fd244, %fd246;
	add.f64 	%fd264, %fd263, %fd263;
	neg.f64 	%fd265, %fd246;
	fma.rn.f64 	%fd266, %fd265, %fd244, %fd264;
	mul.f64 	%fd267, %fd243, %fd266;
	mul.f64 	%fd268, %fd247, %fd262;
	fma.rn.f64 	%fd269, %fd268, %fd246, %fd267;
	xor.b32  	%r44, %r57, -2147483648;
	mov.u32 	%r45, 1127219200;
	mov.b64 	%fd270, {%r44, %r45};
	mov.u32 	%r46, -2147483648;
	mov.b64 	%fd271, {%r46, %r45};
	sub.f64 	%fd272, %fd270, %fd271;
	mov.f64 	%fd273, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd246;
	neg.f64 	%fd275, %fd272;
	fma.rn.f64 	%fd276, %fd275, %fd273, %fd274;
	sub.f64 	%fd277, %fd276, %fd246;
	sub.f64 	%fd278, %fd269, %fd277;
	mov.f64 	%fd279, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd280, %fd272, %fd279, %fd278;
	add.f64 	%fd294, %fd274, %fd280;
	bra.uni 	BB132_21;

BB132_1:
	setp.gt.s32	%p2, %r54, 1073217535;
	@%p2 bra 	BB132_11;
	bra.uni 	BB132_2;

BB132_11:
	add.f64 	%fd146, %fd26, 0dC000000000000000;
	mov.f64 	%fd147, 0dBE71FA71D78C0EE2;
	mov.f64 	%fd148, 0d3E452636124338B3;
	fma.rn.f64 	%fd149, %fd148, %fd146, %fd147;
	mov.f64 	%fd150, 0d3E8D111F31E61306;
	fma.rn.f64 	%fd151, %fd149, %fd146, %fd150;
	mov.f64 	%fd152, 0dBEA0502BBE1B2706;
	fma.rn.f64 	%fd153, %fd151, %fd146, %fd152;
	mov.f64 	%fd154, 0d3EB06850B2970292;
	fma.rn.f64 	%fd155, %fd153, %fd146, %fd154;
	mov.f64 	%fd156, 0dBEC108474875033D;
	fma.rn.f64 	%fd157, %fd155, %fd146, %fd156;
	mov.f64 	%fd158, 0d3ED24ACCC62909DC;
	fma.rn.f64 	%fd159, %fd157, %fd146, %fd158;
	mov.f64 	%fd160, 0dBEE3CB25209E63BE;
	fma.rn.f64 	%fd161, %fd159, %fd146, %fd160;
	mov.f64 	%fd162, 0d3EF581CBBC8CDC7B;
	fma.rn.f64 	%fd163, %fd161, %fd146, %fd162;
	mov.f64 	%fd164, 0dBF078E04B85C7597;
	fma.rn.f64 	%fd165, %fd163, %fd146, %fd164;
	mov.f64 	%fd166, 0d3F1A12730CF45051;
	fma.rn.f64 	%fd167, %fd165, %fd146, %fd166;
	mov.f64 	%fd168, 0dBF2D3FD354062012;
	fma.rn.f64 	%fd169, %fd167, %fd146, %fd168;
	mov.f64 	%fd170, 0d3F40B36B0B4DE323;
	fma.rn.f64 	%fd171, %fd169, %fd146, %fd170;
	mov.f64 	%fd172, 0dBF538AC5C6D0317A;
	fma.rn.f64 	%fd173, %fd171, %fd146, %fd172;
	mov.f64 	%fd174, 0d3F67ADD6EAAB19FC;
	fma.rn.f64 	%fd175, %fd173, %fd146, %fd174;
	mov.f64 	%fd176, 0dBF7E404FC20E4D5B;
	fma.rn.f64 	%fd177, %fd175, %fd146, %fd176;
	mov.f64 	%fd178, 0d3F951322AC7DA390;
	fma.rn.f64 	%fd179, %fd177, %fd146, %fd178;
	mov.f64 	%fd180, 0dBFB13E001A5578A3;
	fma.rn.f64 	%fd181, %fd179, %fd146, %fd180;
	mov.f64 	%fd182, 0d3FD4A34CC4A60FA3;
	fma.rn.f64 	%fd183, %fd181, %fd146, %fd182;
	mov.f64 	%fd184, 0d3FDB0EE6072093CF;
	fma.rn.f64 	%fd185, %fd183, %fd146, %fd184;
	mul.f64 	%fd295, %fd146, %fd185;
	bra.uni 	BB132_22;

BB132_13:
	add.f64 	%fd186, %fd26, 0dC008000000000000;
	mov.f64 	%fd187, 0dC1122B7730207EF3;
	mov.f64 	%fd188, 0dC0AF7040BB18FB05;
	fma.rn.f64 	%fd189, %fd188, %fd186, %fd187;
	mov.f64 	%fd190, 0dC1585A0DB81DE7D0;
	fma.rn.f64 	%fd191, %fd189, %fd186, %fd190;
	mov.f64 	%fd192, 0dC18A992B8BA94677;
	fma.rn.f64 	%fd193, %fd191, %fd186, %fd192;
	mov.f64 	%fd194, 0dC1AAC5CB6957CC20;
	fma.rn.f64 	%fd195, %fd193, %fd186, %fd194;
	mov.f64 	%fd196, 0dC1BC0E2B308774BE;
	fma.rn.f64 	%fd197, %fd195, %fd186, %fd196;
	mov.f64 	%fd198, 0dC1C6BA13DCAE7F67;
	fma.rn.f64 	%fd199, %fd197, %fd186, %fd198;
	mov.f64 	%fd200, 0dC1CCF33B9C3D120C;
	fma.rn.f64 	%fd201, %fd199, %fd186, %fd200;
	add.f64 	%fd202, %fd186, 0dC08FF62E0BE189FE;
	mov.f64 	%fd203, 0dC10074FACE10C93F;
	fma.rn.f64 	%fd204, %fd202, %fd186, %fd203;
	mov.f64 	%fd205, 0dC151B662F8D75791;
	fma.rn.f64 	%fd206, %fd204, %fd186, %fd205;
	mov.f64 	%fd207, 0dC18EE64AB4D207F7;
	fma.rn.f64 	%fd208, %fd206, %fd186, %fd207;
	mov.f64 	%fd209, 0dC1B9051687C9951A;
	fma.rn.f64 	%fd210, %fd208, %fd186, %fd209;
	mov.f64 	%fd211, 0dC1D2B866BF0B853D;
	fma.rn.f64 	%fd212, %fd210, %fd186, %fd211;
	mov.f64 	%fd213, 0dC1D4E2130E9DC133;
	fma.rn.f64 	%fd214, %fd212, %fd186, %fd213;
	div.rn.f64 	%fd215, %fd201, %fd214;
	add.f64 	%fd295, %fd186, %fd215;
	bra.uni 	BB132_22;

BB132_2:
	setp.gt.s32	%p3, %r54, 1072064101;
	@%p3 bra 	BB132_10;
	bra.uni 	BB132_3;

BB132_10:
	mov.f64 	%fd101, 0d3FF0000000000000;
	sub.f64 	%fd102, %fd101, %fd26;
	mov.f64 	%fd103, 0d3FA3EB504359EB88;
	mov.f64 	%fd104, 0d3F881F6D2A4C4310;
	fma.rn.f64 	%fd105, %fd104, %fd102, %fd103;
	mov.f64 	%fd106, 0d3FAE35D8DEB06317;
	fma.rn.f64 	%fd107, %fd105, %fd102, %fd106;
	mov.f64 	%fd108, 0d3FAED469A8B6ECCE;
	fma.rn.f64 	%fd109, %fd107, %fd102, %fd108;
	mov.f64 	%fd110, 0d3FACC1B1C357BEFE;
	fma.rn.f64 	%fd111, %fd109, %fd102, %fd110;
	mov.f64 	%fd112, 0d3FAD7154DB67F79F;
	fma.rn.f64 	%fd113, %fd111, %fd102, %fd112;
	mov.f64 	%fd114, 0d3FAFCC622CF2F7BB;
	fma.rn.f64 	%fd115, %fd113, %fd102, %fd114;
	mov.f64 	%fd116, 0d3FB11747A4D1CC43;
	fma.rn.f64 	%fd117, %fd115, %fd102, %fd116;
	mov.f64 	%fd118, 0d3FB24CE16A21B8AC;
	fma.rn.f64 	%fd119, %fd117, %fd102, %fd118;
	mov.f64 	%fd120, 0d3FB3B1C21A7BCB00;
	fma.rn.f64 	%fd121, %fd119, %fd102, %fd120;
	mov.f64 	%fd122, 0d3FB556723452ED57;
	fma.rn.f64 	%fd123, %fd121, %fd102, %fd122;
	mov.f64 	%fd124, 0d3FB748C00891544F;
	fma.rn.f64 	%fd125, %fd123, %fd102, %fd124;
	mov.f64 	%fd126, 0d3FB9A0207808CF40;
	fma.rn.f64 	%fd127, %fd125, %fd102, %fd126;
	mov.f64 	%fd128, 0d3FBC80673B8AE26B;
	fma.rn.f64 	%fd129, %fd127, %fd102, %fd128;
	mov.f64 	%fd130, 0d3FC010B364B7E555;
	fma.rn.f64 	%fd131, %fd129, %fd102, %fd130;
	mov.f64 	%fd132, 0d3FC2703A1D239658;
	fma.rn.f64 	%fd133, %fd131, %fd102, %fd132;
	mov.f64 	%fd134, 0d3FC5B40CB1137E6E;
	fma.rn.f64 	%fd135, %fd133, %fd102, %fd134;
	mov.f64 	%fd136, 0d3FCA8B9C17AC4F03;
	fma.rn.f64 	%fd137, %fd135, %fd102, %fd136;
	mov.f64 	%fd138, 0d3FD151322AC7CB52;
	fma.rn.f64 	%fd139, %fd137, %fd102, %fd138;
	mov.f64 	%fd140, 0d3FD9A4D55BEAB1D4;
	fma.rn.f64 	%fd141, %fd139, %fd102, %fd140;
	mov.f64 	%fd142, 0d3FEA51A6625307D6;
	fma.rn.f64 	%fd143, %fd141, %fd102, %fd142;
	mov.f64 	%fd144, 0d3FE2788CFC6FB619;
	fma.rn.f64 	%fd145, %fd143, %fd102, %fd144;
	mul.f64 	%fd295, %fd102, %fd145;
	bra.uni 	BB132_22;

BB132_17:
	mov.f64 	%fd235, 0d7FF0000000000000;
	fma.rn.f64 	%fd236, %fd17, %fd235, %fd235;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd17;
	}
	mov.b32 	 %f2, %r37;
	setp.eq.f32	%p11, %f2, 0f00000000;
	selp.f64	%fd294, 0dFFF0000000000000, %fd236, %p11;

BB132_21:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd294;
	}
	add.s32 	%r48, %r47, -1048576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd294;
	}
	mov.b64 	%fd281, {%r49, %r48};
	add.f64 	%fd282, %fd26, 0dBFE0000000000000;
	fma.rn.f64 	%fd283, %fd281, %fd282, %fd15;
	fma.rn.f64 	%fd284, %fd281, %fd282, %fd14;
	add.f64 	%fd285, %fd283, %fd284;
	setp.eq.f64	%p13, %fd26, 0d7FF0000000000000;
	selp.f64	%fd295, %fd26, %fd285, %p13;
	bra.uni 	BB132_22;

BB132_3:
	mov.f64 	%fd27, 0d3EA7B77CEB0625E8;
	mov.f64 	%fd28, 0dBE7844988BFE6590;
	fma.rn.f64 	%fd29, %fd28, %fd26, %fd27;
	mov.f64 	%fd30, 0dBE998C69C8710CC4;
	fma.rn.f64 	%fd31, %fd29, %fd26, %fd30;
	mov.f64 	%fd32, 0dBEF6527A5A11CF6E;
	fma.rn.f64 	%fd33, %fd31, %fd26, %fd32;
	mov.f64 	%fd34, 0d3F20EC2950B1B5DE;
	fma.rn.f64 	%fd35, %fd33, %fd26, %fd34;
	mov.f64 	%fd36, 0dBF2C4D80C24BA278;
	fma.rn.f64 	%fd37, %fd35, %fd26, %fd36;
	mov.f64 	%fd38, 0dBF5315B4E8CC0D09;
	fma.rn.f64 	%fd39, %fd37, %fd26, %fd38;
	mov.f64 	%fd40, 0d3F7D917F15D50020;
	fma.rn.f64 	%fd41, %fd39, %fd26, %fd40;
	mov.f64 	%fd42, 0dBF83B4ABB41CB6FA;
	fma.rn.f64 	%fd43, %fd41, %fd26, %fd42;
	mov.f64 	%fd44, 0dBFA59AF1275B7120;
	fma.rn.f64 	%fd45, %fd43, %fd26, %fd44;
	mov.f64 	%fd46, 0d3FC5512321A168A0;
	fma.rn.f64 	%fd47, %fd45, %fd26, %fd46;
	mov.f64 	%fd48, 0dBFA5815E8FDCE74C;
	fma.rn.f64 	%fd49, %fd47, %fd26, %fd48;
	mov.f64 	%fd50, 0dBFE4FCF4026ADD1A;
	fma.rn.f64 	%fd51, %fd49, %fd26, %fd50;
	mov.f64 	%fd52, 0d3FE2788CFC6FB5C8;
	fma.rn.f64 	%fd53, %fd51, %fd26, %fd52;
	mul.f64 	%fd54, %fd53, %fd26;
	fma.rn.f64 	%fd290, %fd54, %fd26, %fd26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd290;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd290;
	}
	mov.u32 	%r52, -1023;
	setp.gt.s32	%p4, %r50, 1048575;
	@%p4 bra 	BB132_5;

	mul.f64 	%fd290, %fd290, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd290;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r51, %temp}, %fd290;
	}
	mov.u32 	%r52, -1077;

BB132_5:
	add.s32 	%r23, %r50, -1;
	setp.lt.u32	%p5, %r23, 2146435071;
	@%p5 bra 	BB132_7;
	bra.uni 	BB132_6;

BB132_7:
	shr.u32 	%r25, %r50, 20;
	add.s32 	%r53, %r52, %r25;
	and.b32  	%r26, %r50, -2146435073;
	or.b32  	%r27, %r26, 1072693248;
	mov.b64 	%fd291, {%r51, %r27};
	setp.lt.s32	%p7, %r27, 1073127583;
	@%p7 bra 	BB132_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r28, %temp}, %fd291;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd291;
	}
	add.s32 	%r30, %r29, -1048576;
	mov.b64 	%fd291, {%r28, %r30};
	add.s32 	%r53, %r53, 1;

BB132_9:
	add.f64 	%fd58, %fd291, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd57,%fd58;
	// inline asm
	neg.f64 	%fd59, %fd58;
	mov.f64 	%fd60, 0d3FF0000000000000;
	fma.rn.f64 	%fd61, %fd59, %fd57, %fd60;
	fma.rn.f64 	%fd62, %fd61, %fd61, %fd61;
	fma.rn.f64 	%fd63, %fd62, %fd57, %fd57;
	add.f64 	%fd64, %fd291, 0dBFF0000000000000;
	mul.f64 	%fd65, %fd64, %fd63;
	fma.rn.f64 	%fd66, %fd64, %fd63, %fd65;
	mul.f64 	%fd67, %fd66, %fd66;
	mov.f64 	%fd68, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd69, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd70, %fd69, %fd67, %fd68;
	mov.f64 	%fd71, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd72, %fd70, %fd67, %fd71;
	mov.f64 	%fd73, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd74, %fd72, %fd67, %fd73;
	mov.f64 	%fd75, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd76, %fd74, %fd67, %fd75;
	mov.f64 	%fd77, 0d3F624924923BE72D;
	fma.rn.f64 	%fd78, %fd76, %fd67, %fd77;
	mov.f64 	%fd79, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd80, %fd78, %fd67, %fd79;
	mov.f64 	%fd81, 0d3FB5555555555554;
	fma.rn.f64 	%fd82, %fd80, %fd67, %fd81;
	sub.f64 	%fd83, %fd64, %fd66;
	add.f64 	%fd84, %fd83, %fd83;
	neg.f64 	%fd85, %fd66;
	fma.rn.f64 	%fd86, %fd85, %fd64, %fd84;
	mul.f64 	%fd87, %fd63, %fd86;
	mul.f64 	%fd88, %fd67, %fd82;
	fma.rn.f64 	%fd89, %fd88, %fd66, %fd87;
	xor.b32  	%r31, %r53, -2147483648;
	mov.u32 	%r32, 1127219200;
	mov.b64 	%fd90, {%r31, %r32};
	mov.u32 	%r33, -2147483648;
	mov.b64 	%fd91, {%r33, %r32};
	sub.f64 	%fd92, %fd90, %fd91;
	mov.f64 	%fd93, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd94, %fd92, %fd93, %fd66;
	neg.f64 	%fd95, %fd92;
	fma.rn.f64 	%fd96, %fd95, %fd93, %fd94;
	sub.f64 	%fd97, %fd96, %fd66;
	sub.f64 	%fd98, %fd89, %fd97;
	mov.f64 	%fd99, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd100, %fd92, %fd99, %fd98;
	add.f64 	%fd8, %fd94, %fd100;
	neg.f64 	%fd295, %fd8;
	bra.uni 	BB132_22;

BB132_6:
	mov.f64 	%fd55, 0d7FF0000000000000;
	fma.rn.f64 	%fd56, %fd290, %fd55, %fd55;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd290;
	}
	mov.b32 	 %f1, %r24;
	setp.eq.f32	%p6, %f1, 0f00000000;
	selp.f64	%fd4, 0dFFF0000000000000, %fd56, %p6;
	neg.f64 	%fd295, %fd4;

BB132_22:
	st.param.f64	[func_retval0+0], %fd295;
	ret;
}


